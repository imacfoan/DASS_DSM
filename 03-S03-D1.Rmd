---
editor_options: 
  markdown: 
    wrap: 72
---

# Demonstration: Classification Problems {-}

In this demonstration, you will learn how to address classification problems using logistic regression, discriminant analysis, KNN, and Naive Bayes.

You will need the **Weekly** dataset, part of the `ISRL2` package. By loading the package, the **Weekly** dataset loads automatically.

In addition to the `ISLR2` package, will also require the following:  

```{r warning = FALSE, message = FALSE}
library(MASS)
library(class)
library(tidyverse)
library(corrplot)
library(ISLR2)
library(e1071)
```

## Dataset and Variables {-}

The **Weekly** dataset contains weekly percentage returns for the S&P 500 stock index between 1990 and 2010. It is a dataframe with 1098 observations and 9 variables. The variables are:   

-  Year: year observation was recorded  
-  Lag1: Percentage return for previous week  
-  Lag2: Percentage return for 2 weeks previous  
-  Lag3: Percentage return for 3 weeks previous  
-  Lag4: Percentage return for 4 weeks previous  
-  Lag5: Percentage return for 5 weeks previous  
-  Volume: Volume of shares traded (average number of daily shares traded in billions)   
-  Today: percentage return for current week  
-  Direction: whether the market had a positive (up) or negative (down) return on a given week.     

In this demonstration, the goal is to predict whether the market had a positive (up) or negative (down) return on a given week. Therefore, **Direction** will be our response variable. 

Before we consider the model, let's first explore our dataset.  

By exploring the structure of the dataframe, we find out that all variables are numeric, with the exception of the **Direction** variable. 

```{r}
str(Weekly)
```

## Correlation Matrix and Plot {-}

Let's now produce a correlation plot between all pairs of *numeric* variables in the dataset. 

Using the base R `cor()` function, we exclude the 9th variable (which is a factor) and compute the correlation among all pairs of numeric variables. 

```{r}
cor(Weekly[, -9])
```

We store the computed correlation matrix in a new object which we will then use to create a correlation matrix plot with the `corrplot()` function from the `corrplot` package. 

```{r}
cor_matrix <- cor(Weekly[, -9])

corrplot(cor_matrix)
```

By using the default arguments, we obtain the correlation matrix in full, with each circle representing the correlation between each variable. The size of the circle represents the magnitude of the correlation, whilst the shade corresponds to both strength and direction of the correlation. As the legend illustrates, a perfect negative correlation (-1) is represented by dark red and a perfect positive correlation (+1) in dark blue. As you already know, the correlation matrix is symmetric around its diagonal. The diagonal area represents the correlation of each variable with itself, and therefore, this corresponds to a perfect correlation (dark blue).

To facilitate interpretation, particularly when we are dealing with a large number of variables, we can set the `diag` argument to `FALSE` to remove the correlation of each variable with itself from the plot. Because of its symmetric property, we can also display just half of the square since the parts on either side of the diagonal are mirror images. We can achieve this using the `type` argument. We can either choose to display the area above the diagonal or the area below the diagonal, as I did below. There are many other options if you want to further customise your correlation plot such using a different visualisation method of the direction and strength of the correlation using the `method` argument. Have a look at the documentaion of the `corrplot` function using ?.

```{r}
corrplot(cor_matrix, type = "lower", diag = FALSE)
```

Now, the correlation plot only displays the correlations between the 8 numeric variables. We observe a strong positive correlation between volume of daily shares traded and the year the observation was recorded (dark blue). The correlations between other variables are quite weak but notably, we see that Lag1 and Lag3, Lag 2 and Lag4, Lag3 and Lag5, and Today and Lag2 are positively correlated with one another (albeit weakly). Other variables also appear weakly negatively correlated, such as Lag1 and Lag2.   

*Ok, so what was the purpose of computing the correlation matrix? You'll remember that multicollinearity is an issue in model building which can lead to inflated variances of the estimated coefficients. As a result of the shared variance between two highly correlated predictors, our ability to adequately evaluate the effect of the predictors on the outcome will be affected (e.g. increased risk of overfitting). One way to deal with multicollinearity is to eliminate one of the highly correlated predictors.*

## "Classic" Logistic Regression {-}

Now let's build our logistic regression model the classic way.   

Given the high correlation between **Year** and **Volume**, we have to reflect on how we want to address this. Assuming we have knowledge of important factors that can predict market movement (**Direction**), we decide to drop the **Year** variable rather than **Volume*** since the latter measures average number of daily shares traded (in billions).    

You may remember that in R, these models are built using the base R `glm` function within which the `family` argument must be set to `binomial`.    

Note that in this dataset, the **Direction** variable is already a factor so there is no need to perform any recoding/transformations but remember to ALWAYS explore your data in detail before you build any model. 

```{r}
fit <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
           data = Weekly, family = binomial)
summary(fit)
```

The results show that only **Lag2** is significant at an alpha level of 0.05.  

Ok, so let's see now how well our model predicts whether the market had a positive or negative return in a given week.   

### Confusion Matrix {-}

Now, let's assess the effectiveness of our model by comparing the actual values with those predicted by the model. We can do so using a *confusion matrix* which compares the model predictions with the true values.  

Using `predict(fit, type = "response")`, we generate predictions from our model (`fit`) on the scale of the response variable (`type = "response"`). In this case, our response variable is measured on a probability scale. 

We choose the standard threshold of 0.5 such that we label an observation as belonging to the **Up** category if its posterior probability is above 0.5 or as **Down** if the posterior probability is below 0.5. Hence, in this context, the `> 0.5` argument transforms the predicted probabilities into a binary form such that predictions greater than 0.5 are labelled TRUE (so representing upward market movement), whilst the rest are labelled FALSE (representing downward market movement). 

```{r}
pred <- predict(fit, type = "response") > 0.5
```

Now that we have the frequencies of the TRUE and FALSE instances, let's build our two-way confusion matrix using the base R `table()` function. The `ifelse` function nested inside `table()` converts the logical TRUE and FALSE values in the **pred** object intro descriptive labels to facilitate interpretation; so, if **pred** is TRUE (> 0.5), it becomes labelled as `Up (pred)`, whilst it is is FALSE, it is labelled as `"Down (pred)"`. To also include the actual values of market movement from the dataset, we also need to add `Weekly$Direction`as our argument.  

Finally to 'force' R to display the `conf_matrix` object we just created, we can place the entire code in single parentheses. 

```{r}
(conf_matrix <- table(ifelse(pred, "Up (pred)", "Down (pred)"), Weekly$Direction))
```

Now let's interpret the results. The results in diagonal represent correct predictions of market movement whilst those in the off-diagonal represent misclassified observations. We can see that our model incorrectly classified 430 instances of market movement as upward movement when in fact they represented downward movement and 48 instances as downward movement when in fact they represented upward movement. Overall, our logistic regression model correctly predicts upwards movements well but it performs poorly at predicting downward movements.  

We can also compute the overall fraction of correct predictions by dividing the number of correct predictions by total number of predictions. We therefore divide the sum of the diagonal values in our confusion matrix (numerator) by the sum of all elements of the matrix (denominator). We extract the diagonal values from the matrix using the base R `diag()` function.

```{r}
sum(diag(conf_matrix)) / sum(conf_matrix)
```

The overall fraction of correct predictions is 0.561 (so our model makes correct predictions about 56.1% of the time).  

**Right, so does that mean that this model will make correct predictions 56% of the time on a new, unseen dataset?**

You already know the answer :)! We used our entire dataset to fit our model. This means that we cannot say anything about how our model will perform on a different dataset and we no longer have any 'unseen' data left to test this out.

## Logistic Regression in Statistical Learning {-}

Now, let's consider logistic regression as applied in statistical learning. 

We will again consider a basic fixed split. In this example, we will fit our model using data from 1990 up to 2008 and set the data from 2009 and 2010 aside; this will be our test dataset. 

This is easily achieved by creating a vector of logical values from the data according to our **Year** criterion. 

```{r}
train <- Weekly$Year < 2009
```

In our previous model, we observed that **Lag2** was the only statistically significant predictor. To exemplify how we can use logistic regression in statistical learning, we will build a simple simple with only one predictor.

The approach to building the model is the same as the one with which you are already familiar. The exception, of course, is that we will only use part of the dataset to build our model (which in this case is referred to as *training the model*). To subset our dataset to only include data from years previous to 2009, we use the logical vector **train** we just created.

```{r}
fit_log_SL <- glm(Direction ~ Lag2, data = Weekly[train, ], family = binomial)
```

Now let's generate predictions; the function and the overall structure of the code is the same as discussed earlier in the demonstration. The exception is that we used the trained model `fit_log_SL` to make predictions on the *test* dataset (`Weekly[!train, ]`). Using `!`, we tell R to not include the training data when generating predictions. 

```{r}
pred <- predict(fit_log_SL, Weekly[!train, ], type = "response") > 0.5
```

Now let's compute the confusion matrix such that we can compare our predictions on the test data (`pred`) against the actual values in our dataset (`Weekly[!train, ]$Direction)`).

```{r}
(t <- table(ifelse(pred, "Up (pred)", "Down (pred)"), Weekly[!train, ]$Direction))
```

If we then compute the overall fraction of correct predictions for the test data we can see that this is higher than the value we obtained using the classical approach ($0.561$).

```{r}
sum(diag(t)) / sum(t)
```

## Linear Discriminant Analysis {-}

How well would linear discriminant analysis address our binary classification problem?

In R, LDA can be performed using the `lda()` function from the `MASS` package. The basic structure of the function is similar to the other models you have built. 

```{r}
fit_lda <- lda(Direction ~ Lag2, data = Weekly[train, ])
```

The output is, of course, different. 

-  prior probabilities of groups: these tells us the way in which the two classes are distributed in our *training data* (i.e. 44.8 % of the observations correspond to downward market movement whilst 55.2% to upward market movement).  
-  group means: the average of our single predictor **Lag2** within each class, and are used by LDA as an estimate of $μ_{k}$.  
-  coefficient(s) of linear discriminants: tells us how our predictor(s) influence the score that is used to classify the observations into one of the two categories. Here, the coefficient is positive 0.44 and so this indicates that higher values for **Lag2** will make the modelmore likely classify an observation as belonging to the **Up** class; also, the larger the absolute value of the coefficient, the stronger the influence on the model. 

```{r}
fit_lda
```

Now let's consider what the `predict()` function does when applied in the context of LDA.  

```{r}
result_lda <- predict(fit_lda, Weekly[!train, ])
```

The output will contain three components: `class`, `posterior`, and `x`, each of which can be accessed using `$`.

The `class` component is a factor that contains the predictions for market movement (up/down).

```{r}
result_lda$class
```

The `posterior` component is matrix that contains the posterior probability that the corresponding observation belongs to a given class.

```{r}
result_lda$posterior
```

The `x` component contains the linear discriminants.

```{r}
result_lda$x
```

To obtain our predictions, we can simply extract the `class` element. 

Alternatively, if we want to directly extract just the predictions from the `class` element, we can use the `predict` function as we did earlier in the demonstration. 

```{r}
#either 
pred_lda <- result_lda$class

#or
pred_lda <- predict(fit_lda, Weekly[!train, ], type = "response")$class
```

Now let's compute the confusion matrix for our LDA classifier such that we can compare our predictions on the test data against the actual values in our dataset.

```{r}
(t <- table(pred_lda, Weekly[!train, ]$Direction))
```

And now the fraction of correct predictions which, we can see is identical to that obtained for logistic regression. 

```{r}
sum(diag(t)) / sum(t)
```

## Quadratic Discriminant Analysis {-}

Let's now consider how quadratic discriminant analysis would address our binary classification problem. The code syntax is identical to that for linear discriminant analysis and the `qda` function is also part of the `MASS` package.

```{r}
fit_qda <- qda(Direction ~ Lag2, data = Weekly[train, ])
```

In terms of prior probabilities and group means, the output is identical to that of linear discriminant analysis. However, the output does not include the coefficients of the *linear* discriminants for obvious reasons. 

```{r}
fit_qda
```

The prediction function works in the same way as for LDA, except that it will produce only two elements (`class`, and `posterior`); again, the `x` element will not be included since we are dealing with a quadratic function. 

```{r}
pred_qda <- predict(fit_qda, Weekly[!train, ], type = "response")$class
```

The confusion matrix is computed in the same way. 

```{r}
(t <- table(pred_qda, Weekly[!train, ]$Direction))
```

The fraction of correct predictions is lower than that for logistic regression and for LDA. We therefore conclude that in this context, QDA does not perform well in comparison to the previous two approaches.

```{r}
sum(diag(t)) / sum(t)
```

## $K$-nearest neighbours {-}

Earlier in the course, we covered K-nearest neighbour classification. Let's now explore how this approach is used in R and how it performs in the context of our market movement problem. 

To implement KNN in R, the most commonly used package is `class`.

:::attention
Note that it is possible to be confronted with the following error when loading the package:   

`Error: (converted from warning) package ‘class’ was built under R version ...`   

This will occur if you are using an older version of R than that under which the package was built. The best option is to update RStudio. If that is not possible (e.g. due to system requirements), then another option is to suppress it using `suppressWarnings(library(class))` in your console. This should allow you to use the functions from the package.
:::

In R, we build our model using the `knn()` function from the `class` package. This function works differently to those we have covered so far for linear and logistic regression, and for LDA and QDA.  

This is because the `knn()` function both fits the model AND generates predictions. There are four arguments required: 

- argument 1: predictors in our *training* data,  
- argument 2: predictors in our *test* data,  
- argument 3: outcome variable in our *training* data,  
- argument 4: the value for $k$; note the function specifies 1 nearest neighbour by default but I added it here for illustration purposes (this value needs to be added only when using a value other than 1).  

Now, you may wonder why I also included the `drop = FALSE` argument when I subsetting the dataset. 

```{r}
fit_knn <- knn(Weekly[train, "Lag2", drop = FALSE],
               Weekly[!train, "Lag2", drop = FALSE],
               Weekly$Direction[train], 
               k = 1
               )
```

Before we proceed to interpret the results, let's see what output we would produce if we subsetted our dataset such that we extract our predictor from the training data without the `drop = FALSE` argument. This looks like a vector, right?

```{r}
Weekly[train, "Lag2"]
```

If you wrap the code within the `class` function you can indeed confirm that the output is a vector.

```{r}
class(Weekly[train, "Lag2"])
```

The `knn()` requires that the training and test data be specified as either a matrix or a dataframe.  

If we set the `drop` argument to `FALSE`, then we are essentially telling R NOT to delete the dimensions of our object when we are subsetting it such that it keeps the row numbers, thereby producing a dataframe. 

```{r}
class(Weekly[train, "Lag2", drop = FALSE])
```

:::attention
You must pay close attention to the requirements and specifications for the functions with which you build your models. Often, you will be prompted by error messages in the console. 

For example, the `knn()` function expects either a matrix or a dataframe. If you do not set `drop = FALSE` you will not be able to proceed:  

`Error in knn(Weekly[train, "Lag2"], Weekly[!train, "Lag2"], Weekly$Direction[train], : dims of 'test' and 'train' differ`   

Other times, the error may be not so severe as to impede the function from working, but incorrectly structured data or improperly coded variables will lead to the function producing invalid results. Remember to carefully explore the arguments using the Help tab (e.g. ?knn).
:::

Now let's return to our results and produce a confusion matrix. 

```{r}
(t <- table(fit_knn, Weekly[!train, ]$Direction))
```

Our overall fraction of correct predictions is 0.5. Therefore, the KNN classifier ($k = 1$) performs the worst out of all other classifiers we have explored so far (but only slightly worse than QDA). 

```{r}
sum(diag(t)) / sum(t)
```

But before we move on to our next classifier, let's consider other values for $k$ for illustration purposes. To ensure consistent results, we also set the seed (to 1 in this case).

We fit KNN for up to $k = 30$ by using the base R `sapply` to apply the `knn()` function to every integer from 1 to 30 and to then calculate the overall fraction of correct prediction. 

```{r}
set.seed(1)
knn_k <- sapply(1:30, function(k) {
  fit <- knn(Weekly[train, "Lag2", drop = FALSE],
             Weekly[!train, "Lag2", drop = FALSE],
             Weekly$Direction[train],
             k = k
             )
  mean(fit == Weekly[!train, ]$Direction)
  })
```

We can then create a plot to observe at what value for `k` the overall fraction of correct predictions is highest. This fraction stabilises at a value for $k$ somewhere between $k = 10$ and $k = 15$.

```{r}
plot(1:30, knn_k, type = "o", xlab = "k", ylab = "Fraction correct")
```

We can find this out directly by asking R the index of the first time a maximum value among all other values appears. Our classifier appears to perform best when $k = 12$. 

```{r}
(k <- which.max(knn_k))
```

Now let's re-evaluate our KNN classifier on the test data using `$k = 12$ and compute the confusion matrix. 

```{r}
fit_knn <- knn(Weekly[train, "Lag2", drop = FALSE],
               Weekly[!train, "Lag2", drop = FALSE],
               Weekly$Direction[train], 
               k = 12
               )

table(fit_knn , Weekly[!train, ]$Direction)
```

Now, the overall fraction of correct predictions is higher than it was for $k = 1$ but this fraction still does not outperform logistic regression and LDA.

```{r}
mean(fit_knn == Weekly[!train, ]$Direction)
```

## Naive Bayes {-}

Finally, let's evaluate the performance of Naive Bayes and conclude which approach performs best for our market movement classification problem. 

A useful package for Naive Bayes is `e1071`. Like the `class` package, do note that you may be prompted with a similar error when loading the package (this can also be addressed by either updating RStudio or suppressing the warning). 

```{r}
fit_NBayes <- naiveBayes(Direction ~ Lag2, data = Weekly, subset = train)
```

Before generating the predictions, let's explore the output of the fit. There are two important components: 
- A-priori probabilities: i.e. prior probabilities (distribution of the classes for the response variable)  
- Conditional probabilities: parameters of the model for the predictor by class. For a numeric variable (as is our predictor in this case), the parameters shown are the mean `[,1]` and standard deviation `[,2]` for the predictor values in each class; for a categorical variable, these would be conditional probabilities for the predictor in each class.   

The a priori probabilities can be extracted by specifying `fit_NBayes$apriori` and the conditional probabilities can be extracted using `fit_NBayes$tables`. 

```{r}
fit_NBayes
```

Now let's predict market movement. 

```{r}
pred_NBayes <- predict(fit_NBayes, Weekly[!train, ], type = "class")
```

And finally, generate our confusion matrix. 

```{r}
(t <- table(pred_NBayes, Weekly[!train, ]$Direction))
```

Our overall fraction of correct predictions is $0.5865385$. Naive Bayes performs slightly better than KNN with $k = 1$ ($0.5$) and the same as QDA ($0.5865385$).

```{r}
sum(diag(t)) / sum(t)
```

*Based on the approaches we have implemented in this demonstration, logistic regression and LDA perform best.*
