---
editor_options:
  markdown:
    wrap: 72
---

# Demonstration 1: Cross-validation {-}

So far, we have established the importance of training and test data and you now should have a robust understanding of the differences between training and test error rates in relation to model performance. However, several challenges remain and these include:

-  the size of the training set and whether it is sufficient to adequately train our model;  
-  the size of the test set and whether it is sufficient to adequately evaluate our model;  
-  the process of testing our model on just a single test set;    
-  the nature of the training set.    

In this demonstration, you will learn more about how cross-validation provides us with ways to address this gap.  

## Data and Variables {-}

In this demonstration, we will make use the **Auto** dataset from the core textbook (James et. al 2021). This dataset is part of the `ISRL2` package. By loading the package, the **Auto** dataset loads automatically.

You should already be familiar with the **Auto** dataset but below is a reminder of the variables it contains:

-   mpg: miles per gallon
-   cylinders: Number of cylinders between 4 and 8
-   displacement: Engine displacement (cu. inches)
-   horsepower: Engine horsepower
-   weight: Vehicle weight (lbs.)
-   acceleration: Time to accelerate from 0 to 60 mph (sec.)
-   year: Model year
-   origin: Origin of car (1. American, 2. European, 3. Japanese)
-   name: Vehicle name

In addition to `ISRL2`, you will also require the `boot` package which first needs to be installed. This package is required for both LOOCV and $k$-fold CV.


```r
library(ISLR2)
library(boot) #don't forget to install it first
```

## The Validation Set Approach {-}

Let's first consider the validation set approach.  

The **Auto** dataset has a total of 392 observations and the goal is to randomly split these observations using a 50/50 ratio. We can perform this randomised split using the base R `sample()` function. The first argument within `sample()` is 392. This is the total number of observations available to us from the **Auto** dataset. The second argument is 196. This represents the number of observations we want to select from the total we have available which is 392. Therefore, the `sample()` function will return a vector of 196 unique integers which represents a subset of 196 indices from a total of 392.   

*Note: by default, the `sample()` function conducts the sampling without replacement.*


```r
training_data <- sample(392, 196)

training_data
```

```
##   [1] 108 173   4 125   3 283  67 322 299   1 358  73 121 212 186 120 210  81
##  [19] 380 176 229  16 146 236 195  35 310 233 374 292 315 331 328 345 363  44
##  [37]  93  32 364 189 248  63  96 285 330 320 366 314 106   9  19 326 305 343
##  [55] 308 273 342  62  50 249  77 191 171 235 264 332 370  28 126 166 260 241
##  [73]  45 113 381 290 383 382 202 219  39 100 378 111 178 338 246 376 160  75
##  [91] 142 116 168 220 192  56 232 179 282 344 333 298 153 318  25 355 201 130
## [109] 346 218  78 234  87 145 180 252 107 377 196 371 163 109 193 117 256  52
## [127] 124  22  71 123 257 373  97 263 316 141 239 165 309  30 267 360  68 230
## [145] 158 339 362 208 300 132  36 372  91  21 159 228 324  34 144  86  57 103
## [163] 143  88 255 361 303 272 269 169  12  79 237  20 389  54 183  26 207 352
## [181] 270 149 259 216 217  72 367 348 284  27  94  47 110  69 289 258
```

Now we need to tell R to fit a linear regression model using only the observations corresponding to the training set. We can do so by using the `subset` argument which tells R to only select the 196 observations that are indexed at the specific positions as defined by the  **training_data** object. In this way, the model will be fitted using only the observations from **Auto** dataset that are defined by this vector of indices. 


```r
fit <- lm(mpg ~ horsepower, data = Auto, subset = training_data)
```

Let's now calculate the Mean Squared Error for the test dataset. We obtain the predictions using the`predict()` function with which you are already familiar.


```r
mean((Auto$mpg - predict(fit, Auto))[-training_data]^2)
```

```
## [1] 22.50177
```

Let's try applying some transformations to our predictor **horsepower** using the **poly()** function and then calculate the test MSE values to observe how the MSE values change.   

We first fit a second-degree polynomial regression model:  


```r
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = training_data)
```

Then calculate the test MSE:   


```r
mean((Auto$mpg - predict(lm.fit2, Auto))[-training_data]^2)
```

```
## [1] 18.17912
```

Now we fit a third-degree polynomial regression model  


```r
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = training_data)
```

Then calculate the test MSE:    


```r
mean((Auto$mpg - predict(lm.fit3, Auto))[-training_data]^2)
```

```
## [1] 18.15096
```

What would happen if we choose a different training dataset instead? We can try this out by setting the seed to 2, for example.   


```r
set.seed(2)  
training_data2 <- sample(392, 196)
```

We then run a new set of models: a linear regression model, a second degree polynomial model, and a third degree polynomial model and then calculate the test MSE values for each of the three models.    

**linear regression model and corresponding test MSE:**  


```r
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = training_data2)
mean((Auto$mpg - predict(lm.fit, Auto))[-training_data2]^2)
```

```
## [1] 25.72651
```

**quadratic regression model and corresponding test MSE:** 


```r
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = training_data2)
mean((Auto$mpg - predict(lm.fit2, Auto))[-training_data2]^2)
```

```
## [1] 20.43036
```

**cubic regression model and corresponding test MSE:** 


```r
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = training_data2)
mean((Auto$mpg - predict(lm.fit3, Auto))[-training_data2]^2)
```

```
## [1] 20.38533
```

The results obtained using the second training dataset are consistent with the findings from the first training dataset whereby a model that predicts **mpg** using a quadratic function of **horsepower** performs better than a model that involves only a linear function of **horsepower**. Also, there is little evidence in favor of a model that uses a cubic function of **horsepower**.  

Overall, the main take-away point is that if we were to choose different training sets, we will obtain somewhat different errors on the validation set.  

## Leave-One-Out Cross-Validation {-}

Now, let's consider LOOCV. The training set of data is split into two parts, with the difference that here they are not of comparable sizes as the validation set consists of a single observation and the remaining observations make up the training set. 

<img src="images/loocv.png" style="display: block; margin-left: auto; margin-right: auto;" />

Repeating the process $n$ times produces $n$ square errors and the estimate for the test MSE is the average of those $n$ test error estimates:

$$CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n} MSE_i$$
The main advantage of the LOOCV approach over a simple train and test validation is that it produces less bias. The model is fit on $nâˆ’1$ training observations, almost as many as there are in the entire data set. Furthermore, since LOOCV is performed multiple times it yields consistent results with less randomness than in a simple train and test approach. 

One more down side to the generally high value of $k$ is that the computation side of the procedure becomes more taxing especially if the model is rather a complex one. Come to think about it, the LOOCV requires as many model fits as data points and each model fit uses a subset that is ALMOST the same size as the training.  

In R, one way to compute the LOOCV estimate is by using functions from the `boot` package. This package nevertheless requires that the models are built using the `glm()` function, including linear models. The `glm()` function will produce identical results to `lm()` when fitting a linear model. You do not need to specify the `family` argument as you did for logistic regression since it is already set by default to `gaussian`. 

Fitting a linear model with `glm()` is the same as with `lm()`. 


```r
glm.fit <- glm(mpg ~ horsepower, data = Auto)
```

We then perform LOOCV using the `cv.glm()` function from the `boot` package.


```r
cv.err <- cv.glm(Auto, glm.fit)
```

The output includes a list of several components. The component of relevance for LOOCV is `delta`. 


```r
cv.err$delta
```

```
## [1] 24.23151 24.23114
```

The `delta` component therefore provides us with the results of the cross-validation. The first value is the raw cross-validation estimate of prediction error whilst the second value is the adjusted cross-validation estimate that compensates for the bias introduced by not using leave-one-out cross-validation. In our case, the two values are identical to two decimal places and so our cross-validation estimate for the test error is approximately $24.23$. 

We can repeat this procedure for increasingly complex polynomial fits. Instead of writing separate code for each model fit, we can automate the process using the  `for()` function that initiates a *for loop* which iteratively fits polynomial regressions for polynomials of order $i=1$ to $i=10$, computes the associated cross-validation error, and stores it in the $i$th element of the vector `cv.error`.


```r
cv.error <- rep(0, 10)
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
```

The output shows us the estimated test MSE for the linear model and polynomials up to the 10th degree. 


```r
cv.error
```

```
##  [1] 24.23151 19.24821 19.33498 19.42443 19.03321 18.97864 18.83305 18.96115
##  [9] 19.06863 19.49093
```

We can see a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials.

## $k$-Fold Cross-Validation {-}

Finally, let's consider $k$-fold cross-validation. This approach involves randomly dividing the set of observations into $k$ groups (known as folds) of approximately equal size. With cross-validation, the test data is held out (approximately one fifth of data), and the remaining training data is randomly divided into $k$ groups. Several different portions of this training data are used for validation, and the remaining part of the data is used for training as shown in the diagram below. Hence, a fold is treated as a validation set, and the method is fit on to the remaining $kâˆ’1$ folds. The $k$ resampled estimates of performance are summarised and used for testing and model building.

<img src="images/k-fold.png" style="display: block; margin-left: auto; margin-right: auto;" />

In this illustration, the non-holdout data is divided into five portions, and we therefore call this *5-fold cross-validation*. If there had been ten blocks, it would have been 10-fold cross-validation. The model that has been built using k-fold cross-validation is then tested on the originally held out test data subset. The $k$ resampled estimates of modelâ€™s performance are summarised commonly using the mean and standard error to develop a better reasoning of its effectiveness in relation to its tuning parameters. In fact, LOOCV is a special case of k-fold CV in which $k$ is equal to $n$:

$$CV_{(k)} = \frac{1}{k} \sum_{i=1}^{k} MSE_i$$
A typical choice for $k$ is 5 or 10, but there is no formal rule. One should keep in mind when making a choice that as $k$ increases, the difference in size between the training set and the resampling subset gets smaller, causing the bias of the statistical model to become smaller. However, when compared with k-fold CV, LOOCV results in a poorer estimate as it provides an approximately unbiased estimate for the test error that is highly variable.  

Using k-fold cross-validation increases validation sensitivity, allowing better reasoning with the model. One of the key questions is how to choose the number of folds, i.e. how big does $k$ need to be? In general, the choice of the number of folds depends on the size of the dataset. For large datasets, 5-fold cross-validation is considered to be quite accurate and when dealing with very sparse datasets, we may consider using leave-one-out in order to train on as many examples as possible.

There is a bias-variance trade-off associated with the choice of $k$ in k-fold cross-validation. Larger $k$, for which training folds are closer to the total dataset, results in less bias towards overestimating the true expected error but higher variance and higher running time. We can summarise those findings as follows:

For a large number of folds:  

- positives:small bias of the true error rate estimator (as a result of a very accurate estimator)  
- negatives: large variance of the true error rate estimator; computational time is large 

For a small number of folds: 

- positives: small variance of the estimator; the number of experiments and, therefore, computation time are reduced
- negatives: large bias of the estimator  


In practice, typical choices for k in cross-validation are $k=5$ or $k=10$, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.


Let's practice $k$-fold cross-validation. The same function (`cv.glm()`) can be used by setting the value of $k$. By default, the value of K is equal to the number of observations in data which therefore gives us LOOCV.

In this example, we will use $k = 10$. We will again perform the same approach of increasingly complex polynomial fits as we did for LOOCV. The code is identical to the one we used to LOOCV except that of course, we specified `K = 10`.


```r
set.seed(17)
cv.error.10 <- rep(0, 10)
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
```

The output shows us a similar pattern to the estimated test MSE for the linear model and polynomials up to the 10th degree: we see a sharp drop between the linear and quadratic fits but once again no improvement with higher order polynomials. We can therefore conclude that a quadratic fit is suitable.


```r
cv.error.10
```

```
##  [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666
##  [9] 18.87013 20.95520
```

We saw earlier that when we LOOCV, the two values were essentially the same. In the case of $k$-fold CV, these values differ slightly but they are still very similar to each other. 


```r
cv.glm(Auto, glm.fit, K = 10)$delta
```

```
## [1] 19.71557 19.60616
```

