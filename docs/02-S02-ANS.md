---
editor_options:
  markdown:
    wrap: 72
---

# Answers {.unnumbered}

## Practical 1 {-}

::: file
For the tasks below, you will require the **Auto** dataset from the core
textbook (James et. al 2021).

This dataset is part of the `ISRL2` package. By loading the package, the
**Auto** dataset loads automatically:

`library(ISLR2)`

Remember to install it first

`install.packages("ISLR2")`
:::

This data file (text format) contains 398 observations of 9 variables.
The variables are:

-   mpg: miles per gallon
-   cylinders: Number of cylinders between 4 and 8
-   displacement: Engine displacement (cu. inches)
-   horsepower: Engine horsepower
-   weight: Vehicle weight (lbs.)
-   acceleration: Time to accelerate from 0 to 60 mph (sec.)
-   year: Model year
-   origin: Origin of car (1. American, 2. European, 3. Japanese)
-   name: Vehicle name

### Task 1 {.unnumbered}

Use the `lm()` function to perform simple linear regression with **mpg**
as the response and **horsepower** as the predictor. Store the output in
an object called **fit**.






```r
fit <- lm(mpg ~ horsepower, data = Auto)
```

### Task 2 {.unnumbered}

Have a look at the results of the model.


```r
summary(fit)
```

```
## 
## Call:
## lm(formula = mpg ~ horsepower, data = Auto)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -13.5710  -3.2592  -0.3435   2.7630  16.9240 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 39.935861   0.717499   55.66   <2e-16 ***
## horsepower  -0.157845   0.006446  -24.49   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 4.906 on 390 degrees of freedom
## Multiple R-squared:  0.6059,	Adjusted R-squared:  0.6049 
## F-statistic: 599.7 on 1 and 390 DF,  p-value: < 2.2e-16
```

::: question
Is there a relationship between the predictor and the response?
:::

::: answers
The slope coefficient (`-0.157845`) is statistically significant
(`<2e-16 ***`). We can conclude that there is evidence to suggest a
negative relationship between miles per gallon and engine horsepower.
For a one-unit increase in engine horsepower, miles per gallon are
reduced by 0.16.
:::

### Task 3 {.unnumbered}

What is the associated 95% confidence intervals for predicted miles per
gallon associated with an engine horsepower of 98? Hint: use the
`predict()` function. For confidence intervals, set the `interval`
argument to `confidence`.


```r
predict(fit, data.frame(horsepower = 98), interval = "confidence")
```

```
##        fit      lwr      upr
## 1 24.46708 23.97308 24.96108
```

### Task 4 {.unnumbered}

How about the prediction interval for the same value?


```r
predict(fit, data.frame(horsepower = 98), interval = "prediction")
```

```
##        fit     lwr      upr
## 1 24.46708 14.8094 34.12476
```

::: question
Are the two intervals different? Why?
:::

::: answers
The prediction interval (lower limit 14.8094 and upper limit 34.12476)
is wider (and therefore less precise) than the confidence interval
(lower limit 23.97308 and upper limit 24.96108). The confidence interval
measures the uncertainty around the estimate of the conditional mean
whilst the prediction interval takes into account not only uncertainty
but also the variability of the conditional distribution.
:::

### Task 5 {.unnumbered}

Using base R, plot the response and the predictor as well as the least
squares regression line. Add suitable labels to the X and Y axes.


```r
plot(Auto$horsepower, Auto$mpg, xlab = "horsepower", ylab = "mpg")
abline(fit)
```

<img src="02-S02-ANS_files/figure-html/unnamed-chunk-6-1.png" width="672" />

### Task 6 {.unnumbered}

Use base R to produce diagnostic plots of the least squares regression
fit. Display these in a 2X2 grid.


```r
par(mfrow = c(2, 2))
plot(fit, cex = 0.2)
```

<img src="02-S02-ANS_files/figure-html/unnamed-chunk-7-1.png" width="672" />

### Task 7 {.unnumbered}

Subset the **Auto** dataset such that it excludes the **name** and
**origin** variables and store this subsetted dataset in a new object
called **quant_vars**.


```r
quant_vars <- subset(Auto, select = -c(name, origin))
```

### Task 8 {.unnumbered}

Compute a correlation matrix of all variables.


```r
cor(quant_vars)
```

```
##                     mpg  cylinders displacement horsepower     weight
## mpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442
## cylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273
## displacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944
## horsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377
## weight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000
## acceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392
## year          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199
##              acceleration       year
## mpg             0.4233285  0.5805410
## cylinders      -0.5046834 -0.3456474
## displacement   -0.5438005 -0.3698552
## horsepower     -0.6891955 -0.4163615
## weight         -0.4168392 -0.3091199
## acceleration    1.0000000  0.2903161
## year            0.2903161  1.0000000
```

::: question
Did you use the **Auto** dataset or the **quant_vars** object? Why does
it matter which data object you use?
:::

::: answers
To compute the correlation matrix using all variables of a data object,
these variables must all be numeric. In the **Auto** data object, the
**name** variable is coded as a factor.

`class(Auto$name)`\
`[1] "factor"`

Therefore, if you try to use the `cor()` function with **Auto** dataset
without excluding the **name** variable, you will get an error.

`cor(Auto)`\
`Error in cor(Auto) : 'x' must be numeric`.

Also, whilst the **origin** variable is of class integer and will not
pose a problem when you apply the `cor()` function, you'll remember from
the variable description list that this is a nominal variable with its
categories numerically labelled.

Compute the correlation matrix using **quant_vars**.
:::

### Task 9 {.unnumbered}

Using the **quant_vars** object, perform multiple linear regression with
miles per gallon as the response and all other variables as the
predictors.

Store the results in an object called **fit2**.


```r
fit2 <- lm(mpg ~ ., data = quant_vars)
```

### Task 10 {.unnumbered}

Have a look at the results of the multiple regression model.


```r
summary(fit2)
```

```
## 
## Call:
## lm(formula = mpg ~ ., data = quant_vars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.6927 -2.3864 -0.0801  2.0291 14.3607 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  -1.454e+01  4.764e+00  -3.051  0.00244 ** 
## cylinders    -3.299e-01  3.321e-01  -0.993  0.32122    
## displacement  7.678e-03  7.358e-03   1.044  0.29733    
## horsepower   -3.914e-04  1.384e-02  -0.028  0.97745    
## weight       -6.795e-03  6.700e-04 -10.141  < 2e-16 ***
## acceleration  8.527e-02  1.020e-01   0.836  0.40383    
## year          7.534e-01  5.262e-02  14.318  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.435 on 385 degrees of freedom
## Multiple R-squared:  0.8093,	Adjusted R-squared:  0.8063 
## F-statistic: 272.2 on 6 and 385 DF,  p-value: < 2.2e-16
```

::: question
Is there a relationship between the predictors and the response? Which
predictors appear to have a statistically significant relationship to
the response? What does the coefficient for the year variable suggest?
:::

::: answers
Two of the predictors are statistically significant: **weight** and
**year**. The relationship between **weight** and **mpg** is negative
which suggests that for a one pound increase in weight of vehicle, the
number of miles per gallon the vehicle can travel decreases, whilst that
of **mpg** and **year** is positive which suggests that the more recent
the vehicle is, the higher the number of miles per gallon it can travel.
:::

### Task 11 {.unnumbered}

Produce diagnostic plots of the multiple linear regression fit in a 2x2
grid.


```r
par(mfrow = c(2, 2))
plot(fit2, cex = 0.2)
```

<img src="02-S02-ANS_files/figure-html/unnamed-chunk-12-1.png" width="672" />

::: question
Do the residual plots suggest any unusually large outliers? Does the
leverage plot identify any observations with unusually high leverage?
:::

::: answers
One point has high leverage, the residuals also show a trend with fitted
values.
:::

### Task 12 {.unnumbered}

Fit separate linear regression models with interaction effect terms for:
weight and horsepower, acceleration and horsepower, and cylinders and
weight.


```r
summary(lm(mpg ~ . + weight:horsepower, data = quant_vars))
summary(lm(mpg ~ . + acceleration:horsepower, data = quant_vars))
summary(lm(mpg ~ . + cylinders:weight, data = quant_vars))
```

::: question
Are any of the interaction terms statistically significant?
:::

::: answers
For each model, the interaction term is statistically significant.
:::

### Task 13 {.unnumbered}

Using the **Auto** data object, apply transformations for the
**horsepower** variable and plot the relationship between **horsepower**
and **mpg** in a 2x2 grid.

-   First plot: use the original variable;\
-   Second plot: apply log transform;\
-   Third plot: raise it to the power of two.


```r
par(mfrow = c(2, 2))
plot(Auto$horsepower, Auto$mpg, cex = 0.2)
plot(log(Auto$horsepower), Auto$mpg, cex = 0.2)
plot(Auto$horsepower ^ 2, Auto$mpg, cex = 0.2)
```

::: question
Which of these transformations is most suitable?
:::

::: answers
The relationship between horsepower and miles per gallon is clearly
non-linear (plot 1). The log transform seems to address this best.
:::

### Task 14 {.unnumbered}

Now run a multiple regression model with all variables as before but
this time, apply a log transformation of the **horsepower** variable.


```r
quant_vars$horsepower <- log(quant_vars$horsepower)
fit3 <- lm(mpg ~ ., data = quant_vars)
```

### Task 15 {.unnumbered}

Have a look at the results.


```r
summary(fit3)
```

```
## 
## Call:
## lm(formula = mpg ~ ., data = quant_vars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.6778 -2.0080 -0.3142  1.9262 14.0979 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(>|t|)    
## (Intercept)  29.1713000  8.9291383   3.267  0.00118 ** 
## cylinders    -0.3563199  0.3181815  -1.120  0.26347    
## displacement  0.0088277  0.0068866   1.282  0.20066    
## horsepower   -8.7568129  1.5958761  -5.487 7.42e-08 ***
## weight       -0.0044304  0.0007213  -6.142 2.03e-09 ***
## acceleration -0.3317439  0.1077476  -3.079  0.00223 ** 
## year          0.6979715  0.0503916  13.851  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.308 on 385 degrees of freedom
## Multiple R-squared:  0.8231,	Adjusted R-squared:  0.8203 
## F-statistic: 298.5 on 6 and 385 DF,  p-value: < 2.2e-16
```

::: question
How do the results of model **fit3** differ from those of model
**fit2**?
:::

::: answers
The **fit2** model results showed that only two predictors were
statistically significant: **weight** and **year**. The **fit3** model
has two additional predictors that are statistically significant:
**acceleration** as well as **horsepower**.Also, the coefficient values
can now be interpreted more easily.
:::

### Task 16 {.unnumbered}

Produce diagnostic plots for the **fit3** object and display them in a
2x2 grid.


```r
par(mfrow = c(2, 2))
plot(fit3, cex = 0.2)
```

<img src="02-S02-ANS_files/figure-html/unnamed-chunk-17-1.png" width="672" />

::: question
How do the diagnostic plots differ?
:::

::: answers
A log transformation of **horsepower** appears to give a more linear
relationship with **mpg** but this difference does not seem to be
substantial.
:::

## Practical 2 {-}

::: file
For the tasks below, you will require the **Carseats** dataset from the
core textbook (James et. al 2021).

This dataset is part of the `ISRL2` package. By loading the package, the
**Carseats** dataset loads automatically:

`library(ISLR2)`
:::

This dataframe object contains a simulated dataset of sales of child car
seats at 400 different stores.

The 9 variables are:

-   Sales: Unit sales (thousands of dollars) at each location\
-   CompPrice: Price charged by competitor at each location\
-   Income: Community income level (thousands of dollars)\
-   Advertising: Local advertising budget for company at each location
    (thousands of dollars)\
-   Population: Population size in region (thousands of dollars)\
-   Price: Price company charges for car seats at each site\
-   ShelveLoc: Quality of the shelving location for the car seats at
    each site\
-   Age: Average age of the local population\
-   Education: Education level at each location\
-   Urban: Whether the store is in an urban or rural location\
-   US: Whether the store is in the US or not



### Task 1 {.unnumbered}

Fit a multiple regression model to predict unit sales at each location
based on price company charges for car seats, whether the store is in an
urban or rural location, and whether the store is in the US or not.


```r
fit <- lm(Sales ~ Price + Urban + US, data = Carseats)
```

### Task 2 {.unnumbered}

Have a look at the results and interpret the coefficients.


```r
summary(fit)
```

```
## 
## Call:
## lm(formula = Sales ~ Price + Urban + US, data = Carseats)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.9206 -1.6220 -0.0564  1.5786  7.0581 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) 13.043469   0.651012  20.036  < 2e-16 ***
## Price       -0.054459   0.005242 -10.389  < 2e-16 ***
## UrbanYes    -0.021916   0.271650  -0.081    0.936    
## USYes        1.200573   0.259042   4.635 4.86e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.472 on 396 degrees of freedom
## Multiple R-squared:  0.2393,	Adjusted R-squared:  0.2335 
## F-statistic: 41.52 on 3 and 396 DF,  p-value: < 2.2e-16
```

::: question
Which coefficients are statistically significant? What do they indicate?
:::

::: answers
The null hypothesis for the slope being zero is rejected for the
**Price** and **US** variables. The coefficient for **Price** is
statistically significant; since it is negative, as price increases by a
thousand dollars (i.e. one unit increase), the sales of child decrease
by about 0.05. The slope for the **US** variable is also statistically
significant but it is positive. Also, this is a binary factor variable
coded as Yes and No (No is the reference category). Therefore, sales of
child car seats are higher by 1.2 for car seat products that are
produced in the US than for car seat products not produced in the US.
:::

### Task 3 {.unnumbered}

Based on the conclusions you have drawn in Task 2, now fit a smaller
model that only uses the predictors for which there is evidence of
association with sales.


```r
fit2 <- lm(Sales ~ Price + US, data = Carseats)
```

### Task 4 {.unnumbered}

Compare the two models (*fit* and *fit2*).


```r
anova(fit, fit2)
```

```
## Analysis of Variance Table
## 
## Model 1: Sales ~ Price + Urban + US
## Model 2: Sales ~ Price + US
##   Res.Df    RSS Df Sum of Sq      F Pr(>F)
## 1    396 2420.8                           
## 2    397 2420.9 -1  -0.03979 0.0065 0.9357
```

::: question
Which model is the better fit?
:::

::: answers
They have similar r-squared values, and the *fit* model (containing the
extra variable **Urban**) is non-significantly better.
:::

### Task 5 {.unnumbered}

Produce diagnostic plots for *fit2* and display these in a 2x2 grid.


```r
par(mfrow = c(2, 2))
plot(fit2, cex = 0.2)
```

<img src="02-S02-ANS_files/figure-html/unnamed-chunk-23-1.png" width="672" />

::: question
Is there evidence of outliers or high leverage observations in the
*fit2* model?
:::

::: answers
Yes, there is evidence of outliers and high leverage observations for
*fit2*.
:::


