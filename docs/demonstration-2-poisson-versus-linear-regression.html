<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Demonstration 2: Poisson versus Linear Regression | Data Science Modelling</title>
  <meta name="description" content="Notebook hosting practical materials for SOST70033." />
  <meta name="generator" content="bookdown 0.38 and GitBook 2.6.7" />

  <meta property="og:title" content="Demonstration 2: Poisson versus Linear Regression | Data Science Modelling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Notebook hosting practical materials for SOST70033." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Demonstration 2: Poisson versus Linear Regression | Data Science Modelling" />
  
  <meta name="twitter:description" content="Notebook hosting practical materials for SOST70033." />
  

<meta name="author" content="Dr.Â Ioana Macoveciuc" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="demonstration-1-classification-problems.html"/>
<link rel="next" href="practical-predicting-a-companys-bankruptcy.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="images/logos/uom_logo.png"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a></li>
<li class="part"><span><b>Section 1</b></span></li>
<li class="chapter" data-level="" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="" data-path="demonstration-a-more-in-depth-consideration-of-model-accuracy.html"><a href="demonstration-a-more-in-depth-consideration-of-model-accuracy.html"><i class="fa fa-check"></i>Demonstration: A more in-depth consideration of model accuracy</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-a-more-in-depth-consideration-of-model-accuracy.html"><a href="demonstration-a-more-in-depth-consideration-of-model-accuracy.html#the-simulation"><i class="fa fa-check"></i>The Simulation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html"><i class="fa fa-check"></i>Practical 1</a>
<ul>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-1"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-2"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-3"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-4"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-5"><i class="fa fa-check"></i>Task 5</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-6"><i class="fa fa-check"></i>Task 6</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-7"><i class="fa fa-check"></i>Task 7</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-8"><i class="fa fa-check"></i>Task 8</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-9"><i class="fa fa-check"></i>Task 9</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-10"><i class="fa fa-check"></i>Task 10</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-11"><i class="fa fa-check"></i>Task 11</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-2.html"><a href="practical-2.html"><i class="fa fa-check"></i>Practical 2</a>
<ul>
<li class="chapter" data-level="" data-path="practical-2.html"><a href="practical-2.html#task-1-1"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical-2.html"><a href="practical-2.html#task-2-1"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="practical-2.html"><a href="practical-2.html#task-3-1"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="practical-2.html"><a href="practical-2.html#task-4-1"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="practical-2.html"><a href="practical-2.html#task-5-1"><i class="fa fa-check"></i>Task 5</a></li>
<li class="chapter" data-level="" data-path="practical-2.html"><a href="practical-2.html#task-6-1"><i class="fa fa-check"></i>Task 6</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html"><i class="fa fa-check"></i>Answers</a>
<ul>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#practical-1-1"><i class="fa fa-check"></i>Practical 1</a>
<ul>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-1-2"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-2-2"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-3-2"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-4-2"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-5-2"><i class="fa fa-check"></i>Task 5</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-6-2"><i class="fa fa-check"></i>Task 6</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-7-1"><i class="fa fa-check"></i>Task 7</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-8-1"><i class="fa fa-check"></i>Task 8</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-9-1"><i class="fa fa-check"></i>Task 9</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-10-1"><i class="fa fa-check"></i>Task 10</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-11-1"><i class="fa fa-check"></i>Task 11</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#practical-2-1"><i class="fa fa-check"></i>Practical 2</a>
<ul>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-1-3"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-2-3"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-3-3"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-4-3"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-5-3"><i class="fa fa-check"></i>Task 5</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-6-3"><i class="fa fa-check"></i>Task 6</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Section 2</b></span></li>
<li class="chapter" data-level="" data-path="overview-1.html"><a href="overview-1.html"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="" data-path="demonstration-1.html"><a href="demonstration-1.html"><i class="fa fa-check"></i>Demonstration 1</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-1.html"><a href="demonstration-1.html#simple-linear-models-without-intercept"><i class="fa fa-check"></i>Simple Linear Models Without Intercept</a></li>
<li class="chapter" data-level="" data-path="demonstration-1.html"><a href="demonstration-1.html#simple-linear-models-with-intercept"><i class="fa fa-check"></i>Simple Linear Models with Intercept</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="demonstration-2.html"><a href="demonstration-2.html"><i class="fa fa-check"></i>Demonstration 2</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-2.html"><a href="demonstration-2.html#population-parameters-and-estimated-coefficients"><i class="fa fa-check"></i>Population Parameters and Estimated Coefficients</a></li>
<li class="chapter" data-level="" data-path="demonstration-2.html"><a href="demonstration-2.html#what-happens-if-we-reduce-noise"><i class="fa fa-check"></i>What happens if we <em>reduce</em> noise?</a></li>
<li class="chapter" data-level="" data-path="demonstration-2.html"><a href="demonstration-2.html#what-happens-if-we-increase-noise"><i class="fa fa-check"></i>What happens if we <em>increase</em> noise?</a></li>
<li class="chapter" data-level="" data-path="demonstration-2.html"><a href="demonstration-2.html#how-does-noise-affect-confidence-intervals-for-the-coefficients"><i class="fa fa-check"></i>How does noise affect confidence intervals for the coefficients?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html"><i class="fa fa-check"></i>Practical 1</a>
<ul>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-1-4"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-2-4"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-3-4"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-4-4"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-5-4"><i class="fa fa-check"></i>Task 5</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-6-4"><i class="fa fa-check"></i>Task 6</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-7-2"><i class="fa fa-check"></i>Task 7</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-8-2"><i class="fa fa-check"></i>Task 8</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-9-2"><i class="fa fa-check"></i>Task 9</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-10-2"><i class="fa fa-check"></i>Task 10</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-11-2"><i class="fa fa-check"></i>Task 11</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-12"><i class="fa fa-check"></i>Task 12</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-13"><i class="fa fa-check"></i>Task 13</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-14"><i class="fa fa-check"></i>Task 14</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-15"><i class="fa fa-check"></i>Task 15</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-16"><i class="fa fa-check"></i>Task 16</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-2-2.html"><a href="practical-2-2.html"><i class="fa fa-check"></i>Practical 2</a>
<ul>
<li class="chapter" data-level="" data-path="practical-2-2.html"><a href="practical-2-2.html#task-1-5"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical-2-2.html"><a href="practical-2-2.html#task-2-5"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="practical-2-2.html"><a href="practical-2-2.html#task-3-5"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="practical-2-2.html"><a href="practical-2-2.html#task-4-5"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="practical-2-2.html"><a href="practical-2-2.html#task-5-5"><i class="fa fa-check"></i>Task 5</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-3-the-quality-of-red-bordeaux-vintages.html"><a href="practical-3-the-quality-of-red-bordeaux-vintages.html"><i class="fa fa-check"></i>Practical 3: The Quality of Red Bordeaux Vintages</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html"><i class="fa fa-check"></i>Answers</a>
<ul>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#practical-1-3"><i class="fa fa-check"></i>Practical 1</a>
<ul>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-1-6"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-2-6"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-3-6"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-4-6"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-5-6"><i class="fa fa-check"></i>Task 5</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-6-5"><i class="fa fa-check"></i>Task 6</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-7-3"><i class="fa fa-check"></i>Task 7</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-8-3"><i class="fa fa-check"></i>Task 8</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-9-3"><i class="fa fa-check"></i>Task 9</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-10-3"><i class="fa fa-check"></i>Task 10</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-11-3"><i class="fa fa-check"></i>Task 11</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-12-1"><i class="fa fa-check"></i>Task 12</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-13-1"><i class="fa fa-check"></i>Task 13</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-14-1"><i class="fa fa-check"></i>Task 14</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-15-1"><i class="fa fa-check"></i>Task 15</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-16-1"><i class="fa fa-check"></i>Task 16</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#practical-2-3"><i class="fa fa-check"></i>Practical 2</a>
<ul>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-1-7"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-2-7"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-3-7"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-4-7"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-5-7"><i class="fa fa-check"></i>Task 5</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#practical-3-the-quality-of-red-bordeaux-vintages-1"><i class="fa fa-check"></i>Practical 3: The Quality of Red Bordeaux Vintages</a></li>
</ul></li>
<li class="part"><span><b>Section 3</b></span></li>
<li class="chapter" data-level="" data-path="overview-2.html"><a href="overview-2.html"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html"><i class="fa fa-check"></i>Demonstration 1: Classification Problems</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#dataset-and-variables"><i class="fa fa-check"></i>Dataset and Variables</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#correlation-matrix-and-plot"><i class="fa fa-check"></i>Correlation Matrix and Plot</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#classic-logistic-regression"><i class="fa fa-check"></i>âClassicâ Logistic Regression</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#confusion-matrix"><i class="fa fa-check"></i>Confusion Matrix</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#logistic-regression-in-statistical-learning"><i class="fa fa-check"></i>Logistic Regression in Statistical Learning</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#linear-discriminant-analysis"><i class="fa fa-check"></i>Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i>Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#k-nearest-neighbours"><i class="fa fa-check"></i><span class="math inline">\(K\)</span>-nearest neighbours</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#naive-bayes"><i class="fa fa-check"></i>Naive Bayes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="demonstration-2-poisson-versus-linear-regression.html"><a href="demonstration-2-poisson-versus-linear-regression.html"><i class="fa fa-check"></i>Demonstration 2: Poisson versus Linear Regression</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-2-poisson-versus-linear-regression.html"><a href="demonstration-2-poisson-versus-linear-regression.html#the-linear-model"><i class="fa fa-check"></i>The Linear Model</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-2-poisson-versus-linear-regression.html"><a href="demonstration-2-poisson-versus-linear-regression.html#approaches-to-variable-coding"><i class="fa fa-check"></i>Approaches to variable coding</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-poisson-versus-linear-regression.html"><a href="demonstration-2-poisson-versus-linear-regression.html#plotting-coefficient-estimates"><i class="fa fa-check"></i>Plotting Coefficient Estimates</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="demonstration-2-poisson-versus-linear-regression.html"><a href="demonstration-2-poisson-versus-linear-regression.html#the-poisson-model"><i class="fa fa-check"></i>The Poisson Model</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-poisson-versus-linear-regression.html"><a href="demonstration-2-poisson-versus-linear-regression.html#linear-versus-poisson-regression"><i class="fa fa-check"></i>Linear versus Poisson Regression</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html"><i class="fa fa-check"></i>Practical: Predicting a companyâs bankruptcy</a>
<ul>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#data-and-variables"><i class="fa fa-check"></i>Data and Variables</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#importing-the-data"><i class="fa fa-check"></i>Importing the data</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#loading-required-packages"><i class="fa fa-check"></i>Loading required packages</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#correlation-matrix-and-plot-1"><i class="fa fa-check"></i>Correlation Matrix and Plot</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#logistic-regression"><i class="fa fa-check"></i>Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#explaining-the-logit"><i class="fa fa-check"></i>Explaining the Logit</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#tasks-linear-discriminant-analysis"><i class="fa fa-check"></i>Tasks: Linear Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-1-8"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-2-8"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-3-8"><i class="fa fa-check"></i>Task 3</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#tasks-quadratic-discriminant-analysis"><i class="fa fa-check"></i>Tasks: Quadratic Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-1-9"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-2-9"><i class="fa fa-check"></i>Task 2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#tasks-k-nearest-neighbours"><i class="fa fa-check"></i>Tasks: <span class="math inline">\(K\)</span>-nearest neighbours</a>
<ul>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-1-10"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-2-10"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-3-9"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-4-8"><i class="fa fa-check"></i>Task 4</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#tasks-naive-bayes"><i class="fa fa-check"></i>Tasks: Naive Bayes</a>
<ul>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-1-11"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-2-11"><i class="fa fa-check"></i>Task 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html"><i class="fa fa-check"></i>Answers</a>
<ul>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#practical-predicting-a-companys-bankruptcy-1"><i class="fa fa-check"></i>Practical: Predicting a companyâs bankruptcy</a>
<ul>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#data-and-variables-1"><i class="fa fa-check"></i>Data and Variables</a></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#importing-the-data-1"><i class="fa fa-check"></i>Importing the data</a></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#loading-required-packages-1"><i class="fa fa-check"></i>Loading required packages</a></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#correlation-matrix-and-plot-2"><i class="fa fa-check"></i>Correlation Matrix and Plot</a></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#logistic-regression-1"><i class="fa fa-check"></i>Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#linear-discriminant-analysis-1"><i class="fa fa-check"></i>Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#quadratic-discriminant-analysis-1"><i class="fa fa-check"></i>Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#k-nearest-neighbours-1"><i class="fa fa-check"></i><span class="math inline">\(K\)</span>-nearest neighbours</a></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#naive-bayes-1"><i class="fa fa-check"></i>Naive Bayes</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Science Modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<hr>
<center> 
  <div class="header">
   <img src="images/banners/DSM_banner.png" alt="Trulli">
  </div>
</center>
<div id="demonstration-2-poisson-versus-linear-regression" class="section level1 unnumbered hasAnchor">
<h1>Demonstration 2: Poisson versus Linear Regression<a href="demonstration-2-poisson-versus-linear-regression.html#demonstration-2-poisson-versus-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this demonstration, we will cover the basics of building and
interpreting a Poisson regression model.</p>
<p>You will need the <strong>Bikeshare</strong> dataset, part of the <code>ISRL2</code> package. By
loading the package, the <strong>Weekly</strong> dataset loads automatically.</p>
<p>This dataset measures the number of bike rentals (<code>bikers</code>) per hour in
Washington, DC. It contains 8645 observations on several variables. The
variables we will use in this practical and their descriptions are
listed below. For further information on additional variables, type
<code>?Bikeshare</code> in your R console after loading the <code>ISLR2</code> package.</p>
<ul>
<li><p>mnth: Month of the year, coded as a factor.</p></li>
<li><p>hr: Hour of the day, coded as a factor from 0 to 23.</p></li>
<li><p>workingday: Is it a work day? Yes=1, No=0.</p></li>
<li><p>temp: Normalised temperature in Celsius. The values are derived via
(t-t_min)/(t_max-t_min), t_min=-8, t_max=+39;</p></li>
<li><p>weathersit: Weather, coded as a factor.</p></li>
</ul>
<p>Loading the necessary packages:</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="demonstration-2-poisson-versus-linear-regression.html#cb191-1" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span></code></pre></div>
<p>Once you load the <code>ISLR2</code> package, the <strong>Bikeshare</strong> dataset will be
âloadedâ too and can be accessed without needing to assign it to a
separate object.</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="demonstration-2-poisson-versus-linear-regression.html#cb192-1" tabindex="-1"></a><span class="fu">head</span>(Bikeshare)</span></code></pre></div>
<pre><code>##   season mnth day hr holiday weekday workingday   weathersit temp  atemp  hum
## 1      1  Jan   1  0       0       6          0        clear 0.24 0.2879 0.81
## 2      1  Jan   1  1       0       6          0        clear 0.22 0.2727 0.80
## 3      1  Jan   1  2       0       6          0        clear 0.22 0.2727 0.80
## 4      1  Jan   1  3       0       6          0        clear 0.24 0.2879 0.75
## 5      1  Jan   1  4       0       6          0        clear 0.24 0.2879 0.75
## 6      1  Jan   1  5       0       6          0 cloudy/misty 0.24 0.2576 0.75
##   windspeed casual registered bikers
## 1    0.0000      3         13     16
## 2    0.0000      8         32     40
## 3    0.0000      5         27     32
## 4    0.0000      3         10     13
## 5    0.0000      0          1      1
## 6    0.0896      0          1      1</code></pre>
<p>As usual, we can access variables within the dataset by indexing them.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="demonstration-2-poisson-versus-linear-regression.html#cb194-1" tabindex="-1"></a>Bikeshare<span class="sc">$</span>temp</span></code></pre></div>
<pre><code>##    [1] 0.24 0.22 0.22 0.24 0.24 0.24 0.22 0.20 0.24 0.32 0.38 0.36 0.42 0.46
##   [15] 0.46 0.44 0.42 0.44 0.42 0.42 0.40 0.40 0.40 0.46 0.46 0.44 0.42 0.46
##   [29] 0.46 0.42 0.40 0.40 0.38 0.36 0.36 0.36 0.36 0.36 0.34 0.34 0.34 0.36
##   [43] 0.32 0.30 0.26 0.24 0.22 0.22 0.20 0.16 0.16 0.14 0.14 0.14 0.16 0.18
##   [57] 0.20 0.22 0.24 0.26 0.26 0.26 0.24 0.24 0.20 0.20 0.18 0.14 0.18 0.16
##   [71] 0.16 0.14 0.14 0.12 0.12 0.12 0.14 0.16 0.16 0.22 0.22 0.24 0.26 0.28
##   [85] 0.30 0.28 0.26 0.24 0.24 0.22 0.22 0.20 0.20 0.16 0.16 0.24 0.22 0.20
##   [99] 0.18 0.20 0.22 0.22 0.26 0.26 0.28 0.30 0.30 0.30 0.24 0.24 0.24 0.22
##  [113] 0.20 0.18 0.20 0.18 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.18 0.20 0.22
##  [127] 0.26 0.26 0.28 0.28 0.26 0.22 0.22 0.22 0.20 0.22 0.22 0.20 0.20 0.20
##  [141] 0.20 0.20 0.22 0.20 0.20 0.20 0.20 0.22 0.20 0.20 0.20 0.20 0.20 0.20
##  [155] 0.20 0.20 0.16 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.16 0.16
##  [169] 0.16 0.16 0.16 0.18 0.20 0.20 0.20 0.20 0.20 0.18 0.16 0.14 0.14 0.12
##  [183] 0.12 0.12 0.10 0.10 0.10 0.10 0.10 0.08 0.08 0.10 0.08 0.10 0.12 0.14
##  [197] 0.16 0.18 0.20 0.22 0.22 0.20 0.18 0.16 0.16 0.14 0.14 0.14 0.12 0.12
##  [211] 0.12 0.12 0.12 0.10 0.10 0.12 0.12 0.12 0.14 0.14 0.16 0.20 0.20 0.20
##  [225] 0.20 0.20 0.20 0.20 0.16 0.16 0.14 0.14 0.14 0.14 0.14 0.16 0.16 0.16
##  [239] 0.16 0.18 0.18 0.20 0.20 0.20 0.20 0.20 0.16 0.16 0.16 0.16 0.16 0.16
##  [253] 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.12 0.14 0.16 0.16 0.18 0.20 0.20
##  [267] 0.22 0.20 0.20 0.22 0.20 0.20 0.18 0.16 0.16 0.16 0.14 0.14 0.14 0.14
##  [281] 0.14 0.14 0.14 0.12 0.12 0.14 0.14 0.16 0.20 0.20 0.22 0.22 0.24 0.24
##  [295] 0.20 0.20 0.16 0.16 0.14 0.14 0.12 0.12 0.10 0.10 0.10 0.10 0.10 0.10
##  [309] 0.12 0.14 0.18 0.18 0.20 0.22 0.22 0.24 0.22 0.22 0.20 0.16 0.18 0.16
##  [323] 0.16 0.18 0.18 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.14 0.16 0.18 0.20
##  [337] 0.24 0.28 0.30 0.32 0.34 0.32 0.30 0.32 0.32 0.32 0.30 0.30 0.26 0.26
##  [351] 0.26 0.22 0.26 0.26 0.26 0.24 0.22 0.22 0.22 0.24 0.24 0.26 0.28 0.26
##  [365] 0.24 0.22 0.20 0.18 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.18 0.18 0.18
##  [379] 0.18 0.16 0.16 0.16 0.16 0.16 0.18 0.18 0.18 0.20 0.20 0.20 0.18 0.18
##  [393] 0.16 0.16 0.14 0.16 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22
##  [407] 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.24 0.24 0.24 0.26 0.28 0.30
##  [421] 0.40 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.32 0.30 0.30 0.26 0.26 0.26
##  [435] 0.26 0.26 0.24 0.22 0.22 0.22 0.24 0.26 0.28 0.30 0.28 0.30 0.32 0.30
##  [449] 0.30 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.22 0.22 0.24 0.22
##  [463] 0.20 0.20 0.20 0.20 0.22 0.22 0.20 0.20 0.16 0.16 0.14 0.12 0.12 0.10
##  [477] 0.08 0.06 0.06 0.04 0.04 0.04 0.04 0.02 0.02 0.02 0.02 0.04 0.04 0.06
##  [491] 0.06 0.08 0.10 0.12 0.12 0.12 0.08 0.08 0.06 0.06 0.06 0.04 0.04 0.04
##  [505] 0.02 0.02 0.04 0.04 0.08 0.06 0.10 0.14 0.14 0.16 0.14 0.16 0.16 0.16
##  [519] 0.14 0.12 0.12 0.10 0.10 0.08 0.06 0.06 0.04 0.04 0.02 0.02 0.02 0.02
##  [533] 0.04 0.06 0.10 0.10 0.12 0.14 0.14 0.16 0.16 0.14 0.14 0.14 0.14 0.14
##  [547] 0.14 0.16 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.16 0.20 0.22 0.24 0.26
##  [561] 0.26 0.30 0.32 0.32 0.30 0.30 0.26 0.24 0.24 0.22 0.22 0.22 0.24 0.22
##  [575] 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.20 0.22
##  [589] 0.22 0.20 0.20 0.18 0.18 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.18 0.16
##  [603] 0.16 0.18 0.18 0.18 0.18 0.18 0.22 0.20 0.22 0.24 0.24 0.24 0.24 0.22
##  [617] 0.24 0.24 0.22 0.22 0.22 0.20 0.16 0.16 0.16 0.18 0.18 0.18 0.18 0.20
##  [631] 0.22 0.22 0.22 0.24 0.24 0.22 0.22 0.18 0.18 0.16 0.16 0.16 0.14 0.16
##  [645] 0.14 0.14 0.14 0.14 0.14 0.16 0.18 0.22 0.30 0.28 0.28 0.30 0.30 0.30
##  [659] 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.18 0.16 0.16
##  [673] 0.16 0.16 0.16 0.16 0.18 0.16 0.18 0.16 0.16 0.16 0.16 0.30 0.16 0.16
##  [687] 0.16 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.16 0.16 0.18 0.20
##  [701] 0.20 0.22 0.24 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.22 0.22 0.22
##  [715] 0.22 0.22 0.22 0.22 0.22 0.22 0.24 0.22 0.24 0.24 0.34 0.38 0.38 0.36
##  [729] 0.36 0.34 0.28 0.24 0.22 0.22 0.20 0.20 0.20 0.18 0.18 0.16 0.16 0.14
##  [743] 0.14 0.16 0.18 0.18 0.20 0.20 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.18
##  [757] 0.18 0.20 0.20 0.16 0.14 0.14 0.14 0.16 0.14 0.14 0.16 0.20 0.22 0.24
##  [771] 0.26 0.28 0.28 0.30 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24
##  [785] 0.24 0.22 0.20 0.20 0.22 0.20 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22
##  [799] 0.24 0.28 0.28 0.30 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26
##  [813] 0.24 0.24 0.28 0.30 0.32 0.34 0.34 0.34 0.34 0.34 0.34 0.30 0.28 0.28
##  [827] 0.26 0.26 0.24 0.24 0.22 0.20 0.20 0.20 0.20 0.18 0.18 0.16 0.22 0.24
##  [841] 0.30 0.32 0.36 0.36 0.38 0.36 0.32 0.34 0.32 0.32 0.32 0.28 0.30 0.28
##  [855] 0.28 0.26 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.22 0.22 0.24 0.24 0.22
##  [869] 0.22 0.22 0.22 0.20 0.16 0.16 0.14 0.12 0.12 0.10 0.10 0.08 0.06 0.06
##  [883] 0.06 0.06 0.10 0.12 0.14 0.14 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.14
##  [897] 0.14 0.14 0.16 0.16 0.14 0.14 0.14 0.14 0.12 0.12 0.10 0.10 0.12 0.12
##  [911] 0.14 0.16 0.18 0.20 0.20 0.20 0.18 0.16 0.14 0.14 0.14 0.12 0.12 0.10
##  [925] 0.10 0.10 0.08 0.10 0.08 0.10 0.12 0.14 0.22 0.22 0.24 0.30 0.32 0.30
##  [939] 0.30 0.28 0.26 0.22 0.20 0.20 0.18 0.16 0.14 0.14 0.12 0.12 0.12 0.12
##  [953] 0.12 0.14 0.16 0.22 0.30 0.30 0.30 0.34 0.34 0.34 0.32 0.28 0.28 0.26
##  [967] 0.26 0.24 0.22 0.20 0.20 0.20 0.20 0.20 0.20 0.22 0.22 0.24 0.30 0.32
##  [981] 0.36 0.38 0.40 0.40 0.42 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38
##  [995] 0.36 0.34 0.32 0.32 0.34 0.34 0.38 0.40 0.44 0.52 0.56 0.58 0.60 0.56
## [1009] 0.52 0.46 0.40 0.38 0.36 0.36 0.34 0.32 0.30 0.30 0.28 0.22 0.22 0.20
## [1023] 0.20 0.20 0.22 0.24 0.26 0.28 0.32 0.34 0.34 0.34 0.32 0.30 0.28 0.26
## [1037] 0.24 0.24 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.20 0.22 0.24 0.26 0.34
## [1051] 0.38 0.42 0.46 0.46 0.46 0.46 0.40 0.34 0.38 0.36 0.34 0.38 0.34 0.34
## [1065] 0.34 0.34 0.32 0.32 0.30 0.32 0.32 0.36 0.38 0.44 0.48 0.54 0.60 0.60
## [1079] 0.56 0.58 0.54 0.48 0.48 0.52 0.50 0.46 0.44 0.44 0.44 0.46 0.46 0.46
## [1093] 0.44 0.42 0.42 0.42 0.44 0.44 0.50 0.60 0.66 0.66 0.66 0.66 0.64 0.62
## [1107] 0.60 0.58 0.54 0.52 0.48 0.46 0.44 0.42 0.40 0.40 0.40 0.38 0.38 0.40
## [1121] 0.42 0.44 0.44 0.44 0.46 0.44 0.44 0.42 0.36 0.34 0.32 0.32 0.30 0.28
## [1135] 0.26 0.24 0.24 0.22 0.22 0.20 0.18 0.20 0.22 0.26 0.30 0.30 0.34 0.36
## [1149] 0.36 0.36 0.34 0.34 0.34 0.34 0.32 0.32 0.30 0.34 0.34 0.34 0.34 0.32
## [1163] 0.34 0.42 0.42 0.32 0.32 0.32 0.32 0.32 0.30 0.32 0.30 0.28 0.28 0.24
## [1177] 0.24 0.24 0.22 0.20 0.20 0.12 0.12 0.12 0.14 0.16 0.16 0.20 0.22 0.22
## [1191] 0.24 0.22 0.22 0.22 0.20 0.20 0.20 0.16 0.16 0.14 0.14 0.12 0.12 0.12
## [1205] 0.12 0.12 0.14 0.18 0.20 0.24 0.26 0.30 0.32 0.34 0.34 0.34 0.32 0.30
## [1219] 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.24 0.24 0.26
## [1233] 0.32 0.36 0.38 0.40 0.40 0.38 0.36 0.34 0.34 0.34 0.34 0.34 0.32 0.32
## [1247] 0.32 0.32 0.32 0.32 0.34 0.34 0.36 0.34 0.42 0.52 0.54 0.54 0.56 0.46
## [1261] 0.32 0.32 0.32 0.30 0.30 0.28 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22
## [1275] 0.22 0.22 0.24 0.26 0.30 0.30 0.32 0.34 0.34 0.36 0.36 0.36 0.34 0.32
## [1289] 0.30 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.24 0.24 0.24 0.26 0.28 0.30
## [1303] 0.36 0.40 0.42 0.44 0.46 0.48 0.42 0.40 0.40 0.40 0.38 0.38 0.36 0.36
## [1317] 0.34 0.32 0.34 0.34 0.36 0.34 0.42 0.52 0.56 0.56 0.46 0.42 0.42 0.42
## [1331] 0.40 0.46 0.44 0.44 0.38 0.34 0.32 0.30 0.26 0.24 0.22 0.22 0.20 0.20
## [1345] 0.20 0.20 0.22 0.24 0.28 0.30 0.32 0.32 0.34 0.34 0.34 0.32 0.30 0.30
## [1359] 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.20 0.22 0.22 0.22 0.24 0.28 0.32
## [1373] 0.34 0.40 0.50 0.52 0.54 0.54 0.50 0.46 0.40 0.36 0.34 0.30 0.26 0.24
## [1387] 0.24 0.20 0.20 0.16 0.14 0.14 0.12 0.14 0.16 0.18 0.20 0.22 0.22 0.24
## [1401] 0.24 0.26 0.26 0.24 0.20 0.20 0.18 0.20 0.18 0.20 0.18 0.18 0.18 0.18
## [1415] 0.16 0.16 0.16 0.18 0.22 0.24 0.28 0.32 0.34 0.36 0.36 0.36 0.36 0.34
## [1429] 0.32 0.30 0.30 0.30 0.30 0.28 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30
## [1443] 0.32 0.34 0.40 0.44 0.46 0.48 0.46 0.48 0.48 0.48 0.46 0.44 0.44 0.42
## [1457] 0.44 0.42 0.42 0.40 0.42 0.42 0.42 0.42 0.40 0.42 0.42 0.42 0.46 0.46
## [1471] 0.44 0.44 0.36 0.34 0.32 0.30 0.28 0.24 0.22 0.22 0.20 0.20 0.20 0.20
## [1485] 0.20 0.20 0.20 0.20 0.22 0.24 0.26 0.30 0.32 0.32 0.34 0.34 0.34 0.32
## [1499] 0.30 0.30 0.28 0.26 0.28 0.26 0.24 0.24 0.24 0.22 0.20 0.20 0.18 0.22
## [1513] 0.26 0.30 0.36 0.36 0.38 0.38 0.36 0.38 0.36 0.34 0.34 0.32 0.30 0.30
## [1527] 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.26 0.30 0.32 0.32
## [1541] 0.32 0.34 0.36 0.34 0.34 0.34 0.34 0.32 0.32 0.32 0.34 0.34 0.34 0.34
## [1555] 0.36 0.36 0.38 0.38 0.40 0.40 0.40 0.42 0.42 0.44 0.44 0.42 0.44 0.44
## [1569] 0.44 0.36 0.36 0.34 0.34 0.34 0.34 0.34 0.32 0.30 0.26 0.26 0.28 0.30
## [1583] 0.32 0.32 0.34 0.36 0.36 0.34 0.34 0.34 0.32 0.30 0.30 0.30 0.30 0.30
## [1597] 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.24 0.26 0.28 0.32 0.34 0.34 0.36
## [1611] 0.40 0.42 0.46 0.46 0.42 0.42 0.40 0.38 0.36 0.38 0.38 0.36 0.34 0.34
## [1625] 0.36 0.34 0.36 0.40 0.40 0.42 0.44 0.46 0.46 0.46 0.48 0.46 0.44 0.40
## [1639] 0.36 0.32 0.30 0.30 0.26 0.26 0.26 0.26 0.26 0.24 0.26 0.28 0.30 0.32
## [1653] 0.34 0.36 0.38 0.38 0.38 0.38 0.40 0.38 0.36 0.36 0.34 0.34 0.32 0.32
## [1667] 0.32 0.30 0.30 0.24 0.24 0.22 0.24 0.26 0.30 0.32 0.34 0.36 0.36 0.38
## [1681] 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.30
## [1695] 0.30 0.30 0.32 0.32 0.36 0.36 0.40 0.40 0.40 0.40 0.40 0.44 0.44 0.44
## [1709] 0.42 0.42 0.40 0.40 0.38 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.36 0.40
## [1723] 0.44 0.44 0.50 0.52 0.50 0.52 0.50 0.50 0.46 0.44 0.42 0.42 0.40 0.42
## [1737] 0.42 0.40 0.40 0.36 0.38 0.40 0.40 0.42 0.46 0.52 0.54 0.56 0.64 0.66
## [1751] 0.68 0.68 0.70 0.68 0.66 0.62 0.62 0.62 0.60 0.60 0.58 0.56 0.54 0.52
## [1765] 0.52 0.44 0.40 0.42 0.42 0.44 0.46 0.46 0.50 0.50 0.50 0.50 0.48 0.46
## [1779] 0.44 0.42 0.40 0.40 0.38 0.34 0.32 0.30 0.28 0.26 0.26 0.26 0.24 0.28
## [1793] 0.30 0.32 0.34 0.36 0.38 0.40 0.40 0.42 0.40 0.38 0.36 0.36 0.34 0.34
## [1807] 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.30 0.30 0.34 0.38 0.42 0.44 0.50
## [1821] 0.54 0.56 0.54 0.54 0.52 0.58 0.56 0.46 0.46 0.46 0.46 0.42 0.44 0.44
## [1835] 0.42 0.40 0.40 0.40 0.40 0.40 0.44 0.44 0.46 0.50 0.50 0.50 0.50 0.50
## [1849] 0.48 0.44 0.44 0.42 0.40 0.40 0.36 0.34 0.34 0.34 0.32 0.34 0.32 0.32
## [1863] 0.32 0.34 0.34 0.34 0.34 0.36 0.38 0.40 0.40 0.38 0.38 0.36 0.32 0.32
## [1877] 0.32 0.32 0.30 0.30 0.28 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.30 0.30
## [1891] 0.32 0.30 0.30 0.30 0.30 0.30 0.30 0.28 0.26 0.26 0.24 0.24 0.20 0.20
## [1905] 0.20 0.18 0.18 0.18 0.20 0.22 0.24 0.26 0.30 0.32 0.32 0.36 0.34 0.34
## [1919] 0.32 0.30 0.30 0.30 0.30 0.28 0.26 0.26 0.26 0.22 0.20 0.22 0.20 0.18
## [1933] 0.20 0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.34 0.34 0.32 0.32 0.28 0.28
## [1947] 0.26 0.28 0.26 0.26 0.24 0.22 0.20 0.18 0.16 0.16 0.20 0.22 0.22 0.24
## [1961] 0.26 0.30 0.32 0.32 0.34 0.32 0.30 0.30 0.28 0.26 0.26 0.26 0.22 0.22
## [1975] 0.22 0.22 0.18 0.18 0.20 0.20 0.22 0.24 0.26 0.26 0.30 0.32 0.32 0.34
## [1989] 0.34 0.32 0.30 0.32 0.32 0.30 0.28 0.26 0.24 0.24 0.24 0.20 0.22 0.22
## [2003] 0.22 0.24 0.28 0.30 0.34 0.34 0.34 0.36 0.38 0.38 0.40 0.36 0.34 0.36
## [2017] 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30 0.32 0.32 0.32 0.34
## [2031] 0.34 0.34 0.36 0.36 0.28 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24
## [2045] 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.26 0.26 0.28 0.28 0.30 0.30
## [2059] 0.30 0.30 0.30 0.30 0.30 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.24 0.24
## [2073] 0.24 0.24 0.24 0.26 0.32 0.32 0.32 0.34 0.36 0.36 0.34 0.34 0.34 0.34
## [2087] 0.34 0.32 0.32 0.30 0.30 0.30 0.26 0.24 0.24 0.24 0.24 0.26 0.26 0.30
## [2101] 0.34 0.36 0.40 0.32 0.34 0.32 0.34 0.38 0.38 0.38 0.36 0.34 0.32 0.32
## [2115] 0.32 0.30 0.30 0.26 0.30 0.28 0.28 0.28 0.32 0.34 0.36 0.40 0.42 0.44
## [2129] 0.44 0.46 0.46 0.46 0.46 0.46 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.38
## [2143] 0.38 0.38 0.40 0.42 0.44 0.46 0.50 0.54 0.60 0.64 0.68 0.74 0.76 0.76
## [2157] 0.74 0.72 0.70 0.70 0.70 0.68 0.64 0.62 0.62 0.54 0.54 0.50 0.46 0.48
## [2171] 0.48 0.38 0.36 0.34 0.32 0.34 0.36 0.36 0.40 0.38 0.42 0.38 0.36 0.34
## [2185] 0.34 0.32 0.30 0.30 0.26 0.24 0.26 0.24 0.24 0.24 0.26 0.32 0.36 0.40
## [2199] 0.42 0.44 0.46 0.50 0.52 0.54 0.52 0.52 0.50 0.46 0.46 0.46 0.46 0.46
## [2213] 0.42 0.42 0.36 0.34 0.34 0.32 0.34 0.36 0.40 0.42 0.46 0.46 0.52 0.56
## [2227] 0.60 0.60 0.52 0.48 0.46 0.44 0.44 0.40 0.38 0.36 0.34 0.34 0.34 0.34
## [2241] 0.32 0.34 0.34 0.34 0.36 0.36 0.40 0.38 0.36 0.34 0.34 0.32 0.32 0.32
## [2255] 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.30 0.30 0.30 0.30 0.30 0.32
## [2269] 0.34 0.34 0.36 0.36 0.36 0.36 0.36 0.38 0.38 0.38 0.38 0.38 0.36 0.36
## [2283] 0.38 0.38 0.38 0.38 0.38 0.36 0.36 0.36 0.36 0.38 0.38 0.40 0.40 0.42
## [2297] 0.46 0.50 0.50 0.52 0.52 0.50 0.50 0.46 0.44 0.44 0.46 0.48 0.46 0.46
## [2311] 0.46 0.46 0.46 0.50 0.52 0.56 0.56 0.60 0.60 0.64 0.72 0.74 0.74 0.74
## [2325] 0.72 0.72 0.68 0.66 0.64 0.58 0.62 0.62 0.60 0.58 0.56 0.54 0.54 0.54
## [2339] 0.48 0.46 0.50 0.52 0.56 0.54 0.50 0.48 0.48 0.44 0.44 0.42 0.42 0.42
## [2353] 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.38 0.38 0.40 0.42
## [2367] 0.42 0.44 0.42 0.44 0.46 0.46 0.44 0.44 0.44 0.42 0.42 0.40 0.38 0.38
## [2381] 0.36 0.34 0.34 0.34 0.34 0.38 0.42 0.46 0.50 0.52 0.54 0.56 0.56 0.60
## [2395] 0.60 0.60 0.56 0.54 0.50 0.46 0.48 0.46 0.44 0.44 0.40 0.40 0.38 0.36
## [2409] 0.36 0.40 0.44 0.50 0.50 0.52 0.52 0.54 0.54 0.54 0.52 0.50 0.46 0.42
## [2423] 0.40 0.40 0.38 0.36 0.36 0.36 0.36 0.36 0.36 0.38 0.40 0.40 0.40 0.40
## [2437] 0.42 0.42 0.46 0.46 0.52 0.52 0.50 0.50 0.50 0.52 0.44 0.44 0.42 0.44
## [2451] 0.44 0.42 0.40 0.40 0.36 0.36 0.36 0.36 0.38 0.40 0.42 0.46 0.46 0.50
## [2465] 0.52 0.54 0.54 0.56 0.56 0.56 0.52 0.50 0.50 0.44 0.46 0.46 0.42 0.42
## [2479] 0.40 0.40 0.40 0.46 0.46 0.50 0.52 0.54 0.56 0.56 0.58 0.60 0.60 0.58
## [2493] 0.64 0.56 0.60 0.56 0.52 0.50 0.50 0.46 0.46 0.48 0.46 0.46 0.48 0.52
## [2507] 0.50 0.52 0.50 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.54 0.50 0.50 0.50
## [2521] 0.48 0.46 0.44 0.42 0.42 0.42 0.40 0.40 0.42 0.44 0.62 0.66 0.62 0.62
## [2535] 0.70 0.70 0.74 0.76 0.76 0.74 0.74 0.70 0.68 0.66 0.62 0.60 0.56 0.52
## [2549] 0.50 0.46 0.44 0.42 0.40 0.42 0.40 0.42 0.42 0.44 0.46 0.48 0.50 0.52
## [2563] 0.52 0.50 0.50 0.46 0.44 0.42 0.42 0.40 0.36 0.36 0.36 0.36 0.34 0.34
## [2577] 0.34 0.34 0.34 0.34 0.36 0.34 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.32
## [2591] 0.32 0.30 0.30 0.32 0.32 0.32 0.32 0.32 0.34 0.34 0.34 0.34 0.36 0.38
## [2605] 0.42 0.46 0.52 0.52 0.58 0.60 0.60 0.60 0.58 0.56 0.54 0.56 0.58 0.54
## [2619] 0.52 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.52 0.56 0.60 0.66 0.68 0.70
## [2633] 0.70 0.66 0.74 0.66 0.64 0.60 0.60 0.54 0.54 0.54 0.52 0.54 0.54 0.50
## [2647] 0.52 0.46 0.50 0.52 0.56 0.60 0.64 0.64 0.66 0.70 0.72 0.74 0.70 0.70
## [2661] 0.68 0.66 0.66 0.62 0.60 0.58 0.62 0.62 0.56 0.54 0.56 0.54 0.56 0.58
## [2675] 0.58 0.64 0.66 0.68 0.70 0.74 0.72 0.70 0.70 0.68 0.68 0.64 0.64 0.62
## [2689] 0.60 0.60 0.60 0.60 0.58 0.58 0.56 0.56 0.56 0.58 0.58 0.60 0.62 0.64
## [2703] 0.66 0.64 0.68 0.70 0.70 0.66 0.66 0.62 0.64 0.62 0.62 0.62 0.64 0.62
## [2717] 0.62 0.64 0.62 0.62 0.64 0.64 0.66 0.62 0.62 0.62 0.62 0.62 0.62 0.66
## [2731] 0.62 0.64 0.64 0.62 0.58 0.56 0.54 0.54 0.52 0.50 0.46 0.46 0.46 0.46
## [2745] 0.50 0.52 0.54 0.54 0.56 0.60 0.56 0.56 0.60 0.56 0.54 0.52 0.52 0.46
## [2759] 0.48 0.46 0.42 0.44 0.44 0.44 0.44 0.42 0.42 0.42 0.40 0.40 0.40 0.44
## [2773] 0.48 0.50 0.52 0.54 0.54 0.56 0.58 0.56 0.54 0.54 0.44 0.44 0.44 0.44
## [2787] 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.42 0.44 0.46 0.48 0.46 0.48 0.50
## [2801] 0.50 0.50 0.50 0.48 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.44 0.44
## [2815] 0.44 0.44 0.44 0.46 0.48 0.50 0.54 0.58 0.62 0.62 0.64 0.66 0.66 0.66
## [2829] 0.64 0.62 0.62 0.60 0.60 0.56 0.56 0.56 0.56 0.54 0.52 0.52 0.52 0.54
## [2843] 0.56 0.60 0.64 0.66 0.68 0.70 0.70 0.70 0.72 0.70 0.70 0.68 0.66 0.64
## [2857] 0.58 0.56 0.52 0.50 0.50 0.42 0.38 0.36 0.34 0.34 0.34 0.34 0.34 0.40
## [2871] 0.44 0.46 0.50 0.48 0.50 0.40 0.42 0.42 0.40 0.40 0.38 0.36 0.36 0.34
## [2885] 0.34 0.34 0.34 0.34 0.34 0.38 0.42 0.46 0.50 0.52 0.52 0.54 0.54 0.56
## [2899] 0.58 0.56 0.56 0.54 0.50 0.50 0.48 0.46 0.44 0.40 0.38 0.36 0.36 0.34
## [2913] 0.36 0.40 0.42 0.46 0.54 0.54 0.56 0.58 0.60 0.60 0.60 0.58 0.54 0.54
## [2927] 0.52 0.48 0.46 0.44 0.42 0.42 0.42 0.42 0.40 0.46 0.42 0.48 0.52 0.54
## [2941] 0.56 0.56 0.60 0.60 0.60 0.60 0.60 0.58 0.58 0.56 0.56 0.54 0.54 0.50
## [2955] 0.50 0.52 0.48 0.46 0.42 0.44 0.44 0.46 0.52 0.56 0.58 0.58 0.60 0.60
## [2969] 0.60 0.60 0.60 0.58 0.58 0.56 0.52 0.52 0.50 0.46 0.46 0.44 0.44 0.46
## [2983] 0.42 0.42 0.44 0.48 0.52 0.54 0.56 0.60 0.60 0.62 0.62 0.62 0.64 0.62
## [2997] 0.62 0.58 0.54 0.52 0.52 0.50 0.48 0.46 0.44 0.44 0.42 0.40 0.42 0.44
## [3011] 0.50 0.52 0.56 0.56 0.60 0.62 0.62 0.64 0.66 0.64 0.64 0.60 0.54 0.54
## [3025] 0.52 0.52 0.52 0.50 0.52 0.50 0.48 0.46 0.46 0.48 0.48 0.52 0.54 0.56
## [3039] 0.60 0.62 0.62 0.64 0.66 0.64 0.62 0.56 0.54 0.54 0.50 0.46 0.46 0.46
## [3053] 0.44 0.44 0.42 0.42 0.44 0.46 0.48 0.50 0.54 0.58 0.58 0.62 0.62 0.64
## [3067] 0.64 0.64 0.62 0.60 0.60 0.56 0.54 0.54 0.52 0.52 0.50 0.50 0.50 0.50
## [3081] 0.50 0.50 0.50 0.50 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52
## [3095] 0.52 0.52 0.52 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.48 0.50 0.52
## [3109] 0.52 0.52 0.52 0.52 0.54 0.54 0.54 0.56 0.56 0.54 0.54 0.54 0.54 0.52
## [3123] 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.54 0.58 0.58 0.60 0.62 0.62
## [3137] 0.64 0.66 0.64 0.56 0.56 0.56 0.54 0.54 0.56 0.54 0.52 0.52 0.50 0.50
## [3151] 0.50 0.50 0.52 0.52 0.56 0.60 0.62 0.64 0.66 0.68 0.68 0.72 0.60 0.58
## [3165] 0.58 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.56 0.54 0.54 0.52 0.52 0.52
## [3179] 0.52 0.54 0.54 0.56 0.56 0.56 0.62 0.62 0.62 0.62 0.60 0.60 0.58 0.54
## [3193] 0.54 0.54 0.54 0.54 0.52 0.52 0.52 0.52 0.52 0.54 0.56 0.56 0.54 0.54
## [3207] 0.56 0.56 0.58 0.60 0.60 0.60 0.60 0.56 0.56 0.52 0.52 0.52 0.52 0.50
## [3221] 0.50 0.48 0.48 0.48 0.50 0.50 0.52 0.54 0.54 0.54 0.58 0.58 0.54 0.56
## [3235] 0.58 0.56 0.60 0.58 0.54 0.54 0.50 0.48 0.46 0.46 0.44 0.44 0.44 0.44
## [3249] 0.46 0.50 0.54 0.54 0.56 0.58 0.60 0.60 0.62 0.60 0.60 0.62 0.60 0.58
## [3263] 0.58 0.56 0.54 0.52 0.52 0.52 0.52 0.48 0.46 0.46 0.50 0.54 0.56 0.60
## [3277] 0.62 0.64 0.66 0.70 0.72 0.72 0.72 0.72 0.70 0.68 0.62 0.62 0.60 0.58
## [3291] 0.54 0.52 0.52 0.50 0.50 0.50 0.52 0.54 0.60 0.62 0.64 0.70 0.72 0.66
## [3305] 0.62 0.66 0.68 0.70 0.66 0.66 0.64 0.62 0.60 0.58 0.56 0.56 0.56 0.54
## [3319] 0.54 0.54 0.54 0.56 0.60 0.60 0.66 0.68 0.68 0.74 0.72 0.72 0.72 0.72
## [3333] 0.70 0.70 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.58 0.60 0.58 0.58 0.64
## [3347] 0.62 0.64 0.70 0.74 0.76 0.78 0.78 0.74 0.66 0.70 0.70 0.66 0.66 0.64
## [3361] 0.62 0.66 0.60 0.58 0.56 0.54 0.54 0.56 0.56 0.62 0.64 0.68 0.70 0.74
## [3375] 0.74 0.74 0.74 0.76 0.74 0.74 0.72 0.70 0.70 0.66 0.66 0.64 0.64 0.64
## [3389] 0.62 0.60 0.60 0.60 0.60 0.62 0.66 0.72 0.70 0.74 0.78 0.82 0.82 0.82
## [3403] 0.80 0.80 0.80 0.78 0.72 0.72 0.70 0.70 0.68 0.70 0.68 0.66 0.64 0.64
## [3417] 0.64 0.64 0.66 0.70 0.72 0.74 0.76 0.74 0.76 0.78 0.76 0.74 0.72 0.60
## [3431] 0.62 0.60 0.60 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.58 0.60 0.62 0.64
## [3445] 0.70 0.70 0.72 0.72 0.74 0.74 0.76 0.74 0.72 0.70 0.70 0.66 0.66 0.64
## [3459] 0.64 0.62 0.62 0.62 0.60 0.60 0.62 0.62 0.62 0.62 0.66 0.66 0.70 0.72
## [3473] 0.74 0.74 0.74 0.74 0.72 0.72 0.70 0.68 0.66 0.66 0.64 0.64 0.64 0.64
## [3487] 0.62 0.62 0.64 0.64 0.66 0.72 0.80 0.82 0.86 0.86 0.88 0.88 0.88 0.86
## [3501] 0.86 0.52 0.76 0.74 0.72 0.70 0.70 0.68 0.66 0.64 0.66 0.66 0.68 0.74
## [3515] 0.78 0.80 0.82 0.86 0.86 0.90 0.90 0.90 0.90 0.84 0.84 0.78 0.78 0.76
## [3529] 0.74 0.72 0.70 0.70 0.70 0.68 0.68 0.66 0.68 0.70 0.72 0.74 0.76 0.82
## [3543] 0.86 0.90 0.90 0.90 0.86 0.86 0.82 0.74 0.74 0.74 0.74 0.74 0.74 0.72
## [3557] 0.70 0.66 0.66 0.66 0.66 0.70 0.70 0.72 0.72 0.74 0.76 0.80 0.78 0.80
## [3571] 0.80 0.76 0.74 0.72 0.70 0.66 0.64 0.62 0.62 0.60 0.56 0.54 0.52 0.52
## [3585] 0.54 0.54 0.56 0.58 0.62 0.64 0.66 0.68 0.70 0.70 0.72 0.72 0.70 0.68
## [3599] 0.64 0.64 0.62 0.58 0.56 0.54 0.54 0.52 0.52 0.50 0.54 0.56 0.60 0.62
## [3613] 0.64 0.66 0.70 0.74 0.74 0.74 0.72 0.72 0.74 0.70 0.70 0.66 0.64 0.64
## [3627] 0.64 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.64 0.64 0.66
## [3641] 0.70 0.70 0.72 0.74 0.70 0.68 0.66 0.64 0.64 0.62 0.62 0.60 0.58 0.58
## [3655] 0.56 0.56 0.58 0.62 0.64 0.70 0.72 0.74 0.76 0.78 0.78 0.80 0.76 0.78
## [3669] 0.76 0.72 0.68 0.66 0.66 0.64 0.64 0.62 0.60 0.60 0.58 0.58 0.60 0.64
## [3683] 0.68 0.72 0.76 0.76 0.80 0.80 0.80 0.82 0.80 0.80 0.78 0.76 0.74 0.72
## [3697] 0.70 0.68 0.66 0.66 0.64 0.64 0.62 0.62 0.64 0.66 0.76 0.76 0.82 0.84
## [3711] 0.88 0.90 0.92 0.92 0.92 0.92 0.90 0.82 0.80 0.80 0.76 0.76 0.74 0.74
## [3725] 0.72 0.72 0.72 0.70 0.72 0.72 0.76 0.84 0.86 0.90 0.92 0.90 0.92 0.94
## [3739] 0.92 0.90 0.88 0.84 0.80 0.76 0.74 0.74 0.70 0.70 0.68 0.68 0.66 0.66
## [3753] 0.66 0.72 0.74 0.76 0.78 0.82 0.84 0.84 0.86 0.84 0.82 0.82 0.80 0.78
## [3767] 0.76 0.76 0.72 0.72 0.70 0.70 0.70 0.68 0.68 0.68 0.70 0.72 0.74 0.74
## [3781] 0.74 0.76 0.80 0.80 0.82 0.82 0.80 0.74 0.72 0.70 0.68 0.66 0.66 0.66
## [3795] 0.66 0.64 0.64 0.64 0.62 0.62 0.62 0.64 0.70 0.72 0.76 0.78 0.82 0.78
## [3809] 0.82 0.80 0.68 0.68 0.70 0.70 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.60
## [3823] 0.56 0.54 0.54 0.56 0.58 0.62 0.64 0.66 0.66 0.70 0.70 0.70 0.70 0.70
## [3837] 0.70 0.66 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.58 0.54 0.54 0.54 0.56
## [3851] 0.58 0.62 0.62 0.64 0.64 0.64 0.64 0.64 0.68 0.64 0.62 0.64 0.60 0.60
## [3865] 0.58 0.56 0.56 0.54 0.52 0.50 0.50 0.48 0.50 0.54 0.58 0.60 0.64 0.68
## [3879] 0.70 0.74 0.74 0.76 0.76 0.74 0.72 0.70 0.66 0.64 0.62 0.62 0.60 0.58
## [3893] 0.60 0.56 0.56 0.60 0.58 0.56 0.60 0.62 0.66 0.68 0.72 0.72 0.72 0.70
## [3907] 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.56 0.56 0.56 0.56 0.54 0.54
## [3921] 0.56 0.60 0.60 0.70 0.70 0.68 0.70 0.74 0.76 0.78 0.76 0.76 0.76 0.64
## [3935] 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.62 0.64 0.66 0.70
## [3949] 0.72 0.72 0.74 0.76 0.80 0.82 0.76 0.76 0.74 0.74 0.72 0.72 0.72 0.72
## [3963] 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.66 0.68 0.68 0.70 0.72 0.74 0.74
## [3977] 0.74 0.74 0.76 0.74 0.74 0.72 0.70 0.68 0.66 0.66 0.66 0.64 0.64 0.64
## [3991] 0.62 0.62 0.60 0.56 0.56 0.60 0.60 0.60 0.62 0.64 0.66 0.66 0.70 0.70
## [4005] 0.70 0.66 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.60
## [4019] 0.62 0.62 0.64 0.66 0.70 0.74 0.76 0.78 0.80 0.78 0.76 0.74 0.74 0.72
## [4033] 0.70 0.70 0.66 0.66 0.66 0.66 0.64 0.64 0.66 0.70 0.72 0.72 0.74 0.74
## [4047] 0.80 0.82 0.82 0.82 0.82 0.80 0.80 0.80 0.74 0.74 0.72 0.72 0.72 0.72
## [4061] 0.72 0.72 0.70 0.72 0.72 0.72 0.74 0.74 0.76 0.76 0.76 0.76 0.76 0.76
## [4075] 0.74 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.64
## [4089] 0.66 0.66 0.70 0.74 0.80 0.80 0.80 0.80 0.78 0.82 0.80 0.76 0.76 0.74
## [4103] 0.72 0.70 0.70 0.68 0.68 0.66 0.66 0.64 0.64 0.62 0.64 0.64 0.70 0.72
## [4117] 0.72 0.74 0.74 0.74 0.74 0.74 0.76 0.74 0.72 0.72 0.70 0.68 0.68 0.66
## [4131] 0.64 0.64 0.62 0.60 0.60 0.60 0.60 0.62 0.64 0.66 0.72 0.72 0.74 0.74
## [4145] 0.74 0.74 0.74 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.64 0.62 0.62 0.62
## [4159] 0.60 0.62 0.62 0.64 0.66 0.72 0.72 0.70 0.72 0.72 0.72 0.74 0.74 0.74
## [4173] 0.74 0.72 0.70 0.70 0.68 0.68 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.68
## [4187] 0.70 0.74 0.80 0.82 0.84 0.84 0.86 0.86 0.86 0.82 0.80 0.74 0.74 0.74
## [4201] 0.70 0.70 0.68 0.68 0.66 0.66 0.66 0.64 0.66 0.70 0.70 0.72 0.74 0.76
## [4215] 0.76 0.80 0.80 0.82 0.82 0.82 0.80 0.76 0.74 0.72 0.70 0.68 0.66 0.64
## [4229] 0.62 0.60 0.58 0.58 0.60 0.62 0.68 0.70 0.72 0.74 0.76 0.76 0.78 0.78
## [4243] 0.80 0.78 0.76 0.74 0.74 0.72 0.70 0.66 0.66 0.66 0.62 0.64 0.62 0.60
## [4257] 0.62 0.66 0.70 0.74 0.76 0.80 0.80 0.80 0.82 0.82 0.82 0.82 0.80 0.78
## [4271] 0.72 0.70 0.70 0.68 0.68 0.66 0.64 0.64 0.62 0.58 0.62 0.64 0.68 0.74
## [4285] 0.80 0.80 0.82 0.82 0.84 0.86 0.88 0.84 0.82 0.80 0.76 0.74 0.72 0.72
## [4299] 0.70 0.70 0.70 0.68 0.68 0.62 0.62 0.64 0.68 0.70 0.74 0.76 0.80 0.80
## [4313] 0.82 0.84 0.84 0.80 0.80 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.64
## [4327] 0.66 0.64 0.64 0.66 0.70 0.72 0.76 0.76 0.78 0.78 0.80 0.82 0.82 0.80
## [4341] 0.80 0.78 0.76 0.74 0.74 0.72 0.70 0.70 0.66 0.66 0.66 0.66 0.66 0.70
## [4355] 0.72 0.74 0.76 0.78 0.80 0.82 0.80 0.82 0.82 0.82 0.80 0.80 0.76 0.78
## [4369] 0.76 0.74 0.72 0.70 0.70 0.70 0.68 0.68 0.70 0.72 0.72 0.70 0.70 0.72
## [4383] 0.74 0.74 0.76 0.76 0.76 0.78 0.76 0.74 0.72 0.70 0.70 0.68 0.66 0.66
## [4397] 0.66 0.64 0.64 0.64 0.64 0.66 0.70 0.74 0.78 0.82 0.84 0.86 0.86 0.86
## [4411] 0.86 0.86 0.84 0.82 0.76 0.74 0.74 0.72 0.74 0.72 0.70 0.68 0.70 0.68
## [4425] 0.70 0.70 0.72 0.76 0.74 0.74 0.76 0.78 0.80 0.80 0.68 0.66 0.66 0.66
## [4439] 0.66 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.64 0.66 0.70 0.74
## [4453] 0.76 0.78 0.80 0.82 0.84 0.84 0.82 0.84 0.82 0.76 0.76 0.74 0.72 0.72
## [4467] 0.70 0.70 0.68 0.66 0.66 0.66 0.64 0.70 0.72 0.74 0.76 0.78 0.82 0.80
## [4481] 0.82 0.84 0.84 0.84 0.82 0.80 0.76 0.74 0.74 0.72 0.70 0.70 0.70 0.68
## [4495] 0.66 0.66 0.68 0.70 0.74 0.78 0.80 0.82 0.84 0.86 0.86 0.86 0.86 0.86
## [4509] 0.86 0.84 0.72 0.70 0.72 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.74 0.76
## [4523] 0.76 0.78 0.82 0.82 0.86 0.86 0.90 0.90 0.86 0.88 0.86 0.84 0.82 0.82
## [4537] 0.80 0.78 0.76 0.76 0.76 0.74 0.74 0.74 0.74 0.76 0.80 0.82 0.82 0.84
## [4551] 0.84 0.82 0.82 0.64 0.66 0.70 0.72 0.70 0.70 0.70 0.68 0.66 0.66 0.66
## [4565] 0.64 0.62 0.62 0.60 0.60 0.62 0.64 0.68 0.70 0.72 0.74 0.76 0.74 0.76
## [4579] 0.76 0.74 0.72 0.72 0.70 0.66 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.60
## [4593] 0.60 0.64 0.64 0.68 0.66 0.70 0.70 0.70 0.70 0.72 0.72 0.74 0.72 0.70
## [4607] 0.70 0.66 0.66 0.64 0.62 0.60 0.60 0.60 0.60 0.58 0.60 0.62 0.66 0.70
## [4621] 0.72 0.74 0.76 0.76 0.74 0.76 0.76 0.76 0.76 0.74 0.72 0.70 0.70 0.68
## [4635] 0.66 0.64 0.64 0.64 0.62 0.64 0.62 0.64 0.68 0.72 0.74 0.76 0.76 0.80
## [4649] 0.80 0.82 0.80 0.80 0.80 0.76 0.76 0.74 0.72 0.70 0.70 0.70 0.66 0.66
## [4663] 0.64 0.64 0.64 0.68 0.70 0.74 0.76 0.80 0.80 0.82 0.82 0.84 0.84 0.84
## [4677] 0.82 0.80 0.78 0.74 0.76 0.74 0.74 0.74 0.72 0.72 0.72 0.70 0.72 0.74
## [4691] 0.76 0.80 0.82 0.82 0.84 0.86 0.86 0.88 0.90 0.80 0.80 0.76 0.74 0.74
## [4705] 0.74 0.72 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.70 0.74 0.74 0.76 0.80
## [4719] 0.82 0.84 0.86 0.86 0.84 0.84 0.84 0.82 0.80 0.80 0.78 0.76 0.76 0.74
## [4733] 0.74 0.74 0.72 0.72 0.72 0.74 0.76 0.80 0.82 0.86 0.88 0.88 0.90 0.90
## [4747] 0.92 0.92 0.90 0.86 0.84 0.82 0.82 0.80 0.82 0.80 0.78 0.78 0.76 0.74
## [4761] 0.76 0.80 0.84 0.86 0.90 0.90 0.94 0.94 0.96 0.94 0.90 0.88 0.90 0.86
## [4775] 0.84 0.82 0.84 0.80 0.82 0.82 0.82 0.78 0.76 0.76 0.80 0.80 0.84 0.84
## [4789] 0.86 0.90 0.92 0.94 0.92 0.94 0.94 0.94 0.92 0.82 0.82 0.82 0.80 0.80
## [4803] 0.80 0.78 0.80 0.80 0.78 0.78 0.80 0.80 0.82 0.82 0.86 0.84 0.90 0.86
## [4817] 0.86 0.90 0.90 0.90 0.88 0.86 0.84 0.80 0.78 0.76 0.76 0.76 0.74 0.74
## [4831] 0.72 0.72 0.72 0.74 0.76 0.80 0.82 0.84 0.84 0.74 0.72 0.70 0.70 0.72
## [4845] 0.74 0.74 0.72 0.70 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.66 0.68 0.72
## [4859] 0.74 0.76 0.80 0.82 0.84 0.84 0.86 0.88 0.86 0.86 0.84 0.82 0.80 0.78
## [4873] 0.76 0.76 0.78 0.78 0.76 0.72 0.72 0.70 0.70 0.72 0.74 0.76 0.78 0.80
## [4887] 0.82 0.84 0.84 0.86 0.86 0.84 0.82 0.80 0.76 0.74 0.72 0.74 0.70 0.72
## [4901] 0.72 0.72 0.70 0.70 0.70 0.72 0.74 0.78 0.80 0.84 0.84 0.86 0.84 0.86
## [4915] 0.86 0.84 0.84 0.82 0.80 0.78 0.76 0.76 0.74 0.74 0.74 0.74 0.72 0.72
## [4929] 0.72 0.74 0.76 0.86 0.90 0.92 0.96 0.94 0.96 0.96 0.96 0.96 0.92 0.90
## [4943] 0.86 0.82 0.80 0.78 0.76 0.76 0.76 0.74 0.72 0.72 0.72 0.76 0.78 0.82
## [4957] 0.82 0.84 0.84 0.88 0.90 0.90 0.90 0.90 0.88 0.74 0.82 0.80 0.78 0.76
## [4971] 0.76 0.74 0.74 0.74 0.72 0.72 0.74 0.74 0.76 0.80 0.84 0.86 0.90 0.90
## [4985] 0.90 0.92 0.92 0.92 0.86 0.80 0.80 0.78 0.74 0.74 0.72 0.72 0.70 0.70
## [4999] 0.66 0.66 0.66 0.74 0.80 0.82 0.86 0.88 0.90 0.90 0.92 0.90 0.90 0.76
## [5013] 0.78 0.74 0.72 0.70 0.70 0.68 0.66 0.66 0.68 0.66 0.66 0.66 0.68 0.72
## [5027] 0.74 0.78 0.82 0.84 0.86 0.86 0.90 0.90 0.90 0.90 0.86 0.86 0.82 0.80
## [5041] 0.78 0.80 0.80 0.78 0.78 0.76 0.76 0.76 0.72 0.74 0.74 0.74 0.74 0.74
## [5055] 0.76 0.76 0.76 0.70 0.68 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.68 0.66
## [5069] 0.66 0.66 0.68 0.68 0.68 0.68 0.70 0.72 0.72 0.74 0.76 0.76 0.80 0.80
## [5083] 0.76 0.76 0.70 0.70 0.70 0.70 0.68 0.66 0.66 0.64 0.66 0.64 0.64 0.64
## [5097] 0.64 0.66 0.70 0.72 0.74 0.76 0.76 0.78 0.78 0.76 0.80 0.78 0.76 0.74
## [5111] 0.72 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.66 0.64 0.64 0.68 0.68 0.74
## [5125] 0.74 0.78 0.80 0.80 0.82 0.84 0.74 0.74 0.72 0.72 0.72 0.70 0.70 0.70
## [5139] 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.72 0.76 0.80 0.82 0.90 0.90
## [5153] 0.86 0.72 0.72 0.74 0.72 0.72 0.72 0.72 0.70 0.70 0.70 0.68 0.66 0.66
## [5167] 0.66 0.70 0.70 0.72 0.74 0.76 0.80 0.82 0.82 0.84 0.82 0.84 0.86 0.86
## [5181] 0.84 0.82 0.80 0.76 0.76 0.74 0.72 0.72 0.72 0.72 0.70 0.70 0.72 0.72
## [5195] 0.74 0.78 0.80 0.80 0.82 0.84 0.86 0.86 0.86 0.80 0.80 0.80 0.80 0.78
## [5209] 0.78 0.76 0.74 0.72 0.72 0.70 0.70 0.68 0.68 0.70 0.74 0.76 0.80 0.80
## [5223] 0.82 0.82 0.84 0.86 0.86 0.84 0.82 0.80 0.76 0.76 0.74 0.74 0.70 0.70
## [5237] 0.66 0.64 0.64 0.64 0.62 0.66 0.70 0.72 0.74 0.76 0.78 0.76 0.80 0.80
## [5251] 0.80 0.80 0.78 0.74 0.72 0.70 0.70 0.66 0.64 0.64 0.62 0.62 0.62 0.60
## [5265] 0.62 0.64 0.68 0.72 0.74 0.76 0.80 0.78 0.80 0.80 0.82 0.82 0.76 0.74
## [5279] 0.72 0.70 0.68 0.68 0.68 0.68 0.68 0.66 0.64 0.64 0.64 0.66 0.70 0.70
## [5293] 0.72 0.74 0.66 0.74 0.74 0.68 0.70 0.72 0.70 0.68 0.68 0.68 0.68 0.66
## [5307] 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.66 0.66 0.70 0.70 0.70 0.72
## [5321] 0.74 0.74 0.74 0.76 0.74 0.72 0.70 0.60 0.60 0.60 0.60 0.60 0.60 0.60
## [5335] 0.60 0.60 0.60 0.60 0.64 0.64 0.68 0.72 0.74 0.78 0.74 0.74 0.74 0.74
## [5349] 0.70 0.70 0.66 0.66 0.66 0.64 0.64 0.64 0.62 0.62 0.64 0.62 0.62 0.64
## [5363] 0.70 0.68 0.70 0.74 0.76 0.76 0.76 0.80 0.80 0.76 0.76 0.74 0.74 0.72
## [5377] 0.70 0.66 0.66 0.64 0.66 0.64 0.64 0.64 0.62 0.66 0.70 0.74 0.76 0.78
## [5391] 0.80 0.80 0.82 0.80 0.82 0.80 0.76 0.76 0.74 0.72 0.70 0.70 0.68 0.66
## [5405] 0.66 0.66 0.64 0.64 0.64 0.64 0.66 0.72 0.74 0.76 0.80 0.80 0.82 0.80
## [5419] 0.80 0.80 0.76 0.66 0.66 0.68 0.70 0.70 0.68 0.64 0.64 0.64 0.64 0.64
## [5433] 0.64 0.66 0.66 0.70 0.72 0.72 0.74 0.76 0.78 0.80 0.80 0.76 0.76 0.62
## [5447] 0.62 0.62 0.60 0.60 0.60 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.66 0.70
## [5461] 0.72 0.72 0.74 0.76 0.80 0.80 0.80 0.80 0.76 0.74 0.74 0.72 0.70 0.70
## [5475] 0.70 0.68 0.68 0.66 0.68 0.66 0.66 0.68 0.70 0.72 0.74 0.80 0.82 0.80
## [5489] 0.70 0.70 0.72 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.68 0.68 0.70 0.70
## [5503] 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.72 0.74 0.74 0.74 0.74 0.74 0.74
## [5517] 0.72 0.70 0.66 0.64 0.64 0.62 0.60 0.58 0.56 0.56 0.54 0.54 0.54 0.60
## [5531] 0.62 0.66 0.70 0.70 0.70 0.72 0.72 0.72 0.72 0.72 0.72 0.66 0.64 0.62
## [5545] 0.62 0.62 0.62 0.60 0.58 0.58 0.56 0.56 0.56 0.60 0.62 0.64 0.70 0.72
## [5559] 0.74 0.76 0.76 0.76 0.76 0.76 0.74 0.74 0.72 0.70 0.70 0.68 0.68 0.68
## [5573] 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.72 0.74 0.70 0.70 0.70 0.72 0.74
## [5587] 0.72 0.72 0.72 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.60 0.62 0.62
## [5601] 0.62 0.64 0.66 0.70 0.74 0.76 0.76 0.78 0.80 0.80 0.78 0.74 0.74 0.72
## [5615] 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70
## [5629] 0.70 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.62 0.66 0.70 0.70 0.74
## [5643] 0.76 0.78 0.78 0.80 0.76 0.74 0.72 0.72 0.66 0.64 0.62 0.62 0.60 0.60
## [5657] 0.60 0.56 0.56 0.56 0.60 0.62 0.62 0.66 0.66 0.68 0.70 0.70 0.70 0.72
## [5671] 0.70 0.66 0.66 0.64 0.64 0.62 0.60 0.56 0.56 0.54 0.54 0.52 0.52 0.54
## [5685] 0.56 0.62 0.66 0.70 0.72 0.72 0.74 0.74 0.74 0.74 0.72 0.70 0.66 0.66
## [5699] 0.64 0.62 0.62 0.60 0.60 0.56 0.56 0.56 0.54 0.54 0.60 0.62 0.64 0.70
## [5713] 0.72 0.74 0.74 0.74 0.74 0.74 0.72 0.70 0.84 0.66 0.66 0.64 0.60 0.60
## [5727] 0.60 0.58 0.58 0.56 0.56 0.60 0.60 0.62 0.64 0.68 0.72 0.72 0.72 0.72
## [5741] 0.72 0.74 0.72 0.72 0.70 0.66 0.66 0.66 0.64 0.64 0.62 0.62 0.60 0.60
## [5755] 0.60 0.60 0.60 0.62 0.62 0.64 0.66 0.66 0.68 0.70 0.70 0.70 0.68 0.68
## [5769] 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.64
## [5783] 0.66 0.66 0.66 0.70 0.70 0.72 0.72 0.72 0.72 0.72 0.70 0.70 0.68 0.68
## [5797] 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.66 0.66 0.66 0.70 0.74 0.76
## [5811] 0.78 0.78 0.78 0.80 0.76 0.76 0.74 0.74 0.72 0.72 0.72 0.70 0.68 0.68
## [5825] 0.68 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.70 0.72 0.74 0.74 0.68 0.68
## [5839] 0.66 0.66 0.66 0.66 0.66 0.60 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [5853] 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [5867] 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.56 0.60 0.60 0.62 0.60 0.60 0.60
## [5881] 0.56 0.60 0.60 0.62 0.64 0.64 0.64 0.64 0.64 0.64 0.62 0.62 0.60 0.62
## [5895] 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.64 0.64 0.66 0.68 0.70 0.66 0.64
## [5909] 0.64 0.64 0.62 0.62 0.64 0.62 0.62 0.64 0.62 0.62 0.62 0.62 0.62 0.62
## [5923] 0.62 0.62 0.62 0.62 0.62 0.64 0.70 0.72 0.74 0.74 0.70 0.70 0.66 0.64
## [5937] 0.66 0.62 0.62 0.62 0.62 0.60 0.58 0.58 0.58 0.58 0.60 0.62 0.64 0.70
## [5951] 0.72 0.72 0.74 0.74 0.74 0.74 0.74 0.72 0.70 0.66 0.64 0.64 0.62 0.62
## [5965] 0.62 0.62 0.62 0.60 0.60 0.58 0.60 0.64 0.66 0.70 0.70 0.70 0.74 0.72
## [5979] 0.74 0.72 0.72 0.68 0.64 0.62 0.64 0.62 0.58 0.56 0.56 0.56 0.54 0.56
## [5993] 0.56 0.58 0.60 0.64 0.68 0.70 0.72 0.72 0.74 0.74 0.72 0.72 0.70 0.68
## [6007] 0.66 0.64 0.62 0.62 0.60 0.58 0.60 0.58 0.56 0.56 0.56 0.58 0.60 0.64
## [6021] 0.68 0.70 0.72 0.74 0.74 0.74 0.74 0.74 0.70 0.68 0.66 0.64 0.64 0.64
## [6035] 0.62 0.62 0.60 0.60 0.60 0.58 0.58 0.60 0.62 0.64 0.70 0.72 0.74 0.76
## [6049] 0.78 0.78 0.76 0.76 0.72 0.70 0.72 0.66 0.66 0.64 0.64 0.64 0.62 0.62
## [6063] 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.68 0.66 0.64 0.64 0.60 0.54 0.48
## [6077] 0.48 0.46 0.46 0.46 0.44 0.44 0.42 0.42 0.40 0.40 0.40 0.38 0.38 0.40
## [6091] 0.42 0.46 0.50 0.50 0.52 0.54 0.54 0.54 0.52 0.52 0.52 0.50 0.50 0.50
## [6105] 0.48 0.50 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.48 0.50 0.52
## [6119] 0.52 0.52 0.52 0.52 0.54 0.52 0.52 0.52 0.52 0.50 0.50 0.46 0.46 0.46
## [6133] 0.44 0.44 0.44 0.44 0.44 0.46 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.58
## [6147] 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.52 0.52 0.52 0.50 0.50 0.50
## [6161] 0.50 0.50 0.52 0.54 0.56 0.58 0.58 0.60 0.60 0.60 0.60 0.58 0.56 0.56
## [6175] 0.56 0.56 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [6189] 0.56 0.56 0.56 0.56 0.58 0.62 0.60 0.60 0.60 0.58 0.56 0.56 0.56 0.56
## [6203] 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.56 0.60 0.62 0.64 0.66
## [6217] 0.66 0.66 0.66 0.66 0.64 0.62 0.62 0.60 0.62 0.60 0.60 0.60 0.60 0.60
## [6231] 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.66 0.66 0.68 0.68 0.66
## [6245] 0.64 0.64 0.62 0.64 0.62 0.62 0.62 0.60 0.60 0.60 0.60 0.62 0.62 0.62
## [6259] 0.62 0.62 0.62 0.62 0.62 0.60 0.60 0.62 0.62 0.60 0.60 0.62 0.60 0.60
## [6273] 0.60 0.58 0.58 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.58 0.60 0.60 0.62
## [6287] 0.62 0.64 0.66 0.66 0.64 0.66 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60
## [6301] 0.60 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.66 0.68 0.66 0.66
## [6315] 0.70 0.68 0.68 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62
## [6329] 0.62 0.62 0.62 0.64 0.64 0.68 0.68 0.70 0.70 0.72 0.70 0.70 0.66 0.66
## [6343] 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.64 0.64
## [6357] 0.64 0.66 0.66 0.70 0.68 0.68 0.66 0.66 0.64 0.64 0.62 0.60 0.60 0.60
## [6371] 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.62 0.62 0.62 0.66 0.68 0.70
## [6385] 0.72 0.70 0.70 0.70 0.66 0.66 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60
## [6399] 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.62 0.66 0.64 0.68 0.68 0.68 0.66
## [6413] 0.64 0.62 0.60 0.58 0.56 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52
## [6427] 0.54 0.56 0.60 0.64 0.64 0.66 0.64 0.64 0.62 0.62 0.58 0.54 0.54 0.52
## [6441] 0.52 0.52 0.50 0.48 0.46 0.46 0.44 0.44 0.42 0.42 0.40 0.40 0.40 0.38
## [6455] 0.40 0.40 0.42 0.42 0.40 0.40 0.38 0.38 0.36 0.36 0.36 0.36 0.36 0.34
## [6469] 0.34 0.34 0.34 0.32 0.32 0.34 0.34 0.36 0.36 0.38 0.40 0.40 0.36 0.36
## [6483] 0.36 0.36 0.36 0.38 0.36 0.36 0.36 0.36 0.34 0.36 0.36 0.36 0.34 0.36
## [6497] 0.36 0.36 0.36 0.40 0.40 0.40 0.40 0.42 0.40 0.40 0.40 0.40 0.40 0.40
## [6511] 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.42 0.44 0.46
## [6525] 0.48 0.54 0.56 0.56 0.58 0.58 0.58 0.56 0.54 0.52 0.52 0.50 0.50 0.48
## [6539] 0.48 0.48 0.46 0.46 0.44 0.44 0.44 0.46 0.52 0.54 0.56 0.60 0.62 0.64
## [6553] 0.64 0.64 0.64 0.64 0.60 0.58 0.52 0.50 0.52 0.50 0.50 0.48 0.46 0.44
## [6567] 0.44 0.42 0.40 0.42 0.44 0.46 0.52 0.54 0.56 0.56 0.58 0.58 0.58 0.56
## [6581] 0.54 0.52 0.50 0.46 0.46 0.44 0.44 0.44 0.42 0.40 0.40 0.42 0.42 0.42
## [6595] 0.46 0.48 0.52 0.56 0.60 0.60 0.62 0.66 0.64 0.60 0.56 0.56 0.54 0.50
## [6609] 0.50 0.50 0.48 0.46 0.46 0.44 0.42 0.42 0.42 0.42 0.46 0.50 0.52 0.58
## [6623] 0.62 0.62 0.62 0.66 0.64 0.62 0.60 0.54 0.52 0.52 0.50 0.48 0.46 0.46
## [6637] 0.46 0.44 0.44 0.44 0.44 0.44 0.46 0.50 0.56 0.62 0.64 0.66 0.68 0.68
## [6651] 0.66 0.62 0.64 0.56 0.56 0.54 0.52 0.50 0.50 0.48 0.48 0.46 0.46 0.46
## [6665] 0.44 0.46 0.52 0.54 0.56 0.62 0.70 0.72 0.74 0.72 0.70 0.66 0.64 0.58
## [6679] 0.60 0.56 0.56 0.54 0.54 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.56 0.60
## [6693] 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.58 0.58 0.58 0.56 0.56
## [6707] 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [6721] 0.54 0.54 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [6735] 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.62 0.62 0.64 0.66 0.66 0.66 0.62
## [6749] 0.62 0.62 0.62 0.62 0.62 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.56 0.56
## [6763] 0.56 0.56 0.54 0.52 0.52 0.56 0.62 0.62 0.60 0.58 0.54 0.54 0.52 0.50
## [6777] 0.46 0.46 0.46 0.46 0.46 0.44 0.42 0.42 0.40 0.40 0.46 0.52 0.54 0.56
## [6791] 0.58 0.60 0.60 0.62 0.62 0.58 0.54 0.50 0.50 0.50 0.50 0.48 0.46 0.44
## [6805] 0.42 0.42 0.42 0.42 0.38 0.40 0.44 0.50 0.54 0.56 0.58 0.60 0.60 0.62
## [6819] 0.60 0.58 0.56 0.54 0.54 0.54 0.56 0.56 0.54 0.56 0.56 0.56 0.52 0.50
## [6833] 0.50 0.50 0.50 0.50 0.52 0.56 0.56 0.56 0.58 0.56 0.58 0.56 0.56 0.54
## [6847] 0.52 0.50 0.50 0.48 0.48 0.46 0.44 0.44 0.44 0.46 0.46 0.46 0.50 0.52
## [6861] 0.56 0.60 0.62 0.64 0.62 0.62 0.62 0.60 0.56 0.56 0.54 0.54 0.52 0.52
## [6875] 0.52 0.52 0.52 0.50 0.50 0.52 0.52 0.54 0.52 0.52 0.52 0.52 0.54 0.54
## [6889] 0.54 0.56 0.56 0.56 0.60 0.60 0.58 0.58 0.58 0.56 0.56 0.56 0.50 0.50
## [6903] 0.48 0.44 0.42 0.44 0.44 0.46 0.48 0.48 0.50 0.48 0.48 0.48 0.48 0.46
## [6917] 0.46 0.46 0.44 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.34 0.36 0.36 0.40
## [6931] 0.42 0.46 0.50 0.50 0.48 0.52 0.50 0.50 0.46 0.44 0.44 0.44 0.42 0.42
## [6945] 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.36 0.40 0.42 0.44 0.44 0.46
## [6959] 0.50 0.48 0.48 0.50 0.46 0.44 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.34
## [6973] 0.34 0.34 0.32 0.32 0.34 0.36 0.40 0.42 0.46 0.50 0.52 0.52 0.52 0.52
## [6987] 0.50 0.50 0.46 0.44 0.44 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.38 0.40
## [7001] 0.38 0.42 0.44 0.46 0.50 0.52 0.54 0.54 0.56 0.54 0.52 0.54 0.48 0.48
## [7015] 0.46 0.48 0.46 0.44 0.44 0.42 0.40 0.38 0.38 0.38 0.40 0.44 0.48 0.50
## [7029] 0.52 0.54 0.56 0.56 0.56 0.56 0.56 0.52 0.48 0.46 0.44 0.46 0.44 0.44
## [7043] 0.44 0.44 0.44 0.42 0.42 0.42 0.42 0.44 0.48 0.52 0.52 0.58 0.56 0.52
## [7057] 0.52 0.52 0.52 0.52 0.50 0.50 0.52 0.48 0.48 0.46 0.46 0.46 0.46 0.48
## [7071] 0.48 0.50 0.46 0.48 0.50 0.50 0.50 0.50 0.50 0.52 0.52 0.56 0.50 0.48
## [7085] 0.44 0.42 0.40 0.36 0.34 0.34 0.32 0.32 0.30 0.30 0.30 0.28 0.28 0.30
## [7099] 0.32 0.34 0.36 0.38 0.38 0.36 0.36 0.36 0.36 0.36 0.36 0.34 0.32 0.30
## [7113] 0.30 0.28 0.30 0.30 0.30 0.30 0.26 0.26 0.26 0.28 0.28 0.26 0.26 0.24
## [7127] 0.24 0.24 0.22 0.22 0.22 0.22 0.24 0.24 0.24 0.22 0.22 0.22 0.22 0.22
## [7141] 0.24 0.22 0.24 0.24 0.24 0.26 0.30 0.32 0.36 0.38 0.40 0.42 0.42 0.42
## [7155] 0.40 0.36 0.56 0.34 0.32 0.30 0.26 0.26 0.26 0.24 0.24 0.24 0.22 0.24
## [7169] 0.24 0.28 0.32 0.36 0.40 0.42 0.44 0.44 0.44 0.42 0.42 0.40 0.40 0.40
## [7183] 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.34 0.32 0.32 0.34 0.36 0.40 0.44
## [7197] 0.46 0.50 0.48 0.50 0.50 0.48 0.44 0.42 0.42 0.40 0.36 0.36 0.34 0.32
## [7211] 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.34 0.40 0.42 0.46 0.48 0.50
## [7225] 0.48 0.48 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32
## [7239] 0.32 0.34 0.32 0.34 0.36 0.40 0.44 0.50 0.52 0.52 0.52 0.52 0.48 0.46
## [7253] 0.44 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.38 0.38 0.40 0.40
## [7267] 0.42 0.42 0.44 0.48 0.44 0.44 0.46 0.46 0.42 0.42 0.40 0.36 0.34 0.34
## [7281] 0.32 0.32 0.32 0.30 0.30 0.28 0.26 0.26 0.26 0.28 0.30 0.32 0.36 0.36
## [7295] 0.40 0.42 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.30 0.28 0.28 0.26 0.26
## [7309] 0.24 0.24 0.24 0.26 0.26 0.28 0.30 0.36 0.42 0.44 0.46 0.46 0.48 0.46
## [7323] 0.44 0.42 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.30 0.28
## [7337] 0.28 0.30 0.34 0.36 0.42 0.46 0.54 0.56 0.56 0.52 0.50 0.46 0.46 0.40
## [7351] 0.38 0.36 0.36 0.34 0.32 0.32 0.32 0.30 0.30 0.30 0.30 0.32 0.36 0.42
## [7365] 0.46 0.52 0.54 0.56 0.58 0.56 0.52 0.48 0.46 0.40 0.40 0.36 0.36 0.36
## [7379] 0.34 0.32 0.32 0.32 0.30 0.30 0.30 0.32 0.34 0.40 0.46 0.50 0.50 0.52
## [7393] 0.52 0.52 0.46 0.44 0.44 0.44 0.40 0.40 0.38 0.40 0.40 0.38 0.38 0.38
## [7407] 0.36 0.36 0.38 0.40 0.42 0.44 0.46 0.42 0.36 0.36 0.36 0.36 0.36 0.36
## [7421] 0.36 0.36 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.30 0.30 0.28 0.28 0.30
## [7435] 0.32 0.32 0.34 0.34 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.32
## [7449] 0.30 0.24 0.24 0.24 0.24 0.20 0.22 0.22 0.22 0.26 0.30 0.34 0.38 0.44
## [7463] 0.48 0.50 0.52 0.52 0.50 0.42 0.42 0.42 0.42 0.42 0.40 0.40 0.36 0.36
## [7477] 0.36 0.36 0.34 0.34 0.34 0.34 0.40 0.44 0.46 0.52 0.52 0.54 0.50 0.54
## [7491] 0.52 0.52 0.50 0.50 0.48 0.48 0.46 0.46 0.46 0.46 0.44 0.44 0.44 0.44
## [7505] 0.44 0.46 0.48 0.50 0.54 0.56 0.60 0.62 0.64 0.62 0.62 0.56 0.60 0.60
## [7519] 0.60 0.58 0.56 0.56 0.56 0.56 0.54 0.56 0.54 0.56 0.54 0.54 0.56 0.56
## [7533] 0.56 0.54 0.54 0.54 0.54 0.52 0.50 0.50 0.50 0.48 0.50 0.46 0.46 0.46
## [7547] 0.46 0.46 0.46 0.44 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46
## [7561] 0.48 0.48 0.48 0.46 0.46 0.44 0.44 0.42 0.42 0.42 0.42 0.42 0.42 0.40
## [7575] 0.34 0.36 0.34 0.34 0.34 0.34 0.32 0.34 0.34 0.34 0.34 0.32 0.32 0.32
## [7589] 0.30 0.30 0.30 0.26 0.26 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.22
## [7603] 0.26 0.26 0.30 0.32 0.34 0.34 0.34 0.34 0.32 0.30 0.28 0.28 0.28 0.26
## [7617] 0.26 0.26 0.26 0.24 0.26 0.26 0.24 0.24 0.24 0.26 0.28 0.32 0.36 0.40
## [7631] 0.42 0.42 0.42 0.42 0.40 0.38 0.34 0.36 0.36 0.38 0.38 0.38 0.40 0.40
## [7645] 0.40 0.40 0.42 0.42 0.42 0.42 0.44 0.44 0.50 0.50 0.54 0.52 0.52 0.52
## [7659] 0.52 0.54 0.50 0.52 0.48 0.46 0.46 0.46 0.46 0.44 0.44 0.44 0.44 0.42
## [7673] 0.46 0.46 0.48 0.52 0.52 0.50 0.50 0.48 0.44 0.44 0.42 0.42 0.40 0.40
## [7687] 0.40 0.40 0.40 0.38 0.40 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.40 0.40
## [7701] 0.40 0.40 0.42 0.42 0.44 0.44 0.44 0.44 0.46 0.46 0.46 0.50 0.48 0.48
## [7715] 0.48 0.50 0.50 0.52 0.46 0.44 0.46 0.48 0.52 0.52 0.50 0.48 0.44 0.42
## [7729] 0.42 0.40 0.38 0.40 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.28 0.30
## [7743] 0.30 0.30 0.26 0.30 0.34 0.36 0.42 0.46 0.48 0.50 0.50 0.50 0.48 0.42
## [7757] 0.40 0.36 0.36 0.36 0.34 0.34 0.34 0.28 0.28 0.30 0.28 0.26 0.26 0.26
## [7771] 0.32 0.36 0.40 0.46 0.50 0.52 0.52 0.50 0.50 0.46 0.42 0.40 0.36 0.34
## [7785] 0.34 0.34 0.32 0.30 0.30 0.30 0.30 0.30 0.26 0.32 0.34 0.36 0.40 0.44
## [7799] 0.48 0.48 0.50 0.46 0.46 0.42 0.40 0.42 0.38 0.38 0.36 0.36 0.36 0.34
## [7813] 0.34 0.34 0.36 0.38 0.38 0.40 0.46 0.46 0.50 0.54 0.54 0.62 0.62 0.56
## [7827] 0.54 0.50 0.48 0.50 0.48 0.48 0.48 0.46 0.46 0.44 0.44 0.40 0.42 0.42
## [7841] 0.42 0.44 0.48 0.52 0.56 0.58 0.60 0.58 0.56 0.58 0.56 0.54 0.54 0.54
## [7855] 0.52 0.52 0.52 0.52 0.50 0.50 0.52 0.50 0.52 0.52 0.54 0.56 0.56 0.50
## [7869] 0.42 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.38 0.36 0.36 0.34
## [7883] 0.32 0.32 0.30 0.28 0.26 0.26 0.28 0.30 0.34 0.38 0.36 0.38 0.38 0.36
## [7897] 0.36 0.34 0.34 0.32 0.32 0.32 0.30 0.28 0.28 0.26 0.26 0.26 0.26 0.26
## [7911] 0.24 0.24 0.26 0.30 0.32 0.34 0.36 0.40 0.40 0.40 0.40 0.36 0.36 0.34
## [7925] 0.34 0.30 0.30 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.24 0.28
## [7939] 0.30 0.34 0.36 0.40 0.42 0.42 0.42 0.42 0.40 0.34 0.38 0.34 0.32 0.32
## [7953] 0.30 0.26 0.26 0.24 0.22 0.24 0.24 0.22 0.24 0.26 0.32 0.32 0.36 0.36
## [7967] 0.36 0.38 0.38 0.36 0.34 0.30 0.30 0.30 0.32 0.30 0.30 0.30 0.26 0.28
## [7981] 0.26 0.26 0.24 0.26 0.26 0.30 0.32 0.34 0.36 0.40 0.42 0.42 0.42 0.38
## [7995] 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.32 0.32 0.30 0.30 0.30 0.32
## [8009] 0.32 0.36 0.36 0.36 0.40 0.42 0.46 0.46 0.50 0.42 0.44 0.46 0.46 0.44
## [8023] 0.44 0.46 0.50 0.46 0.46 0.46 0.46 0.46 0.44 0.46 0.46 0.46 0.46 0.46
## [8037] 0.46 0.46 0.48 0.50 0.46 0.46 0.46 0.46 0.46 0.44 0.46 0.46 0.44 0.46
## [8051] 0.46 0.46 0.46 0.46 0.48 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.42
## [8065] 0.42 0.40 0.40 0.34 0.34 0.30 0.24 0.24 0.26 0.26 0.28 0.26 0.24 0.22
## [8079] 0.22 0.22 0.22 0.24 0.26 0.28 0.28 0.30 0.32 0.32 0.32 0.30 0.28 0.26
## [8093] 0.26 0.28 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.24 0.26
## [8107] 0.30 0.32 0.36 0.38 0.38 0.38 0.36 0.34 0.34 0.34 0.30 0.30 0.28 0.28
## [8121] 0.26 0.26 0.28 0.28 0.26 0.26 0.24 0.24 0.26 0.28 0.32 0.32 0.32 0.34
## [8135] 0.34 0.34 0.32 0.28 0.26 0.26 0.24 0.22 0.22 0.20 0.20 0.16 0.18 0.16
## [8149] 0.16 0.18 0.16 0.18 0.16 0.20 0.24 0.26 0.26 0.30 0.30 0.30 0.30 0.28
## [8163] 0.24 0.22 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.18 0.18 0.16 0.16 0.14
## [8177] 0.16 0.18 0.22 0.26 0.28 0.30 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.26
## [8191] 0.26 0.24 0.22 0.20 0.20 0.18 0.20 0.18 0.20 0.16 0.20 0.26 0.30 0.34
## [8205] 0.36 0.40 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.30 0.30 0.26 0.26 0.26
## [8219] 0.26 0.28 0.26 0.26 0.26 0.28 0.26 0.30 0.32 0.34 0.36 0.36 0.38 0.38
## [8233] 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.32
## [8247] 0.32 0.32 0.34 0.36 0.40 0.40 0.46 0.50 0.52 0.52 0.46 0.52 0.52 0.52
## [8261] 0.52 0.50 0.52 0.52 0.50 0.50 0.48 0.46 0.50 0.48 0.44 0.38 0.36 0.36
## [8275] 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30
## [8289] 0.30 0.30 0.28 0.26 0.24 0.26 0.26 0.26 0.26 0.26 0.26 0.28 0.28 0.28
## [8303] 0.28 0.28 0.28 0.26 0.26 0.24 0.22 0.20 0.20 0.20 0.20 0.20 0.22 0.22
## [8317] 0.22 0.20 0.20 0.20 0.20 0.22 0.24 0.24 0.26 0.30 0.30 0.32 0.28 0.28
## [8331] 0.28 0.26 0.24 0.22 0.22 0.20 0.20 0.18 0.18 0.16 0.16 0.14 0.16 0.18
## [8345] 0.20 0.22 0.24 0.26 0.30 0.34 0.36 0.38 0.40 0.38 0.36 0.36 0.40 0.36
## [8359] 0.36 0.36 0.36 0.36 0.34 0.34 0.36 0.36 0.36 0.36 0.42 0.36 0.42 0.40
## [8373] 0.44 0.44 0.44 0.44 0.44 0.40 0.38 0.38 0.36 0.36 0.36 0.38 0.34 0.36
## [8387] 0.36 0.36 0.36 0.38 0.36 0.36 0.36 0.40 0.48 0.48 0.46 0.44 0.48 0.48
## [8401] 0.44 0.44 0.44 0.50 0.50 0.50 0.50 0.50 0.50 0.44 0.48 0.44 0.38 0.38
## [8415] 0.36 0.34 0.36 0.38 0.40 0.44 0.48 0.46 0.46 0.48 0.46 0.44 0.44 0.44
## [8429] 0.42 0.42 0.36 0.40 0.40 0.40 0.38 0.38 0.38 0.36 0.38 0.38 0.40 0.40
## [8443] 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.36 0.34 0.32 0.32 0.32 0.32
## [8457] 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.30 0.32 0.34 0.34
## [8471] 0.34 0.32 0.32 0.28 0.30 0.30 0.26 0.26 0.24 0.24 0.24 0.22 0.22 0.22
## [8485] 0.20 0.20 0.22 0.20 0.24 0.26 0.30 0.30 0.32 0.34 0.34 0.36 0.34 0.32
## [8499] 0.32 0.32 0.30 0.28 0.26 0.22 0.28 0.34 0.34 0.34 0.32 0.32 0.32 0.34
## [8513] 0.34 0.34 0.36 0.38 0.38 0.38 0.36 0.34 0.32 0.30 0.30 0.26 0.26 0.26
## [8527] 0.26 0.26 0.26 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.32 0.32 0.30 0.30
## [8541] 0.42 0.42 0.44 0.40 0.38 0.34 0.32 0.32 0.32 0.30 0.32 0.32 0.32 0.32
## [8555] 0.32 0.32 0.32 0.32 0.32 0.34 0.36 0.34 0.34 0.32 0.32 0.30 0.28 0.26
## [8569] 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.18 0.20
## [8583] 0.20 0.22 0.24 0.26 0.28 0.30 0.30 0.30 0.30 0.30 0.30 0.28 0.28 0.28
## [8597] 0.30 0.28 0.26 0.24 0.24 0.24 0.22 0.24 0.24 0.24 0.26 0.30 0.32 0.32
## [8611] 0.36 0.40 0.42 0.42 0.38 0.36 0.34 0.34 0.36 0.34 0.36 0.38 0.40 0.40
## [8625] 0.40 0.38 0.36 0.40 0.38 0.34 0.38 0.40 0.42 0.52 0.50 0.46 0.46 0.44
## [8639] 0.42 0.42 0.42 0.42 0.40 0.38 0.36</code></pre>
<p>However, if we want to access variables within the dataset without
needing to index them we can use the base R <code>attach()</code> function.</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="demonstration-2-poisson-versus-linear-regression.html#cb196-1" tabindex="-1"></a><span class="fu">attach</span>(Bikeshare)</span></code></pre></div>
<p>So now, we can call on the variables from the dataset directly (if we do
not have other datasets loaded in the R environment with variables named
in the same way).</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="demonstration-2-poisson-versus-linear-regression.html#cb197-1" tabindex="-1"></a>temp</span></code></pre></div>
<pre><code>##    [1] 0.24 0.22 0.22 0.24 0.24 0.24 0.22 0.20 0.24 0.32 0.38 0.36 0.42 0.46
##   [15] 0.46 0.44 0.42 0.44 0.42 0.42 0.40 0.40 0.40 0.46 0.46 0.44 0.42 0.46
##   [29] 0.46 0.42 0.40 0.40 0.38 0.36 0.36 0.36 0.36 0.36 0.34 0.34 0.34 0.36
##   [43] 0.32 0.30 0.26 0.24 0.22 0.22 0.20 0.16 0.16 0.14 0.14 0.14 0.16 0.18
##   [57] 0.20 0.22 0.24 0.26 0.26 0.26 0.24 0.24 0.20 0.20 0.18 0.14 0.18 0.16
##   [71] 0.16 0.14 0.14 0.12 0.12 0.12 0.14 0.16 0.16 0.22 0.22 0.24 0.26 0.28
##   [85] 0.30 0.28 0.26 0.24 0.24 0.22 0.22 0.20 0.20 0.16 0.16 0.24 0.22 0.20
##   [99] 0.18 0.20 0.22 0.22 0.26 0.26 0.28 0.30 0.30 0.30 0.24 0.24 0.24 0.22
##  [113] 0.20 0.18 0.20 0.18 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.18 0.20 0.22
##  [127] 0.26 0.26 0.28 0.28 0.26 0.22 0.22 0.22 0.20 0.22 0.22 0.20 0.20 0.20
##  [141] 0.20 0.20 0.22 0.20 0.20 0.20 0.20 0.22 0.20 0.20 0.20 0.20 0.20 0.20
##  [155] 0.20 0.20 0.16 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.16 0.16
##  [169] 0.16 0.16 0.16 0.18 0.20 0.20 0.20 0.20 0.20 0.18 0.16 0.14 0.14 0.12
##  [183] 0.12 0.12 0.10 0.10 0.10 0.10 0.10 0.08 0.08 0.10 0.08 0.10 0.12 0.14
##  [197] 0.16 0.18 0.20 0.22 0.22 0.20 0.18 0.16 0.16 0.14 0.14 0.14 0.12 0.12
##  [211] 0.12 0.12 0.12 0.10 0.10 0.12 0.12 0.12 0.14 0.14 0.16 0.20 0.20 0.20
##  [225] 0.20 0.20 0.20 0.20 0.16 0.16 0.14 0.14 0.14 0.14 0.14 0.16 0.16 0.16
##  [239] 0.16 0.18 0.18 0.20 0.20 0.20 0.20 0.20 0.16 0.16 0.16 0.16 0.16 0.16
##  [253] 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.12 0.14 0.16 0.16 0.18 0.20 0.20
##  [267] 0.22 0.20 0.20 0.22 0.20 0.20 0.18 0.16 0.16 0.16 0.14 0.14 0.14 0.14
##  [281] 0.14 0.14 0.14 0.12 0.12 0.14 0.14 0.16 0.20 0.20 0.22 0.22 0.24 0.24
##  [295] 0.20 0.20 0.16 0.16 0.14 0.14 0.12 0.12 0.10 0.10 0.10 0.10 0.10 0.10
##  [309] 0.12 0.14 0.18 0.18 0.20 0.22 0.22 0.24 0.22 0.22 0.20 0.16 0.18 0.16
##  [323] 0.16 0.18 0.18 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.14 0.16 0.18 0.20
##  [337] 0.24 0.28 0.30 0.32 0.34 0.32 0.30 0.32 0.32 0.32 0.30 0.30 0.26 0.26
##  [351] 0.26 0.22 0.26 0.26 0.26 0.24 0.22 0.22 0.22 0.24 0.24 0.26 0.28 0.26
##  [365] 0.24 0.22 0.20 0.18 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.18 0.18 0.18
##  [379] 0.18 0.16 0.16 0.16 0.16 0.16 0.18 0.18 0.18 0.20 0.20 0.20 0.18 0.18
##  [393] 0.16 0.16 0.14 0.16 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22
##  [407] 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.24 0.24 0.24 0.26 0.28 0.30
##  [421] 0.40 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.32 0.30 0.30 0.26 0.26 0.26
##  [435] 0.26 0.26 0.24 0.22 0.22 0.22 0.24 0.26 0.28 0.30 0.28 0.30 0.32 0.30
##  [449] 0.30 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.22 0.22 0.24 0.22
##  [463] 0.20 0.20 0.20 0.20 0.22 0.22 0.20 0.20 0.16 0.16 0.14 0.12 0.12 0.10
##  [477] 0.08 0.06 0.06 0.04 0.04 0.04 0.04 0.02 0.02 0.02 0.02 0.04 0.04 0.06
##  [491] 0.06 0.08 0.10 0.12 0.12 0.12 0.08 0.08 0.06 0.06 0.06 0.04 0.04 0.04
##  [505] 0.02 0.02 0.04 0.04 0.08 0.06 0.10 0.14 0.14 0.16 0.14 0.16 0.16 0.16
##  [519] 0.14 0.12 0.12 0.10 0.10 0.08 0.06 0.06 0.04 0.04 0.02 0.02 0.02 0.02
##  [533] 0.04 0.06 0.10 0.10 0.12 0.14 0.14 0.16 0.16 0.14 0.14 0.14 0.14 0.14
##  [547] 0.14 0.16 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.16 0.20 0.22 0.24 0.26
##  [561] 0.26 0.30 0.32 0.32 0.30 0.30 0.26 0.24 0.24 0.22 0.22 0.22 0.24 0.22
##  [575] 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.20 0.22
##  [589] 0.22 0.20 0.20 0.18 0.18 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.18 0.16
##  [603] 0.16 0.18 0.18 0.18 0.18 0.18 0.22 0.20 0.22 0.24 0.24 0.24 0.24 0.22
##  [617] 0.24 0.24 0.22 0.22 0.22 0.20 0.16 0.16 0.16 0.18 0.18 0.18 0.18 0.20
##  [631] 0.22 0.22 0.22 0.24 0.24 0.22 0.22 0.18 0.18 0.16 0.16 0.16 0.14 0.16
##  [645] 0.14 0.14 0.14 0.14 0.14 0.16 0.18 0.22 0.30 0.28 0.28 0.30 0.30 0.30
##  [659] 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.18 0.16 0.16
##  [673] 0.16 0.16 0.16 0.16 0.18 0.16 0.18 0.16 0.16 0.16 0.16 0.30 0.16 0.16
##  [687] 0.16 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.16 0.16 0.18 0.20
##  [701] 0.20 0.22 0.24 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.22 0.22 0.22
##  [715] 0.22 0.22 0.22 0.22 0.22 0.22 0.24 0.22 0.24 0.24 0.34 0.38 0.38 0.36
##  [729] 0.36 0.34 0.28 0.24 0.22 0.22 0.20 0.20 0.20 0.18 0.18 0.16 0.16 0.14
##  [743] 0.14 0.16 0.18 0.18 0.20 0.20 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.18
##  [757] 0.18 0.20 0.20 0.16 0.14 0.14 0.14 0.16 0.14 0.14 0.16 0.20 0.22 0.24
##  [771] 0.26 0.28 0.28 0.30 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24
##  [785] 0.24 0.22 0.20 0.20 0.22 0.20 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22
##  [799] 0.24 0.28 0.28 0.30 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26
##  [813] 0.24 0.24 0.28 0.30 0.32 0.34 0.34 0.34 0.34 0.34 0.34 0.30 0.28 0.28
##  [827] 0.26 0.26 0.24 0.24 0.22 0.20 0.20 0.20 0.20 0.18 0.18 0.16 0.22 0.24
##  [841] 0.30 0.32 0.36 0.36 0.38 0.36 0.32 0.34 0.32 0.32 0.32 0.28 0.30 0.28
##  [855] 0.28 0.26 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.22 0.22 0.24 0.24 0.22
##  [869] 0.22 0.22 0.22 0.20 0.16 0.16 0.14 0.12 0.12 0.10 0.10 0.08 0.06 0.06
##  [883] 0.06 0.06 0.10 0.12 0.14 0.14 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.14
##  [897] 0.14 0.14 0.16 0.16 0.14 0.14 0.14 0.14 0.12 0.12 0.10 0.10 0.12 0.12
##  [911] 0.14 0.16 0.18 0.20 0.20 0.20 0.18 0.16 0.14 0.14 0.14 0.12 0.12 0.10
##  [925] 0.10 0.10 0.08 0.10 0.08 0.10 0.12 0.14 0.22 0.22 0.24 0.30 0.32 0.30
##  [939] 0.30 0.28 0.26 0.22 0.20 0.20 0.18 0.16 0.14 0.14 0.12 0.12 0.12 0.12
##  [953] 0.12 0.14 0.16 0.22 0.30 0.30 0.30 0.34 0.34 0.34 0.32 0.28 0.28 0.26
##  [967] 0.26 0.24 0.22 0.20 0.20 0.20 0.20 0.20 0.20 0.22 0.22 0.24 0.30 0.32
##  [981] 0.36 0.38 0.40 0.40 0.42 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38
##  [995] 0.36 0.34 0.32 0.32 0.34 0.34 0.38 0.40 0.44 0.52 0.56 0.58 0.60 0.56
## [1009] 0.52 0.46 0.40 0.38 0.36 0.36 0.34 0.32 0.30 0.30 0.28 0.22 0.22 0.20
## [1023] 0.20 0.20 0.22 0.24 0.26 0.28 0.32 0.34 0.34 0.34 0.32 0.30 0.28 0.26
## [1037] 0.24 0.24 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.20 0.22 0.24 0.26 0.34
## [1051] 0.38 0.42 0.46 0.46 0.46 0.46 0.40 0.34 0.38 0.36 0.34 0.38 0.34 0.34
## [1065] 0.34 0.34 0.32 0.32 0.30 0.32 0.32 0.36 0.38 0.44 0.48 0.54 0.60 0.60
## [1079] 0.56 0.58 0.54 0.48 0.48 0.52 0.50 0.46 0.44 0.44 0.44 0.46 0.46 0.46
## [1093] 0.44 0.42 0.42 0.42 0.44 0.44 0.50 0.60 0.66 0.66 0.66 0.66 0.64 0.62
## [1107] 0.60 0.58 0.54 0.52 0.48 0.46 0.44 0.42 0.40 0.40 0.40 0.38 0.38 0.40
## [1121] 0.42 0.44 0.44 0.44 0.46 0.44 0.44 0.42 0.36 0.34 0.32 0.32 0.30 0.28
## [1135] 0.26 0.24 0.24 0.22 0.22 0.20 0.18 0.20 0.22 0.26 0.30 0.30 0.34 0.36
## [1149] 0.36 0.36 0.34 0.34 0.34 0.34 0.32 0.32 0.30 0.34 0.34 0.34 0.34 0.32
## [1163] 0.34 0.42 0.42 0.32 0.32 0.32 0.32 0.32 0.30 0.32 0.30 0.28 0.28 0.24
## [1177] 0.24 0.24 0.22 0.20 0.20 0.12 0.12 0.12 0.14 0.16 0.16 0.20 0.22 0.22
## [1191] 0.24 0.22 0.22 0.22 0.20 0.20 0.20 0.16 0.16 0.14 0.14 0.12 0.12 0.12
## [1205] 0.12 0.12 0.14 0.18 0.20 0.24 0.26 0.30 0.32 0.34 0.34 0.34 0.32 0.30
## [1219] 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.24 0.24 0.26
## [1233] 0.32 0.36 0.38 0.40 0.40 0.38 0.36 0.34 0.34 0.34 0.34 0.34 0.32 0.32
## [1247] 0.32 0.32 0.32 0.32 0.34 0.34 0.36 0.34 0.42 0.52 0.54 0.54 0.56 0.46
## [1261] 0.32 0.32 0.32 0.30 0.30 0.28 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22
## [1275] 0.22 0.22 0.24 0.26 0.30 0.30 0.32 0.34 0.34 0.36 0.36 0.36 0.34 0.32
## [1289] 0.30 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.24 0.24 0.24 0.26 0.28 0.30
## [1303] 0.36 0.40 0.42 0.44 0.46 0.48 0.42 0.40 0.40 0.40 0.38 0.38 0.36 0.36
## [1317] 0.34 0.32 0.34 0.34 0.36 0.34 0.42 0.52 0.56 0.56 0.46 0.42 0.42 0.42
## [1331] 0.40 0.46 0.44 0.44 0.38 0.34 0.32 0.30 0.26 0.24 0.22 0.22 0.20 0.20
## [1345] 0.20 0.20 0.22 0.24 0.28 0.30 0.32 0.32 0.34 0.34 0.34 0.32 0.30 0.30
## [1359] 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.20 0.22 0.22 0.22 0.24 0.28 0.32
## [1373] 0.34 0.40 0.50 0.52 0.54 0.54 0.50 0.46 0.40 0.36 0.34 0.30 0.26 0.24
## [1387] 0.24 0.20 0.20 0.16 0.14 0.14 0.12 0.14 0.16 0.18 0.20 0.22 0.22 0.24
## [1401] 0.24 0.26 0.26 0.24 0.20 0.20 0.18 0.20 0.18 0.20 0.18 0.18 0.18 0.18
## [1415] 0.16 0.16 0.16 0.18 0.22 0.24 0.28 0.32 0.34 0.36 0.36 0.36 0.36 0.34
## [1429] 0.32 0.30 0.30 0.30 0.30 0.28 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30
## [1443] 0.32 0.34 0.40 0.44 0.46 0.48 0.46 0.48 0.48 0.48 0.46 0.44 0.44 0.42
## [1457] 0.44 0.42 0.42 0.40 0.42 0.42 0.42 0.42 0.40 0.42 0.42 0.42 0.46 0.46
## [1471] 0.44 0.44 0.36 0.34 0.32 0.30 0.28 0.24 0.22 0.22 0.20 0.20 0.20 0.20
## [1485] 0.20 0.20 0.20 0.20 0.22 0.24 0.26 0.30 0.32 0.32 0.34 0.34 0.34 0.32
## [1499] 0.30 0.30 0.28 0.26 0.28 0.26 0.24 0.24 0.24 0.22 0.20 0.20 0.18 0.22
## [1513] 0.26 0.30 0.36 0.36 0.38 0.38 0.36 0.38 0.36 0.34 0.34 0.32 0.30 0.30
## [1527] 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.26 0.30 0.32 0.32
## [1541] 0.32 0.34 0.36 0.34 0.34 0.34 0.34 0.32 0.32 0.32 0.34 0.34 0.34 0.34
## [1555] 0.36 0.36 0.38 0.38 0.40 0.40 0.40 0.42 0.42 0.44 0.44 0.42 0.44 0.44
## [1569] 0.44 0.36 0.36 0.34 0.34 0.34 0.34 0.34 0.32 0.30 0.26 0.26 0.28 0.30
## [1583] 0.32 0.32 0.34 0.36 0.36 0.34 0.34 0.34 0.32 0.30 0.30 0.30 0.30 0.30
## [1597] 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.24 0.26 0.28 0.32 0.34 0.34 0.36
## [1611] 0.40 0.42 0.46 0.46 0.42 0.42 0.40 0.38 0.36 0.38 0.38 0.36 0.34 0.34
## [1625] 0.36 0.34 0.36 0.40 0.40 0.42 0.44 0.46 0.46 0.46 0.48 0.46 0.44 0.40
## [1639] 0.36 0.32 0.30 0.30 0.26 0.26 0.26 0.26 0.26 0.24 0.26 0.28 0.30 0.32
## [1653] 0.34 0.36 0.38 0.38 0.38 0.38 0.40 0.38 0.36 0.36 0.34 0.34 0.32 0.32
## [1667] 0.32 0.30 0.30 0.24 0.24 0.22 0.24 0.26 0.30 0.32 0.34 0.36 0.36 0.38
## [1681] 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.30
## [1695] 0.30 0.30 0.32 0.32 0.36 0.36 0.40 0.40 0.40 0.40 0.40 0.44 0.44 0.44
## [1709] 0.42 0.42 0.40 0.40 0.38 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.36 0.40
## [1723] 0.44 0.44 0.50 0.52 0.50 0.52 0.50 0.50 0.46 0.44 0.42 0.42 0.40 0.42
## [1737] 0.42 0.40 0.40 0.36 0.38 0.40 0.40 0.42 0.46 0.52 0.54 0.56 0.64 0.66
## [1751] 0.68 0.68 0.70 0.68 0.66 0.62 0.62 0.62 0.60 0.60 0.58 0.56 0.54 0.52
## [1765] 0.52 0.44 0.40 0.42 0.42 0.44 0.46 0.46 0.50 0.50 0.50 0.50 0.48 0.46
## [1779] 0.44 0.42 0.40 0.40 0.38 0.34 0.32 0.30 0.28 0.26 0.26 0.26 0.24 0.28
## [1793] 0.30 0.32 0.34 0.36 0.38 0.40 0.40 0.42 0.40 0.38 0.36 0.36 0.34 0.34
## [1807] 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.30 0.30 0.34 0.38 0.42 0.44 0.50
## [1821] 0.54 0.56 0.54 0.54 0.52 0.58 0.56 0.46 0.46 0.46 0.46 0.42 0.44 0.44
## [1835] 0.42 0.40 0.40 0.40 0.40 0.40 0.44 0.44 0.46 0.50 0.50 0.50 0.50 0.50
## [1849] 0.48 0.44 0.44 0.42 0.40 0.40 0.36 0.34 0.34 0.34 0.32 0.34 0.32 0.32
## [1863] 0.32 0.34 0.34 0.34 0.34 0.36 0.38 0.40 0.40 0.38 0.38 0.36 0.32 0.32
## [1877] 0.32 0.32 0.30 0.30 0.28 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.30 0.30
## [1891] 0.32 0.30 0.30 0.30 0.30 0.30 0.30 0.28 0.26 0.26 0.24 0.24 0.20 0.20
## [1905] 0.20 0.18 0.18 0.18 0.20 0.22 0.24 0.26 0.30 0.32 0.32 0.36 0.34 0.34
## [1919] 0.32 0.30 0.30 0.30 0.30 0.28 0.26 0.26 0.26 0.22 0.20 0.22 0.20 0.18
## [1933] 0.20 0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.34 0.34 0.32 0.32 0.28 0.28
## [1947] 0.26 0.28 0.26 0.26 0.24 0.22 0.20 0.18 0.16 0.16 0.20 0.22 0.22 0.24
## [1961] 0.26 0.30 0.32 0.32 0.34 0.32 0.30 0.30 0.28 0.26 0.26 0.26 0.22 0.22
## [1975] 0.22 0.22 0.18 0.18 0.20 0.20 0.22 0.24 0.26 0.26 0.30 0.32 0.32 0.34
## [1989] 0.34 0.32 0.30 0.32 0.32 0.30 0.28 0.26 0.24 0.24 0.24 0.20 0.22 0.22
## [2003] 0.22 0.24 0.28 0.30 0.34 0.34 0.34 0.36 0.38 0.38 0.40 0.36 0.34 0.36
## [2017] 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30 0.32 0.32 0.32 0.34
## [2031] 0.34 0.34 0.36 0.36 0.28 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24
## [2045] 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.26 0.26 0.28 0.28 0.30 0.30
## [2059] 0.30 0.30 0.30 0.30 0.30 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.24 0.24
## [2073] 0.24 0.24 0.24 0.26 0.32 0.32 0.32 0.34 0.36 0.36 0.34 0.34 0.34 0.34
## [2087] 0.34 0.32 0.32 0.30 0.30 0.30 0.26 0.24 0.24 0.24 0.24 0.26 0.26 0.30
## [2101] 0.34 0.36 0.40 0.32 0.34 0.32 0.34 0.38 0.38 0.38 0.36 0.34 0.32 0.32
## [2115] 0.32 0.30 0.30 0.26 0.30 0.28 0.28 0.28 0.32 0.34 0.36 0.40 0.42 0.44
## [2129] 0.44 0.46 0.46 0.46 0.46 0.46 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.38
## [2143] 0.38 0.38 0.40 0.42 0.44 0.46 0.50 0.54 0.60 0.64 0.68 0.74 0.76 0.76
## [2157] 0.74 0.72 0.70 0.70 0.70 0.68 0.64 0.62 0.62 0.54 0.54 0.50 0.46 0.48
## [2171] 0.48 0.38 0.36 0.34 0.32 0.34 0.36 0.36 0.40 0.38 0.42 0.38 0.36 0.34
## [2185] 0.34 0.32 0.30 0.30 0.26 0.24 0.26 0.24 0.24 0.24 0.26 0.32 0.36 0.40
## [2199] 0.42 0.44 0.46 0.50 0.52 0.54 0.52 0.52 0.50 0.46 0.46 0.46 0.46 0.46
## [2213] 0.42 0.42 0.36 0.34 0.34 0.32 0.34 0.36 0.40 0.42 0.46 0.46 0.52 0.56
## [2227] 0.60 0.60 0.52 0.48 0.46 0.44 0.44 0.40 0.38 0.36 0.34 0.34 0.34 0.34
## [2241] 0.32 0.34 0.34 0.34 0.36 0.36 0.40 0.38 0.36 0.34 0.34 0.32 0.32 0.32
## [2255] 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.30 0.30 0.30 0.30 0.30 0.32
## [2269] 0.34 0.34 0.36 0.36 0.36 0.36 0.36 0.38 0.38 0.38 0.38 0.38 0.36 0.36
## [2283] 0.38 0.38 0.38 0.38 0.38 0.36 0.36 0.36 0.36 0.38 0.38 0.40 0.40 0.42
## [2297] 0.46 0.50 0.50 0.52 0.52 0.50 0.50 0.46 0.44 0.44 0.46 0.48 0.46 0.46
## [2311] 0.46 0.46 0.46 0.50 0.52 0.56 0.56 0.60 0.60 0.64 0.72 0.74 0.74 0.74
## [2325] 0.72 0.72 0.68 0.66 0.64 0.58 0.62 0.62 0.60 0.58 0.56 0.54 0.54 0.54
## [2339] 0.48 0.46 0.50 0.52 0.56 0.54 0.50 0.48 0.48 0.44 0.44 0.42 0.42 0.42
## [2353] 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.38 0.38 0.40 0.42
## [2367] 0.42 0.44 0.42 0.44 0.46 0.46 0.44 0.44 0.44 0.42 0.42 0.40 0.38 0.38
## [2381] 0.36 0.34 0.34 0.34 0.34 0.38 0.42 0.46 0.50 0.52 0.54 0.56 0.56 0.60
## [2395] 0.60 0.60 0.56 0.54 0.50 0.46 0.48 0.46 0.44 0.44 0.40 0.40 0.38 0.36
## [2409] 0.36 0.40 0.44 0.50 0.50 0.52 0.52 0.54 0.54 0.54 0.52 0.50 0.46 0.42
## [2423] 0.40 0.40 0.38 0.36 0.36 0.36 0.36 0.36 0.36 0.38 0.40 0.40 0.40 0.40
## [2437] 0.42 0.42 0.46 0.46 0.52 0.52 0.50 0.50 0.50 0.52 0.44 0.44 0.42 0.44
## [2451] 0.44 0.42 0.40 0.40 0.36 0.36 0.36 0.36 0.38 0.40 0.42 0.46 0.46 0.50
## [2465] 0.52 0.54 0.54 0.56 0.56 0.56 0.52 0.50 0.50 0.44 0.46 0.46 0.42 0.42
## [2479] 0.40 0.40 0.40 0.46 0.46 0.50 0.52 0.54 0.56 0.56 0.58 0.60 0.60 0.58
## [2493] 0.64 0.56 0.60 0.56 0.52 0.50 0.50 0.46 0.46 0.48 0.46 0.46 0.48 0.52
## [2507] 0.50 0.52 0.50 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.54 0.50 0.50 0.50
## [2521] 0.48 0.46 0.44 0.42 0.42 0.42 0.40 0.40 0.42 0.44 0.62 0.66 0.62 0.62
## [2535] 0.70 0.70 0.74 0.76 0.76 0.74 0.74 0.70 0.68 0.66 0.62 0.60 0.56 0.52
## [2549] 0.50 0.46 0.44 0.42 0.40 0.42 0.40 0.42 0.42 0.44 0.46 0.48 0.50 0.52
## [2563] 0.52 0.50 0.50 0.46 0.44 0.42 0.42 0.40 0.36 0.36 0.36 0.36 0.34 0.34
## [2577] 0.34 0.34 0.34 0.34 0.36 0.34 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.32
## [2591] 0.32 0.30 0.30 0.32 0.32 0.32 0.32 0.32 0.34 0.34 0.34 0.34 0.36 0.38
## [2605] 0.42 0.46 0.52 0.52 0.58 0.60 0.60 0.60 0.58 0.56 0.54 0.56 0.58 0.54
## [2619] 0.52 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.52 0.56 0.60 0.66 0.68 0.70
## [2633] 0.70 0.66 0.74 0.66 0.64 0.60 0.60 0.54 0.54 0.54 0.52 0.54 0.54 0.50
## [2647] 0.52 0.46 0.50 0.52 0.56 0.60 0.64 0.64 0.66 0.70 0.72 0.74 0.70 0.70
## [2661] 0.68 0.66 0.66 0.62 0.60 0.58 0.62 0.62 0.56 0.54 0.56 0.54 0.56 0.58
## [2675] 0.58 0.64 0.66 0.68 0.70 0.74 0.72 0.70 0.70 0.68 0.68 0.64 0.64 0.62
## [2689] 0.60 0.60 0.60 0.60 0.58 0.58 0.56 0.56 0.56 0.58 0.58 0.60 0.62 0.64
## [2703] 0.66 0.64 0.68 0.70 0.70 0.66 0.66 0.62 0.64 0.62 0.62 0.62 0.64 0.62
## [2717] 0.62 0.64 0.62 0.62 0.64 0.64 0.66 0.62 0.62 0.62 0.62 0.62 0.62 0.66
## [2731] 0.62 0.64 0.64 0.62 0.58 0.56 0.54 0.54 0.52 0.50 0.46 0.46 0.46 0.46
## [2745] 0.50 0.52 0.54 0.54 0.56 0.60 0.56 0.56 0.60 0.56 0.54 0.52 0.52 0.46
## [2759] 0.48 0.46 0.42 0.44 0.44 0.44 0.44 0.42 0.42 0.42 0.40 0.40 0.40 0.44
## [2773] 0.48 0.50 0.52 0.54 0.54 0.56 0.58 0.56 0.54 0.54 0.44 0.44 0.44 0.44
## [2787] 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.42 0.44 0.46 0.48 0.46 0.48 0.50
## [2801] 0.50 0.50 0.50 0.48 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.44 0.44
## [2815] 0.44 0.44 0.44 0.46 0.48 0.50 0.54 0.58 0.62 0.62 0.64 0.66 0.66 0.66
## [2829] 0.64 0.62 0.62 0.60 0.60 0.56 0.56 0.56 0.56 0.54 0.52 0.52 0.52 0.54
## [2843] 0.56 0.60 0.64 0.66 0.68 0.70 0.70 0.70 0.72 0.70 0.70 0.68 0.66 0.64
## [2857] 0.58 0.56 0.52 0.50 0.50 0.42 0.38 0.36 0.34 0.34 0.34 0.34 0.34 0.40
## [2871] 0.44 0.46 0.50 0.48 0.50 0.40 0.42 0.42 0.40 0.40 0.38 0.36 0.36 0.34
## [2885] 0.34 0.34 0.34 0.34 0.34 0.38 0.42 0.46 0.50 0.52 0.52 0.54 0.54 0.56
## [2899] 0.58 0.56 0.56 0.54 0.50 0.50 0.48 0.46 0.44 0.40 0.38 0.36 0.36 0.34
## [2913] 0.36 0.40 0.42 0.46 0.54 0.54 0.56 0.58 0.60 0.60 0.60 0.58 0.54 0.54
## [2927] 0.52 0.48 0.46 0.44 0.42 0.42 0.42 0.42 0.40 0.46 0.42 0.48 0.52 0.54
## [2941] 0.56 0.56 0.60 0.60 0.60 0.60 0.60 0.58 0.58 0.56 0.56 0.54 0.54 0.50
## [2955] 0.50 0.52 0.48 0.46 0.42 0.44 0.44 0.46 0.52 0.56 0.58 0.58 0.60 0.60
## [2969] 0.60 0.60 0.60 0.58 0.58 0.56 0.52 0.52 0.50 0.46 0.46 0.44 0.44 0.46
## [2983] 0.42 0.42 0.44 0.48 0.52 0.54 0.56 0.60 0.60 0.62 0.62 0.62 0.64 0.62
## [2997] 0.62 0.58 0.54 0.52 0.52 0.50 0.48 0.46 0.44 0.44 0.42 0.40 0.42 0.44
## [3011] 0.50 0.52 0.56 0.56 0.60 0.62 0.62 0.64 0.66 0.64 0.64 0.60 0.54 0.54
## [3025] 0.52 0.52 0.52 0.50 0.52 0.50 0.48 0.46 0.46 0.48 0.48 0.52 0.54 0.56
## [3039] 0.60 0.62 0.62 0.64 0.66 0.64 0.62 0.56 0.54 0.54 0.50 0.46 0.46 0.46
## [3053] 0.44 0.44 0.42 0.42 0.44 0.46 0.48 0.50 0.54 0.58 0.58 0.62 0.62 0.64
## [3067] 0.64 0.64 0.62 0.60 0.60 0.56 0.54 0.54 0.52 0.52 0.50 0.50 0.50 0.50
## [3081] 0.50 0.50 0.50 0.50 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52
## [3095] 0.52 0.52 0.52 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.48 0.50 0.52
## [3109] 0.52 0.52 0.52 0.52 0.54 0.54 0.54 0.56 0.56 0.54 0.54 0.54 0.54 0.52
## [3123] 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.54 0.58 0.58 0.60 0.62 0.62
## [3137] 0.64 0.66 0.64 0.56 0.56 0.56 0.54 0.54 0.56 0.54 0.52 0.52 0.50 0.50
## [3151] 0.50 0.50 0.52 0.52 0.56 0.60 0.62 0.64 0.66 0.68 0.68 0.72 0.60 0.58
## [3165] 0.58 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.56 0.54 0.54 0.52 0.52 0.52
## [3179] 0.52 0.54 0.54 0.56 0.56 0.56 0.62 0.62 0.62 0.62 0.60 0.60 0.58 0.54
## [3193] 0.54 0.54 0.54 0.54 0.52 0.52 0.52 0.52 0.52 0.54 0.56 0.56 0.54 0.54
## [3207] 0.56 0.56 0.58 0.60 0.60 0.60 0.60 0.56 0.56 0.52 0.52 0.52 0.52 0.50
## [3221] 0.50 0.48 0.48 0.48 0.50 0.50 0.52 0.54 0.54 0.54 0.58 0.58 0.54 0.56
## [3235] 0.58 0.56 0.60 0.58 0.54 0.54 0.50 0.48 0.46 0.46 0.44 0.44 0.44 0.44
## [3249] 0.46 0.50 0.54 0.54 0.56 0.58 0.60 0.60 0.62 0.60 0.60 0.62 0.60 0.58
## [3263] 0.58 0.56 0.54 0.52 0.52 0.52 0.52 0.48 0.46 0.46 0.50 0.54 0.56 0.60
## [3277] 0.62 0.64 0.66 0.70 0.72 0.72 0.72 0.72 0.70 0.68 0.62 0.62 0.60 0.58
## [3291] 0.54 0.52 0.52 0.50 0.50 0.50 0.52 0.54 0.60 0.62 0.64 0.70 0.72 0.66
## [3305] 0.62 0.66 0.68 0.70 0.66 0.66 0.64 0.62 0.60 0.58 0.56 0.56 0.56 0.54
## [3319] 0.54 0.54 0.54 0.56 0.60 0.60 0.66 0.68 0.68 0.74 0.72 0.72 0.72 0.72
## [3333] 0.70 0.70 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.58 0.60 0.58 0.58 0.64
## [3347] 0.62 0.64 0.70 0.74 0.76 0.78 0.78 0.74 0.66 0.70 0.70 0.66 0.66 0.64
## [3361] 0.62 0.66 0.60 0.58 0.56 0.54 0.54 0.56 0.56 0.62 0.64 0.68 0.70 0.74
## [3375] 0.74 0.74 0.74 0.76 0.74 0.74 0.72 0.70 0.70 0.66 0.66 0.64 0.64 0.64
## [3389] 0.62 0.60 0.60 0.60 0.60 0.62 0.66 0.72 0.70 0.74 0.78 0.82 0.82 0.82
## [3403] 0.80 0.80 0.80 0.78 0.72 0.72 0.70 0.70 0.68 0.70 0.68 0.66 0.64 0.64
## [3417] 0.64 0.64 0.66 0.70 0.72 0.74 0.76 0.74 0.76 0.78 0.76 0.74 0.72 0.60
## [3431] 0.62 0.60 0.60 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.58 0.60 0.62 0.64
## [3445] 0.70 0.70 0.72 0.72 0.74 0.74 0.76 0.74 0.72 0.70 0.70 0.66 0.66 0.64
## [3459] 0.64 0.62 0.62 0.62 0.60 0.60 0.62 0.62 0.62 0.62 0.66 0.66 0.70 0.72
## [3473] 0.74 0.74 0.74 0.74 0.72 0.72 0.70 0.68 0.66 0.66 0.64 0.64 0.64 0.64
## [3487] 0.62 0.62 0.64 0.64 0.66 0.72 0.80 0.82 0.86 0.86 0.88 0.88 0.88 0.86
## [3501] 0.86 0.52 0.76 0.74 0.72 0.70 0.70 0.68 0.66 0.64 0.66 0.66 0.68 0.74
## [3515] 0.78 0.80 0.82 0.86 0.86 0.90 0.90 0.90 0.90 0.84 0.84 0.78 0.78 0.76
## [3529] 0.74 0.72 0.70 0.70 0.70 0.68 0.68 0.66 0.68 0.70 0.72 0.74 0.76 0.82
## [3543] 0.86 0.90 0.90 0.90 0.86 0.86 0.82 0.74 0.74 0.74 0.74 0.74 0.74 0.72
## [3557] 0.70 0.66 0.66 0.66 0.66 0.70 0.70 0.72 0.72 0.74 0.76 0.80 0.78 0.80
## [3571] 0.80 0.76 0.74 0.72 0.70 0.66 0.64 0.62 0.62 0.60 0.56 0.54 0.52 0.52
## [3585] 0.54 0.54 0.56 0.58 0.62 0.64 0.66 0.68 0.70 0.70 0.72 0.72 0.70 0.68
## [3599] 0.64 0.64 0.62 0.58 0.56 0.54 0.54 0.52 0.52 0.50 0.54 0.56 0.60 0.62
## [3613] 0.64 0.66 0.70 0.74 0.74 0.74 0.72 0.72 0.74 0.70 0.70 0.66 0.64 0.64
## [3627] 0.64 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.64 0.64 0.66
## [3641] 0.70 0.70 0.72 0.74 0.70 0.68 0.66 0.64 0.64 0.62 0.62 0.60 0.58 0.58
## [3655] 0.56 0.56 0.58 0.62 0.64 0.70 0.72 0.74 0.76 0.78 0.78 0.80 0.76 0.78
## [3669] 0.76 0.72 0.68 0.66 0.66 0.64 0.64 0.62 0.60 0.60 0.58 0.58 0.60 0.64
## [3683] 0.68 0.72 0.76 0.76 0.80 0.80 0.80 0.82 0.80 0.80 0.78 0.76 0.74 0.72
## [3697] 0.70 0.68 0.66 0.66 0.64 0.64 0.62 0.62 0.64 0.66 0.76 0.76 0.82 0.84
## [3711] 0.88 0.90 0.92 0.92 0.92 0.92 0.90 0.82 0.80 0.80 0.76 0.76 0.74 0.74
## [3725] 0.72 0.72 0.72 0.70 0.72 0.72 0.76 0.84 0.86 0.90 0.92 0.90 0.92 0.94
## [3739] 0.92 0.90 0.88 0.84 0.80 0.76 0.74 0.74 0.70 0.70 0.68 0.68 0.66 0.66
## [3753] 0.66 0.72 0.74 0.76 0.78 0.82 0.84 0.84 0.86 0.84 0.82 0.82 0.80 0.78
## [3767] 0.76 0.76 0.72 0.72 0.70 0.70 0.70 0.68 0.68 0.68 0.70 0.72 0.74 0.74
## [3781] 0.74 0.76 0.80 0.80 0.82 0.82 0.80 0.74 0.72 0.70 0.68 0.66 0.66 0.66
## [3795] 0.66 0.64 0.64 0.64 0.62 0.62 0.62 0.64 0.70 0.72 0.76 0.78 0.82 0.78
## [3809] 0.82 0.80 0.68 0.68 0.70 0.70 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.60
## [3823] 0.56 0.54 0.54 0.56 0.58 0.62 0.64 0.66 0.66 0.70 0.70 0.70 0.70 0.70
## [3837] 0.70 0.66 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.58 0.54 0.54 0.54 0.56
## [3851] 0.58 0.62 0.62 0.64 0.64 0.64 0.64 0.64 0.68 0.64 0.62 0.64 0.60 0.60
## [3865] 0.58 0.56 0.56 0.54 0.52 0.50 0.50 0.48 0.50 0.54 0.58 0.60 0.64 0.68
## [3879] 0.70 0.74 0.74 0.76 0.76 0.74 0.72 0.70 0.66 0.64 0.62 0.62 0.60 0.58
## [3893] 0.60 0.56 0.56 0.60 0.58 0.56 0.60 0.62 0.66 0.68 0.72 0.72 0.72 0.70
## [3907] 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.56 0.56 0.56 0.56 0.54 0.54
## [3921] 0.56 0.60 0.60 0.70 0.70 0.68 0.70 0.74 0.76 0.78 0.76 0.76 0.76 0.64
## [3935] 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.62 0.64 0.66 0.70
## [3949] 0.72 0.72 0.74 0.76 0.80 0.82 0.76 0.76 0.74 0.74 0.72 0.72 0.72 0.72
## [3963] 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.66 0.68 0.68 0.70 0.72 0.74 0.74
## [3977] 0.74 0.74 0.76 0.74 0.74 0.72 0.70 0.68 0.66 0.66 0.66 0.64 0.64 0.64
## [3991] 0.62 0.62 0.60 0.56 0.56 0.60 0.60 0.60 0.62 0.64 0.66 0.66 0.70 0.70
## [4005] 0.70 0.66 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.60
## [4019] 0.62 0.62 0.64 0.66 0.70 0.74 0.76 0.78 0.80 0.78 0.76 0.74 0.74 0.72
## [4033] 0.70 0.70 0.66 0.66 0.66 0.66 0.64 0.64 0.66 0.70 0.72 0.72 0.74 0.74
## [4047] 0.80 0.82 0.82 0.82 0.82 0.80 0.80 0.80 0.74 0.74 0.72 0.72 0.72 0.72
## [4061] 0.72 0.72 0.70 0.72 0.72 0.72 0.74 0.74 0.76 0.76 0.76 0.76 0.76 0.76
## [4075] 0.74 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.64
## [4089] 0.66 0.66 0.70 0.74 0.80 0.80 0.80 0.80 0.78 0.82 0.80 0.76 0.76 0.74
## [4103] 0.72 0.70 0.70 0.68 0.68 0.66 0.66 0.64 0.64 0.62 0.64 0.64 0.70 0.72
## [4117] 0.72 0.74 0.74 0.74 0.74 0.74 0.76 0.74 0.72 0.72 0.70 0.68 0.68 0.66
## [4131] 0.64 0.64 0.62 0.60 0.60 0.60 0.60 0.62 0.64 0.66 0.72 0.72 0.74 0.74
## [4145] 0.74 0.74 0.74 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.64 0.62 0.62 0.62
## [4159] 0.60 0.62 0.62 0.64 0.66 0.72 0.72 0.70 0.72 0.72 0.72 0.74 0.74 0.74
## [4173] 0.74 0.72 0.70 0.70 0.68 0.68 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.68
## [4187] 0.70 0.74 0.80 0.82 0.84 0.84 0.86 0.86 0.86 0.82 0.80 0.74 0.74 0.74
## [4201] 0.70 0.70 0.68 0.68 0.66 0.66 0.66 0.64 0.66 0.70 0.70 0.72 0.74 0.76
## [4215] 0.76 0.80 0.80 0.82 0.82 0.82 0.80 0.76 0.74 0.72 0.70 0.68 0.66 0.64
## [4229] 0.62 0.60 0.58 0.58 0.60 0.62 0.68 0.70 0.72 0.74 0.76 0.76 0.78 0.78
## [4243] 0.80 0.78 0.76 0.74 0.74 0.72 0.70 0.66 0.66 0.66 0.62 0.64 0.62 0.60
## [4257] 0.62 0.66 0.70 0.74 0.76 0.80 0.80 0.80 0.82 0.82 0.82 0.82 0.80 0.78
## [4271] 0.72 0.70 0.70 0.68 0.68 0.66 0.64 0.64 0.62 0.58 0.62 0.64 0.68 0.74
## [4285] 0.80 0.80 0.82 0.82 0.84 0.86 0.88 0.84 0.82 0.80 0.76 0.74 0.72 0.72
## [4299] 0.70 0.70 0.70 0.68 0.68 0.62 0.62 0.64 0.68 0.70 0.74 0.76 0.80 0.80
## [4313] 0.82 0.84 0.84 0.80 0.80 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.64
## [4327] 0.66 0.64 0.64 0.66 0.70 0.72 0.76 0.76 0.78 0.78 0.80 0.82 0.82 0.80
## [4341] 0.80 0.78 0.76 0.74 0.74 0.72 0.70 0.70 0.66 0.66 0.66 0.66 0.66 0.70
## [4355] 0.72 0.74 0.76 0.78 0.80 0.82 0.80 0.82 0.82 0.82 0.80 0.80 0.76 0.78
## [4369] 0.76 0.74 0.72 0.70 0.70 0.70 0.68 0.68 0.70 0.72 0.72 0.70 0.70 0.72
## [4383] 0.74 0.74 0.76 0.76 0.76 0.78 0.76 0.74 0.72 0.70 0.70 0.68 0.66 0.66
## [4397] 0.66 0.64 0.64 0.64 0.64 0.66 0.70 0.74 0.78 0.82 0.84 0.86 0.86 0.86
## [4411] 0.86 0.86 0.84 0.82 0.76 0.74 0.74 0.72 0.74 0.72 0.70 0.68 0.70 0.68
## [4425] 0.70 0.70 0.72 0.76 0.74 0.74 0.76 0.78 0.80 0.80 0.68 0.66 0.66 0.66
## [4439] 0.66 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.64 0.66 0.70 0.74
## [4453] 0.76 0.78 0.80 0.82 0.84 0.84 0.82 0.84 0.82 0.76 0.76 0.74 0.72 0.72
## [4467] 0.70 0.70 0.68 0.66 0.66 0.66 0.64 0.70 0.72 0.74 0.76 0.78 0.82 0.80
## [4481] 0.82 0.84 0.84 0.84 0.82 0.80 0.76 0.74 0.74 0.72 0.70 0.70 0.70 0.68
## [4495] 0.66 0.66 0.68 0.70 0.74 0.78 0.80 0.82 0.84 0.86 0.86 0.86 0.86 0.86
## [4509] 0.86 0.84 0.72 0.70 0.72 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.74 0.76
## [4523] 0.76 0.78 0.82 0.82 0.86 0.86 0.90 0.90 0.86 0.88 0.86 0.84 0.82 0.82
## [4537] 0.80 0.78 0.76 0.76 0.76 0.74 0.74 0.74 0.74 0.76 0.80 0.82 0.82 0.84
## [4551] 0.84 0.82 0.82 0.64 0.66 0.70 0.72 0.70 0.70 0.70 0.68 0.66 0.66 0.66
## [4565] 0.64 0.62 0.62 0.60 0.60 0.62 0.64 0.68 0.70 0.72 0.74 0.76 0.74 0.76
## [4579] 0.76 0.74 0.72 0.72 0.70 0.66 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.60
## [4593] 0.60 0.64 0.64 0.68 0.66 0.70 0.70 0.70 0.70 0.72 0.72 0.74 0.72 0.70
## [4607] 0.70 0.66 0.66 0.64 0.62 0.60 0.60 0.60 0.60 0.58 0.60 0.62 0.66 0.70
## [4621] 0.72 0.74 0.76 0.76 0.74 0.76 0.76 0.76 0.76 0.74 0.72 0.70 0.70 0.68
## [4635] 0.66 0.64 0.64 0.64 0.62 0.64 0.62 0.64 0.68 0.72 0.74 0.76 0.76 0.80
## [4649] 0.80 0.82 0.80 0.80 0.80 0.76 0.76 0.74 0.72 0.70 0.70 0.70 0.66 0.66
## [4663] 0.64 0.64 0.64 0.68 0.70 0.74 0.76 0.80 0.80 0.82 0.82 0.84 0.84 0.84
## [4677] 0.82 0.80 0.78 0.74 0.76 0.74 0.74 0.74 0.72 0.72 0.72 0.70 0.72 0.74
## [4691] 0.76 0.80 0.82 0.82 0.84 0.86 0.86 0.88 0.90 0.80 0.80 0.76 0.74 0.74
## [4705] 0.74 0.72 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.70 0.74 0.74 0.76 0.80
## [4719] 0.82 0.84 0.86 0.86 0.84 0.84 0.84 0.82 0.80 0.80 0.78 0.76 0.76 0.74
## [4733] 0.74 0.74 0.72 0.72 0.72 0.74 0.76 0.80 0.82 0.86 0.88 0.88 0.90 0.90
## [4747] 0.92 0.92 0.90 0.86 0.84 0.82 0.82 0.80 0.82 0.80 0.78 0.78 0.76 0.74
## [4761] 0.76 0.80 0.84 0.86 0.90 0.90 0.94 0.94 0.96 0.94 0.90 0.88 0.90 0.86
## [4775] 0.84 0.82 0.84 0.80 0.82 0.82 0.82 0.78 0.76 0.76 0.80 0.80 0.84 0.84
## [4789] 0.86 0.90 0.92 0.94 0.92 0.94 0.94 0.94 0.92 0.82 0.82 0.82 0.80 0.80
## [4803] 0.80 0.78 0.80 0.80 0.78 0.78 0.80 0.80 0.82 0.82 0.86 0.84 0.90 0.86
## [4817] 0.86 0.90 0.90 0.90 0.88 0.86 0.84 0.80 0.78 0.76 0.76 0.76 0.74 0.74
## [4831] 0.72 0.72 0.72 0.74 0.76 0.80 0.82 0.84 0.84 0.74 0.72 0.70 0.70 0.72
## [4845] 0.74 0.74 0.72 0.70 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.66 0.68 0.72
## [4859] 0.74 0.76 0.80 0.82 0.84 0.84 0.86 0.88 0.86 0.86 0.84 0.82 0.80 0.78
## [4873] 0.76 0.76 0.78 0.78 0.76 0.72 0.72 0.70 0.70 0.72 0.74 0.76 0.78 0.80
## [4887] 0.82 0.84 0.84 0.86 0.86 0.84 0.82 0.80 0.76 0.74 0.72 0.74 0.70 0.72
## [4901] 0.72 0.72 0.70 0.70 0.70 0.72 0.74 0.78 0.80 0.84 0.84 0.86 0.84 0.86
## [4915] 0.86 0.84 0.84 0.82 0.80 0.78 0.76 0.76 0.74 0.74 0.74 0.74 0.72 0.72
## [4929] 0.72 0.74 0.76 0.86 0.90 0.92 0.96 0.94 0.96 0.96 0.96 0.96 0.92 0.90
## [4943] 0.86 0.82 0.80 0.78 0.76 0.76 0.76 0.74 0.72 0.72 0.72 0.76 0.78 0.82
## [4957] 0.82 0.84 0.84 0.88 0.90 0.90 0.90 0.90 0.88 0.74 0.82 0.80 0.78 0.76
## [4971] 0.76 0.74 0.74 0.74 0.72 0.72 0.74 0.74 0.76 0.80 0.84 0.86 0.90 0.90
## [4985] 0.90 0.92 0.92 0.92 0.86 0.80 0.80 0.78 0.74 0.74 0.72 0.72 0.70 0.70
## [4999] 0.66 0.66 0.66 0.74 0.80 0.82 0.86 0.88 0.90 0.90 0.92 0.90 0.90 0.76
## [5013] 0.78 0.74 0.72 0.70 0.70 0.68 0.66 0.66 0.68 0.66 0.66 0.66 0.68 0.72
## [5027] 0.74 0.78 0.82 0.84 0.86 0.86 0.90 0.90 0.90 0.90 0.86 0.86 0.82 0.80
## [5041] 0.78 0.80 0.80 0.78 0.78 0.76 0.76 0.76 0.72 0.74 0.74 0.74 0.74 0.74
## [5055] 0.76 0.76 0.76 0.70 0.68 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.68 0.66
## [5069] 0.66 0.66 0.68 0.68 0.68 0.68 0.70 0.72 0.72 0.74 0.76 0.76 0.80 0.80
## [5083] 0.76 0.76 0.70 0.70 0.70 0.70 0.68 0.66 0.66 0.64 0.66 0.64 0.64 0.64
## [5097] 0.64 0.66 0.70 0.72 0.74 0.76 0.76 0.78 0.78 0.76 0.80 0.78 0.76 0.74
## [5111] 0.72 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.66 0.64 0.64 0.68 0.68 0.74
## [5125] 0.74 0.78 0.80 0.80 0.82 0.84 0.74 0.74 0.72 0.72 0.72 0.70 0.70 0.70
## [5139] 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.72 0.76 0.80 0.82 0.90 0.90
## [5153] 0.86 0.72 0.72 0.74 0.72 0.72 0.72 0.72 0.70 0.70 0.70 0.68 0.66 0.66
## [5167] 0.66 0.70 0.70 0.72 0.74 0.76 0.80 0.82 0.82 0.84 0.82 0.84 0.86 0.86
## [5181] 0.84 0.82 0.80 0.76 0.76 0.74 0.72 0.72 0.72 0.72 0.70 0.70 0.72 0.72
## [5195] 0.74 0.78 0.80 0.80 0.82 0.84 0.86 0.86 0.86 0.80 0.80 0.80 0.80 0.78
## [5209] 0.78 0.76 0.74 0.72 0.72 0.70 0.70 0.68 0.68 0.70 0.74 0.76 0.80 0.80
## [5223] 0.82 0.82 0.84 0.86 0.86 0.84 0.82 0.80 0.76 0.76 0.74 0.74 0.70 0.70
## [5237] 0.66 0.64 0.64 0.64 0.62 0.66 0.70 0.72 0.74 0.76 0.78 0.76 0.80 0.80
## [5251] 0.80 0.80 0.78 0.74 0.72 0.70 0.70 0.66 0.64 0.64 0.62 0.62 0.62 0.60
## [5265] 0.62 0.64 0.68 0.72 0.74 0.76 0.80 0.78 0.80 0.80 0.82 0.82 0.76 0.74
## [5279] 0.72 0.70 0.68 0.68 0.68 0.68 0.68 0.66 0.64 0.64 0.64 0.66 0.70 0.70
## [5293] 0.72 0.74 0.66 0.74 0.74 0.68 0.70 0.72 0.70 0.68 0.68 0.68 0.68 0.66
## [5307] 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.66 0.66 0.70 0.70 0.70 0.72
## [5321] 0.74 0.74 0.74 0.76 0.74 0.72 0.70 0.60 0.60 0.60 0.60 0.60 0.60 0.60
## [5335] 0.60 0.60 0.60 0.60 0.64 0.64 0.68 0.72 0.74 0.78 0.74 0.74 0.74 0.74
## [5349] 0.70 0.70 0.66 0.66 0.66 0.64 0.64 0.64 0.62 0.62 0.64 0.62 0.62 0.64
## [5363] 0.70 0.68 0.70 0.74 0.76 0.76 0.76 0.80 0.80 0.76 0.76 0.74 0.74 0.72
## [5377] 0.70 0.66 0.66 0.64 0.66 0.64 0.64 0.64 0.62 0.66 0.70 0.74 0.76 0.78
## [5391] 0.80 0.80 0.82 0.80 0.82 0.80 0.76 0.76 0.74 0.72 0.70 0.70 0.68 0.66
## [5405] 0.66 0.66 0.64 0.64 0.64 0.64 0.66 0.72 0.74 0.76 0.80 0.80 0.82 0.80
## [5419] 0.80 0.80 0.76 0.66 0.66 0.68 0.70 0.70 0.68 0.64 0.64 0.64 0.64 0.64
## [5433] 0.64 0.66 0.66 0.70 0.72 0.72 0.74 0.76 0.78 0.80 0.80 0.76 0.76 0.62
## [5447] 0.62 0.62 0.60 0.60 0.60 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.66 0.70
## [5461] 0.72 0.72 0.74 0.76 0.80 0.80 0.80 0.80 0.76 0.74 0.74 0.72 0.70 0.70
## [5475] 0.70 0.68 0.68 0.66 0.68 0.66 0.66 0.68 0.70 0.72 0.74 0.80 0.82 0.80
## [5489] 0.70 0.70 0.72 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.68 0.68 0.70 0.70
## [5503] 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.72 0.74 0.74 0.74 0.74 0.74 0.74
## [5517] 0.72 0.70 0.66 0.64 0.64 0.62 0.60 0.58 0.56 0.56 0.54 0.54 0.54 0.60
## [5531] 0.62 0.66 0.70 0.70 0.70 0.72 0.72 0.72 0.72 0.72 0.72 0.66 0.64 0.62
## [5545] 0.62 0.62 0.62 0.60 0.58 0.58 0.56 0.56 0.56 0.60 0.62 0.64 0.70 0.72
## [5559] 0.74 0.76 0.76 0.76 0.76 0.76 0.74 0.74 0.72 0.70 0.70 0.68 0.68 0.68
## [5573] 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.72 0.74 0.70 0.70 0.70 0.72 0.74
## [5587] 0.72 0.72 0.72 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.60 0.62 0.62
## [5601] 0.62 0.64 0.66 0.70 0.74 0.76 0.76 0.78 0.80 0.80 0.78 0.74 0.74 0.72
## [5615] 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70
## [5629] 0.70 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.62 0.66 0.70 0.70 0.74
## [5643] 0.76 0.78 0.78 0.80 0.76 0.74 0.72 0.72 0.66 0.64 0.62 0.62 0.60 0.60
## [5657] 0.60 0.56 0.56 0.56 0.60 0.62 0.62 0.66 0.66 0.68 0.70 0.70 0.70 0.72
## [5671] 0.70 0.66 0.66 0.64 0.64 0.62 0.60 0.56 0.56 0.54 0.54 0.52 0.52 0.54
## [5685] 0.56 0.62 0.66 0.70 0.72 0.72 0.74 0.74 0.74 0.74 0.72 0.70 0.66 0.66
## [5699] 0.64 0.62 0.62 0.60 0.60 0.56 0.56 0.56 0.54 0.54 0.60 0.62 0.64 0.70
## [5713] 0.72 0.74 0.74 0.74 0.74 0.74 0.72 0.70 0.84 0.66 0.66 0.64 0.60 0.60
## [5727] 0.60 0.58 0.58 0.56 0.56 0.60 0.60 0.62 0.64 0.68 0.72 0.72 0.72 0.72
## [5741] 0.72 0.74 0.72 0.72 0.70 0.66 0.66 0.66 0.64 0.64 0.62 0.62 0.60 0.60
## [5755] 0.60 0.60 0.60 0.62 0.62 0.64 0.66 0.66 0.68 0.70 0.70 0.70 0.68 0.68
## [5769] 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.64
## [5783] 0.66 0.66 0.66 0.70 0.70 0.72 0.72 0.72 0.72 0.72 0.70 0.70 0.68 0.68
## [5797] 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.66 0.66 0.66 0.70 0.74 0.76
## [5811] 0.78 0.78 0.78 0.80 0.76 0.76 0.74 0.74 0.72 0.72 0.72 0.70 0.68 0.68
## [5825] 0.68 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.70 0.72 0.74 0.74 0.68 0.68
## [5839] 0.66 0.66 0.66 0.66 0.66 0.60 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [5853] 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [5867] 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.56 0.60 0.60 0.62 0.60 0.60 0.60
## [5881] 0.56 0.60 0.60 0.62 0.64 0.64 0.64 0.64 0.64 0.64 0.62 0.62 0.60 0.62
## [5895] 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.64 0.64 0.66 0.68 0.70 0.66 0.64
## [5909] 0.64 0.64 0.62 0.62 0.64 0.62 0.62 0.64 0.62 0.62 0.62 0.62 0.62 0.62
## [5923] 0.62 0.62 0.62 0.62 0.62 0.64 0.70 0.72 0.74 0.74 0.70 0.70 0.66 0.64
## [5937] 0.66 0.62 0.62 0.62 0.62 0.60 0.58 0.58 0.58 0.58 0.60 0.62 0.64 0.70
## [5951] 0.72 0.72 0.74 0.74 0.74 0.74 0.74 0.72 0.70 0.66 0.64 0.64 0.62 0.62
## [5965] 0.62 0.62 0.62 0.60 0.60 0.58 0.60 0.64 0.66 0.70 0.70 0.70 0.74 0.72
## [5979] 0.74 0.72 0.72 0.68 0.64 0.62 0.64 0.62 0.58 0.56 0.56 0.56 0.54 0.56
## [5993] 0.56 0.58 0.60 0.64 0.68 0.70 0.72 0.72 0.74 0.74 0.72 0.72 0.70 0.68
## [6007] 0.66 0.64 0.62 0.62 0.60 0.58 0.60 0.58 0.56 0.56 0.56 0.58 0.60 0.64
## [6021] 0.68 0.70 0.72 0.74 0.74 0.74 0.74 0.74 0.70 0.68 0.66 0.64 0.64 0.64
## [6035] 0.62 0.62 0.60 0.60 0.60 0.58 0.58 0.60 0.62 0.64 0.70 0.72 0.74 0.76
## [6049] 0.78 0.78 0.76 0.76 0.72 0.70 0.72 0.66 0.66 0.64 0.64 0.64 0.62 0.62
## [6063] 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.68 0.66 0.64 0.64 0.60 0.54 0.48
## [6077] 0.48 0.46 0.46 0.46 0.44 0.44 0.42 0.42 0.40 0.40 0.40 0.38 0.38 0.40
## [6091] 0.42 0.46 0.50 0.50 0.52 0.54 0.54 0.54 0.52 0.52 0.52 0.50 0.50 0.50
## [6105] 0.48 0.50 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.48 0.50 0.52
## [6119] 0.52 0.52 0.52 0.52 0.54 0.52 0.52 0.52 0.52 0.50 0.50 0.46 0.46 0.46
## [6133] 0.44 0.44 0.44 0.44 0.44 0.46 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.58
## [6147] 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.52 0.52 0.52 0.50 0.50 0.50
## [6161] 0.50 0.50 0.52 0.54 0.56 0.58 0.58 0.60 0.60 0.60 0.60 0.58 0.56 0.56
## [6175] 0.56 0.56 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [6189] 0.56 0.56 0.56 0.56 0.58 0.62 0.60 0.60 0.60 0.58 0.56 0.56 0.56 0.56
## [6203] 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.56 0.60 0.62 0.64 0.66
## [6217] 0.66 0.66 0.66 0.66 0.64 0.62 0.62 0.60 0.62 0.60 0.60 0.60 0.60 0.60
## [6231] 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.66 0.66 0.68 0.68 0.66
## [6245] 0.64 0.64 0.62 0.64 0.62 0.62 0.62 0.60 0.60 0.60 0.60 0.62 0.62 0.62
## [6259] 0.62 0.62 0.62 0.62 0.62 0.60 0.60 0.62 0.62 0.60 0.60 0.62 0.60 0.60
## [6273] 0.60 0.58 0.58 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.58 0.60 0.60 0.62
## [6287] 0.62 0.64 0.66 0.66 0.64 0.66 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60
## [6301] 0.60 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.66 0.68 0.66 0.66
## [6315] 0.70 0.68 0.68 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62
## [6329] 0.62 0.62 0.62 0.64 0.64 0.68 0.68 0.70 0.70 0.72 0.70 0.70 0.66 0.66
## [6343] 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.64 0.64
## [6357] 0.64 0.66 0.66 0.70 0.68 0.68 0.66 0.66 0.64 0.64 0.62 0.60 0.60 0.60
## [6371] 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.62 0.62 0.62 0.66 0.68 0.70
## [6385] 0.72 0.70 0.70 0.70 0.66 0.66 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60
## [6399] 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.62 0.66 0.64 0.68 0.68 0.68 0.66
## [6413] 0.64 0.62 0.60 0.58 0.56 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52
## [6427] 0.54 0.56 0.60 0.64 0.64 0.66 0.64 0.64 0.62 0.62 0.58 0.54 0.54 0.52
## [6441] 0.52 0.52 0.50 0.48 0.46 0.46 0.44 0.44 0.42 0.42 0.40 0.40 0.40 0.38
## [6455] 0.40 0.40 0.42 0.42 0.40 0.40 0.38 0.38 0.36 0.36 0.36 0.36 0.36 0.34
## [6469] 0.34 0.34 0.34 0.32 0.32 0.34 0.34 0.36 0.36 0.38 0.40 0.40 0.36 0.36
## [6483] 0.36 0.36 0.36 0.38 0.36 0.36 0.36 0.36 0.34 0.36 0.36 0.36 0.34 0.36
## [6497] 0.36 0.36 0.36 0.40 0.40 0.40 0.40 0.42 0.40 0.40 0.40 0.40 0.40 0.40
## [6511] 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.42 0.44 0.46
## [6525] 0.48 0.54 0.56 0.56 0.58 0.58 0.58 0.56 0.54 0.52 0.52 0.50 0.50 0.48
## [6539] 0.48 0.48 0.46 0.46 0.44 0.44 0.44 0.46 0.52 0.54 0.56 0.60 0.62 0.64
## [6553] 0.64 0.64 0.64 0.64 0.60 0.58 0.52 0.50 0.52 0.50 0.50 0.48 0.46 0.44
## [6567] 0.44 0.42 0.40 0.42 0.44 0.46 0.52 0.54 0.56 0.56 0.58 0.58 0.58 0.56
## [6581] 0.54 0.52 0.50 0.46 0.46 0.44 0.44 0.44 0.42 0.40 0.40 0.42 0.42 0.42
## [6595] 0.46 0.48 0.52 0.56 0.60 0.60 0.62 0.66 0.64 0.60 0.56 0.56 0.54 0.50
## [6609] 0.50 0.50 0.48 0.46 0.46 0.44 0.42 0.42 0.42 0.42 0.46 0.50 0.52 0.58
## [6623] 0.62 0.62 0.62 0.66 0.64 0.62 0.60 0.54 0.52 0.52 0.50 0.48 0.46 0.46
## [6637] 0.46 0.44 0.44 0.44 0.44 0.44 0.46 0.50 0.56 0.62 0.64 0.66 0.68 0.68
## [6651] 0.66 0.62 0.64 0.56 0.56 0.54 0.52 0.50 0.50 0.48 0.48 0.46 0.46 0.46
## [6665] 0.44 0.46 0.52 0.54 0.56 0.62 0.70 0.72 0.74 0.72 0.70 0.66 0.64 0.58
## [6679] 0.60 0.56 0.56 0.54 0.54 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.56 0.60
## [6693] 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.58 0.58 0.58 0.56 0.56
## [6707] 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [6721] 0.54 0.54 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [6735] 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.62 0.62 0.64 0.66 0.66 0.66 0.62
## [6749] 0.62 0.62 0.62 0.62 0.62 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.56 0.56
## [6763] 0.56 0.56 0.54 0.52 0.52 0.56 0.62 0.62 0.60 0.58 0.54 0.54 0.52 0.50
## [6777] 0.46 0.46 0.46 0.46 0.46 0.44 0.42 0.42 0.40 0.40 0.46 0.52 0.54 0.56
## [6791] 0.58 0.60 0.60 0.62 0.62 0.58 0.54 0.50 0.50 0.50 0.50 0.48 0.46 0.44
## [6805] 0.42 0.42 0.42 0.42 0.38 0.40 0.44 0.50 0.54 0.56 0.58 0.60 0.60 0.62
## [6819] 0.60 0.58 0.56 0.54 0.54 0.54 0.56 0.56 0.54 0.56 0.56 0.56 0.52 0.50
## [6833] 0.50 0.50 0.50 0.50 0.52 0.56 0.56 0.56 0.58 0.56 0.58 0.56 0.56 0.54
## [6847] 0.52 0.50 0.50 0.48 0.48 0.46 0.44 0.44 0.44 0.46 0.46 0.46 0.50 0.52
## [6861] 0.56 0.60 0.62 0.64 0.62 0.62 0.62 0.60 0.56 0.56 0.54 0.54 0.52 0.52
## [6875] 0.52 0.52 0.52 0.50 0.50 0.52 0.52 0.54 0.52 0.52 0.52 0.52 0.54 0.54
## [6889] 0.54 0.56 0.56 0.56 0.60 0.60 0.58 0.58 0.58 0.56 0.56 0.56 0.50 0.50
## [6903] 0.48 0.44 0.42 0.44 0.44 0.46 0.48 0.48 0.50 0.48 0.48 0.48 0.48 0.46
## [6917] 0.46 0.46 0.44 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.34 0.36 0.36 0.40
## [6931] 0.42 0.46 0.50 0.50 0.48 0.52 0.50 0.50 0.46 0.44 0.44 0.44 0.42 0.42
## [6945] 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.36 0.40 0.42 0.44 0.44 0.46
## [6959] 0.50 0.48 0.48 0.50 0.46 0.44 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.34
## [6973] 0.34 0.34 0.32 0.32 0.34 0.36 0.40 0.42 0.46 0.50 0.52 0.52 0.52 0.52
## [6987] 0.50 0.50 0.46 0.44 0.44 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.38 0.40
## [7001] 0.38 0.42 0.44 0.46 0.50 0.52 0.54 0.54 0.56 0.54 0.52 0.54 0.48 0.48
## [7015] 0.46 0.48 0.46 0.44 0.44 0.42 0.40 0.38 0.38 0.38 0.40 0.44 0.48 0.50
## [7029] 0.52 0.54 0.56 0.56 0.56 0.56 0.56 0.52 0.48 0.46 0.44 0.46 0.44 0.44
## [7043] 0.44 0.44 0.44 0.42 0.42 0.42 0.42 0.44 0.48 0.52 0.52 0.58 0.56 0.52
## [7057] 0.52 0.52 0.52 0.52 0.50 0.50 0.52 0.48 0.48 0.46 0.46 0.46 0.46 0.48
## [7071] 0.48 0.50 0.46 0.48 0.50 0.50 0.50 0.50 0.50 0.52 0.52 0.56 0.50 0.48
## [7085] 0.44 0.42 0.40 0.36 0.34 0.34 0.32 0.32 0.30 0.30 0.30 0.28 0.28 0.30
## [7099] 0.32 0.34 0.36 0.38 0.38 0.36 0.36 0.36 0.36 0.36 0.36 0.34 0.32 0.30
## [7113] 0.30 0.28 0.30 0.30 0.30 0.30 0.26 0.26 0.26 0.28 0.28 0.26 0.26 0.24
## [7127] 0.24 0.24 0.22 0.22 0.22 0.22 0.24 0.24 0.24 0.22 0.22 0.22 0.22 0.22
## [7141] 0.24 0.22 0.24 0.24 0.24 0.26 0.30 0.32 0.36 0.38 0.40 0.42 0.42 0.42
## [7155] 0.40 0.36 0.56 0.34 0.32 0.30 0.26 0.26 0.26 0.24 0.24 0.24 0.22 0.24
## [7169] 0.24 0.28 0.32 0.36 0.40 0.42 0.44 0.44 0.44 0.42 0.42 0.40 0.40 0.40
## [7183] 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.34 0.32 0.32 0.34 0.36 0.40 0.44
## [7197] 0.46 0.50 0.48 0.50 0.50 0.48 0.44 0.42 0.42 0.40 0.36 0.36 0.34 0.32
## [7211] 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.34 0.40 0.42 0.46 0.48 0.50
## [7225] 0.48 0.48 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32
## [7239] 0.32 0.34 0.32 0.34 0.36 0.40 0.44 0.50 0.52 0.52 0.52 0.52 0.48 0.46
## [7253] 0.44 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.38 0.38 0.40 0.40
## [7267] 0.42 0.42 0.44 0.48 0.44 0.44 0.46 0.46 0.42 0.42 0.40 0.36 0.34 0.34
## [7281] 0.32 0.32 0.32 0.30 0.30 0.28 0.26 0.26 0.26 0.28 0.30 0.32 0.36 0.36
## [7295] 0.40 0.42 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.30 0.28 0.28 0.26 0.26
## [7309] 0.24 0.24 0.24 0.26 0.26 0.28 0.30 0.36 0.42 0.44 0.46 0.46 0.48 0.46
## [7323] 0.44 0.42 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.30 0.28
## [7337] 0.28 0.30 0.34 0.36 0.42 0.46 0.54 0.56 0.56 0.52 0.50 0.46 0.46 0.40
## [7351] 0.38 0.36 0.36 0.34 0.32 0.32 0.32 0.30 0.30 0.30 0.30 0.32 0.36 0.42
## [7365] 0.46 0.52 0.54 0.56 0.58 0.56 0.52 0.48 0.46 0.40 0.40 0.36 0.36 0.36
## [7379] 0.34 0.32 0.32 0.32 0.30 0.30 0.30 0.32 0.34 0.40 0.46 0.50 0.50 0.52
## [7393] 0.52 0.52 0.46 0.44 0.44 0.44 0.40 0.40 0.38 0.40 0.40 0.38 0.38 0.38
## [7407] 0.36 0.36 0.38 0.40 0.42 0.44 0.46 0.42 0.36 0.36 0.36 0.36 0.36 0.36
## [7421] 0.36 0.36 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.30 0.30 0.28 0.28 0.30
## [7435] 0.32 0.32 0.34 0.34 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.32
## [7449] 0.30 0.24 0.24 0.24 0.24 0.20 0.22 0.22 0.22 0.26 0.30 0.34 0.38 0.44
## [7463] 0.48 0.50 0.52 0.52 0.50 0.42 0.42 0.42 0.42 0.42 0.40 0.40 0.36 0.36
## [7477] 0.36 0.36 0.34 0.34 0.34 0.34 0.40 0.44 0.46 0.52 0.52 0.54 0.50 0.54
## [7491] 0.52 0.52 0.50 0.50 0.48 0.48 0.46 0.46 0.46 0.46 0.44 0.44 0.44 0.44
## [7505] 0.44 0.46 0.48 0.50 0.54 0.56 0.60 0.62 0.64 0.62 0.62 0.56 0.60 0.60
## [7519] 0.60 0.58 0.56 0.56 0.56 0.56 0.54 0.56 0.54 0.56 0.54 0.54 0.56 0.56
## [7533] 0.56 0.54 0.54 0.54 0.54 0.52 0.50 0.50 0.50 0.48 0.50 0.46 0.46 0.46
## [7547] 0.46 0.46 0.46 0.44 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46
## [7561] 0.48 0.48 0.48 0.46 0.46 0.44 0.44 0.42 0.42 0.42 0.42 0.42 0.42 0.40
## [7575] 0.34 0.36 0.34 0.34 0.34 0.34 0.32 0.34 0.34 0.34 0.34 0.32 0.32 0.32
## [7589] 0.30 0.30 0.30 0.26 0.26 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.22
## [7603] 0.26 0.26 0.30 0.32 0.34 0.34 0.34 0.34 0.32 0.30 0.28 0.28 0.28 0.26
## [7617] 0.26 0.26 0.26 0.24 0.26 0.26 0.24 0.24 0.24 0.26 0.28 0.32 0.36 0.40
## [7631] 0.42 0.42 0.42 0.42 0.40 0.38 0.34 0.36 0.36 0.38 0.38 0.38 0.40 0.40
## [7645] 0.40 0.40 0.42 0.42 0.42 0.42 0.44 0.44 0.50 0.50 0.54 0.52 0.52 0.52
## [7659] 0.52 0.54 0.50 0.52 0.48 0.46 0.46 0.46 0.46 0.44 0.44 0.44 0.44 0.42
## [7673] 0.46 0.46 0.48 0.52 0.52 0.50 0.50 0.48 0.44 0.44 0.42 0.42 0.40 0.40
## [7687] 0.40 0.40 0.40 0.38 0.40 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.40 0.40
## [7701] 0.40 0.40 0.42 0.42 0.44 0.44 0.44 0.44 0.46 0.46 0.46 0.50 0.48 0.48
## [7715] 0.48 0.50 0.50 0.52 0.46 0.44 0.46 0.48 0.52 0.52 0.50 0.48 0.44 0.42
## [7729] 0.42 0.40 0.38 0.40 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.28 0.30
## [7743] 0.30 0.30 0.26 0.30 0.34 0.36 0.42 0.46 0.48 0.50 0.50 0.50 0.48 0.42
## [7757] 0.40 0.36 0.36 0.36 0.34 0.34 0.34 0.28 0.28 0.30 0.28 0.26 0.26 0.26
## [7771] 0.32 0.36 0.40 0.46 0.50 0.52 0.52 0.50 0.50 0.46 0.42 0.40 0.36 0.34
## [7785] 0.34 0.34 0.32 0.30 0.30 0.30 0.30 0.30 0.26 0.32 0.34 0.36 0.40 0.44
## [7799] 0.48 0.48 0.50 0.46 0.46 0.42 0.40 0.42 0.38 0.38 0.36 0.36 0.36 0.34
## [7813] 0.34 0.34 0.36 0.38 0.38 0.40 0.46 0.46 0.50 0.54 0.54 0.62 0.62 0.56
## [7827] 0.54 0.50 0.48 0.50 0.48 0.48 0.48 0.46 0.46 0.44 0.44 0.40 0.42 0.42
## [7841] 0.42 0.44 0.48 0.52 0.56 0.58 0.60 0.58 0.56 0.58 0.56 0.54 0.54 0.54
## [7855] 0.52 0.52 0.52 0.52 0.50 0.50 0.52 0.50 0.52 0.52 0.54 0.56 0.56 0.50
## [7869] 0.42 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.38 0.36 0.36 0.34
## [7883] 0.32 0.32 0.30 0.28 0.26 0.26 0.28 0.30 0.34 0.38 0.36 0.38 0.38 0.36
## [7897] 0.36 0.34 0.34 0.32 0.32 0.32 0.30 0.28 0.28 0.26 0.26 0.26 0.26 0.26
## [7911] 0.24 0.24 0.26 0.30 0.32 0.34 0.36 0.40 0.40 0.40 0.40 0.36 0.36 0.34
## [7925] 0.34 0.30 0.30 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.24 0.28
## [7939] 0.30 0.34 0.36 0.40 0.42 0.42 0.42 0.42 0.40 0.34 0.38 0.34 0.32 0.32
## [7953] 0.30 0.26 0.26 0.24 0.22 0.24 0.24 0.22 0.24 0.26 0.32 0.32 0.36 0.36
## [7967] 0.36 0.38 0.38 0.36 0.34 0.30 0.30 0.30 0.32 0.30 0.30 0.30 0.26 0.28
## [7981] 0.26 0.26 0.24 0.26 0.26 0.30 0.32 0.34 0.36 0.40 0.42 0.42 0.42 0.38
## [7995] 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.32 0.32 0.30 0.30 0.30 0.32
## [8009] 0.32 0.36 0.36 0.36 0.40 0.42 0.46 0.46 0.50 0.42 0.44 0.46 0.46 0.44
## [8023] 0.44 0.46 0.50 0.46 0.46 0.46 0.46 0.46 0.44 0.46 0.46 0.46 0.46 0.46
## [8037] 0.46 0.46 0.48 0.50 0.46 0.46 0.46 0.46 0.46 0.44 0.46 0.46 0.44 0.46
## [8051] 0.46 0.46 0.46 0.46 0.48 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.42
## [8065] 0.42 0.40 0.40 0.34 0.34 0.30 0.24 0.24 0.26 0.26 0.28 0.26 0.24 0.22
## [8079] 0.22 0.22 0.22 0.24 0.26 0.28 0.28 0.30 0.32 0.32 0.32 0.30 0.28 0.26
## [8093] 0.26 0.28 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.24 0.26
## [8107] 0.30 0.32 0.36 0.38 0.38 0.38 0.36 0.34 0.34 0.34 0.30 0.30 0.28 0.28
## [8121] 0.26 0.26 0.28 0.28 0.26 0.26 0.24 0.24 0.26 0.28 0.32 0.32 0.32 0.34
## [8135] 0.34 0.34 0.32 0.28 0.26 0.26 0.24 0.22 0.22 0.20 0.20 0.16 0.18 0.16
## [8149] 0.16 0.18 0.16 0.18 0.16 0.20 0.24 0.26 0.26 0.30 0.30 0.30 0.30 0.28
## [8163] 0.24 0.22 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.18 0.18 0.16 0.16 0.14
## [8177] 0.16 0.18 0.22 0.26 0.28 0.30 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.26
## [8191] 0.26 0.24 0.22 0.20 0.20 0.18 0.20 0.18 0.20 0.16 0.20 0.26 0.30 0.34
## [8205] 0.36 0.40 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.30 0.30 0.26 0.26 0.26
## [8219] 0.26 0.28 0.26 0.26 0.26 0.28 0.26 0.30 0.32 0.34 0.36 0.36 0.38 0.38
## [8233] 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.32
## [8247] 0.32 0.32 0.34 0.36 0.40 0.40 0.46 0.50 0.52 0.52 0.46 0.52 0.52 0.52
## [8261] 0.52 0.50 0.52 0.52 0.50 0.50 0.48 0.46 0.50 0.48 0.44 0.38 0.36 0.36
## [8275] 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30
## [8289] 0.30 0.30 0.28 0.26 0.24 0.26 0.26 0.26 0.26 0.26 0.26 0.28 0.28 0.28
## [8303] 0.28 0.28 0.28 0.26 0.26 0.24 0.22 0.20 0.20 0.20 0.20 0.20 0.22 0.22
## [8317] 0.22 0.20 0.20 0.20 0.20 0.22 0.24 0.24 0.26 0.30 0.30 0.32 0.28 0.28
## [8331] 0.28 0.26 0.24 0.22 0.22 0.20 0.20 0.18 0.18 0.16 0.16 0.14 0.16 0.18
## [8345] 0.20 0.22 0.24 0.26 0.30 0.34 0.36 0.38 0.40 0.38 0.36 0.36 0.40 0.36
## [8359] 0.36 0.36 0.36 0.36 0.34 0.34 0.36 0.36 0.36 0.36 0.42 0.36 0.42 0.40
## [8373] 0.44 0.44 0.44 0.44 0.44 0.40 0.38 0.38 0.36 0.36 0.36 0.38 0.34 0.36
## [8387] 0.36 0.36 0.36 0.38 0.36 0.36 0.36 0.40 0.48 0.48 0.46 0.44 0.48 0.48
## [8401] 0.44 0.44 0.44 0.50 0.50 0.50 0.50 0.50 0.50 0.44 0.48 0.44 0.38 0.38
## [8415] 0.36 0.34 0.36 0.38 0.40 0.44 0.48 0.46 0.46 0.48 0.46 0.44 0.44 0.44
## [8429] 0.42 0.42 0.36 0.40 0.40 0.40 0.38 0.38 0.38 0.36 0.38 0.38 0.40 0.40
## [8443] 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.36 0.34 0.32 0.32 0.32 0.32
## [8457] 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.30 0.32 0.34 0.34
## [8471] 0.34 0.32 0.32 0.28 0.30 0.30 0.26 0.26 0.24 0.24 0.24 0.22 0.22 0.22
## [8485] 0.20 0.20 0.22 0.20 0.24 0.26 0.30 0.30 0.32 0.34 0.34 0.36 0.34 0.32
## [8499] 0.32 0.32 0.30 0.28 0.26 0.22 0.28 0.34 0.34 0.34 0.32 0.32 0.32 0.34
## [8513] 0.34 0.34 0.36 0.38 0.38 0.38 0.36 0.34 0.32 0.30 0.30 0.26 0.26 0.26
## [8527] 0.26 0.26 0.26 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.32 0.32 0.30 0.30
## [8541] 0.42 0.42 0.44 0.40 0.38 0.34 0.32 0.32 0.32 0.30 0.32 0.32 0.32 0.32
## [8555] 0.32 0.32 0.32 0.32 0.32 0.34 0.36 0.34 0.34 0.32 0.32 0.30 0.28 0.26
## [8569] 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.18 0.20
## [8583] 0.20 0.22 0.24 0.26 0.28 0.30 0.30 0.30 0.30 0.30 0.30 0.28 0.28 0.28
## [8597] 0.30 0.28 0.26 0.24 0.24 0.24 0.22 0.24 0.24 0.24 0.26 0.30 0.32 0.32
## [8611] 0.36 0.40 0.42 0.42 0.38 0.36 0.34 0.34 0.36 0.34 0.36 0.38 0.40 0.40
## [8625] 0.40 0.38 0.36 0.40 0.38 0.34 0.38 0.40 0.42 0.52 0.50 0.46 0.46 0.44
## [8639] 0.42 0.42 0.42 0.42 0.40 0.38 0.36</code></pre>
<div id="the-linear-model" class="section level2 unnumbered hasAnchor">
<h2>The Linear Model<a href="demonstration-2-poisson-versus-linear-regression.html#the-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="approaches-to-variable-coding" class="section level3 unnumbered hasAnchor">
<h3>Approaches to variable coding<a href="demonstration-2-poisson-versus-linear-regression.html#approaches-to-variable-coding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The purpose of this task is to predict the number of bikers (bikers)
using month (mnth), hour (hr), whether itâs a working day (workingday),
temperature (temp), and weather situation (weathersit).</p>
<p>We begin by fitting a least squares linear regression model to the data.</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="demonstration-2-poisson-versus-linear-regression.html#cb199-1" tabindex="-1"></a>mod.lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(bikers <span class="sc">~</span> mnth <span class="sc">+</span> hr <span class="sc">+</span> workingday <span class="sc">+</span> temp <span class="sc">+</span> weathersit)</span>
<span id="cb199-2"><a href="demonstration-2-poisson-versus-linear-regression.html#cb199-2" tabindex="-1"></a><span class="fu">summary</span>(mod.lm)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = bikers ~ mnth + hr + workingday + temp + weathersit)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -299.00  -45.70   -6.23   41.08  425.29 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                -68.632      5.307 -12.932  &lt; 2e-16 ***
## mnthFeb                      6.845      4.287   1.597 0.110398    
## mnthMarch                   16.551      4.301   3.848 0.000120 ***
## mnthApril                   41.425      4.972   8.331  &lt; 2e-16 ***
## mnthMay                     72.557      5.641  12.862  &lt; 2e-16 ***
## mnthJune                    67.819      6.544  10.364  &lt; 2e-16 ***
## mnthJuly                    45.324      7.081   6.401 1.63e-10 ***
## mnthAug                     53.243      6.640   8.019 1.21e-15 ***
## mnthSept                    66.678      5.925  11.254  &lt; 2e-16 ***
## mnthOct                     75.834      4.950  15.319  &lt; 2e-16 ***
## mnthNov                     60.310      4.610  13.083  &lt; 2e-16 ***
## mnthDec                     46.458      4.271  10.878  &lt; 2e-16 ***
## hr1                        -14.579      5.699  -2.558 0.010536 *  
## hr2                        -21.579      5.733  -3.764 0.000168 ***
## hr3                        -31.141      5.778  -5.389 7.26e-08 ***
## hr4                        -36.908      5.802  -6.361 2.11e-10 ***
## hr5                        -24.135      5.737  -4.207 2.61e-05 ***
## hr6                         20.600      5.704   3.612 0.000306 ***
## hr7                        120.093      5.693  21.095  &lt; 2e-16 ***
## hr8                        223.662      5.690  39.310  &lt; 2e-16 ***
## hr9                        120.582      5.693  21.182  &lt; 2e-16 ***
## hr10                        83.801      5.705  14.689  &lt; 2e-16 ***
## hr11                       105.423      5.722  18.424  &lt; 2e-16 ***
## hr12                       137.284      5.740  23.916  &lt; 2e-16 ***
## hr13                       136.036      5.760  23.617  &lt; 2e-16 ***
## hr14                       126.636      5.776  21.923  &lt; 2e-16 ***
## hr15                       132.087      5.780  22.852  &lt; 2e-16 ***
## hr16                       178.521      5.772  30.927  &lt; 2e-16 ***
## hr17                       296.267      5.749  51.537  &lt; 2e-16 ***
## hr18                       269.441      5.736  46.976  &lt; 2e-16 ***
## hr19                       186.256      5.714  32.596  &lt; 2e-16 ***
## hr20                       125.549      5.704  22.012  &lt; 2e-16 ***
## hr21                        87.554      5.693  15.378  &lt; 2e-16 ***
## hr22                        59.123      5.689  10.392  &lt; 2e-16 ***
## hr23                        26.838      5.688   4.719 2.41e-06 ***
## workingday                   1.270      1.784   0.711 0.476810    
## temp                       157.209     10.261  15.321  &lt; 2e-16 ***
## weathersitcloudy/misty     -12.890      1.964  -6.562 5.60e-11 ***
## weathersitlight rain/snow  -66.494      2.965 -22.425  &lt; 2e-16 ***
## weathersitheavy rain/snow -109.745     76.667  -1.431 0.152341    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 76.5 on 8605 degrees of freedom
## Multiple R-squared:  0.6745, Adjusted R-squared:  0.6731 
## F-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>In <code>mod.lm</code>, the first level of hr (0) and mnth (January) are treated as
baseline values, meaning no coefficient estimates are provided for them.
Implicitly, their coefficient estimates are zero, and all other levels
are measured relative to these baselines. For instance, the February
coefficient of <span class="math inline">\(6.845\)</span> indicates that, holding all other variables
constant, there are on average about 7 more riders in February than in
January. Similarly, there are about 16.5 more riders in March compared
to January.</p>
<p>However, what if we were to recode the <strong>hr</strong> and <strong>mnth</strong> variables?</p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="demonstration-2-poisson-versus-linear-regression.html#cb201-1" tabindex="-1"></a><span class="fu">contrasts</span>(Bikeshare<span class="sc">$</span>hr) <span class="ot">=</span> <span class="fu">contr.sum</span>(<span class="dv">24</span>)</span>
<span id="cb201-2"><a href="demonstration-2-poisson-versus-linear-regression.html#cb201-2" tabindex="-1"></a><span class="fu">contrasts</span>(Bikeshare<span class="sc">$</span>mnth) <span class="ot">=</span> <span class="fu">contr.sum</span>(<span class="dv">12</span>)</span>
<span id="cb201-3"><a href="demonstration-2-poisson-versus-linear-regression.html#cb201-3" tabindex="-1"></a>mod.lm2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(bikers <span class="sc">~</span> mnth <span class="sc">+</span> hr <span class="sc">+</span> workingday <span class="sc">+</span> temp <span class="sc">+</span> weathersit,</span>
<span id="cb201-4"><a href="demonstration-2-poisson-versus-linear-regression.html#cb201-4" tabindex="-1"></a>              <span class="at">data =</span> Bikeshare)</span>
<span id="cb201-5"><a href="demonstration-2-poisson-versus-linear-regression.html#cb201-5" tabindex="-1"></a><span class="fu">summary</span>(mod.lm2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, 
##     data = Bikeshare)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -299.00  -45.70   -6.23   41.08  425.29 
## 
## Coefficients:
##                            Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                 73.5974     5.1322  14.340  &lt; 2e-16 ***
## mnth1                      -46.0871     4.0855 -11.281  &lt; 2e-16 ***
## mnth2                      -39.2419     3.5391 -11.088  &lt; 2e-16 ***
## mnth3                      -29.5357     3.1552  -9.361  &lt; 2e-16 ***
## mnth4                       -4.6622     2.7406  -1.701  0.08895 .  
## mnth5                       26.4700     2.8508   9.285  &lt; 2e-16 ***
## mnth6                       21.7317     3.4651   6.272 3.75e-10 ***
## mnth7                       -0.7626     3.9084  -0.195  0.84530    
## mnth8                        7.1560     3.5347   2.024  0.04295 *  
## mnth9                       20.5912     3.0456   6.761 1.46e-11 ***
## mnth10                      29.7472     2.6995  11.019  &lt; 2e-16 ***
## mnth11                      14.2229     2.8604   4.972 6.74e-07 ***
## hr1                        -96.1420     3.9554 -24.307  &lt; 2e-16 ***
## hr2                       -110.7213     3.9662 -27.916  &lt; 2e-16 ***
## hr3                       -117.7212     4.0165 -29.310  &lt; 2e-16 ***
## hr4                       -127.2828     4.0808 -31.191  &lt; 2e-16 ***
## hr5                       -133.0495     4.1168 -32.319  &lt; 2e-16 ***
## hr6                       -120.2775     4.0370 -29.794  &lt; 2e-16 ***
## hr7                        -75.5424     3.9916 -18.925  &lt; 2e-16 ***
## hr8                         23.9511     3.9686   6.035 1.65e-09 ***
## hr9                        127.5199     3.9500  32.284  &lt; 2e-16 ***
## hr10                        24.4399     3.9360   6.209 5.57e-10 ***
## hr11                       -12.3407     3.9361  -3.135  0.00172 ** 
## hr12                         9.2814     3.9447   2.353  0.01865 *  
## hr13                        41.1417     3.9571  10.397  &lt; 2e-16 ***
## hr14                        39.8939     3.9750  10.036  &lt; 2e-16 ***
## hr15                        30.4940     3.9910   7.641 2.39e-14 ***
## hr16                        35.9445     3.9949   8.998  &lt; 2e-16 ***
## hr17                        82.3786     3.9883  20.655  &lt; 2e-16 ***
## hr18                       200.1249     3.9638  50.488  &lt; 2e-16 ***
## hr19                       173.2989     3.9561  43.806  &lt; 2e-16 ***
## hr20                        90.1138     3.9400  22.872  &lt; 2e-16 ***
## hr21                        29.4071     3.9362   7.471 8.74e-14 ***
## hr22                        -8.5883     3.9332  -2.184  0.02902 *  
## hr23                       -37.0194     3.9344  -9.409  &lt; 2e-16 ***
## workingday                   1.2696     1.7845   0.711  0.47681    
## temp                       157.2094    10.2612  15.321  &lt; 2e-16 ***
## weathersitcloudy/misty     -12.8903     1.9643  -6.562 5.60e-11 ***
## weathersitlight rain/snow  -66.4944     2.9652 -22.425  &lt; 2e-16 ***
## weathersitheavy rain/snow -109.7446    76.6674  -1.431  0.15234    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 76.5 on 8605 degrees of freedom
## Multiple R-squared:  0.6745, Adjusted R-squared:  0.6731 
## F-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>What is the difference between the two codings? In <code>mod.lm2</code>, a
coefficient estimate is reported for all but the last level of <strong>hr</strong>
and <strong>mnth</strong>. Importantly, in <code>mod.lm2</code>, the coefficient estimate for
the last level of <strong>mnth</strong> is not zero; instead, it equals the negative
of the sum of the coefficient estimates for all the other levels.
Similarly, in <code>mod.lm2</code>, the coefficient estimate for the last level of
<strong>hr</strong> is the negative of the sum of the coefficient estimates for all
the other levels. This means the coefficients of hr and <strong>mnth</strong> in
<code>mod.lm2</code> will always sum to zero and can be interpreted as the
difference from the mean level. For example, the coefficient for January
of <span class="math inline">\(-46.087\)</span> indicates that, holding all other variables constant, there
are typically 46 fewer riders in January compared to the yearly average.</p>
<p>The predictions from either of the two models is identical, as confirmed
below. However, the choice of coding is crucial for correct
interpretation of the model results.</p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="demonstration-2-poisson-versus-linear-regression.html#cb203-1" tabindex="-1"></a><span class="fu">sum</span>((<span class="fu">predict</span>(mod.lm) <span class="sc">-</span> <span class="fu">predict</span>(mod.lm2))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1.426274e-18</code></pre>
<p>The sum of squared differences is zero. We can also see this using the
<code>all.equal()</code> function:</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="demonstration-2-poisson-versus-linear-regression.html#cb205-1" tabindex="-1"></a><span class="fu">all.equal</span>(<span class="fu">predict</span>(mod.lm), <span class="fu">predict</span>(mod.lm2))</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
<div id="plotting-coefficient-estimates" class="section level3 unnumbered hasAnchor">
<h3>Plotting Coefficient Estimates<a href="demonstration-2-poisson-versus-linear-regression.html#plotting-coefficient-estimates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Letâs plot the coefficient estimates for the variable <strong>mnth</strong>.</p>
<p>First we obtain the coefficients for January through November from the
<code>mod.lm2</code> object. The coefficient for December must be explicitly
computed as the negative sum of all the other months.</p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="demonstration-2-poisson-versus-linear-regression.html#cb207-1" tabindex="-1"></a>coef.months <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">coef</span>(mod.lm2)[<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>], <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">coef</span>(mod.lm2)[<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>]))</span></code></pre></div>
<p>To make the plot, we manually label the <span class="math inline">\(x\)</span>-axis with the names of the
months.</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="demonstration-2-poisson-versus-linear-regression.html#cb208-1" tabindex="-1"></a><span class="fu">plot</span>(coef.months, <span class="at">xlab =</span> <span class="st">&quot;Month&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Coefficient&quot;</span>,</span>
<span id="cb208-2"><a href="demonstration-2-poisson-versus-linear-regression.html#cb208-2" tabindex="-1"></a>    <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">type =</span> <span class="st">&quot;o&quot;</span>)</span>
<span id="cb208-3"><a href="demonstration-2-poisson-versus-linear-regression.html#cb208-3" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">at =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>, <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;J&quot;</span>, <span class="st">&quot;F&quot;</span>, <span class="st">&quot;M&quot;</span>, <span class="st">&quot;A&quot;</span>,</span>
<span id="cb208-4"><a href="demonstration-2-poisson-versus-linear-regression.html#cb208-4" tabindex="-1"></a>    <span class="st">&quot;M&quot;</span>, <span class="st">&quot;J&quot;</span>, <span class="st">&quot;J&quot;</span>, <span class="st">&quot;A&quot;</span>, <span class="st">&quot;S&quot;</span>, <span class="st">&quot;O&quot;</span>, <span class="st">&quot;N&quot;</span>, <span class="st">&quot;D&quot;</span>))</span></code></pre></div>
<p><img src="03-S03-D2_files/figure-html/chunk41-1.png" width="672" /></p>
<p>Now letâs do the same for the variable <strong>mhr</strong>.</p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="demonstration-2-poisson-versus-linear-regression.html#cb209-1" tabindex="-1"></a>coef.hours <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">coef</span>(mod.lm2)[<span class="dv">13</span><span class="sc">:</span><span class="dv">35</span>], <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">coef</span>(mod.lm2)[<span class="dv">13</span><span class="sc">:</span><span class="dv">35</span>]))</span>
<span id="cb209-2"><a href="demonstration-2-poisson-versus-linear-regression.html#cb209-2" tabindex="-1"></a></span>
<span id="cb209-3"><a href="demonstration-2-poisson-versus-linear-regression.html#cb209-3" tabindex="-1"></a><span class="fu">plot</span>(coef.hours, <span class="at">xlab =</span> <span class="st">&quot;Hour&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Coefficient&quot;</span>,</span>
<span id="cb209-4"><a href="demonstration-2-poisson-versus-linear-regression.html#cb209-4" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">type =</span> <span class="st">&quot;o&quot;</span>)</span></code></pre></div>
<p><img src="03-S03-D2_files/figure-html/chunk42-1.png" width="672" /></p>
<div class="question">
<p>What do these two plots show?</p>
</div>
</div>
</div>
<div id="the-poisson-model" class="section level2 unnumbered hasAnchor">
<h2>The Poisson Model<a href="demonstration-2-poisson-versus-linear-regression.html#the-poisson-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now letâs fit a Poisson model that is more appropriate given that we are
dealing with count data. The approach to fitting the model is very
similar to that of logistic regression except that we use the argument
<code>family = poisson</code>.</p>
<p>As with the linear model, the purpose is to predict the number of bikers
(bikers) using month (mnth), hour (hr), whether itâs a working day
(workingday), temperature (temp), and weather situation (weathersit).</p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="demonstration-2-poisson-versus-linear-regression.html#cb210-1" tabindex="-1"></a>mod.pois <span class="ot">&lt;-</span> <span class="fu">glm</span>(bikers <span class="sc">~</span> mnth <span class="sc">+</span> hr <span class="sc">+</span> workingday <span class="sc">+</span> temp <span class="sc">+</span> weathersit,</span>
<span id="cb210-2"><a href="demonstration-2-poisson-versus-linear-regression.html#cb210-2" tabindex="-1"></a>                <span class="at">data =</span> Bikeshare, <span class="at">family =</span> poisson)</span>
<span id="cb210-3"><a href="demonstration-2-poisson-versus-linear-regression.html#cb210-3" tabindex="-1"></a></span>
<span id="cb210-4"><a href="demonstration-2-poisson-versus-linear-regression.html#cb210-4" tabindex="-1"></a><span class="fu">summary</span>(mod.pois)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, 
##     family = poisson, data = Bikeshare)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -20.7574   -3.3441   -0.6549    2.6999   21.9628  
## 
## Coefficients:
##                            Estimate Std. Error  z value Pr(&gt;|z|)    
## (Intercept)                4.118245   0.006021  683.964  &lt; 2e-16 ***
## mnth1                     -0.670170   0.005907 -113.445  &lt; 2e-16 ***
## mnth2                     -0.444124   0.004860  -91.379  &lt; 2e-16 ***
## mnth3                     -0.293733   0.004144  -70.886  &lt; 2e-16 ***
## mnth4                      0.021523   0.003125    6.888 5.66e-12 ***
## mnth5                      0.240471   0.002916   82.462  &lt; 2e-16 ***
## mnth6                      0.223235   0.003554   62.818  &lt; 2e-16 ***
## mnth7                      0.103617   0.004125   25.121  &lt; 2e-16 ***
## mnth8                      0.151171   0.003662   41.281  &lt; 2e-16 ***
## mnth9                      0.233493   0.003102   75.281  &lt; 2e-16 ***
## mnth10                     0.267573   0.002785   96.091  &lt; 2e-16 ***
## mnth11                     0.150264   0.003180   47.248  &lt; 2e-16 ***
## hr1                       -0.754386   0.007879  -95.744  &lt; 2e-16 ***
## hr2                       -1.225979   0.009953 -123.173  &lt; 2e-16 ***
## hr3                       -1.563147   0.011869 -131.702  &lt; 2e-16 ***
## hr4                       -2.198304   0.016424 -133.846  &lt; 2e-16 ***
## hr5                       -2.830484   0.022538 -125.586  &lt; 2e-16 ***
## hr6                       -1.814657   0.013464 -134.775  &lt; 2e-16 ***
## hr7                       -0.429888   0.006896  -62.341  &lt; 2e-16 ***
## hr8                        0.575181   0.004406  130.544  &lt; 2e-16 ***
## hr9                        1.076927   0.003563  302.220  &lt; 2e-16 ***
## hr10                       0.581769   0.004286  135.727  &lt; 2e-16 ***
## hr11                       0.336852   0.004720   71.372  &lt; 2e-16 ***
## hr12                       0.494121   0.004392  112.494  &lt; 2e-16 ***
## hr13                       0.679642   0.004069  167.040  &lt; 2e-16 ***
## hr14                       0.673565   0.004089  164.722  &lt; 2e-16 ***
## hr15                       0.624910   0.004178  149.570  &lt; 2e-16 ***
## hr16                       0.653763   0.004132  158.205  &lt; 2e-16 ***
## hr17                       0.874301   0.003784  231.040  &lt; 2e-16 ***
## hr18                       1.294635   0.003254  397.848  &lt; 2e-16 ***
## hr19                       1.212281   0.003321  365.084  &lt; 2e-16 ***
## hr20                       0.914022   0.003700  247.065  &lt; 2e-16 ***
## hr21                       0.616201   0.004191  147.045  &lt; 2e-16 ***
## hr22                       0.364181   0.004659   78.173  &lt; 2e-16 ***
## hr23                       0.117493   0.005225   22.488  &lt; 2e-16 ***
## workingday                 0.014665   0.001955    7.502 6.27e-14 ***
## temp                       0.785292   0.011475   68.434  &lt; 2e-16 ***
## weathersitcloudy/misty    -0.075231   0.002179  -34.528  &lt; 2e-16 ***
## weathersitlight rain/snow -0.575800   0.004058 -141.905  &lt; 2e-16 ***
## weathersitheavy rain/snow -0.926287   0.166782   -5.554 2.79e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 1052921  on 8644  degrees of freedom
## Residual deviance:  228041  on 8605  degrees of freedom
## AIC: 281159
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Below is a table which summarises what the coefficients tell us:</p>
<table style="width:96%;">
<colgroup>
<col width="22%" />
<col width="73%" />
</colgroup>
<tbody>
<tr class="odd">
<td>intercept</td>
<td>baseline log count of bikers when all predictors
are at their reference levels</td>
</tr>
<tr class="even">
<td>mnth</td>
<td><p>effects of different months (1-11) relative to the
reference month (mnth 12 - December)</p>
<p>For example, the <strong>estimate for January (mnth1)</strong>
is <span class="math inline">\(-0.670170\)</span>. This tells us that the log count
of bikers in January is lower by approximately
0.67 units compared to the reference month
(December).</p>
<p>To facilitate interpretation, we exponentiate the
coefficient: <code>exp(-0.670170) â 0.5116</code>. Therefore,
the expected number of bikers in January is about
<span class="math inline">\(51\%\)</span> that of December. This indicates a decrease
of approximately
<span class="math inline">\(0.511 6-1 = 0.4884(100) = 48.84\%\)</span> compared to
December, holding all other variables constant.</p></td>
</tr>
<tr class="odd">
<td>hr</td>
<td><p>effects of different hours (1-23) relative to the
reference hour (hr24)</p>
<p>For example, the <strong>estimate for 8:00 (hr8)</strong> is
<span class="math inline">\(0.575181\)</span>. This tells us that the log count of
bikers at 8 in the morning is higher by
approximately 0.58 units compared to the reference
hour (midnight). We exponentiate the coefficient:
<code>exp(0.575181) â 1.777452</code>. Therefore, the
expected number of bikers at 8 in the morning is
about <span class="math inline">\(1 .777-1 = 0.77(100) = 77.7\%\)</span> higher than
the number of bikers at midnight, holding all
other variables constant.</p></td>
</tr>
<tr class="even">
<td>workingday</td>
<td><p>since the coefficient is positive, it suggests
that there are more bikers on working days</p>
<p>We exponentiate the coefficient:
<code>exp(0.014665) â 1.014773</code>. Therefore, the
expected number of bikers on working day is about
<span class="math inline">\(1.01 4773-1 = 0.015(100) = 1.5\%\)</span> higher than the
number of bikers during a non-working day, holding
all other variables constant.</p></td>
</tr>
<tr class="odd">
<td>temp</td>
<td><p>since the coefficient is positive, it suggests
that higher temperatures are associated with more
bikers</p>
<p>We exponentiate the coefficient:
<code>exp(0.785292) â 2.193047</code>.</p>
<p>Therefore, for each one-unit increase in the
normalised temperature, the expected number of
bikers is about
<span class="math inline">\(2.193047- 1 = 1.193047(100) = 119.3\%\)</span> higher,
holding all other variables constant</p></td>
</tr>
<tr class="even">
<td>weathersit</td>
<td><p>indicates the effect of different weather
conditions on the number of bikers relative to the
reference (clear weather). Since the coefficients
are negative, this suggests that adverse weather
conditions (e.g., heavy rain/snow) significantly
reduce the number of bikers</p>
<ul>
<li>For cloudy/misty weather, the expected number
of bikers is
<ul>
<li><code>exp(-0.075231) â 0.9275</code> times the number
of bikers in clear weather, indicating a
decrease of about 7.25%.</li>
</ul></li>
<li>For light rain/snow, the expected number of
bikers is
<ul>
<li><code>exp(-0.575800) â 0.5621</code> times the number
of bikers in clear weather, indicating a
decrease of about 43.79%.</li>
</ul></li>
<li>For heavy rain/snow, the expected number of
bikers is
<ul>
<li><code>exp(-0.926287) â 0.3957</code> times the number
of bikers in clear weather, indicating a
decrease of about 60.43%.</li>
</ul></li>
</ul></td>
</tr>
</tbody>
</table>
<p>Letâs plot the coefficients associated with the <strong>mnth</strong> variable.</p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="demonstration-2-poisson-versus-linear-regression.html#cb212-1" tabindex="-1"></a>coef.mnth <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">coef</span>(mod.pois)[<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>], <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">coef</span>(mod.pois)[<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>]))</span>
<span id="cb212-2"><a href="demonstration-2-poisson-versus-linear-regression.html#cb212-2" tabindex="-1"></a></span>
<span id="cb212-3"><a href="demonstration-2-poisson-versus-linear-regression.html#cb212-3" tabindex="-1"></a><span class="fu">plot</span>(coef.mnth, <span class="at">xlab =</span> <span class="st">&quot;Month&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Coefficient&quot;</span>,</span>
<span id="cb212-4"><a href="demonstration-2-poisson-versus-linear-regression.html#cb212-4" tabindex="-1"></a>     <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">type =</span> <span class="st">&quot;o&quot;</span>)</span>
<span id="cb212-5"><a href="demonstration-2-poisson-versus-linear-regression.html#cb212-5" tabindex="-1"></a></span>
<span id="cb212-6"><a href="demonstration-2-poisson-versus-linear-regression.html#cb212-6" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">at =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>, </span>
<span id="cb212-7"><a href="demonstration-2-poisson-versus-linear-regression.html#cb212-7" tabindex="-1"></a>     <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;J&quot;</span>, <span class="st">&quot;F&quot;</span>, <span class="st">&quot;M&quot;</span>, <span class="st">&quot;A&quot;</span>, <span class="st">&quot;M&quot;</span>, <span class="st">&quot;J&quot;</span>, <span class="st">&quot;J&quot;</span>, <span class="st">&quot;A&quot;</span>, <span class="st">&quot;S&quot;</span>, <span class="st">&quot;O&quot;</span>, <span class="st">&quot;N&quot;</span>, <span class="st">&quot;D&quot;</span>))</span></code></pre></div>
<p><img src="03-S03-D2_files/figure-html/chunk44-1.png" width="672" /></p>
<p>And the coefficients associated with the <strong>hr</strong> variable.</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="demonstration-2-poisson-versus-linear-regression.html#cb213-1" tabindex="-1"></a>coef.hours <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">coef</span>(mod.pois)[<span class="dv">13</span><span class="sc">:</span><span class="dv">35</span>], <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">coef</span>(mod.pois)[<span class="dv">13</span><span class="sc">:</span><span class="dv">35</span>]))</span>
<span id="cb213-2"><a href="demonstration-2-poisson-versus-linear-regression.html#cb213-2" tabindex="-1"></a></span>
<span id="cb213-3"><a href="demonstration-2-poisson-versus-linear-regression.html#cb213-3" tabindex="-1"></a><span class="fu">plot</span>(coef.hours, <span class="at">xlab =</span> <span class="st">&quot;Hour&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Coefficient&quot;</span>,</span>
<span id="cb213-4"><a href="demonstration-2-poisson-versus-linear-regression.html#cb213-4" tabindex="-1"></a>    <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">type =</span> <span class="st">&quot;o&quot;</span>)</span></code></pre></div>
<p><img src="03-S03-D2_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<div class="question">
<p>What do these plots suggest?</p>
</div>
</div>
<div id="linear-versus-poisson-regression" class="section level2 unnumbered hasAnchor">
<h2>Linear versus Poisson Regression<a href="demonstration-2-poisson-versus-linear-regression.html#linear-versus-poisson-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Letâs plot the fitted values from the Poisson model and compare them to
those of the linear model. For the Poisson predictions, we must use the
argument <code>type = "response"</code> to specify that we want <code>R</code> to output
<span class="math inline">\(\exp(\hat\beta_0 + \hat\beta_1 X_1 + \ldots +\hat\beta_p X_p)\)</span> rather
than <span class="math inline">\(\hat\beta_0 + \hat\beta_1 X_1 + \ldots + \hat\beta_p X_p\)</span>, which
it will output by default.</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="demonstration-2-poisson-versus-linear-regression.html#cb214-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict</span>(mod.lm2), <span class="fu">predict</span>(mod.pois, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>))</span>
<span id="cb214-2"><a href="demonstration-2-poisson-versus-linear-regression.html#cb214-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">col =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="03-S03-D2_files/figure-html/chunk45-1.png" width="672" /></p>
<p>As you can see, the predictions from the Poisson regression model are
correlated with those from the linear model; however, the former are
non-negative. As a result the Poisson regression predictions tend to be
larger than those from the linear model for either very low or very high
levels of ridership.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="demonstration-1-classification-problems.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="practical-predicting-a-companys-bankruptcy.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
