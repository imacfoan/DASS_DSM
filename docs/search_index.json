[["index.html", "Data Science Modelling About", " Data Science Modelling Dr. Ioana Macoveciuc About Welcome to SOST70033 Data Science Modelling! This notebook will host the materials for all R practicals and demonstrations for this course unit. The notebook follows the same section-based structure as the learning materials on Blackboard. To access this notebook, you can bookmark it like any other website in your favourite web browser. For each section, you will have at least one practical to complete and each of these can be accessed by using the sidebar menu on the left hand side of your screen. Clicking on the headings in each section will expand the menu and each task can be accessed individually. The sidebar menu can be toggled on and off by clicking on the Toggle Sidebar button. Other customisation options include changing the font, font size, and the appearance. You also have a handy search button. This notebook is also designed to work well on tablet and mobile devices; to enhance your experience on these devices, it is recommended that you hide the sidebar menu and navigate across sections and subsections using the right and left arrows. The code, as well as the output and answers are provided at the end of each section. The R code can be copied and pasted directly in your R console or script by clicking on the following icon: 1: Before beginning, it is recommended that you create a RStudio project for this course and work through the exercises and tasks in each section using this project. 2: You should write and save your answers to the exercises and tasks in R scripts. You should have at least one R script for each course section. 3: The recommended approach for a ‘clean’ working directory is to place all the data files you plan to use in a separate folder (e.g. a folder called data) within your R project working directory. You should always use simple names that allow you easy access to the contents when you want to either explore the folder on your machine or specify the path to these folders in R. 4: To build a robust knowledge basis and adequately develop your practical programming skills, it is absolutely essential that you first attempt all tasks and exercises on your own before comparing your answers with those provided in this notebook. "],["overview.html", "Overview", " Overview Section 1: Introduction to Data Science - The Basics of Statistical Learning This section is comprised of a demonstration and two practicals. The two practicals will make use of exercises adapted from the core textbook for this course: James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd ed. New York: Springer. https://www.statlearning.com/ Learning Outcomes: appreciate the importance of the mean squared error; indexing using base R; creating scatterplot matrices; creating new variables; transforming existing variables; using functionals; ‘calling’ on masked functions from specific packages; translating base R code to tidyverse and vice versa. In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. You can access the R documentation in the Help tab using? (e.g. ?read.csv) Function Description Package read.csv() read csv files base R read_csv() read csv files tidyverse co l umn_to_rownames() convert column to row names tidyverse rownames() obtain names of rows base R summary() obtain summary statistics base R summarise() object summaries tidyverse (dplyr) group_by() group by one or more variables tidyverse (dplyr) pairs() produce a matrix of scatterplots base R plot() create a plot base R ggplot() generic function for creating a plot tidyverse (ggplot2) mutate() create, modify, and delete columns tidyverse (dplyr) if_else() condition-based function tidyverse (dplyr) as_factor() create factor using existing levels tidyverse (forcats) par() set graphical parameters base R mfrow() par() parameter base R slice_min() and slice_max() index rows by location (smallest and largest values of a variable respectively) tidyverse (dplyr) sapply() applying a function over list or vector base R select() keep or drop columns tidyverse (dplyr) note that this function is also available through the MASS package (we will not cover this in this section) pivot_longer() lengthen data tidyverse (tidyr) where() selection helper tidyverse median(), mean(), sd() median, mean, standard deviation base R "],["demonstration-1.html", "Demonstration 1 A more in-depth consideration of model accuracy The Simulation", " Demonstration 1 A more in-depth consideration of model accuracy So, I mentioned earlier that statistical modeling allows us to explain and dig deeper into understanding the nature of a process of a phenomenon of interest and that we can mathematically describe this phenomenon as an unknown function \\(f\\). In its related data, we annotate the information that can explain the problem’s behaviour as \\(x\\) (this could be a single value or a vector or matrix of values or something more complex) and the results of its behaviour as \\(y\\) (this could also be a single value or something more complex). Mathematically we present this as: \\[y_i = f(x_i)\\] And of course, let’s not forget about the variation of the \\(f\\)’s behaviour: \\[y_i = f(x_i) + \\epsilon_i\\] The standard approach for the error/noise is to adopt the following distribution structure \\(\\epsilon \\sim N(0,\\sigma)\\), meaning that the \\(\\epsilon\\) value is considered to be normally distributed with a mean of \\(0\\) and standard deviation \\(\\sigma\\). Remember that this implies that the negative and positive impacts from the noise are considered equally likely, and that small errors are much more likely than extreme ones. To evaluate the performance of a statistical model on a given data set, we “observe” the discrepancies between the predicted response values for given observations obtained by the chosen statistical model (\\(\\hat{f}(x_i)\\)) and the true response values for these observations (\\(y_i\\)). As I already mentioned, the most commonly used performance measure for regression problems is the mean squared error (MSE): \\[MSE = \\frac{1}{n} \\sum^{n}_{i=1}(y_i - \\hat{f}(x_i))^2\\] The MSE above is computed using the training data, used to fit the model, and as such it would be more correct to refer to it as the training MSE. But we have already discussed that we are not really interested how well the model “works” on the training data, ie. \\(\\hat{f}(x_i) \\approx y_i\\). We are more interested in the accuracy of the predictions \\(\\hat{f}(x_0)\\) that are obtained when we apply the model to previously unseen test data \\((x_0, y_0)\\), ie. \\(\\hat{f}(x_0) \\approx y_0\\). In other words, we want to chose the model with the lowest \\(test\\) \\(MSE\\) and to do so we need a large enough number of observations in the test data to calculate the mean square prediction error for the test observations (\\(x_0\\), \\(y_0\\)), to which we can refer to as the \\(test\\) \\(MSE\\). \\[mean(y_0 - \\hat{f}(x_0))^2\\] In statistics nothing is black and white. In other words, nothing is straightforward and there are many considerations one needs to take into account when applying statistical modelling. The same applies in this situation. We need to realise that when a given model yields a small training MSE but a large test MSE, we are said to be overfitting the data. The statistical model is too ‘preoccupied’ to find patterns in the training data and consequently is modelling the patterns that are caused by random effects, rather than by true features of the unknown function \\(f\\). When the model overfits the training data, the test MSE will be large because the modelled features that the model identifies in the training data just do not exist in the test data. Saying that, regardless of overfitting occurring or not, we expect the training MSE to be smaller than the test MSE, as most of the statistical models either directly or indirectly seek to minimise the training MSE. We need to be aware that the chosen model needs to be flexible and not rigid and glued to the training data. MSE is simple to calculate and yet, despite its simplicity, it can provide us with a vital insight into modelling. It consists of two intrinsic components that can provide greater enlightenment about how the model works: variance: degree to which \\(\\hat{f}\\) would differ if we estimated it using a different training dataset (ideally, it should not vary greatly). bias: average difference between the estimator \\(\\hat{y}\\) and true value \\(y\\). Mathematically we write bias as: \\[E[\\hat{y} – y]\\] As it is not squared difference, it can be either positive or negative. Positive or negative bias implies that the model is over or under “predicting”, while the value of zero would indicate that the model is likely to predict too much as it is to predict too little. The latter implies that the model can be completely wrong in its prediction and still provide us with the bias of zero. This implies that bias on its own provides little information about how correct the model is in its prediction. Remember that \\(y = f(x) + \\epsilon\\) and therefore \\(\\hat{f}\\) is not directly approximating \\(f\\). \\(\\hat{f}\\) models \\(y\\) that includes the noise. It can be challenging and in some cases even impossible to meaningfully capture the behaviour of \\(f\\) itself when the noise term is very large. We have discussed earlier that we assess model accuracy using MSE which is calculated by: obtaining the error (i.e. discrepancy between \\(\\hat{f}(x_i)\\) and \\(y_i\\)) squaring this value (making negative into the positive same, and greater error gets more severe penalty) then averaging these results The mean of the squared error is the same as the expectation\\(^*\\) of our squared error so we can go ahead and simplify this a slightly: \\[MSE=E[(y-\\hat{f}(x))^2]\\] Now, we can break this further and write it as: \\[MSE = E[(f(x)+ \\epsilon - \\hat{f}(x))^2]\\] Knowing that computing the expectation of adding two random variables is the same as computing the expectation of each random variable and then adding them up: \\[E[X+Y]=E[X] +E[Y]\\] and recalling that \\(\\sigma^2\\) represent the variance of \\(\\epsilon\\), where the variance is calculated as: \\[E[X^2]-E[X]^2,\\] and therefore: \\[Var(\\epsilon) = \\sigma^2 = E[\\epsilon^2] - E[\\epsilon]^2,\\] with \\(\\epsilon \\sim N(0,\\sigma)\\) \\[E[\\epsilon]^2=\\mu^2=0^2=0\\] we get: \\[E[\\epsilon^2] = \\sigma^2\\] This helps us to rearranging MSE further and calculate it as: \\[MSE=σ^2+E[−2f(x)\\hat{f}(x)+f(x)^2+\\hat{f}(x)^2]\\] where \\(\\sigma^2\\) is the variance of the noise (i.e. \\(\\epsilon\\)). Therefore, the variance of the noise in data is an irreducible part of the MSE. Regardless of how good the model is, it can never reduce the MSE to being less than the variance related to the noise (i.e. error). This error represents the lack of information in data used to adequately explain everything that can be known about the phenomena being modelled. We should not look at it as a nuisance, as it can often guide us to further explore the problem and look into other factors that might be related to it. Knowing that: \\[Var(X) = E[X^2] - E[X]^2\\] we can apply further transformation and break MSE into: \\[MSE = \\sigma^{2}+Var[f(x)-\\hat{f}(x)]+E[f(x)-\\hat{f}(x)]^2\\] The term \\(Var[f(x)-\\hat{f}(x)]\\) is the variance in the model predictions from the true output values and the last term \\(E[f(x)-\\hat{f}(x)]^2\\) is just the bias squared. We mentioned earlier that that unlike variance, bias can be positive or negative, so we square this value in order to make sure it is always positive. With this in mind, we realise that MSE consists of: model variance model bias and irreducible error \\[\\text{Mean Squared Error}=\\text{Model Variance} + \\text{Model Bias}^2 + \\text{Irreducible Error}\\] We come to the conclusion that in order to minimise the expected test error, we need to select a statistical model that simultaneously achieves low variance and low bias. Note that in practice we will never know what the variance \\(\\sigma^2\\) of the error \\(\\epsilon\\) is, and therefore we will not be able to determine the variance and the bias of the model. However, since \\(\\sigma^2\\) is constant, we have to decrease either bias or variance to improve the model. Testing the model using the test data and observing its bias and variance can help us address some important issues, allowing us to reason with the model. If the model fails to find the \\(f\\) in data and is systematically over or under predicting, this will indicate underfitting and it will be reflected through high bias. However, high variance when working with test data indicates the issue of overfitting. What happens is that the model has learnt the training data really well and is too close to the data, so much so that it starts to mistake the \\(f(x) + \\epsilon\\) for true \\(f(x)\\). The Simulation To better understand these concepts, let us run a small simulation study. We will: simulate a function \\(f\\) apply the error, i.e. noise sampled from a distribution with a known variance To make it very simple and illustrative we will use a linear function \\(f(x) = 3 + 2x\\) to simulate response \\(y\\) with the error \\(e\\thicksim N(\\mu =0, \\sigma^2 =4)\\), where \\(x\\) is going to be a sequence of numbers between \\(0\\) and \\(10\\) in steps of \\(0.1\\). We will examine the simulations for the models that over and under estimate the true \\(f\\), and since it is a linear function we will not have a problem identifying using simple linear regression modelling. Let’s start with a simulation in which we will model the true function with \\(\\hat{f}_{1} = 4 + 2x\\) and \\(\\hat{f}_{2} = 1 + 2x\\). set.seed(123) ## set the seed of R‘s random number generator # simulate function f(x) = 3 + 2x f &lt;- function(x){ 3 + 2 * x } # generate vector X x &lt;- seq(0, 10, by = 0.05) # the error term coming from N(mean = 0, variance = 4) e &lt;- rnorm(length(x), mean = 0, sd = 2) # simulate the response vector Y y &lt;- f(x) + e # plot the simulated data plot(x, y, cex = 0.75, pch = 16, main = &quot;Simulation: 1&quot;) abline(3, 2, col =&quot;gray&quot;, lwd = 2, lty = 1) # model fitted to simulated data f_hat_1&lt;- function(x){ 4 + 2 * x } f_hat_2 &lt;- function(x){ 1 + 2 * x } y_bar = mean(y) # average value of the response variable y f_hat_3 &lt;- function(x){ y_bar } # add the line representing the fitted model abline(1, 2, col = &quot;red&quot;, lwd = 2, lty = 2) abline(4, 2, col = &quot;blue&quot;, lwd = 2, lty = 1) abline(y_bar, 0, col = &quot;darkgreen&quot;, lwd = 2, lty = 3) legend(7.5, 10, legend=c(&quot;f_hat_1&quot;, &quot;f_hat_2&quot;, &quot;f_hat_3&quot;, &quot;f&quot;), col = c(&quot;blue&quot;, &quot;red&quot;, &quot;darkgreen&quot;, &quot;gray&quot;), lwd = c(2, 2, 2, 2), lty = c(1:3, 1), text.font = 4, bg = &#39;lightyellow&#39;) Observing the graph, we notice that \\(\\hat{f}_1\\) and \\(\\hat{f}_2\\), depicted in blue and red lines respectively, follow the data nicely, but are also systematically over (in the case of \\(\\hat{f}_1\\) and under (in the case of \\(\\hat{f}_2\\)) estimating the values. In the simple model \\(\\hat{f}_3\\), the line represents the value \\(\\bar{y}\\), which cuts the data in half. As we mentioned earlier, knowing the true function \\(f\\) and the distribution of \\(\\epsilon\\) we can calculate: - the MSE using the simulated data and the estimated model, - the model’s bias and variance which will allow for the calculation of the “theoretical” MSE. This will allow for more detailed illustration about the information contained in the model’s bias and variance. # calculate MSE from data MSE_data1 = mean((y - f_hat_1(x))^2) MSE_data2 = mean((y - f_hat_2(x))^2) MSE_data3 = mean((y - f_hat_3(x))^2) # model bias bias_1 = mean(f_hat_1(x) - f(x)) bias_2 = mean(f_hat_2(x) - f(x)) bias_3 = mean(f_hat_3(x) - f(x)) # model variance var_1 = var(f(x) - f_hat_1(x)) var_2 = var(f(x) - f_hat_2(x)) var_3 = var(f(x) - f_hat_3(x)) # calculate &#39;theoretical&#39; MSE MSE_1 = bias_1^2 + var_1 + 2^2 MSE_2 = bias_2^2 + var_2 + 2^2 MSE_3 = bias_3^2 + var_3 + 2^2 for (i in 1:1){ cat (c(&quot;==============================================&quot;,&quot;\\n&quot;)) cat (c(&quot;=============== f_hat_1 ================&quot;,&quot;\\n&quot;)) cat(c(&quot;MSE_data1 = &quot;, round(MSE_data1, 2), sep = &#39;\\n&#39;)) cat(c(&quot;bias_1 = &quot;, bias_1, sep = &#39;\\n&#39; )) cat(c(&quot;variance_1 = &quot;, round(var_1, 2), sep = &#39;\\n&#39; )) cat(c(&quot;MSE_1 = 4 + bias_1^2 + variance_1 = &quot;, MSE_1, sep = &#39;\\n&#39; )) cat(c(&quot;==============================================&quot;,&quot;\\n&quot;)) cat(c(&quot;=============== f_hat_2 ================&quot;,&quot;\\n&quot;)) cat(c(&quot;MSE_data2 = &quot;, round(MSE_data2, 2), sep = &#39;\\n&#39;)) cat(c(&quot;bias_2 = &quot;, bias_2, sep = &#39;\\n&#39; )) cat(c(&quot;variance_2 = &quot;, round(var_2, 2), sep = &#39;\\n&#39; )) cat(c(&quot;MSE_2 = 4 + bias_2^2 + variance_2 = &quot;, MSE_2, sep = &#39;\\n&#39; )) cat(c(&quot;==============================================&quot;,&quot;\\n&quot;)) cat(c(&quot;=============== f_hat_3 ================&quot;,&quot;\\n&quot;)) cat(c(&quot;average y = &quot;, round(y_bar, 2), sep = &#39;\\n&#39;)) cat(c(&quot;MSE_data3 = &quot;, round(MSE_data3, 2), sep = &#39;\\n&#39;)) cat(c(&quot;bias_3 = &quot;, round(bias_3, 2), sep = &#39;\\n&#39; )) cat(c(&quot;variance_3 = &quot;, round(var_3, 2), sep = &#39;\\n&#39; )) cat(c(&quot;MSE_3 = 4 + bias_3^2 + variance_3 = &quot;, round(MSE_3, 2), sep = &#39;\\n&#39; )) cat(c(&quot;==============================================&quot;,&quot;\\n&quot;)) } ## ============================================== ## =============== f_hat_1 ================ ## MSE_data1 = 4.61 ## bias_1 = 1 ## variance_1 = 0 ## MSE_1 = 4 + bias_1^2 + variance_1 = 5 ## ============================================== ## =============== f_hat_2 ================ ## MSE_data2 = 7.64 ## bias_2 = -2 ## variance_2 = 0 ## MSE_2 = 4 + bias_2^2 + variance_2 = 8 ## ============================================== ## =============== f_hat_3 ================ ## average y = 13 ## MSE_data3 = 36.7 ## bias_3 = 0 ## variance_3 = 33.84 ## MSE_3 = 4 + bias_3^2 + variance_3 = 37.84 ## ============================================== \\(\\hat{f}_1\\) has a positive bias because it is overestimating data points more often than it is underestimating, but as it does it so consistently in comparison to \\(f\\) that produces variance of zero. In contrast \\(\\hat{f}_2\\) has a negative bias as it is underestimating simulated data, but nonetheless it also does it consistently, resulting in zero variance with \\(f\\). Unlike in the previous two model estimates which follow the data points, \\(\\hat{f}_3\\) predicts the mean value of data, resulting in no bias since it evenly underestimates and overestimates \\(f(x)\\). However, the variation in prediction between \\(f\\) and \\(\\hat{f}_3\\) is obvious. Given that the true function \\(f\\) is linear, by applying simple regression modelling, we should be able to estimate it easily in R using the \\(lm()\\) function. # model fitted to simulated data f_hat_4&lt;- function(x){ lm(y~x) } # plot the simulated data plot(x, y, cex = 0.75, pch = 16, main = &quot;Simulation: 2&quot;) abline(3, 2, col =&quot;gray&quot;, lwd = 2, lty = 1) # add the line representing the fitted model abline(lm(y~x), col =&quot;red&quot;, lwd = 2, lty = 3) legend(7.5, 8, legend=c(&quot;f_hat_4&quot;, &quot;f&quot;), col = c(&quot;red&quot;, &quot;gray&quot;), lwd = c(2, 2), lty = c(3, 1), text.font = 4, bg = &#39;lightyellow&#39;) Since the true function \\(f\\) is a linear model it is not surprising that \\(\\hat{f}_4\\) can learn it, resulting in zero values of the model’s bias and variance. # calculate MSE from data MSE_data4 = mean((y - predict(f_hat_4(x)))^2) # model bias bias_4 = mean(predict(f_hat_4(x)) - f(x)) # model variance var_4 = var(f(x) - predict(f_hat_4(x))) # calculate &#39;theoretical&#39; MSE MSE_4 = bias_4^2 + var_4 + 2^2 for (i in 1:1){ cat (c(&quot;==============================================&quot;,&quot;\\n&quot;)) cat (c(&quot;=============== f_hat_4 ================&quot;,&quot;\\n&quot;)) cat(c(&quot;MSE_data4 = &quot;, round(MSE_data4, 2), sep = &#39;\\n&#39;)) cat(c(&quot;bias_4 = &quot;, round(bias_4, 2), sep = &#39;\\n&#39; )) cat(c(&quot;variance_4 = &quot;, round(var_4, 2), sep = &#39;\\n&#39; )) cat(c(&quot;MSE_4 = 4 + bias_4^2 + variance_4 = &quot;, round(MSE_4, 2), sep = &#39;\\n&#39; )) cat (c(&quot;==============================================&quot;,&quot;\\n&quot;)) } ## ============================================== ## =============== f_hat_4 ================ ## MSE_data4 = 3.62 ## bias_4 = 0 ## variance_4 = 0 ## MSE_4 = 4 + bias_4^2 + variance_4 = 4 ## ============================================== We realise that the \\(MSE\\) is more than just a simple error measurement. It is a tool that informs and educates the modeller about the performance of the model being used in the analysis of a problem. It is packed with information that when unwrapped can provide a greater insight into not just the fitted model, but the nature of the problem and its data. \\(^*\\) The Expectation of a Random Variable is the sum of its values weighted by their probability. For example: What is the average toss of a fair six-sided die? If the random variable is the top face of a tossed, fair six-sided die, then the probability of die landing on \\(X\\) is: \\[f(x) = \\frac{1}{6}\\] for \\(x = 1, 2,... 6\\). Therefore, the average toss, i.e. the expected value of \\(X\\) is: \\[E(X) = 1(\\frac{1}{6}) + 2(\\frac{1}{6}) + 3(\\frac{1}{6}) + 4(\\frac{1}{6}) + 5(\\frac{1}{6}) + 6(\\frac{1}{6}) = 3.5\\] Of course, we do not expect to get a fraction when tossing a die, i.e. we do not expect the toss to be 3.5, but rather an integer number between 1 to 6. So, what the expected value is really saying is what is the expected average of a large number of tosses will be. If we toss a fair, six-side die hundreds of times and calculate the average of the tosses we will not get the exact 3.5 figure, but we will expect it to be close to 3.5. This is a theoretical average, not the exact one that is realised. "],["practical-1.html", "Practical 1", " Practical 1 For the tasks below, you will require the College dataset from the core textbook (James et. al 2021). Click here to download the file: College.csv . Remember to place your data file in a separate subfolder within your R project working directory. The College dataset contains statistics for a large number of US Colleges from the 1995 issue of US News and World Report. It is a data frame with 777 observations and 18 variables. The variables are: Private : Public/private indicator Apps : Number of applications received Accept : Number of applicants accepted Enroll : Number of new students enrolled Top10perc : New students from top 10% of high school class Top25perc : New students from top 25% of high school class F.Undergrad : Number of full-time undergraduates P.Undergrad : Number of part-time undergraduates Outstate : Out-of-state tuition Room.Board : Room and board costs Books : Estimated book costs Personal : Estimated personal spending PhD : Percent of faculty with Ph.D.’s Terminal : Percent of faculty with terminal degree S.F.Ratio: Student/faculty ratio perc.alumni : Percent of alumni who donate Expend : Instructional expenditure per student Grad.Rate : Graduation rate Task 1 Import the dataset in an object called college using the tidyverse read_csv() function. If you then have a look at the contents of the data object using View(), you will notice that the first column contains the names of all of the universities in the dataset. You will also notice that it has a strange name. Actually, these data should not be treated as a variable (column) since it is just a list of university names. Task 2 Keeping the list of names in the data object, transform this column such that the university names in the column become row names. Hint: use the column_to_rownames() function from dplyr. How would have your approach to this task differed if you would have imported the dataset using base R? Try it! Task 3 Produce summary statistics for all variables in the data object. Task 4 Create a scatterplot matrix of the first three numeric variables. Task 5 Produce side by side box plots of Outstate versus Private using base R. Did this work? Why? Task 6 Using the Top10perc variable, create a new categorical variable called Elite such that universities are divided into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%. Hint: use a combination of mutate() and if_else(). Task 7 Produce side by side box plots of the new Elite variable and Outstate. How would you produce a similar plot using base R? Task 8 Use base R to produce a multipanel plot that displays histograms of the following variables: Apps, perc.alumni, S.F.Ratio, Expend. Hint: use par(mfrow=c(2,2)) to set up a 2x2 panel. Try to adjust the specifications (e.g. breaks). Task 9 Using Accept and Apps, create a new variable that describes acceptance rate. Name this variable acceptance_rate. Hint: use mutate(). Task 10 Using the acceptance_rate variable, find out which university has the lowest acceptance rate. Hint: for a tidyverse approach, you can use slice_min(). Task 11 Using the acceptance_rate variable, find out which university has the highest acceptance rate. "],["practical-2.html", "Practical 2", " Practical 2 For the tasks below, you will require the Boston dataset. This dataset is part of the MASS R package. To access the dataset, load the MASS package (install the package first, if you have not done so previously). Task 1 Install and load the MASS package. You will also require tidyverse. Does R provide any message when loading MASS? Why does this matter? Task 2 Find out more about the Boston dataset variables by accessing the R Documentation. To explore the Boston dataset, simply type the name of the data object into the console or use View() What type of data structure is the Boston dataset? What are its dimensions? How many categorical and quantitative variables are there? Task 3 Find the class of all 14 variables. Hint: use sapply. Task 4 Using a tidyverse approach, calculate the mean, median, and standard deviation of all variables of class numeric. What is the mean pupil-teacher ratio? What is the median and mean per capita crime rate? Which value do you think is more suitable to describe per capita crime rate? Task 5 Using a base R approach, create a 2x2 multipanel plot of crim versus age, dis, rad, tax and ptratio. What can you say about the relationships between age, dis, rad, tax, and crim? Task 6 Using a base R approach, create and display histograms of crim, tax and ptratio in a 1x2 multipanel plot. Set the breaks argument to 25 . What do these histograms indicate? "],["answers.html", "Answers Practical 1 Practical 2", " Answers Practical 1 For the tasks below, you will require the College dataset from the core textbook (James et. al 2021). Click here to download the file: College.csv . Remember to place your data file in a separate subfolder within your R project working directory. This data file contains 18 variables for 777 different universities and colleges in the United States. The variables are: Private : Public/private indicator Apps : Number of applications received Accept : Number of applicants accepted Enroll : Number of new students enrolled Top10perc : New students from top 10% of high school class Top25perc : New students from top 25% of high school class F.Undergrad : Number of full-time undergraduates P.Undergrad : Number of part-time undergraduates Outstate : Out-of-state tuition Room.Board : Room and board costs Books : Estimated book costs Personal : Estimated personal spending PhD : Percent of faculty with Ph.D.’s Terminal : Percent of faculty with terminal degree S.F.Ratio: Student/faculty ratio perc.alumni : Percent of alumni who donate Expend : Instructional expenditure per student Grad.Rate : Graduation rate Task 1 Import the dataset in an object called college using the tidyverse read_csv() function. # Remember to load tidyverse first library(tidyverse) college &lt;- read_csv(&quot;data/College.csv&quot;) If you have a look at the contents of the data object using View(), you will notice that the first column contains the names of all of the universities in the dataset. You will also notice that it has a strange name. Actually, these data should not be treated as a variable (column) since it is just a list of university names. Task 2 Keeping the list of names in the data object, transform this column such that the university names in the column become row names. Hint: use the column_to_rownames() function from dplyr. college &lt;- college %&gt;% column_to_rownames(var = &quot;...1&quot;) How would have your approach to this task differed if you would have imported the dataset using base R? Try it! The data file could have instead been imported using read.csv(): college &lt;- read.csv(\"data/College.csv\") Using the base R approach, the first column containing the university names would have been named “X”, as shown below using View(). Now, how would be go about transforming the contents of the first column into row names? This would require two steps. First, we assign the column contents to rows names. rownames(college) &lt;- college[, 1] If you have another look at the data object, you will see that the rows have now been renamed using the university names in the “X” column, but the column is still part of the dataset. We therefore need to tell R to delete the column. college &lt;- college[, -1] Task 3 Produce summary statistics for all variables in the data object. summary(college) ## Private Apps Accept Enroll ## Length:777 Min. : 81 Min. : 72 Min. : 35 ## Class :character 1st Qu.: 776 1st Qu.: 604 1st Qu.: 242 ## Mode :character Median : 1558 Median : 1110 Median : 434 ## Mean : 3002 Mean : 2019 Mean : 780 ## 3rd Qu.: 3624 3rd Qu.: 2424 3rd Qu.: 902 ## Max. :48094 Max. :26330 Max. :6392 ## Top10perc Top25perc F.Undergrad P.Undergrad ## Min. : 1.00 Min. : 9.0 Min. : 139 Min. : 1.0 ## 1st Qu.:15.00 1st Qu.: 41.0 1st Qu.: 992 1st Qu.: 95.0 ## Median :23.00 Median : 54.0 Median : 1707 Median : 353.0 ## Mean :27.56 Mean : 55.8 Mean : 3700 Mean : 855.3 ## 3rd Qu.:35.00 3rd Qu.: 69.0 3rd Qu.: 4005 3rd Qu.: 967.0 ## Max. :96.00 Max. :100.0 Max. :31643 Max. :21836.0 ## Outstate Room.Board Books Personal ## Min. : 2340 Min. :1780 Min. : 96.0 Min. : 250 ## 1st Qu.: 7320 1st Qu.:3597 1st Qu.: 470.0 1st Qu.: 850 ## Median : 9990 Median :4200 Median : 500.0 Median :1200 ## Mean :10441 Mean :4358 Mean : 549.4 Mean :1341 ## 3rd Qu.:12925 3rd Qu.:5050 3rd Qu.: 600.0 3rd Qu.:1700 ## Max. :21700 Max. :8124 Max. :2340.0 Max. :6800 ## PhD Terminal S.F.Ratio perc.alumni ## Min. : 8.00 Min. : 24.0 Min. : 2.50 Min. : 0.00 ## 1st Qu.: 62.00 1st Qu.: 71.0 1st Qu.:11.50 1st Qu.:13.00 ## Median : 75.00 Median : 82.0 Median :13.60 Median :21.00 ## Mean : 72.66 Mean : 79.7 Mean :14.09 Mean :22.74 ## 3rd Qu.: 85.00 3rd Qu.: 92.0 3rd Qu.:16.50 3rd Qu.:31.00 ## Max. :103.00 Max. :100.0 Max. :39.80 Max. :64.00 ## Expend Grad.Rate ## Min. : 3186 Min. : 10.00 ## 1st Qu.: 6751 1st Qu.: 53.00 ## Median : 8377 Median : 65.00 ## Mean : 9660 Mean : 65.46 ## 3rd Qu.:10830 3rd Qu.: 78.00 ## Max. :56233 Max. :118.00 Task 4 Create a scatterplot matrix of the first three numeric variables. pairs(college[,2:4]) Task 5 Produce side by side box plots of Outstate versus Private using base R. plot(college$Private, college$Outstate, xlab = &quot;Private&quot;, ylab = &quot;Outstate&quot;) Did this work? Why? Using the plot() base R function to produce a box plot would produce an error since the Private variable is of class character. Most statistical functions will not work with character vectors. Error in plot.window(...) : need finite 'xlim' values In addition: Warning messages: 1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion 2: In min(x) : no non-missing arguments to min; returning Inf 3: In max(x) : no non-missing arguments to max; returning -Inf Creating a box plot with tidyverse would work. college %&gt;% ggplot(aes(x = Private, y = Outstate)) + geom_boxplot() However, it is important to note that if a variable is not of the right class, then this might have unintended consequences for example, when building models. In this case, the Private variable must be transformed into a factor. Task 6 Using the Top10perc variable, create a new categorical variable called Elite such that universities are divided into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%. Hint: use a combination of mutate() and if_else(). college &lt;- college %&gt;% mutate(Elite = if_else(Top10perc &gt; 50, &quot;Yes&quot;, &quot;No&quot;), Elite = as_factor(Elite)) #do not forget the factor transformation step (categorical variables are factors in R) Task 7 Produce side by side box plots of the new Elite variable and Outstate. college %&gt;% ggplot(aes(x = Elite, y = Outstate)) + geom_boxplot() How would you produce a similar plot using base R? plot(college$Elite, college$Outstate, xlab = \"Elite\", ylab = \"Outstate\") Task 8 Use base R to produce a multipanel plot that displays histograms of the following variables: Apps, perc.alumni, S.F.Ratio, Expend. Hint: use par(mfrow=c(2,2)) to set up a 2x2 panel. Try to adjust the specifications (e.g. breaks). # An example is shown below. Note that the purpose of the mfrow parameter is # to change the default way in which R displays plots. # Once applied, all plots you create later will also be displayed in a 2x2 grid. # To revert back, you need to enter par(mfrow=c(1,1)) into the console. par(mfrow=c(2,2)) hist(college$Apps) hist(college$perc.alumni, col=2) hist(college$S.F.Ratio, col=3, breaks=10) hist(college$Expend, breaks=100) Task 9 Using Accept and Apps, create a new variable that describes acceptance rate. Name this variable acceptance_rate. Hint: use mutate(). college &lt;- college %&gt;% mutate(acceptance_rate = Accept / Apps) Task 10 Using the acceptance_rate variable, find out which university has the lowest acceptance rate. Hint: for a tidyverse approach, you can use slice_min(). college %&gt;% slice_min(order_by = acceptance_rate, n = 1) ## Private Apps Accept Enroll Top10perc Top25perc ## Princeton University Yes 13218 2042 1153 90 98 ## F.Undergrad P.Undergrad Outstate Room.Board Books Personal ## Princeton University 4540 146 19900 5910 675 1575 ## PhD Terminal S.F.Ratio perc.alumni Expend Grad.Rate Elite ## Princeton University 91 96 8.4 54 28320 99 Yes ## acceptance_rate ## Princeton University 0.1544863 Task 11 Using the acceptance_rate variable, find out which university has the highest acceptance rate. college %&gt;% slice_max(order_by = acceptance_rate, n = 1) ## Private Apps Accept Enroll Top10perc Top25perc ## Emporia State University No 1256 1256 853 43 79 ## Mayville State University No 233 233 153 5 12 ## MidAmerica Nazarene College Yes 331 331 225 15 36 ## Southwest Baptist University Yes 1093 1093 642 12 32 ## University of Wisconsin-Superior No 910 910 342 14 53 ## Wayne State College No 1373 1373 724 6 21 ## F.Undergrad P.Undergrad Outstate Room.Board ## Emporia State University 3957 588 5401 3144 ## Mayville State University 658 58 4486 2516 ## MidAmerica Nazarene College 1100 166 6840 3720 ## Southwest Baptist University 1770 967 7070 2500 ## University of Wisconsin-Superior 1434 417 7032 2780 ## Wayne State College 2754 474 2700 2660 ## Books Personal PhD Terminal S.F.Ratio ## Emporia State University 450 1888 72 75 19.3 ## Mayville State University 600 1900 68 68 15.7 ## MidAmerica Nazarene College 1100 4913 33 33 15.4 ## Southwest Baptist University 400 1000 52 54 15.9 ## University of Wisconsin-Superior 550 1960 75 81 15.2 ## Wayne State College 540 1660 60 68 20.3 ## perc.alumni Expend Grad.Rate Elite ## Emporia State University 4 5527 50 No ## Mayville State University 11 6971 51 No ## MidAmerica Nazarene College 20 5524 49 No ## Southwest Baptist University 13 4718 71 No ## University of Wisconsin-Superior 15 6490 36 No ## Wayne State College 29 4550 52 No ## acceptance_rate ## Emporia State University 1 ## Mayville State University 1 ## MidAmerica Nazarene College 1 ## Southwest Baptist University 1 ## University of Wisconsin-Superior 1 ## Wayne State College 1 Practical 2 For the tasks below, you will require the Boston dataset. This dataset is part of the MASS R package. To access the dataset, load the MASS package (install the package first, if you have not done so previously). Task 1 Install and load the MASS package. You will also require tidyverse. # if you haven&#39;t already, install the MASS package before loading install.packages(&quot;MASS&quot;) library(MASS) library(tidyverse) Does R provide any message when loading MASS? Why does this matter? One important message that R provides when loading MASS is that this package masks the select() function from tidyverse. Attaching package: ‘MASS’ The following object is masked from ‘package:dplyr’: `select` When masking occurs, this means that both packages contain the same function. If you were to use the select() function, R will call the function from the MASS package, rather than from tidyverse (dplyr) package. This is because the MASS package is the one masking the function. If you intend to use the select() function as defined by the tidyverse package, it may not work as intended and/or you may be prompted by an error message such as: Error in select(...): unused argument (...) To avoid such issues, you must ‘call’ on the package from which you want R to use the masked function (e.g. dplyr::select()). This is why it is important to read through all warnings and messages provided in the console. Task 2 Find out more about the Boston dataset variables by accessing the R Documentation. ?Boston To explore the Boston dataset, simply type the name of the data object into the console or use View() What type of data structure is the Boston dataset? What are its dimensions? How many categorical and quantitative variables are there? The Boston dataset is a data frame with 506 rows (observations) and 14 columns (variables). There is one categorical variable (chas), and 13 quantitative variables. Task 3 Find the class of all 14 variables. Hint: use sapply. sapply(Boston, class) ## crim zn indus chas nox rm age dis ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;integer&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## rad tax ptratio black lstat medv ## &quot;integer&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; Task 4 Using a tidyverse approach, calculate the mean, median, and standard deviation of all variables of class numeric. Boston %&gt;% dplyr::select(dplyr::where(is.numeric)) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(Mean = mean(value, na.rm = TRUE), SD = sd(value, na.rm = TRUE), Median = median(value, na.rm = TRUE)) ## # A tibble: 14 × 4 ## name Mean SD Median ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 age 68.6 28.1 77.5 ## 2 black 357. 91.3 391. ## 3 chas 0.0692 0.254 0 ## 4 crim 3.61 8.60 0.257 ## 5 dis 3.80 2.11 3.21 ## 6 indus 11.1 6.86 9.69 ## 7 lstat 12.7 7.14 11.4 ## 8 medv 22.5 9.20 21.2 ## 9 nox 0.555 0.116 0.538 ## 10 ptratio 18.5 2.16 19.0 ## 11 rad 9.55 8.71 5 ## 12 rm 6.28 0.703 6.21 ## 13 tax 408. 169. 330 ## 14 zn 11.4 23.3 0 What is the mean pupil-teacher ratio? What is the median and mean per capita crime rate? Which value do you think is more suitable to describe per capita crime rate? The mean pupil-teacher ratio is about 19. The median crime rate is 0.257 whilst the mean is larger at 3.61. Given the difference between the median and the mean, a skewed distribution is expected, therefore, the median may be a more a suitable summary statistic to describe crime rate (a histogram would be needed). Task 5 Using a base R approach, create a 2x2 multipanel plot of crim versus age, dis, rad, tax and ptratio. par(mfrow = c(2,2)) plot(Boston$age, Boston$crim) plot(Boston$dis, Boston$crim) plot(Boston$rad, Boston$crim) plot(Boston$tax, Boston$crim) What can you say about the relationships between age, dis, rad, tax, and crim? As the age of the home increases (age), crime also increases. There is also higher crime around employment centers (dis). With very high index of accessibility to radial highways (rad), and tax rates (tax) there also appears to be high crime rates. Task 6 Using a base R approach, create and display histograms of crim, tax and ptratio in a 1x2 multipanel plot. Set the breaks argument to 25 . par(mfrow=c(1,2)) hist(Boston$crim, breaks=25) hist(Boston$tax, breaks=25) What do these histograms indicate? Most areas have low crime rates, but there is a rather long tail showing high crime rates (although the frequency seems to be very low). Given the degree of skew, the mean would not be a good measure of central tendency. With respect to tax rates, there appears to be a large divide between low taxation and high taxation, with the highest peak at around 670. Remember to revert back to single panel display. "],["overview-1.html", "Overview", " Overview Section 2: Linear Regression and Prediction This section is comprised of two demonstrations and two practicals. The two practicals will make use of exercises adapted from the core textbook for this course: James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd ed. New York: Springer. https://www.statlearning.com/ Learning Outcomes: explain the relevance of the intercept; appreciate the impact of noise on coefficient estimates; produce and interpret diagnostic plots with base R; apply non-linear transformations; compare model fit; compute and interpret confidence intervals. In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. Function Description Package lm() fit linear models base R predict() generic function for predictions from results of different models (e.g. predict.lm()) base R confint() compute confidence intervals base R plot() generic function for plotting base R legend() add legend (to plot()) arguments such as col, lty, and cex control colour, line type, and font size respectively base R abline() adding one or more straight lines to plot base R cor() computes correlation between variables base R rnorm() generates normal distribution base R poly() returns or evaluates polynomials base R par() set graphical parameters base R mfrow() par() parameter base R subset() return subset of a data object (vector, matrix, or dataframe) according to condition(s) base R anova() compute analysis of variance for base R rnorm() density, distribution function, quantile function and random generation for the normal distribution base R sqrt() compute square root base R "],["demonstration-1-1.html", "Demonstration 1 Simple Linear Models Without Intercept Simple Linear Models with Intercept", " Demonstration 1 Simple Linear Models Without Intercept Let’s investigate the t-statistic for the null hypothesis \\(H_{0}:\\beta = 0\\) in simple linear regression without an intercept. The equation for a model without the intercept would therefore be \\(Y = \\beta X\\). By excluding the intercept, the model is constrained to pass through the origin \\((0,0)\\), allowing the relationship between the response and predictor to be interpreted as proportional. In other words, the removal of the intercept forces the regression line to start at \\((0,0)\\), so when \\(x = 0\\), then \\(y = 0\\). Let’s first generate some data for a predictor \\(x\\) and a response \\(y\\). We select a seed value to ensure that we generate the same data every time. To generate values, we use the rnorm function to produce 100 data values drawn from a normal distribution (hence rnorm()). set.seed(1) x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100) Now that we generated our predictor and our response variable, let’s run a simple linear regression without an intercept using \\(y\\) as the response and \\(x\\) as a predictor. One way to do so is by adding \\(0\\) into the formula. fit &lt;- lm(y ~ x + 0) And now, let’s have a look at the results. coef(summary(fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## x 1.993876 0.1064767 18.72593 2.642197e-34 We can see a significant positive relationship between y and x. The coefficient estimate for \\(x\\) is \\(1.993876\\), and since the relationship between \\(x\\) and \\(y\\) is proportional, we interpret the estimate as the \\(y\\) values being predicted to be (a little below) twice the \\(x\\) values. But what happens if we swap \\(x\\) and \\(y\\) and run a model using \\(y\\) as the predictor and \\(x\\) as the response? fit2 &lt;- lm(x ~ y + 0) coef(summary(fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## y 0.3911145 0.02088625 18.72593 2.642197e-34 We again observe a significant positive relationship between \\(x\\) and \\(y\\), except that the \\(x\\) values are predicted to be (a little below) half the \\(y\\) values (since the coefficient estimate is \\(0.3911145\\)). Note also the t-statistic values for the two models. They are identical and of course so is the p-value (therefore, there is a significant relationships between \\(x\\) and \\(y\\)). Therefore, the results of the models of \\(y\\) onto \\(x\\) and \\(x\\) onto \\(y\\) indicate that the coefficients would be the inverse of each other (2 and 1/2) whilst the t-statistic values (and p-values) remain the same. Why are the t-statistic values identical? For each coefficient, the t statistic is calculated by dividing the coefficient estimate by its standard error. For example, for the fit2 model, we have a coefficient estimate of \\(0.3911145\\) and a standard error of \\(0.02088625\\) and so dividing \\(0.3911145\\) by \\(0.02088625\\) gives us \\(18.72593\\). You’ll also remember that the correlation coefficient between two variables is symmetric and so the correlation between \\(X\\) and \\(Y\\) is the same as for \\(Y\\) and \\(X\\). This is the reason why it is incorrect to state that “\\(X\\) causes a change in \\(Y\\)”. In a linear model, we are testing whether there is a linear association between \\(x\\) and \\(y\\) but not if X causes Y or Y causes X. Therefore, irrespective of whether we are regressing \\(y\\) onto \\(x\\) or \\(y\\) onto \\(y\\), the t-statistic is testing the same null hypothesis \\(H_{0} : \\beta = 0\\) (i.e. fundamentally, it is testing whether there is a linear correlation between \\(x\\) and \\(y\\)). So what exactly is the role of the intercept? As you already know, the intercept represents the value of \\(y\\) when \\(x = 0\\) which can be thought of as the initial value effect that exists independently of \\(x\\). This not only applies to the simple linear regression model but also to the multiple linear regression model (i.e. the intercept is the value of \\(y\\) when all predictors are zero). In other words, the intercept adjusts the starting point of regression line and allows for the line to shift up or down on the y-axis thus reflecting a “baseline” level of \\(y\\) that is not dependent on \\(x\\). With an intercept, the slope coefficient still tells us how much \\(Y\\) changes with a one-unit change in \\(x\\), but this change is relative to the value of \\(y\\) when \\(x = 0\\) and this is important when \\(x\\) can take a value of \\(0\\) that is meaningful to the model. Without the intercept, the line is forced to pass through the origin \\((0,0)\\), which may not be suitable unless the data naturally begin at zero (or there are some other theoretical or practical reasons which warrant the line passing through the origin). With an intercept, the regression line is no longer forced to pass through zero (and will only do so if the data naturally begin at zero). The intercept therefore allows for the regression line to better fit the data, particularly when the data do not actually begin at zero. In this way, the model can capture the average outcome when the predictor(s) is/are zero. Simple Linear Models with Intercept Do you think that the t-statistic will be the same for both regression of Y onto X and X onto Y if we were to include the intercept? We use the same data as before and run a regression with \\(y\\) as response and \\(x\\) as predictor and include the intercept. fit3 &lt;- lm(y ~ x) We then extract the model coefficients from the summary results of the model. coef(summary(fit3)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.03769261 0.09698729 -0.3886346 6.983896e-01 ## x 1.99893961 0.10772703 18.5555993 7.723851e-34 How does coefficient for fit3 compare to fit? How about the t-statistic value? coef(summary(fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## x 1.993876 0.1064767 18.72593 2.642197e-34 As you can see, the coefficient for the model with the intercept (fit3) is very similar to the coefficient for the model without the intercept (fit). The t-statistic is also very close (\\(18.72593\\) for the model without intercept and \\(18.5555993\\) for the model with the intercept). Now we run a regression with \\(x\\) as response and \\(y\\) as predictor. fit4 &lt;- lm(x ~ y) We then extract the model coefficients from the summary results of the model. coef(summary(fit4)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.03880394 0.04266144 0.9095787 3.652764e-01 ## y 0.38942451 0.02098690 18.5555993 7.723851e-34 How does the coefficient for fit4 compare to fit2? How about the t-statistic value? Are the t-statistic values different between the fit3 and fit4 models? coef(summary(fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## y 0.3911145 0.02088625 18.72593 2.642197e-34 The slope coefficient for the model with the intercept (\\(0.38942451\\)) is very similar to the coefficient for the model without the intercept (\\(0.3911145\\)) and so is the t-statistic. Also, as expected, the t-statistic value for the two models for which we included the intercept are identical. Therefore, irrespective of whether we include the intercept or not, the t-statistic value for the regression of \\(y\\) onto \\(x\\) will be identical to the t-statistic value for the regression of \\(x\\) onto \\(y\\). "],["demonstration-2.html", "Demonstration 2 2.1 Population Parameters and Estimated Coefficients 2.2 What happens if we reduce noise? 2.3 What happens if we increase noise? 2.4 How does noise affect confidence intervals for the coefficients?", " Demonstration 2 2.1 Population Parameters and Estimated Coefficients Let’s explore the differences between population parameters and estimated coefficients. We will generate a “true” population dataset. We create a variable \\(X\\) with 100 observations drawn from a normal distribution. To be more specific about the characteristic of our variable \\(X\\), we will not only specify the total number of observations (100), but also the mean (0), and standard deviation (1). This will be our predictor. set.seed(1) x &lt;- rnorm(100, 0, 1) We now create another vector called eps containing 100 observations drawn from a normal distribution with a mean of zero and a variance of 0.25. This will be the our error term \\(\\epsilon\\). eps &lt;- rnorm(100, 0, sqrt(0.25)) Using \\(X\\) and \\(\\epsilon\\), we now generate a vector \\(Y\\) according to the following formula: \\(Y = -1 + 0.5X + \\epsilon\\). Essentially, we specify our intercept, the slope coefficient, the predictor variable and the error to obtain our response variable. y &lt;- -1 + 0.5 * x + eps The values \\(-1\\) and \\(0.5\\) represent the “true” population values for the intercept \\(\\beta_{0}\\) and slope \\(\\beta_{1}\\) respectively. Now we can create a scatterplot to observe the association between \\(X\\) and \\(Y\\). plot(x, y) The plot indicates a linear relationship between \\(X\\) and \\(Y\\). The relationship is clearly not perfectly linear due to noise. If we were to instead estimate the intercept and slope, to what degree do you think these estimated coefficients will differ from the true population values? Ok, so we have the variables we generated, so our predictor X and our response X and we run a regression model. fit &lt;- lm(y ~ x) summary(fit) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.93842 -0.30688 -0.06975 0.26970 1.17309 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.01885 0.04849 -21.010 &lt; 2e-16 *** ## x 0.49947 0.05386 9.273 4.58e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4814 on 98 degrees of freedom ## Multiple R-squared: 0.4674, Adjusted R-squared: 0.4619 ## F-statistic: 85.99 on 1 and 98 DF, p-value: 4.583e-15 The results of the model show an estimated slope coefficient (\\(\\hat{\\beta_{1}}\\)) for \\(x\\) of \\(0.49947\\). This is very close to the population value (\\(\\beta_{1}\\)) which is \\(0.5\\). We see a similar estimated value for the intercept (\\(\\hat{\\beta_{0}}\\)) which is \\(-1.01885\\), again very close to the true value for the intercept (\\(\\beta_{0}\\)) which is \\(-1\\). Therefore, if we were to plot the population regression line and the estimated regression line, we would see that the two are difficult to distinguish (given the similarity of the estimated and true values for the coefficients). plot(x, y) abline(fit) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2), cex = 0.72, ) What if we were to fit a polynomial regression model? Would there be any evidence that adding a quadratic term improves the model fit? To add a polynomial term of degree two, we can use the poly base R function directly in the code for the model. Since the F-test is not statistically significant, there is no evidence that adding a quadratic term improves the model fit. fit2 &lt;- lm(y ~ poly(x, 2)) anova(fit, fit2) ## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ poly(x, 2) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 98 22.709 ## 2 97 22.257 1 0.45163 1.9682 0.1638 2.2 What happens if we reduce noise? For our first model (fit), we specified a variance of \\(0.5\\) (standard deviation 0.25) for \\(\\epsilon\\) and we noted an \\(R^2\\) value of \\(0.4674\\). set.seed(1) x &lt;- rnorm(100, 0, 1) eps &lt;- rnorm(100, 0, sqrt(0.25)) y &lt;- -1 + 0.5 * x + eps fit &lt;- lm(y ~ x) summary(fit) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.93842 -0.30688 -0.06975 0.26970 1.17309 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.01885 0.04849 -21.010 &lt; 2e-16 *** ## x 0.49947 0.05386 9.273 4.58e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4814 on 98 degrees of freedom ## Multiple R-squared: 0.4674, Adjusted R-squared: 0.4619 ## F-statistic: 85.99 on 1 and 98 DF, p-value: 4.583e-15 Now, let’s observe what happens to the \\(R^2\\) value if we reduce noise from 0.25 to 0.05. We can do so directly when we generate data for our variable \\(y\\) without needing to create a new eps object. The results show that the \\(R^{2}\\) value for fit3 is much higher than the \\(R^{2}\\) value for fit. x &lt;- rnorm(100, 0, 1) y &lt;- -1 + 0.5 * x + rnorm(100, 0, sqrt(0.05)) fit3 &lt;- lm(y ~ x) summary(fit3) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.61308 -0.12553 -0.00391 0.15199 0.41332 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.98917 0.02216 -44.64 &lt;2e-16 *** ## x 0.52375 0.02152 24.33 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2215 on 98 degrees of freedom ## Multiple R-squared: 0.858, Adjusted R-squared: 0.8565 ## F-statistic: 592.1 on 1 and 98 DF, p-value: &lt; 2.2e-16 By plotting the data we can clearly see that the data points are less dispersed than before and therefore, the association between x and y appears more linear. We can also observe that the estimated regression line deviates slightly from the population regression line (particularly at lowest and highest values). plot(x, y) abline(fit3) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2), cex = 0.72 ) 2.3 What happens if we increase noise? Using the same approach as before, we now increase the standard deviation to 1. Now, the \\(R^{2}\\) value for fit4 is much lower than that of either fit3 or fit. x &lt;- rnorm(100, 0, 1) y &lt;- -1 + 0.5 * x + rnorm(100, 0, 1) fit4 &lt;- lm(y ~ x) summary(fit4) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.51014 -0.60549 0.02065 0.70483 2.08980 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.04745 0.09676 -10.825 &lt; 2e-16 *** ## x 0.42505 0.08310 5.115 1.56e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9671 on 98 degrees of freedom ## Multiple R-squared: 0.2107, Adjusted R-squared: 0.2027 ## F-statistic: 26.16 on 1 and 98 DF, p-value: 1.56e-06 If we plot the data, we can observe that the data points are more dispersed and therefore, the estimated regression line deviates to a greater extent from the population regression line. plot(x, y) abline(fit4) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2), cex = 0.72, ) 2.4 How does noise affect confidence intervals for the coefficients? The fit3 model is the model with the lowest amount of noise (standard deviation of 0.05), whilst fit4 is the model with the largest amount of noise (standard deviation of 1). The confidence interval for the coefficient for the model with the highest noise is the widest. The larger the amount of noise, the wider the interval and therefore, the less precise the coefficient estimates will be. Conversely, the narrowest interval is that of the model with the lowest noise which yielded the most precise estimates. confint(fit) ## 2.5 % 97.5 % ## (Intercept) -1.1150804 -0.9226122 ## x 0.3925794 0.6063602 confint(fit3) ## 2.5 % 97.5 % ## (Intercept) -1.033141 -0.9451916 ## x 0.481037 0.5664653 confint(fit4) ## 2.5 % 97.5 % ## (Intercept) -1.2394772 -0.8554276 ## x 0.2601391 0.5899632 "],["practical-1-2.html", "Practical 1", " Practical 1 For the tasks below, you will require the Auto dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Auto dataset loads automatically: library(ISLR2) The Auto dataset contains information such as engine horsepower, gas mileage, model year, and origin of car, for 392 vehicles. It is a dataframe with 392 observations and 9 variables. The variables are: mpg: miles per gallon cylinders: Number of cylinders between 4 and 8 displacement: Engine displacement (cu. inches) horsepower: Engine horsepower weight: Vehicle weight (lbs.) acceleration: Time to accelerate from 0 to 60 mph (sec.) year: Model year origin: Origin of car (1. American, 2. European, 3. Japanese) name: Vehicle name Task 1 Use the lm() function to perform simple linear regression with mpg as the response and horsepower as the predictor. Store the output in an object called fit. Task 2 Have a look at the results of the model. Is there a relationship between the predictor and the response? Task 3 What is the associated 95% confidence intervals for predicted miles per gallon associated with an engine horsepower of 98? Hint: use the predict() function. For confidence intervals, set the interval argument to confidence. Task 4 How about the prediction interval for the same value? Are the two intervals different? Why? Task 5 Using base R, plot the response and the predictor as well as the least squares regression line. Add suitable labels to the X and Y axes. Task 6 Use base R to produce diagnostic plots of the least squares regression fit. Display these in a 2X2 grid. Task 7 Subset the Auto dataset such that it excludes the name and origin variables and store this subsetted dataset in a new object called quant_vars. Task 8 Compute a correlation matrix of all variables. Did you use the Auto dataset or the quant_vars object? Why does it matter which data object you use? Task 9 Using the quant_vars object, perform multiple linear regression with miles per gallon as the response and all other variables as the predictors. Store the results in an object called fit2. Task 10 Have a look at the results of the multiple regression model. Is there a relationship between the predictors and the response? Which predictors appear to have a statistically significant relationship to the response? What does the coefficient for the year variable suggest? Task 11 Produce diagnostic plots of the multiple linear regression fit in a 2x2 grid. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage? Task 12 Fit separate linear regression models with interaction effect terms for: weight and horsepower, acceleration and horsepower, and cylinders and weight. Are any of the interaction terms statistically significant? Task 13 Using the Auto data object, apply transformations for the horsepower variable and plot the relationship between horsepower and mpg in a 2x2 grid. First plot: use the original variable; Second plot: apply log transform; Third plot: raise it to the power of two. Which of these transformations is most suitable? Task 14 Now run a multiple regression model with all variables as before but this time, apply a log transformation of the horsepower variable. Task 15 Have a look at the results. How do the results of model fit3 differ from those of model fit2? Task 16 Produce diagnostic plots for the fit3 object and display them in a 2x2 grid. How do the diagnostic plots differ? "],["practical-2-2.html", "Practical 2 Task 1 Task 2 Task 3 Task 4 Task 5", " Practical 2 For the tasks below, you will require the Carseats dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Carseats dataset loads automatically: library(ISLR2) Carseats is a simulated dataset comprising of sales of child car seats at 400 different stores. It is a datafrate with 400 observations and 11 variables. The variables are: Sales: Unit sales (thousands of dollars) at each location CompPrice: Price charged by competitor at each location Income: Community income level (thousands of dollars) Advertising: Local advertising budget for company at each location (thousands of dollars) Population: Population size in region (thousands of dollars) Price: Price company charges for car seats at each site ShelveLoc: Quality of the shelving location for the car seats at each site Age: Average age of the local population Education: Education level at each location Urban: Whether the store is in an urban or rural location US: Whether the store is in the US or not Task 1 Fit a multiple regression model to predict unit sales at each location based on price company charges for car seats, whether the store is in an urban or rural location, and whether the store is in the US or not. Task 2 Have a look at the results and interpret the coefficients. Which coefficients are statistically significant? What do they indicate? Task 3 Based on the conclusions you have drawn in Task 2, now fit a smaller model that only uses the predictors for which there is evidence of association with sales. Task 4 Compare the two models (fit and fit2). Which model is the better fit? Task 5 Produce diagnostic plots for fit2 and display these in a 2x2 grid. Is there evidence of outliers or high leverage observations in the fit2 model? "],["answers-1.html", "Answers Practical 1 Practical 2", " Answers Practical 1 For the tasks below, you will require the Auto dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Auto dataset loads automatically: library(ISLR2) Remember to install it first install.packages(\"ISLR2\") This data file (text format) contains 398 observations of 9 variables. The variables are: mpg: miles per gallon cylinders: Number of cylinders between 4 and 8 displacement: Engine displacement (cu. inches) horsepower: Engine horsepower weight: Vehicle weight (lbs.) acceleration: Time to accelerate from 0 to 60 mph (sec.) year: Model year origin: Origin of car (1. American, 2. European, 3. Japanese) name: Vehicle name Task 1 Use the lm() function to perform simple linear regression with mpg as the response and horsepower as the predictor. Store the output in an object called fit. fit &lt;- lm(mpg ~ horsepower, data = Auto) Task 2 Have a look at the results of the model. summary(fit) ## ## Call: ## lm(formula = mpg ~ horsepower, data = Auto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.5710 -3.2592 -0.3435 2.7630 16.9240 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.935861 0.717499 55.66 &lt;2e-16 *** ## horsepower -0.157845 0.006446 -24.49 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.906 on 390 degrees of freedom ## Multiple R-squared: 0.6059, Adjusted R-squared: 0.6049 ## F-statistic: 599.7 on 1 and 390 DF, p-value: &lt; 2.2e-16 Is there a relationship between the predictor and the response? The slope coefficient (-0.157845) is statistically significant (&lt;2e-16 ***). We can conclude that there is evidence to suggest a negative relationship between miles per gallon and engine horsepower. For a one-unit increase in engine horsepower, miles per gallon are reduced by 0.16. Task 3 What is the associated 95% confidence intervals for predicted miles per gallon associated with an engine horsepower of 98? Hint: use the predict() function. For confidence intervals, set the interval argument to confidence. predict(fit, data.frame(horsepower = 98), interval = &quot;confidence&quot;) ## fit lwr upr ## 1 24.46708 23.97308 24.96108 Task 4 How about the prediction interval for the same value? predict(fit, data.frame(horsepower = 98), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 24.46708 14.8094 34.12476 Are the two intervals different? Why? The prediction interval (lower limit 14.8094 and upper limit 34.12476) is wider (and therefore less precise) than the confidence interval (lower limit 23.97308 and upper limit 24.96108). The confidence interval measures the uncertainty around the estimate of the conditional mean whilst the prediction interval takes into account not only uncertainty but also the variability of the conditional distribution. Task 5 Using base R, plot the response and the predictor as well as the least squares regression line. Add suitable labels to the X and Y axes. plot(Auto$horsepower, Auto$mpg, xlab = &quot;horsepower&quot;, ylab = &quot;mpg&quot;) abline(fit) Task 6 Use base R to produce diagnostic plots of the least squares regression fit. Display these in a 2X2 grid. par(mfrow = c(2, 2)) plot(fit, cex = 0.2) Task 7 Subset the Auto dataset such that it excludes the name and origin variables and store this subsetted dataset in a new object called quant_vars. quant_vars &lt;- subset(Auto, select = -c(name, origin)) Task 8 Compute a correlation matrix of all variables. cor(quant_vars) ## mpg cylinders displacement horsepower weight ## mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 ## cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 ## displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 ## horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 ## weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 ## acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 ## year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 ## acceleration year ## mpg 0.4233285 0.5805410 ## cylinders -0.5046834 -0.3456474 ## displacement -0.5438005 -0.3698552 ## horsepower -0.6891955 -0.4163615 ## weight -0.4168392 -0.3091199 ## acceleration 1.0000000 0.2903161 ## year 0.2903161 1.0000000 Did you use the Auto dataset or the quant_vars object? Why does it matter which data object you use? To compute the correlation matrix using all variables of a data object, these variables must all be numeric. In the Auto data object, the name variable is coded as a factor. class(Auto$name) [1] \"factor\" Therefore, if you try to use the cor() function with Auto dataset without excluding the name variable, you will get an error. cor(Auto) Error in cor(Auto) : 'x' must be numeric. Also, whilst the origin variable is of class integer and will not pose a problem when you apply the cor() function, you’ll remember from the variable description list that this is a nominal variable with its categories numerically labelled. Compute the correlation matrix using quant_vars. Task 9 Using the quant_vars object, perform multiple linear regression with miles per gallon as the response and all other variables as the predictors. Store the results in an object called fit2. fit2 &lt;- lm(mpg ~ ., data = quant_vars) Task 10 Have a look at the results of the multiple regression model. summary(fit2) ## ## Call: ## lm(formula = mpg ~ ., data = quant_vars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.6927 -2.3864 -0.0801 2.0291 14.3607 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.454e+01 4.764e+00 -3.051 0.00244 ** ## cylinders -3.299e-01 3.321e-01 -0.993 0.32122 ## displacement 7.678e-03 7.358e-03 1.044 0.29733 ## horsepower -3.914e-04 1.384e-02 -0.028 0.97745 ## weight -6.795e-03 6.700e-04 -10.141 &lt; 2e-16 *** ## acceleration 8.527e-02 1.020e-01 0.836 0.40383 ## year 7.534e-01 5.262e-02 14.318 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.435 on 385 degrees of freedom ## Multiple R-squared: 0.8093, Adjusted R-squared: 0.8063 ## F-statistic: 272.2 on 6 and 385 DF, p-value: &lt; 2.2e-16 Is there a relationship between the predictors and the response? Which predictors appear to have a statistically significant relationship to the response? What does the coefficient for the year variable suggest? Two of the predictors are statistically significant: weight and year. The relationship between weight and mpg is negative which suggests that for a one pound increase in weight of vehicle, the number of miles per gallon the vehicle can travel decreases, whilst that of mpg and year is positive which suggests that the more recent the vehicle is, the higher the number of miles per gallon it can travel. Task 11 Produce diagnostic plots of the multiple linear regression fit in a 2x2 grid. par(mfrow = c(2, 2)) plot(fit2, cex = 0.2) Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage? One point has high leverage, the residuals also show a trend with fitted values. Task 12 Fit separate linear regression models with interaction effect terms for: weight and horsepower, acceleration and horsepower, and cylinders and weight. summary(lm(mpg ~ . + weight:horsepower, data = quant_vars)) summary(lm(mpg ~ . + acceleration:horsepower, data = quant_vars)) summary(lm(mpg ~ . + cylinders:weight, data = quant_vars)) Are any of the interaction terms statistically significant? For each model, the interaction term is statistically significant. Task 13 Using the Auto data object, apply transformations for the horsepower variable and plot the relationship between horsepower and mpg in a 2x2 grid. First plot: use the original variable; Second plot: apply log transform; Third plot: raise it to the power of two. par(mfrow = c(2, 2)) plot(Auto$horsepower, Auto$mpg, cex = 0.2) plot(log(Auto$horsepower), Auto$mpg, cex = 0.2) plot(Auto$horsepower ^ 2, Auto$mpg, cex = 0.2) Which of these transformations is most suitable? The relationship between horsepower and miles per gallon is clearly non-linear (plot 1). The log transform seems to address this best. Task 14 Now run a multiple regression model with all variables as before but this time, apply a log transformation of the horsepower variable. quant_vars$horsepower &lt;- log(quant_vars$horsepower) fit3 &lt;- lm(mpg ~ ., data = quant_vars) Task 15 Have a look at the results. summary(fit3) ## ## Call: ## lm(formula = mpg ~ ., data = quant_vars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.6778 -2.0080 -0.3142 1.9262 14.0979 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.1713000 8.9291383 3.267 0.00118 ** ## cylinders -0.3563199 0.3181815 -1.120 0.26347 ## displacement 0.0088277 0.0068866 1.282 0.20066 ## horsepower -8.7568129 1.5958761 -5.487 7.42e-08 *** ## weight -0.0044304 0.0007213 -6.142 2.03e-09 *** ## acceleration -0.3317439 0.1077476 -3.079 0.00223 ** ## year 0.6979715 0.0503916 13.851 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.308 on 385 degrees of freedom ## Multiple R-squared: 0.8231, Adjusted R-squared: 0.8203 ## F-statistic: 298.5 on 6 and 385 DF, p-value: &lt; 2.2e-16 How do the results of model fit3 differ from those of model fit2? The fit2 model results showed that only two predictors were statistically significant: weight and year. The fit3 model has two additional predictors that are statistically significant: acceleration as well as horsepower.Also, the coefficient values can now be interpreted more easily. Task 16 Produce diagnostic plots for the fit3 object and display them in a 2x2 grid. par(mfrow = c(2, 2)) plot(fit3, cex = 0.2) How do the diagnostic plots differ? A log transformation of horsepower appears to give a more linear relationship with mpg but this difference does not seem to be substantial. Practical 2 For the tasks below, you will require the Carseats dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Carseats dataset loads automatically: library(ISLR2) This dataframe object contains a simulated dataset of sales of child car seats at 400 different stores. The 9 variables are: Sales: Unit sales (thousands of dollars) at each location CompPrice: Price charged by competitor at each location Income: Community income level (thousands of dollars) Advertising: Local advertising budget for company at each location (thousands of dollars) Population: Population size in region (thousands of dollars) Price: Price company charges for car seats at each site ShelveLoc: Quality of the shelving location for the car seats at each site Age: Average age of the local population Education: Education level at each location Urban: Whether the store is in an urban or rural location US: Whether the store is in the US or not Task 1 Fit a multiple regression model to predict unit sales at each location based on price company charges for car seats, whether the store is in an urban or rural location, and whether the store is in the US or not. fit &lt;- lm(Sales ~ Price + Urban + US, data = Carseats) Task 2 Have a look at the results and interpret the coefficients. summary(fit) ## ## Call: ## lm(formula = Sales ~ Price + Urban + US, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9206 -1.6220 -0.0564 1.5786 7.0581 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.043469 0.651012 20.036 &lt; 2e-16 *** ## Price -0.054459 0.005242 -10.389 &lt; 2e-16 *** ## UrbanYes -0.021916 0.271650 -0.081 0.936 ## USYes 1.200573 0.259042 4.635 4.86e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.472 on 396 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2335 ## F-statistic: 41.52 on 3 and 396 DF, p-value: &lt; 2.2e-16 Which coefficients are statistically significant? What do they indicate? The null hypothesis for the slope being zero is rejected for the Price and US variables. The coefficient for Price is statistically significant; since it is negative, as price increases by a thousand dollars (i.e. one unit increase), the sales of child decrease by about 0.05. The slope for the US variable is also statistically significant but it is positive. Also, this is a binary factor variable coded as Yes and No (No is the reference category). Therefore, sales of child car seats are higher by 1.2 for car seat products that are produced in the US than for car seat products not produced in the US. Task 3 Based on the conclusions you have drawn in Task 2, now fit a smaller model that only uses the predictors for which there is evidence of association with sales. fit2 &lt;- lm(Sales ~ Price + US, data = Carseats) Task 4 Compare the two models (fit and fit2). anova(fit, fit2) ## Analysis of Variance Table ## ## Model 1: Sales ~ Price + Urban + US ## Model 2: Sales ~ Price + US ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 396 2420.8 ## 2 397 2420.9 -1 -0.03979 0.0065 0.9357 Which model is the better fit? They have similar r-squared values, and the fit model (containing the extra variable Urban) is non-significantly better. Task 5 Produce diagnostic plots for fit2 and display these in a 2x2 grid. par(mfrow = c(2, 2)) plot(fit2, cex = 0.2) Is there evidence of outliers or high leverage observations in the fit2 model? Yes, there is evidence of outliers and high leverage observations for fit2. "],["overview-2.html", "Overview", " Overview Section 3: Classification This section is comprised of one, expanded demonstration which is adapted from the core textbook for this course: James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd ed. New York: Springer. https://www.statlearning.com/ Learning Outcomes: appreciate the difference between classic modelling and supervised learning; compute and plot a correlation matrix; compute and interpret a confusion matrix and overall prediction accuracy; perform a variety of classification methods and appreciate their advantages and limitations. In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. Function Description Package str() display structure of object base R cor() compute correlation base R corrplot() plot correlation matrix arguments such as type and diag control the structure of the plot corrplot glm() fit a generalised linear model arguments such as family control the distribution base R predict() generic function for predictions from results of models arguments such as type specify the type of predictions required base R diag() extract/replace diagonal of a matrix base R sum() compute the sum of values base R lda() perform LDA MASS table() build contingency table base R qda() perform QDA MASS knn() perform KNN classification arguments such as k control number of neighbours class naiveBayes() apply the Naive Bayes classifier e1071 "],["demonstration-1-2.html", "Demonstration 1 Dataset and Variables Correlation Matrix and Plot “Classic” Logistic Regression Logistic Regression in Statistical Learning Linear Discriminant Analysis Quadratic Discriminant Analysis \\(K\\)-nearest neighbours Naive Bayes", " Demonstration 1 In this demonstration, you will learn how to address classification problems using logistic regression, discriminant analysis, KNN, and Naive Bayes. You will need the Weekly dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Weekly dataset loads automatically. In addition to the ISLR2 package, will also require the following: library(MASS) library(class) library(tidyverse) library(corrplot) library(ISLR2) library(e1071) Dataset and Variables The Weekly dataset contains weekly percentage returns for the S&amp;P 500 stock index between 1990 and 2010. It is a dataframe with 1098 observations and 9 variables. The variables are: Year: year observation was recorded Lag1: Percentage return for previous week Lag2: Percentage return for 2 weeks previous Lag3: Percentage return for 3 weeks previous Lag4: Percentage return for 4 weeks previous Lag5: Percentage return for 5 weeks previous Volume: Volume of shares traded (average number of daily shares traded in billions) Today: percentage return for current week Direction: whether the market had a positive (up) or negative (down) return on a given week. In this demonstration, the goal is to predict whether the market had a positive (up) or negative (down) return on a given week. Therefore, Direction will be our response variable. Before we consider the model, let’s first explore our dataset. By exploring the structure of the dataframe, we find out that all variables are numeric, with the exception of the Direction variable. str(Weekly) ## &#39;data.frame&#39;: 1089 obs. of 9 variables: ## $ Year : num 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ... ## $ Lag1 : num 0.816 -0.27 -2.576 3.514 0.712 ... ## $ Lag2 : num 1.572 0.816 -0.27 -2.576 3.514 ... ## $ Lag3 : num -3.936 1.572 0.816 -0.27 -2.576 ... ## $ Lag4 : num -0.229 -3.936 1.572 0.816 -0.27 ... ## $ Lag5 : num -3.484 -0.229 -3.936 1.572 0.816 ... ## $ Volume : num 0.155 0.149 0.16 0.162 0.154 ... ## $ Today : num -0.27 -2.576 3.514 0.712 1.178 ... ## $ Direction: Factor w/ 2 levels &quot;Down&quot;,&quot;Up&quot;: 1 1 2 2 2 1 2 2 2 1 ... Correlation Matrix and Plot Let’s now produce a correlation plot between all pairs of numeric variables in the dataset. Using the base R cor() function, we exclude the 9th variable (which is a factor) and compute the correlation among all pairs of numeric variables. cor(Weekly[, -9]) ## Year Lag1 Lag2 Lag3 Lag4 ## Year 1.00000000 -0.032289274 -0.03339001 -0.03000649 -0.031127923 ## Lag1 -0.03228927 1.000000000 -0.07485305 0.05863568 -0.071273876 ## Lag2 -0.03339001 -0.074853051 1.00000000 -0.07572091 0.058381535 ## Lag3 -0.03000649 0.058635682 -0.07572091 1.00000000 -0.075395865 ## Lag4 -0.03112792 -0.071273876 0.05838153 -0.07539587 1.000000000 ## Lag5 -0.03051910 -0.008183096 -0.07249948 0.06065717 -0.075675027 ## Volume 0.84194162 -0.064951313 -0.08551314 -0.06928771 -0.061074617 ## Today -0.03245989 -0.075031842 0.05916672 -0.07124364 -0.007825873 ## Lag5 Volume Today ## Year -0.030519101 0.84194162 -0.032459894 ## Lag1 -0.008183096 -0.06495131 -0.075031842 ## Lag2 -0.072499482 -0.08551314 0.059166717 ## Lag3 0.060657175 -0.06928771 -0.071243639 ## Lag4 -0.075675027 -0.06107462 -0.007825873 ## Lag5 1.000000000 -0.05851741 0.011012698 ## Volume -0.058517414 1.00000000 -0.033077783 ## Today 0.011012698 -0.03307778 1.000000000 We store the computed correlation matrix in a new object which we will then use to create a correlation matrix plot with the corrplot() function from the corrplot package. cor_matrix &lt;- cor(Weekly[, -9]) corrplot(cor_matrix) By using the default arguments, we obtain the correlation matrix in full, with each circle representing the correlation between each variable. The size of the circle represents the magnitude of the correlation, whilst the shade corresponds to both strength and direction of the correlation. As the legend illustrates, a perfect negative correlation (-1) is represented by dark red and a perfect positive correlation (+1) in dark blue. As you already know, the correlation matrix is symmetric around its diagonal. The diagonal area represents the correlation of each variable with itself, and therefore, this corresponds to a perfect correlation (dark blue). To facilitate interpretation, particularly when we are dealing with a large number of variables, we can set the diag argument to FALSE to remove the correlation of each variable with itself from the plot. Because of its symmetric property, we can also display just half of the square since the parts on either side of the diagonal are mirror images. We can achieve this using the type argument. We can either choose to display the area above the diagonal or the area below the diagonal, as I did below. There are many other options if you want to further customise your correlation plot such using a different visualisation method of the direction and strength of the correlation using the method argument. Have a look at the documentaion of the corrplot function using ?. corrplot(cor_matrix, type = &quot;lower&quot;, diag = FALSE) Now, the correlation plot only displays the correlations between the 8 numeric variables. We observe a strong positive correlation between volume of daily shares traded and the year the observation was recorded (dark blue). The correlations between other variables are quite weak but notably, we see that Lag1 and Lag3, Lag 2 and Lag4, Lag3 and Lag5, and Today and Lag2 are positively correlated with one another (albeit weakly). Other variables also appear weakly negatively correlated, such as Lag1 and Lag2. Ok, so what was the purpose of computing the correlation matrix? You’ll remember that multicollinearity is an issue in model building which can lead to inflated variances of the estimated coefficients. As a result of the shared variance between two highly correlated predictors, our ability to adequately evaluate the effect of the predictors on the outcome will be affected (e.g. increased risk of overfitting). One way to deal with multicollinearity is to eliminate one of the highly correlated predictors. “Classic” Logistic Regression Now let’s build our logistic regression model the classic way. Given the high correlation between Year and Volume, we have to reflect on how we want to address this. Assuming we have knowledge of important factors that can predict market movement (Direction), we decide to drop the Year variable rather than Volume* since the latter measures average number of daily shares traded (in billions). You may remember that in R, these models are built using the base R glm function within which the family argument must be set to binomial. Note that in this dataset, the Direction variable is already a factor so there is no need to perform any recoding/transformations but remember to ALWAYS explore your data in detail before you build any model. fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial) summary(fit) ## ## Call: ## glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + ## Volume, family = binomial, data = Weekly) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6949 -1.2565 0.9913 1.0849 1.4579 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.26686 0.08593 3.106 0.0019 ** ## Lag1 -0.04127 0.02641 -1.563 0.1181 ## Lag2 0.05844 0.02686 2.175 0.0296 * ## Lag3 -0.01606 0.02666 -0.602 0.5469 ## Lag4 -0.02779 0.02646 -1.050 0.2937 ## Lag5 -0.01447 0.02638 -0.549 0.5833 ## Volume -0.02274 0.03690 -0.616 0.5377 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1496.2 on 1088 degrees of freedom ## Residual deviance: 1486.4 on 1082 degrees of freedom ## AIC: 1500.4 ## ## Number of Fisher Scoring iterations: 4 The results show that only Lag2 is significant at an alpha level of 0.05. Ok, so let’s see now how well our model predicts whether the market had a positive or negative return in a given week. Confusion Matrix Now, let’s assess the effectiveness of our model by comparing the actual values with those predicted by the model. We can do so using a confusion matrix which compares the model predictions with the true values. Using predict(fit, type = \"response\"), we generate predictions from our model (fit) on the scale of the response variable (type = \"response\"). In this case, our response variable is measured on a probability scale. We choose the standard threshold of 0.5 such that we label an observation as belonging to the Up category if its posterior probability is above 0.5 or as Down if the posterior probability is below 0.5. Hence, in this context, the &gt; 0.5 argument transforms the predicted probabilities into a binary form such that predictions greater than 0.5 are labelled TRUE (so representing upward market movement), whilst the rest are labelled FALSE (representing downward market movement). pred &lt;- predict(fit, type = &quot;response&quot;) &gt; 0.5 Now that we have the frequencies of the TRUE and FALSE instances, let’s build our two-way confusion matrix using the base R table() function. The ifelse function nested inside table() converts the logical TRUE and FALSE values in the pred object intro descriptive labels to facilitate interpretation; so, if pred is TRUE (&gt; 0.5), it becomes labelled as Up (pred), whilst it is is FALSE, it is labelled as \"Down (pred)\". To also include the actual values of market movement from the dataset, we also need to add Weekly$Directionas our argument. Finally to ‘force’ R to display the conf_matrix object we just created, we can place the entire code in single parentheses. (conf_matrix &lt;- table(ifelse(pred, &quot;Up (pred)&quot;, &quot;Down (pred)&quot;), Weekly$Direction)) ## ## Down Up ## Down (pred) 54 48 ## Up (pred) 430 557 Now let’s interpret the results. The results in diagonal represent correct predictions of market movement whilst those in the off-diagonal represent misclassified observations. We can see that our model incorrectly classified 430 instances of market movement as upward movement when in fact they represented downward movement and 48 instances as downward movement when in fact they represented upward movement. Overall, our logistic regression model correctly predicts upwards movements well but it performs poorly at predicting downward movements. We can also compute the overall fraction of correct predictions by dividing the number of correct predictions by total number of predictions. We therefore divide the sum of the diagonal values in our confusion matrix (numerator) by the sum of all elements of the matrix (denominator). We extract the diagonal values from the matrix using the base R diag() function. sum(diag(conf_matrix)) / sum(conf_matrix) ## [1] 0.5610652 The overall fraction of correct predictions is 0.561 (so our model makes correct predictions about 56.1% of the time). Right, so does that mean that this model will make correct predictions 56% of the time on a new, unseen dataset? You already know the answer :)! We used our entire dataset to fit our model. This means that we cannot say anything about how our model will perform on a different dataset and we no longer have any ‘unseen’ data left to test this out. Logistic Regression in Statistical Learning Now, let’s consider logistic regression as applied in statistical learning. We will again consider a basic fixed split. In this example, we will fit our model using data from 1990 up to 2008 and set the data from 2009 and 2010 aside; this will be our test dataset. This is easily achieved by creating a vector of logical values from the data according to our Year criterion. train &lt;- Weekly$Year &lt; 2009 In our previous model, we observed that Lag2 was the only statistically significant predictor. To exemplify how we can use logistic regression in statistical learning, we will build a simple simple with only one predictor. The approach to building the model is the same as the one with which you are already familiar. The exception, of course, is that we will only use part of the dataset to build our model (which in this case is referred to as training the model). To subset our dataset to only include data from years previous to 2009, we use the logical vector train we just created. fit_log_SL &lt;- glm(Direction ~ Lag2, data = Weekly[train, ], family = binomial) Now let’s generate predictions; the function and the overall structure of the code is the same as discussed earlier in the demonstration. The exception is that we used the trained model fit_log_SL to make predictions on the test dataset (Weekly[!train, ]). Using !, we tell R to not include the training data when generating predictions. pred &lt;- predict(fit_log_SL, Weekly[!train, ], type = &quot;response&quot;) &gt; 0.5 Now let’s compute the confusion matrix such that we can compare our predictions on the test data (pred) against the actual values in our dataset (Weekly[!train, ]$Direction)). (t &lt;- table(ifelse(pred, &quot;Up (pred)&quot;, &quot;Down (pred)&quot;), Weekly[!train, ]$Direction)) ## ## Down Up ## Down (pred) 9 5 ## Up (pred) 34 56 If we then compute the overall fraction of correct predictions for the test data we can see that this is higher than the value we obtained using the classical approach (\\(0.561\\)). sum(diag(t)) / sum(t) ## [1] 0.625 Linear Discriminant Analysis How well would linear discriminant analysis address our binary classification problem? In R, LDA can be performed using the lda() function from the MASS package. The basic structure of the function is similar to the other models you have built. fit_lda &lt;- lda(Direction ~ Lag2, data = Weekly[train, ]) The output is, of course, different. prior probabilities of groups: these tells us the way in which the two classes are distributed in our training data (i.e. 44.8 % of the observations correspond to downward market movement whilst 55.2% to upward market movement). group means: the average of our single predictor Lag2 within each class, and are used by LDA as an estimate of \\(μ_{k}\\). coefficient(s) of linear discriminants: tells us how our predictor(s) influence the score that is used to classify the observations into one of the two categories. Here, the coefficient is positive 0.44 and so this indicates that higher values for Lag2 will make the modelmore likely classify an observation as belonging to the Up class; also, the larger the absolute value of the coefficient, the stronger the influence on the model. fit_lda ## Call: ## lda(Direction ~ Lag2, data = Weekly[train, ]) ## ## Prior probabilities of groups: ## Down Up ## 0.4477157 0.5522843 ## ## Group means: ## Lag2 ## Down -0.03568254 ## Up 0.26036581 ## ## Coefficients of linear discriminants: ## LD1 ## Lag2 0.4414162 Now let’s consider what the predict() function does when applied in the context of LDA. result_lda &lt;- predict(fit_lda, Weekly[!train, ]) The output will contain three components: class, posterior, and x, each of which can be accessed using $. The class component is a factor that contains the predictions for market movement (up/down). result_lda$class ## [1] Up Up Down Down Up Up Up Down Down Down Down Up Up Up Up ## [16] Up Up Up Up Up Down Up Up Up Up Up Up Up Up Up ## [31] Up Up Up Up Up Up Up Up Up Up Up Up Up Up Down ## [46] Up Up Up Up Up Up Up Up Up Up Up Down Up Up Up ## [61] Up Up Up Up Up Up Up Up Up Up Up Down Up Down Up ## [76] Up Up Up Down Down Up Up Up Up Up Down Up Up Up Up ## [91] Up Up Up Up Up Up Up Up Up Up Up Up Up Up ## Levels: Down Up The posterior component is matrix that contains the posterior probability that the corresponding observation belongs to a given class. result_lda$posterior ## Down Up ## 986 0.4736555 0.5263445 ## 987 0.3558617 0.6441383 ## 988 0.5132860 0.4867140 ## 989 0.5142948 0.4857052 ## 990 0.4799727 0.5200273 ## 991 0.4597586 0.5402414 ## 992 0.3771117 0.6228883 ## 993 0.5184724 0.4815276 ## 994 0.5480397 0.4519603 ## 995 0.5146118 0.4853882 ## 996 0.5504246 0.4495754 ## 997 0.3055404 0.6944596 ## 998 0.4268160 0.5731840 ## 999 0.3637275 0.6362725 ## 1000 0.4034316 0.5965684 ## 1001 0.4256310 0.5743690 ## 1002 0.4277053 0.5722947 ## 1003 0.4548626 0.5451374 ## 1004 0.4308002 0.5691998 ## 1005 0.3674066 0.6325934 ## 1006 0.5210641 0.4789359 ## 1007 0.4426627 0.5573373 ## 1008 0.3983332 0.6016668 ## 1009 0.4170520 0.5829480 ## 1010 0.4400457 0.5599543 ## 1011 0.4872186 0.5127814 ## 1012 0.4529323 0.5470677 ## 1013 0.4844231 0.5155769 ## 1014 0.4769786 0.5230214 ## 1015 0.3531293 0.6468707 ## 1016 0.3912903 0.6087097 ## 1017 0.4373753 0.5626247 ## 1018 0.4163510 0.5836490 ## 1019 0.4583549 0.5416451 ## 1020 0.4182305 0.5817695 ## 1021 0.4454253 0.5545747 ## 1022 0.4667580 0.5332420 ## 1023 0.4126831 0.5873169 ## 1024 0.4146279 0.5853721 ## 1025 0.4814414 0.5185586 ## 1026 0.4756405 0.5243595 ## 1027 0.3860819 0.6139181 ## 1028 0.4278606 0.5721394 ## 1029 0.4599449 0.5400551 ## 1030 0.5071309 0.4928691 ## 1031 0.4042648 0.5957352 ## 1032 0.4173045 0.5826955 ## 1033 0.4520606 0.5479394 ## 1034 0.4491759 0.5508241 ## 1035 0.4304467 0.5695533 ## 1036 0.4487621 0.5512379 ## 1037 0.4544049 0.5455951 ## 1038 0.4184691 0.5815309 ## 1039 0.4637729 0.5362271 ## 1040 0.4114393 0.5885607 ## 1041 0.4605038 0.5394962 ## 1042 0.5053429 0.4946571 ## 1043 0.4728071 0.5271929 ## 1044 0.4595437 0.5404563 ## 1045 0.4368785 0.5631215 ## 1046 0.4051682 0.5948318 ## 1047 0.4553490 0.5446510 ## 1048 0.4056270 0.5943730 ## 1049 0.4352188 0.5647812 ## 1050 0.4370488 0.5629512 ## 1051 0.4410978 0.5589022 ## 1052 0.4352756 0.5647244 ## 1053 0.4296973 0.5703027 ## 1054 0.4520034 0.5479966 ## 1055 0.4194240 0.5805760 ## 1056 0.4853885 0.5146115 ## 1057 0.5411727 0.4588273 ## 1058 0.4177113 0.5822887 ## 1059 0.5100863 0.4899137 ## 1060 0.4470646 0.5529354 ## 1061 0.4816287 0.5183713 ## 1062 0.4138300 0.5861700 ## 1063 0.4157203 0.5842797 ## 1064 0.5017234 0.4982766 ## 1065 0.5216975 0.4783025 ## 1066 0.3738247 0.6261753 ## 1067 0.4666863 0.5333137 ## 1068 0.3993705 0.6006295 ## 1069 0.4506892 0.5493108 ## 1070 0.4235170 0.5764830 ## 1071 0.5036414 0.4963586 ## 1072 0.4593288 0.5406712 ## 1073 0.4587988 0.5412012 ## 1074 0.3965787 0.6034213 ## 1075 0.4428192 0.5571808 ## 1076 0.4287787 0.5712213 ## 1077 0.4202670 0.5797330 ## 1078 0.4523464 0.5476536 ## 1079 0.4258989 0.5741011 ## 1080 0.4358286 0.5641714 ## 1081 0.4409698 0.5590302 ## 1082 0.4491046 0.5508954 ## 1083 0.3986650 0.6013350 ## 1084 0.4804910 0.5195090 ## 1085 0.4487050 0.5512950 ## 1086 0.4616361 0.5383639 ## 1087 0.4074084 0.5925916 ## 1088 0.4311115 0.5688885 ## 1089 0.4452828 0.5547172 The x component contains the linear discriminants. result_lda$x ## LD1 ## 986 -0.80594669 ## 987 2.92755168 ## 988 -2.01984129 ## 989 -2.05074043 ## 990 -0.99972841 ## 991 -0.37865579 ## 992 2.22702414 ## 993 -2.17875113 ## 994 -3.08806854 ## 995 -2.06045158 ## 996 -3.16178505 ## 997 4.66982149 ## 998 0.64322275 ## 999 2.66623327 ## 1000 1.38038783 ## 1001 0.68030171 ## 1002 0.61541353 ## 1003 -0.22769145 ## 1004 0.51874338 ## 1005 2.54484381 ## 1006 -2.25820605 ## 1007 0.14971942 ## 1008 1.54282900 ## 1009 0.94956560 ## 1010 0.23094000 ## 1011 -1.22176077 ## 1012 -0.16810026 ## 1013 -1.13612602 ## 1014 -0.90791384 ## 1015 3.01892483 ## 1016 1.76839269 ## 1017 0.31392625 ## 1018 0.97163642 ## 1019 -0.33539700 ## 1020 0.91248664 ## 1021 0.06408467 ## 1022 -0.59406691 ## 1023 1.08728746 ## 1024 1.02593061 ## 1025 -1.04475287 ## 1026 -0.86686213 ## 1027 1.93613085 ## 1028 0.61055795 ## 1029 -0.38439421 ## 1030 -1.83135657 ## 1031 1.35390286 ## 1032 0.94162011 ## 1033 -0.14117387 ## 1034 -0.05200779 ## 1035 0.52977878 ## 1036 -0.03920672 ## 1037 -0.21356613 ## 1038 0.90498257 ## 1039 -0.50225234 ## 1040 1.12657351 ## 1041 -0.40160944 ## 1042 -1.77662096 ## 1043 -0.77990314 ## 1044 -0.37203455 ## 1045 0.32937582 ## 1046 1.32521081 ## 1047 -0.24269960 ## 1048 1.31064407 ## 1049 0.38102152 ## 1050 0.32407882 ## 1051 0.19827520 ## 1052 0.37925585 ## 1053 0.55317384 ## 1054 -0.13940820 ## 1055 0.87496626 ## 1056 -1.16570091 ## 1057 -2.87618875 ## 1058 0.92881904 ## 1059 -1.92184689 ## 1060 0.01332181 ## 1061 -1.05049128 ## 1062 1.05109133 ## 1063 0.99150015 ## 1064 -1.66582548 ## 1065 -2.27762836 ## 1066 2.33428828 ## 1067 -0.59185983 ## 1068 1.50972278 ## 1069 -0.09879791 ## 1070 0.74651414 ## 1071 -1.72453384 ## 1072 -0.36541331 ## 1073 -0.34908091 ## 1074 1.59888886 ## 1075 0.14486384 ## 1076 0.58186590 ## 1077 0.84848129 ## 1078 -0.15000219 ## 1079 0.67191480 ## 1080 0.36204062 ## 1081 0.20224795 ## 1082 -0.04980071 ## 1083 1.53223501 ## 1084 -1.01561940 ## 1085 -0.03744106 ## 1086 -0.43648132 ## 1087 1.25414279 ## 1088 0.50903222 ## 1089 0.06849883 To obtain our predictions, we can simply extract the class element. Alternatively, if we want to directly extract just the predictions from the class element, we can use the predict function as we did earlier in the demonstration. #either pred_lda &lt;- result_lda$class #or pred_lda &lt;- predict(fit_lda, Weekly[!train, ], type = &quot;response&quot;)$class Now let’s compute the confusion matrix for our LDA classifier such that we can compare our predictions on the test data against the actual values in our dataset. (t &lt;- table(pred_lda, Weekly[!train, ]$Direction)) ## ## pred_lda Down Up ## Down 9 5 ## Up 34 56 And now the fraction of correct predictions which, we can see is identical to that obtained for logistic regression. sum(diag(t)) / sum(t) ## [1] 0.625 Quadratic Discriminant Analysis Let’s now consider how quadratic discriminant analysis would address our binary classification problem. The code syntax is identical to that for linear discriminant analysis and the qda function is also part of the MASS package. fit_qda &lt;- qda(Direction ~ Lag2, data = Weekly[train, ]) In terms of prior probabilities and group means, the output is identical to that of linear discriminant analysis. However, the output does not include the coefficients of the linear discriminants for obvious reasons. fit_qda ## Call: ## qda(Direction ~ Lag2, data = Weekly[train, ]) ## ## Prior probabilities of groups: ## Down Up ## 0.4477157 0.5522843 ## ## Group means: ## Lag2 ## Down -0.03568254 ## Up 0.26036581 The prediction function works in the same way as for LDA, except that it will produce only two elements (class, and posterior); again, the x element will not be included since we are dealing with a quadratic function. pred_qda &lt;- predict(fit_qda, Weekly[!train, ], type = &quot;response&quot;)$class The confusion matrix is computed in the same way. (t &lt;- table(pred_qda, Weekly[!train, ]$Direction)) ## ## pred_qda Down Up ## Down 0 0 ## Up 43 61 The fraction of correct predictions is lower than that for logistic regression and for LDA. We therefore conclude that in this context, QDA does not perform well in comparison to the previous two approaches. sum(diag(t)) / sum(t) ## [1] 0.5865385 \\(K\\)-nearest neighbours Earlier in the course, we covered K-nearest neighbour classification. Let’s now explore how this approach is used in R and how it performs in the context of our market movement problem. To implement KNN in R, the most commonly used package is class. Note that it is possible to be confronted with the following error when loading the package: Error: (converted from warning) package ‘class’ was built under R version ... This will occur if you are using an older version of R than that under which the package was built. The best option is to update RStudio. If that is not possible (e.g. due to system requirements), then another option is to suppress it using suppressWarnings(library(class)) in your console. This should allow you to use the functions from the package. In R, we build our model using the knn() function from the class package. This function works differently to those we have covered so far for linear and logistic regression, and for LDA and QDA. This is because the knn() function both fits the model AND generates predictions. There are four arguments required: argument 1: predictors in our training data, argument 2: predictors in our test data, argument 3: outcome variable in our training data, argument 4: the value for \\(k\\); note the function specifies 1 nearest neighbour by default but I added it here for illustration purposes (this value needs to be added only when using a value other than 1). Now, you may wonder why I also included the drop = FALSE argument when I subsetting the dataset. fit_knn &lt;- knn(Weekly[train, &quot;Lag2&quot;, drop = FALSE], Weekly[!train, &quot;Lag2&quot;, drop = FALSE], Weekly$Direction[train], k = 1 ) Before we proceed to interpret the results, let’s see what output we would produce if we subsetted our dataset such that we extract our predictor from the training data without the drop = FALSE argument. This looks like a vector, right? Weekly[train, &quot;Lag2&quot;] ## [1] 1.572 0.816 -0.270 -2.576 3.514 0.712 1.178 -1.372 0.807 ## [10] 0.041 1.253 -2.678 -1.793 2.820 4.022 0.750 -0.017 2.420 ## [19] -1.225 1.171 -2.061 0.729 0.112 2.480 -1.552 -2.259 -2.428 ## [28] -2.708 -2.292 -4.978 3.547 0.260 -2.032 -1.739 -1.693 1.781 ## [37] -3.682 4.150 -2.487 2.343 0.606 1.077 -0.637 2.260 1.716 ## [46] -0.284 1.508 -0.913 -2.349 -1.798 5.393 1.156 2.077 4.751 ## [55] 2.702 -0.924 1.318 1.209 -0.363 -1.635 2.106 0.037 1.343 ## [64] 0.999 -1.348 0.470 -1.329 -0.892 1.370 3.269 -2.668 0.754 ## [73] -1.188 -1.745 0.787 1.649 1.044 -0.856 1.641 -0.015 -0.398 ## [82] 2.228 0.320 -1.601 -1.416 1.129 -0.521 -1.205 0.052 2.897 ## [91] -2.115 1.853 0.401 -2.614 -1.694 -0.245 1.034 1.417 0.668 ## [100] 5.018 3.169 -1.011 0.906 -0.807 -1.613 0.565 0.338 -0.255 ## [109] 0.309 -2.001 0.346 1.345 -1.896 -0.483 0.682 2.906 -1.687 ## [118] 0.858 0.853 -1.433 0.958 0.321 -0.450 -0.900 -1.486 -0.054 ## [127] 2.062 0.692 0.241 -0.967 3.064 -1.256 0.246 -1.205 -0.002 ## [136] 0.540 0.599 0.798 -2.029 -0.936 -1.903 2.253 0.576 1.106 ## [145] -0.263 1.161 0.999 0.823 0.442 0.387 1.741 -0.342 -0.923 ## [154] -1.529 1.888 -0.238 0.612 2.313 -0.969 -2.330 2.110 0.616 ## [163] 0.834 0.078 -0.533 -1.427 0.102 1.607 -2.653 0.723 0.482 ## [172] -0.622 1.429 0.976 -0.029 -0.622 -0.800 0.884 -0.393 0.509 ## [181] -0.527 0.303 0.230 0.123 0.325 1.337 0.960 0.174 0.082 ## [190] -0.626 -0.262 0.798 -0.210 1.996 -1.327 0.984 -1.766 1.266 ## [199] -0.599 0.099 0.395 -0.207 0.528 0.214 -0.199 0.740 1.066 ## [208] -0.040 0.838 -1.857 0.079 -0.530 -0.346 -0.285 0.366 0.990 ## [217] -2.225 -3.216 0.298 -0.206 0.325 0.733 -0.685 -0.822 2.427 ## [226] 0.530 0.612 -0.317 -0.048 -3.414 0.768 0.751 1.025 -0.231 ## [235] 1.137 -0.255 1.061 0.377 2.183 -0.593 -0.597 0.643 -2.445 ## [244] 0.661 -1.645 3.076 -0.897 1.910 -2.425 0.015 -0.190 -1.989 ## [253] 0.223 -1.399 2.649 0.224 -0.122 0.307 1.148 -0.255 1.207 ## [262] 1.756 0.587 0.106 1.274 -0.551 0.855 1.215 1.100 -0.052 ## [271] 1.140 0.555 -0.145 1.223 1.051 1.044 -1.210 0.859 1.692 ## [280] -0.858 2.252 1.830 -0.902 2.133 0.633 -1.120 1.682 -0.709 ## [289] -0.685 0.739 0.159 0.668 1.568 1.863 -0.278 0.461 -0.329 ## [298] 0.345 0.506 -1.321 1.875 0.364 1.240 -0.017 1.168 1.730 ## [307] -0.185 -0.712 0.650 0.127 -2.416 1.665 1.600 2.288 3.229 ## [316] -1.278 1.713 -2.232 -1.687 1.252 1.433 -0.787 1.605 -2.920 ## [325] 1.313 1.301 -1.810 1.630 2.579 1.435 -1.384 0.626 -1.108 ## [334] 0.149 0.568 -1.967 -1.711 -1.154 -0.443 4.181 -0.059 0.470 ## [343] 0.274 -2.255 0.566 3.791 0.954 -0.122 2.225 -0.114 1.450 ## [352] -1.393 0.407 3.844 0.930 1.506 1.107 -2.301 -1.482 2.776 ## [361] 1.058 -1.158 1.533 2.195 -0.728 2.030 0.432 2.396 -0.830 ## [370] -1.366 1.789 -1.466 -1.144 -1.303 -2.065 -2.672 3.889 -0.127 ## [379] 6.219 1.453 0.603 2.083 0.148 1.147 4.110 0.608 -1.268 ## [388] 3.338 -0.026 -0.151 2.566 0.889 -1.436 -3.506 2.523 -2.606 ## [397] 3.289 -0.553 2.879 -0.557 2.096 0.202 -2.360 -0.267 -2.869 ## [406] 1.409 0.091 3.742 -0.798 2.972 -3.090 -0.693 -1.090 4.120 ## [415] -4.856 3.646 -0.408 2.369 3.283 0.754 1.384 1.463 0.605 ## [424] 1.224 2.859 -0.338 2.488 -1.072 1.085 -1.320 1.182 -1.147 ## [433] 0.053 0.157 -1.770 2.112 -1.348 0.165 2.957 1.167 1.562 ## [442] 1.926 -3.872 -1.765 -2.786 -2.451 1.740 -5.004 -5.184 3.611 ## [451] 1.093 2.417 -4.034 -1.816 7.317 1.349 2.615 3.854 -1.340 ## [460] 3.361 2.473 -1.308 -0.874 1.849 3.219 0.241 3.731 -2.496 ## [469] -1.453 4.444 -3.145 -0.748 0.739 -0.072 2.999 1.499 0.363 ## [478] -1.269 0.851 4.223 -2.177 2.870 -1.597 0.735 -0.535 -0.561 ## [487] -2.139 1.990 -2.569 3.803 -2.050 5.771 0.867 1.105 -4.359 ## [496] -2.080 -2.140 2.106 0.673 0.872 0.665 -0.411 -1.201 -4.348 ## [505] 0.427 4.148 -6.632 4.348 4.708 0.536 1.885 1.858 -0.378 ## [514] 1.177 -1.134 0.282 2.626 0.748 -1.891 1.643 -1.624 -5.634 ## [523] 4.721 -2.615 -2.958 -0.946 5.686 -1.001 4.975 4.301 -1.891 ## [532] 1.186 -10.538 5.748 1.247 -1.363 -0.815 -0.986 -2.056 7.202 ## [541] -1.375 0.515 -1.569 0.910 1.671 2.102 -1.973 -4.074 3.031 ## [550] 0.609 1.351 0.987 0.951 -1.727 -1.920 -1.166 -0.843 -1.916 ## [559] -2.471 1.656 -1.242 3.415 -4.255 0.127 -1.897 -1.978 4.156 ## [568] -4.215 -0.473 1.097 -1.661 1.556 1.819 0.924 -0.404 -2.572 ## [577] -1.006 -4.277 -0.938 -0.062 -6.720 -0.930 1.799 -2.749 4.880 ## [586] 5.026 0.810 1.082 -1.653 3.716 -1.089 -1.348 0.340 -4.000 ## [595] 0.905 -0.079 -2.760 2.107 -0.397 -0.415 0.707 -1.992 -2.369 ## [604] 1.976 -4.334 -4.217 -11.050 7.780 2.924 1.892 -1.664 2.900 ## [613] -1.576 3.045 1.637 1.027 -0.947 1.655 -3.041 1.941 1.409 ## [622] 0.990 -2.295 -1.573 0.506 -0.978 -2.315 0.726 -1.299 3.848 ## [631] 2.874 0.159 -1.497 -0.114 -2.149 -1.044 1.275 -4.342 -0.269 ## [640] -1.718 4.891 -2.058 -1.539 -3.712 -1.972 -1.800 0.069 -0.080 ## [649] -6.839 -7.992 0.600 1.337 5.137 2.215 1.302 -2.635 -2.418 ## [658] -0.460 -4.992 -2.132 -3.238 4.339 5.874 1.499 0.369 -0.690 ## [667] 1.687 2.277 0.619 -2.572 -2.494 0.706 -2.273 3.791 2.089 ## [676] -2.780 -4.478 -0.662 -3.040 0.627 1.591 -0.828 -1.458 0.528 ## [685] 7.503 -3.605 1.778 -1.200 2.911 0.585 3.479 0.358 1.167 ## [694] -1.173 3.254 2.508 0.086 0.716 -1.955 0.971 1.262 -0.483 ## [703] 0.540 -1.855 -0.261 1.338 0.241 1.505 1.327 -0.270 1.735 ## [712] -3.807 3.310 0.797 0.121 -1.002 2.119 0.238 -0.272 -1.435 ## [721] 2.214 0.312 1.191 1.352 0.664 1.149 1.207 1.602 0.151 ## [730] -0.913 1.028 0.267 -0.148 0.073 1.041 -3.137 -0.963 -0.155 ## [739] 3.046 -0.218 -0.413 0.528 -2.920 -0.777 -0.273 -0.195 2.480 ## [748] 0.162 1.245 -0.128 -0.052 -0.798 -1.117 -1.026 -1.379 1.429 ## [757] -3.426 0.078 3.151 0.858 0.529 0.924 0.412 -1.634 1.927 ## [766] -0.827 -1.242 -1.124 3.145 3.183 1.544 -1.168 1.052 0.720 ## [775] -0.266 0.522 1.334 0.148 -2.123 -0.141 -1.406 0.299 2.704 ## [784] 0.189 -0.308 0.814 0.887 -1.803 -0.869 -1.532 0.128 0.706 ## [793] -3.266 0.831 0.411 1.253 -1.477 3.053 0.799 -0.230 0.175 ## [802] 1.573 -2.086 0.241 1.458 1.325 0.469 0.041 -0.629 0.324 ## [811] -0.868 -1.198 1.072 1.926 -0.288 -1.827 1.112 -2.678 -0.780 ## [820] -0.588 1.595 1.813 1.195 1.097 1.601 -0.250 -0.451 0.631 ## [829] 0.106 -1.606 2.977 0.168 -2.029 1.762 -1.534 0.234 1.598 ## [838] 0.170 -0.171 -0.451 2.016 -0.329 -0.620 0.049 -0.492 1.719 ## [847] -0.051 1.156 -2.604 -1.875 1.036 0.630 -2.788 -0.061 -0.563 ## [856] 2.065 -0.372 -2.314 0.331 3.085 0.063 -0.986 2.807 -0.554 ## [865] 1.229 -0.922 1.597 -0.370 1.603 1.029 1.188 0.218 0.639 ## [874] -0.947 1.217 1.470 -0.018 -0.303 0.940 1.224 -1.144 0.534 ## [883] -0.606 1.491 -0.016 -0.582 1.843 -0.713 1.216 -0.299 -4.412 ## [892] 1.130 -1.133 3.544 -1.062 1.612 0.630 2.168 0.655 0.773 ## [901] 0.015 1.122 -0.461 1.360 -1.866 1.674 -1.980 0.053 1.802 ## [910] 1.441 -1.185 -4.899 -1.775 1.436 -0.530 2.312 -0.364 -1.387 ## [919] 2.112 2.796 0.066 2.020 0.270 -3.917 2.309 -1.669 -3.706 ## [928] 0.347 -1.237 2.807 1.588 -2.440 1.125 -0.402 -4.522 -0.752 ## [937] -5.412 0.409 4.871 -4.596 1.405 0.231 -1.661 -2.800 -0.404 ## [946] 3.212 -1.075 4.195 -2.742 4.314 0.540 1.149 -1.812 2.670 ## [955] -3.467 1.777 -2.835 -0.048 -3.096 -3.001 -1.211 -1.854 1.710 ## [964] -0.232 0.203 2.857 0.145 -0.462 -0.725 -3.159 0.756 0.270 ## [973] -3.331 -9.399 -18.195 4.596 -6.781 10.491 -3.898 -6.198 -8.389 ## [982] 12.026 -2.251 0.418 0.926 If you wrap the code within the class function you can indeed confirm that the output is a vector. class(Weekly[train, &quot;Lag2&quot;]) ## [1] &quot;numeric&quot; The knn() requires that the training and test data be specified as either a matrix or a dataframe. If we set the drop argument to FALSE, then we are essentially telling R NOT to delete the dimensions of our object when we are subsetting it such that it keeps the row numbers, thereby producing a dataframe. class(Weekly[train, &quot;Lag2&quot;, drop = FALSE]) ## [1] &quot;data.frame&quot; You must pay close attention to the requirements and specifications for the functions with which you build your models. Often, you will be prompted by error messages in the console. For example, the knn() function expects either a matrix or a dataframe. If you do not set drop = FALSE you will not be able to proceed: Error in knn(Weekly[train, \"Lag2\"], Weekly[!train, \"Lag2\"], Weekly$Direction[train], : dims of 'test' and 'train' differ Other times, the error may be not so severe as to impede the function from working, but incorrectly structured data or improperly coded variables will lead to the function producing invalid results. Remember to carefully explore the arguments using the Help tab (e.g. ?knn). Now let’s return to our results and produce a confusion matrix. (t &lt;- table(fit_knn, Weekly[!train, ]$Direction)) ## ## fit_knn Down Up ## Down 21 29 ## Up 22 32 Our overall fraction of correct predictions is 0.5. Therefore, the KNN classifier (\\(k = 1\\)) performs the worst out of all other classifiers we have explored so far (but only slightly worse than QDA). sum(diag(t)) / sum(t) ## [1] 0.5096154 But before we move on to our next classifier, let’s consider other values for \\(k\\) for illustration purposes. To ensure consistent results, we also set the seed (to 1 in this case). We fit KNN for up to \\(k = 30\\) by using the base R sapply to apply the knn() function to every integer from 1 to 30 and to then calculate the overall fraction of correct prediction. set.seed(1) knn_k &lt;- sapply(1:30, function(k) { fit &lt;- knn(Weekly[train, &quot;Lag2&quot;, drop = FALSE], Weekly[!train, &quot;Lag2&quot;, drop = FALSE], Weekly$Direction[train], k = k ) mean(fit == Weekly[!train, ]$Direction) }) We can then create a plot to observe at what value for k the overall fraction of correct predictions is highest. This fraction stabilises at a value for \\(k\\) somewhere between \\(k = 10\\) and \\(k = 15\\). plot(1:30, knn_k, type = &quot;o&quot;, xlab = &quot;k&quot;, ylab = &quot;Fraction correct&quot;) We can find this out directly by asking R the index of the first time a maximum value among all other values appears. Our classifier appears to perform best when \\(k = 12\\). (k &lt;- which.max(knn_k)) ## [1] 12 Now let’s re-evaluate our KNN classifier on the test data using `\\(k = 12\\) and compute the confusion matrix. fit_knn &lt;- knn(Weekly[train, &quot;Lag2&quot;, drop = FALSE], Weekly[!train, &quot;Lag2&quot;, drop = FALSE], Weekly$Direction[train], k = 12 ) table(fit_knn , Weekly[!train, ]$Direction) ## ## fit_knn Down Up ## Down 18 18 ## Up 25 43 Now, the overall fraction of correct predictions is higher than it was for \\(k = 1\\) but this fraction still does not outperform logistic regression and LDA. mean(fit_knn == Weekly[!train, ]$Direction) ## [1] 0.5865385 Naive Bayes Finally, let’s evaluate the performance of Naive Bayes and conclude which approach performs best for our market movement classification problem. A useful package for Naive Bayes is e1071. Like the class package, do note that you may be prompted with a similar error when loading the package (this can also be addressed by either updating RStudio or suppressing the warning). fit_NBayes &lt;- naiveBayes(Direction ~ Lag2, data = Weekly, subset = train) Before generating the predictions, let’s explore the output of the fit. There are two important components: - A-priori probabilities: i.e. prior probabilities (distribution of the classes for the response variable) - Conditional probabilities: parameters of the model for the predictor by class. For a numeric variable (as is our predictor in this case), the parameters shown are the mean [,1] and standard deviation [,2] for the predictor values in each class; for a categorical variable, these would be conditional probabilities for the predictor in each class. The a priori probabilities can be extracted by specifying fit_NBayes$apriori and the conditional probabilities can be extracted using fit_NBayes$tables. fit_NBayes ## ## Naive Bayes Classifier for Discrete Predictors ## ## Call: ## naiveBayes.default(x = X, y = Y, laplace = laplace) ## ## A-priori probabilities: ## Y ## Down Up ## 0.4477157 0.5522843 ## ## Conditional probabilities: ## Lag2 ## Y [,1] [,2] ## Down -0.03568254 2.199504 ## Up 0.26036581 2.317485 Now let’s predict market movement. pred_NBayes &lt;- predict(fit_NBayes, Weekly[!train, ], type = &quot;class&quot;) And finally, generate our confusion matrix. (t &lt;- table(pred_NBayes, Weekly[!train, ]$Direction)) ## ## pred_NBayes Down Up ## Down 0 0 ## Up 43 61 Our overall fraction of correct predictions is \\(0.5865385\\). Naive Bayes performs slightly better than KNN with \\(k = 1\\) (\\(0.5\\)) and the same as QDA (\\(0.5865385\\)). sum(diag(t)) / sum(t) ## [1] 0.5865385 Based on the approaches we have implemented in this demonstration, logistic regression and LDA perform best. "],["overview-3.html", "Overview", " Overview Section 4: Resampling Methods This section is comprised two demonstrations adapted from the core textbook for this course: James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd ed.New York: Springer. https://www.statlearning.com/ Learning Outcomes: appreciate the importance of resampling methods in gauging model accuracy and performance; appreciate the differences between cross-validation and bootstrapping; apply cross-validation and bootstrapping in R and interpret the output. Function Description Package sample() obtain a random sample with or without replacement base R lm() fit linear model base R mean() compute the arithmetic mean base R poly() apply polynomials base R glm() fit generalised linear model base R cv.glm() calculate estimates for LOOCV (default) or K-fold CV (by specifying K) boot boot() generate bootstrap estimates boot "],["demonstration-1-3.html", "Demonstration 1 Data and Variables 1.1 The Validation Set Approach 1.2 Leave-One-Out Cross-Validation 1.3 \\(k\\)-Fold Cross-Validation", " Demonstration 1 So far, we have established the importance of training and test data and you now should have a robust understanding of the differences between training and test error rates in relation to model performance. However, several challenges remain and these include: the size of the training set and whether it is sufficient to adequately train our model; the size of the test set and whether it is sufficient to adequately evaluate our model; the process of testing our model on just a single test set; the nature of the training set. We are dealing with a gap here: the gap between the need to reliably measure model performance and the process of splitting the data we have available. In this demonstration, you will learn more about how cross-validation provides us with ways to address this gap. Data and Variables In this demonstration, we will make use the Auto dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Auto dataset loads automatically. You should already be familiar with the Auto dataset but below is a reminder of the variables it contains: mpg: miles per gallon cylinders: Number of cylinders between 4 and 8 displacement: Engine displacement (cu. inches) horsepower: Engine horsepower weight: Vehicle weight (lbs.) acceleration: Time to accelerate from 0 to 60 mph (sec.) year: Model year origin: Origin of car (1. American, 2. European, 3. Japanese) name: Vehicle name In addition to ISRL2, you will also require the boot package which first needs to be installed. This package is required for both LOOCV and \\(k\\)-fold CV. library(ISLR2) library(boot) #don&#39;t forget to install it first 1.1 The Validation Set Approach Let’s first consider the validation set approach. The Auto dataset has a total of 392 observations and the goal is to randomly split these observations using a 50/50 ratio. We can perform this randomised split using the base R sample() function. The first argument within sample() is 392. This is the total number of observations available to us from the Auto dataset. The second argument is 196. This represents the number of observations we want to select from the total we have available which is 392. Therefore, the sample() function will return a vector of 196 unique integers which represents a subset of 196 indices from a total of 392. Note: by default, the sample() function conducts the sampling without replacement. training_data &lt;- sample(392, 196) training_data ## [1] 362 319 118 371 87 59 375 50 300 132 160 212 272 170 63 128 384 18 ## [19] 333 303 291 226 177 357 202 184 62 72 391 108 39 223 78 189 348 149 ## [37] 193 378 232 136 156 28 385 258 325 312 20 379 296 346 194 126 373 61 ## [55] 354 119 266 40 173 192 268 182 143 288 388 80 14 24 358 162 100 157 ## [73] 48 133 67 70 90 117 287 104 8 289 310 98 228 33 120 22 6 166 ## [91] 209 129 153 251 381 204 196 313 122 301 304 152 340 30 275 77 220 45 ## [109] 349 94 260 316 5 242 109 53 224 352 309 7 96 23 389 365 37 321 ## [127] 246 278 284 35 335 75 171 139 323 331 214 215 217 17 343 198 185 95 ## [145] 154 27 106 195 337 68 32 326 245 240 269 93 237 89 85 3 147 165 ## [163] 172 285 332 297 10 342 216 225 262 46 213 127 187 236 203 322 293 218 ## [181] 359 146 44 334 92 26 86 233 201 176 360 238 211 336 239 25 Now we need to tell R to fit a linear regression model using only the observations corresponding to the training set. We can do so by using the subset argument which tells R to only select the 196 observations that are indexed at the specific positions as defined by the training_data object. In this way, the model will be fitted using only the observations from Auto dataset that are defined by this vector of indices. fit &lt;- lm(mpg ~ horsepower, data = Auto, subset = training_data) Let’s now calculate the Mean Squared Error for the test dataset. We obtain the predictions using thepredict() function with which you are already familiar. mean((Auto$mpg - predict(fit, Auto))[-training_data]^2) ## [1] 22.05303 Let’s try applying some transformations to our predictor horsepower using the poly() function and then calculate the test MSE values to observe how the MSE values change. We first fit a second-degree polynomial regression model: lm.fit2 &lt;- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = training_data) Then calculate the test MSE: mean((Auto$mpg - predict(lm.fit2, Auto))[-training_data]^2) ## [1] 16.46636 Now we fit a third-degree polynomial regression model lm.fit3 &lt;- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = training_data) Then calculate the test MSE: mean((Auto$mpg - predict(lm.fit3, Auto))[-training_data]^2) ## [1] 16.43655 What would happen if we choose a different training dataset instead? We can try this out by setting the seed to 2, for example. set.seed(2) training_data2 &lt;- sample(392, 196) We then run a new set of models: a linear regression model, a second degree polynomial model, and a third degree polynomial model and then calculate the test MSE values for each of the three models. linear regression model and corresponding test MSE: lm.fit &lt;- lm(mpg ~ horsepower, data = Auto, subset = training_data2) mean((Auto$mpg - predict(lm.fit, Auto))[-training_data2]^2) ## [1] 25.72651 quadratic regression model and corresponding test MSE: lm.fit2 &lt;- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = training_data2) mean((Auto$mpg - predict(lm.fit2, Auto))[-training_data2]^2) ## [1] 20.43036 cubic regression model and corresponding test MSE: lm.fit3 &lt;- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = training_data2) mean((Auto$mpg - predict(lm.fit3, Auto))[-training_data2]^2) ## [1] 20.38533 The results obtained using the second training dataset are consistent with the findings from the first training dataset whereby a model that predicts mpg using a quadratic function of horsepower performs better than a model that involves only a linear function of horsepower. Also, there is little evidence in favor of a model that uses a cubic function of horsepower. Overall, the main take-away point is that if we were to choose different training sets, we will obtain somewhat different errors on the validation set. This is an important point of reflection: if we implement the fixed split approach directly such that we fit our model once on our single training set and then immediately evaluate the predictive accuracy of that model once on our single test set, Nevertheless, the validation set approach still does not solve this challenge. 1.2 Leave-One-Out Cross-Validation Now, let’s consider LOOCV. In R, one way to compute the LOOCV estimate is by using functions from the boot package. This package nevertheless requires that the models are built using the glm() function, including linear models. The glm() function will produce identical results to lm() when fitting a linear model. You do not need to specify the family argument as you did for logistic regression since it is already set by default to gaussian. Fitting a linear model with glm() is the same as with lm(). glm.fit &lt;- glm(mpg ~ horsepower, data = Auto) We then perform LOOCV using the cv.glm() function from the boot package. cv.err &lt;- cv.glm(Auto, glm.fit) The output includes a list of several components. The component of relevance for LOOCV is delta. cv.err$delta ## [1] 24.23151 24.23114 The delta component therefore provides us with the results of the cross-validation. The first value is the raw cross-validation estimate of prediction error whilst the second value is the adjusted cross-validation estimate that compensates for the bias introduced by not using leave-one-out cross-validation. In our case, the two values are identical to two decimal places and so our cross-validation estimate for the test error is approximately \\(24.23\\). We can repeat this procedure for increasingly complex polynomial fits. Instead of writing separate code for each model fit, we can automate the process using the for() function that initiates a for loop which iteratively fits polynomial regressions for polynomials of order \\(i=1\\) to \\(i=10\\), computes the associated cross-validation error, and stores it in the \\(i\\)th element of the vector cv.error. cv.error &lt;- rep(0, 10) for (i in 1:10) { glm.fit &lt;- glm(mpg ~ poly(horsepower, i), data = Auto) cv.error[i] &lt;- cv.glm(Auto, glm.fit)$delta[1] } The output shows us the estimated test MSE for the linear model and polynomials up to the 10th degree. cv.error ## [1] 24.23151 19.24821 19.33498 19.42443 19.03321 18.97864 18.83305 18.96115 ## [9] 19.06863 19.49093 We can see a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials. 1.3 \\(k\\)-Fold Cross-Validation Finally, let’s consider \\(k\\)-fold cross-Validation. The same function (cv.glm()) can be used by setting the value of K. By default, the value of K is equal to the number of observations in data which therefore gives us LOOCV. In this example, we will use \\(k=10\\), a common choice for \\(k\\). We will again perform the same approach of increasingly complex polynomial fits as we did for LOOCV. The code is identical to the one we used to LOOCV except that of course, we specified K = 10. set.seed(17) cv.error.10 &lt;- rep(0, 10) for (i in 1:10) { glm.fit &lt;- glm(mpg ~ poly(horsepower, i), data = Auto) cv.error.10[i] &lt;- cv.glm(Auto, glm.fit, K = 10)$delta[1] } The output shows us a similar pattern to the estimated test MSE for the linear model and polynomials up to the 10th degree: we see a sharp drop between the linear and quadratic fits but once again no improvement with higher order polynomials. We can therefore conclude that a quadratic fit is suitable. cv.error.10 ## [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666 ## [9] 18.87013 20.95520 We saw earlier that when we LOOCV, the two values were essentially the same. In the case of \\(k\\)-fold CV, these values differ slightly but they are still very similar to each other. cv.glm(Auto, glm.fit, K = 10)$delta ## [1] 19.71557 19.60616 "],["demonstration-2-1.html", "Demonstration 2 Data and Variables Estimating the Accuracy of a Statistic of Interest Estimating the Accuracy of a Linear Regression Model", " Demonstration 2 How can we implement bootstrapping in R? Follow the steps outlined in this demonstration to build basic coding skills required to perform bootstrapping. Data and Variables In this demonstration, we will make use the Portfolio and Auto package from the core textbook (James et. al 2021). These datasets are part of the ISRL2 package. Portfolio is a dataframe contains 100 observations specifically designed to illustrate the concept of bootstrapping and the way in which it is applied in R. The dataframe contains only two variables: X: returns for Asset X, and Y: returns for Asset Y. In addition to ISRL2, you will also require the boot package. library(ISLR2) library(boot) #you should have already installed this for Demo 1 Estimating the Accuracy of a Statistic of Interest One of the advantages of bootstrapping as a resampling method is that it can be applied in almost all situations and it is quite simple to perform it with R. The first step is to create a function that computes our statistic of interest. This function should take as input the \\((X,Y)\\) data as well as a vector indicating which observations should be used to estimate \\(\\alpha\\). The function then outputs the estimate for \\(\\alpha\\) based on the selected observations. We will call our function alpha.fn(), alpha.fn &lt;- function(data, index) { X &lt;- data$X[index] Y &lt;- data$Y[index] (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y)) } For example, the following command tells R to estimate \\(\\alpha\\) using all \\(100\\) observations from our dataset. alpha.fn(Portfolio, 1:100) ## [1] 0.5758321 We now use the sample() function to randomly select \\(100\\) observations from the range \\(1\\) to \\(100\\), with replacement. This is equivalent to constructing one new bootstrap dataset and recomputing \\(\\hat{\\alpha}\\) based on the new data set. We can perform this command many, many times, recording all of the corresponding estimates for \\(\\alpha\\), and computing the resulting standard deviation. set.seed(7) alpha.fn(Portfolio, sample(100, 100, replace = T)) ## [1] 0.5385326 However, the boot() function automates this approach. For example, we can tell R to repeat this command 1000 times and we obtain \\(R=1,000\\) bootstrap estimates for \\(\\alpha\\). The final output shows that using the original data,\\(\\hat{\\alpha}=0.5758\\), and that the bootstrap estimate for \\({\\rm SE}(\\hat{\\alpha})\\) is \\(0.0897\\). boot(Portfolio, alpha.fn, R = 1000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Portfolio, statistic = alpha.fn, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 0.5758321 0.0007959475 0.08969074 Estimating the Accuracy of a Linear Regression Model The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. In this example, we will assess the variability of the estimates for \\(\\beta_0\\) and \\(\\beta_1\\), the intercept and slope terms for a simple linear regression model that uses horsepower to predict mpg in the Auto data set. We will compare the estimates obtained using the bootstrap to those obtained using the formulas for \\({\\rm SE}(\\hat{\\beta}_0)\\) and \\({\\rm SE}(\\hat{\\beta}_1)\\). We first create a simple function, boot.fn(), which takes in the Auto data set as well as a set of indices for the observations, and returns the intercept and slope estimates for the linear regression model. We then apply this function to the full set of \\(392\\) observations in order to compute the estimates of \\(\\beta_0\\) and \\(\\beta_1\\) on the entire data set. Note that we do not need the { and } at the beginning and end of the function because it is only one line long. boot.fn &lt;- function(data, index) coef(lm(mpg ~ horsepower, data = data, subset = index)) boot.fn(Auto, 1:392) ## (Intercept) horsepower ## 39.9358610 -0.1578447 The boot.fn() function can also be used in order to create bootstrap estimates for the intercept and slope terms by randomly sampling from among the observations with replacement (as we did in the previous example with the `Portfolio dataset). We can see slight differences for the coefficient estimates each time we repeat this procedure. set.seed(1) boot.fn(Auto, sample(392, 392, replace = T)) ## (Intercept) horsepower ## 40.3404517 -0.1634868 boot.fn(Auto, sample(392, 392, replace = T)) ## (Intercept) horsepower ## 40.1186906 -0.1577063 Now, we use the boot() function to compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms. boot(Auto, boot.fn, 1000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Auto, statistic = boot.fn, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 39.9358610 0.0544513229 0.841289790 ## t2* -0.1578447 -0.0006170901 0.007343073 The results show that the bootstrap estimate for \\({\\rm SE}(\\hat{\\beta}_0)\\) is \\(0.84\\), and that the bootstrap estimate for \\({\\rm SE}(\\hat{\\beta}_1)\\) is \\(0.0073\\). How different are those estimates from those provided by fitting the model? Let’s now compute the standard errors for the regression coefficients in a linear model (we use summary and then extract the coefficients using $coef) summary(lm(mpg ~ horsepower, data = Auto))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.9358610 0.717498656 55.65984 1.220362e-187 ## horsepower -0.1578447 0.006445501 -24.48914 7.031989e-81 The standard error estimates for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) obtained from fitting the model using lm() are \\(0.717\\) for the intercept and \\(0.0064\\) for the slope. Interestingly, these are somewhat different from the estimates obtained using the bootstrap. Does this indicate a problem with the bootstrap? In fact, it suggests the opposite. Reflect on the formulae we covered earlier in the course and the assumptions on which these formulae rely. For example, they depend on the unknown parameter \\(\\sigma^2\\) (the noise variance) and so we estimate \\(\\sigma^2\\) using the RSS. Now although the formulas for the standard errors do not rely on the linear model being correct, the estimate for \\(\\sigma^2\\) does. If we create a scatterplot and examine the relationship between mpg and horsepower, we can see that there is a non-linear relationship and so the residuals from a linear fit will be inflated, and so will \\(\\hat{\\sigma}^2\\). plot(Auto$mpg, Auto$horsepower) Secondly, the standard formulas assume (somewhat unrealistically) that the \\(x_i\\) are fixed, and all the variability comes from the variation in the errors \\(\\epsilon_i\\). The bootstrap approach does not rely on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) than is the summary() function. Given the non-linear association betwen the two variables, let’s now compute the bootstrap standard error estimates and the standard linear regression estimates that result from fitting a quadratic model to the data. boot.fn &lt;- function(data, index) coef( lm(mpg ~ horsepower + I(horsepower^2), data = data, subset = index) ) set.seed(1) boot(Auto, boot.fn, 1000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Auto, statistic = boot.fn, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 56.900099702 3.511640e-02 2.0300222526 ## t2* -0.466189630 -7.080834e-04 0.0324241984 ## t3* 0.001230536 2.840324e-06 0.0001172164 Since this model provides a good fit to the data, there is now a better correspondence between the bootstrap estimates and the standard estimates of \\({\\rm SE}(\\hat{\\beta}_0)\\), \\({\\rm SE}(\\hat{\\beta}_1)\\) and \\({\\rm SE}(\\hat{\\beta}_2)\\). summary(lm(mpg ~ horsepower + I(horsepower^2), data = Auto))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 56.900099702 1.8004268063 31.60367 1.740911e-109 ## horsepower -0.466189630 0.0311246171 -14.97816 2.289429e-40 ## I(horsepower^2) 0.001230536 0.0001220759 10.08009 2.196340e-21 "],["overview-4.html", "Overview", " Overview Section 5 In this section, we will cover the following functions: Learning Outcomes: "],["practical.html", "Practical", " Practical "],["overview-5.html", "Overview", " Overview Section 6 In this section, we will cover the following functions: Learning Outcomes: "],["practical-3.html", "Practical", " Practical "],["overview-6.html", "Overview", " Overview Section 7 In this section, we will cover the following functions: Learning Outcomes: "],["practical-4.html", "Practical", " Practical "],["overview-7.html", "Overview", " Overview Section 8 In this section, we will cover the following functions: Learning Outcomes: "],["practical-5.html", "Practical", " Practical "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
