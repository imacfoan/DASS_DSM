[["index.html", "R Notebook About", " R Notebook Dr. Ioana Macoveciuc About Welcome to SOST70033 Data Science Modelling! This notebook will host the materials for all R practicals and demonstrations for this course unit. The notebook follows the same section-based structure as the learning materials on Blackboard. To access this notebook, you can bookmark it like any other website in your favourite web browser. For each section, you will have at least one practical to complete and each of these can be accessed by using the sidebar menu on the left hand side of your screen. Clicking on the headings in each section will expand the menu and each task can be accessed individually. The sidebar menu can be toggled on and off by clicking on the Toggle Sidebar button. Other customisation options include changing the font, font size, and the appearance. You also have a handy search button. This notebook is also designed to work well on tablet and mobile devices; to enhance your experience on these devices, it is recommended that you hide the sidebar menu and navigate across sections and subsections using the right and left arrows. The code, as well as the output and answers are provided at the end of each section. The R code can be copied and pasted directly in your R console or script by clicking on the following icon: 1: Before beginning, it is recommended that you create a RStudio project for this course and work through the exercises and tasks in each section using this project. 2: You should write and save your answers to the exercises and tasks in R scripts. You should have at least one R script for each course section. 3: The recommended approach for a ‘clean’ working directory is to place all the data files you plan to use in a separate folder (e.g. a folder called data) within your R project working directory. You should always use simple names that allow you easy access to the contents when you want to either explore the folder on your machine or specify the path to these folders in R. 4: To build a robust knowledge basis and adequately develop your practical programming skills, it is absolutely essential that you first attempt all tasks and exercises on your own before comparing your answers with those provided in this notebook. "],["overview.html", "Overview", " Overview Section 1: Introduction to Data Science - The Basics of Statistical Learning This section is comprised of a demonstration and two practicals. The two practicals will make use of exercises adapted from the core textbook for this course: James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd ed. New York: Springer. https://www.statlearning.com/ Learning Outcomes: appreciate the importance of the mean squared error; indexing using base R; creating scatterplot matrices; creating new variables; transforming existing variables; using functionals; ‘calling’ on masked functions from specific packages; translating base R code to tidyverse and vice versa. In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. You can access the R documentation in the Help tab using? (e.g. ?read.csv) Function Description Package read.csv() read csv files base R read_csv() read csv files tidyverse co l umn_to_rownames() convert column to row names tidyverse rownames() obtain names of rows base R summary() obtain summary statistics base R summarise() object summaries tidyverse (dplyr) group_by() group by one or more variables tidyverse (dplyr) pairs() produce a matrix of scatterplots base R plot() create a plot base R ggplot() generic function for creating a plot tidyverse (ggplot2) mutate() create, modify, and delete columns tidyverse (dplyr) if_else() condition-based function tidyverse (dplyr) as_factor() create factor using existing levels tidyverse (forcats) par() set graphical parameters base R mfrow() par() parameter base R slice_min() and slice_max() index rows by location (smallest and largest values of a variable respectively) tidyverse (dplyr) sapply() applying a function over list or vector base R select() keep or drop columns tidyverse (dplyr) note that this function is also available through the MASS package (we will not cover this in this section) pivot_longer() lengthen data tidyverse (tidyr) where() selection helper tidyverse median(), mean(), sd() median, mean, standard deviation base R "],["demonstration-a-more-in-depth-consideration-of-model-accuracy.html", "Demonstration: A more in-depth consideration of model accuracy The Simulation", " Demonstration: A more in-depth consideration of model accuracy So, I mentioned earlier that statistical modeling allows us to explain and dig deeper into understanding the nature of a process of a phenomenon of interest and that we can mathematically describe this phenomenon as an unknown function \\(f\\). In its related data, we annotate the information that can explain the problem’s behaviour as \\(x\\) (this could be a single value or a vector or matrix of values or something more complex) and the results of its behaviour as \\(y\\) (this could also be a single value or something more complex). Mathematically we present this as: \\[y_i = f(x_i)\\] And of course, let’s not forget about the variation of the \\(f\\)’s behaviour: \\[y_i = f(x_i) + \\epsilon_i\\] The standard approach for the error/noise is to adopt the following distribution structure \\(\\epsilon \\sim N(0,\\sigma)\\), meaning that the \\(\\epsilon\\) value is considered to be normally distributed with a mean of \\(0\\) and standard deviation \\(\\sigma\\). Remember that this implies that the negative and positive impacts from the noise are considered equally likely, and that small errors are much more likely than extreme ones. To evaluate the performance of a statistical model on a given data set, we “observe” the discrepancies between the predicted response values for given observations obtained by the chosen statistical model (\\(\\hat{f}(x_i)\\)) and the true response values for these observations (\\(y_i\\)). As I already mentioned, the most commonly used performance measure for regression problems is the mean squared error (MSE): \\[MSE = \\frac{1}{n} \\sum^{n}_{i=1}(y_i - \\hat{f}(x_i))^2\\] The MSE above is computed using the training data, used to fit the model, and as such it would be more correct to refer to it as the training MSE. But we have already discussed that we are not really interested how well the model “works” on the training data, ie. \\(\\hat{f}(x_i) \\approx y_i\\). We are more interested in the accuracy of the predictions \\(\\hat{f}(x_0)\\) that are obtained when we apply the model to previously unseen test data \\((x_0, y_0)\\), ie. \\(\\hat{f}(x_0) \\approx y_0\\). In other words, we want to chose the model with the lowest \\(test\\) \\(MSE\\) and to do so we need a large enough number of observations in the test data to calculate the mean square prediction error for the test observations (\\(x_0\\), \\(y_0\\)), to which we can refer to as the \\(test\\) \\(MSE\\). \\[mean(y_0 - \\hat{f}(x_0))^2\\] In statistics nothing is black and white. In other words, nothing is straightforward and there are many considerations one needs to take into account when applying statistical modelling. The same applies in this situation. We need to realise that when a given model yields a small training MSE but a large test MSE, we are said to be overfitting the data. The statistical model is too ‘preoccupied’ to find patterns in the training data and consequently is modelling the patterns that are caused by random effects, rather than by true features of the unknown function \\(f\\). When the model overfits the training data, the test MSE will be large because the modelled features that the model identifies in the training data just do not exist in the test data. Saying that, regardless of overfitting occurring or not, we expect the training MSE to be smaller than the test MSE, as most of the statistical models either directly or indirectly seek to minimise the training MSE. We need to be aware that the chosen model needs to be flexible and not rigid and glued to the training data. MSE is simple to calculate and yet, despite its simplicity, it can provide us with a vital insight into modelling. It consists of two intrinsic components that can provide greater enlightenment about how the model works: variance: degree to which \\(\\hat{f}\\) would differ if we estimated it using a different training dataset (ideally, it should not vary greatly). bias: average difference between the estimator \\(\\hat{y}\\) and true value \\(y\\). Mathematically we write bias as: \\[E[\\hat{y} – y]\\] As it is not squared difference, it can be either positive or negative. Positive or negative bias implies that the model is over or under “predicting”, while the value of zero would indicate that the model is likely to predict too much as it is to predict too little. The latter implies that the model can be completely wrong in its prediction and still provide us with the bias of zero. This implies that bias on its own provides little information about how correct the model is in its prediction. Remember that \\(y = f(x) + \\epsilon\\) and therefore \\(\\hat{f}\\) is not directly approximating \\(f\\). \\(\\hat{f}\\) models \\(y\\) that includes the noise. It can be challenging and in some cases even impossible to meaningfully capture the behaviour of \\(f\\) itself when the noise term is very large. We have discussed earlier that we assess model accuracy using MSE which is calculated by: obtaining the error (i.e. discrepancy between \\(\\hat{f}(x_i)\\) and \\(y_i\\)) squaring this value (making negative into the positive same, and greater error gets more severe penalty) then averaging these results The mean of the squared error is the same as the expectation\\(^*\\) of our squared error so we can go ahead and simplify this a slightly: \\[MSE=E[(y-\\hat{f}(x))^2]\\] Now, we can break this further and write it as: \\[MSE = E[(f(x)+ \\epsilon - \\hat{f}(x))^2]\\] Knowing that computing the expectation of adding two random variables is the same as computing the expectation of each random variable and then adding them up: \\[E[X+Y]=E[X] +E[Y]\\] and recalling that \\(\\sigma^2\\) represent the variance of \\(\\epsilon\\), where the variance is calculated as: \\[E[X^2]-E[X]^2,\\] and therefore: \\[Var(\\epsilon) = \\sigma^2 = E[\\epsilon^2] - E[\\epsilon]^2,\\] with \\(\\epsilon \\sim N(0,\\sigma)\\) \\[E[\\epsilon]^2=\\mu^2=0^2=0\\] we get: \\[E[\\epsilon^2] = \\sigma^2\\] This helps us to rearranging MSE further and calculate it as: \\[MSE=σ^2+E[−2f(x)\\hat{f}(x)+f(x)^2+\\hat{f}(x)^2]\\] where \\(\\sigma^2\\) is the variance of the noise (i.e. \\(\\epsilon\\)). Therefore, the variance of the noise in data is an irreducible part of the MSE. Regardless of how good the model is, it can never reduce the MSE to being less than the variance related to the noise (i.e. error). This error represents the lack of information in data used to adequately explain everything that can be known about the phenomena being modelled. We should not look at it as a nuisance, as it can often guide us to further explore the problem and look into other factors that might be related to it. Knowing that: \\[Var(X) = E[X^2] - E[X]^2\\] we can apply further transformation and break MSE into: \\[MSE = \\sigma^{2}+Var[f(x)-\\hat{f}(x)]+E[f(x)-\\hat{f}(x)]^2\\] The term \\(Var[f(x)-\\hat{f}(x)]\\) is the variance in the model predictions from the true output values and the last term \\(E[f(x)-\\hat{f}(x)]^2\\) is just the bias squared. We mentioned earlier that that unlike variance, bias can be positive or negative, so we square this value in order to make sure it is always positive. With this in mind, we realise that MSE consists of: model variance model bias and irreducible error \\[\\text{Mean Squared Error}=\\text{Model Variance} + \\text{Model Bias}^2 + \\text{Irreducible Error}\\] We come to the conclusion that in order to minimise the expected test error, we need to select a statistical model that simultaneously achieves low variance and low bias. Note that in practice we will never know what the variance \\(\\sigma^2\\) of the error \\(\\epsilon\\) is, and therefore we will not be able to determine the variance and the bias of the model. However, since \\(\\sigma^2\\) is constant, we have to decrease either bias or variance to improve the model. Testing the model using the test data and observing its bias and variance can help us address some important issues, allowing us to reason with the model. If the model fails to find the \\(f\\) in data and is systematically over or under predicting, this will indicate underfitting and it will be reflected through high bias. However, high variance when working with test data indicates the issue of overfitting. What happens is that the model has learnt the training data really well and is too close to the data, so much so that it starts to mistake the \\(f(x) + \\epsilon\\) for true \\(f(x)\\). The Simulation To better understand these concepts, let us run a small simulation study. We will: simulate a function \\(f\\) apply the error, i.e. noise sampled from a distribution with a known variance To make it very simple and illustrative we will use a linear function \\(f(x) = 3 + 2x\\) to simulate response \\(y\\) with the error \\(e\\thicksim N(\\mu =0, \\sigma^2 =4)\\), where \\(x\\) is going to be a sequence of numbers between \\(0\\) and \\(10\\) in steps of \\(0.1\\). We will examine the simulations for the models that over and under estimate the true \\(f\\), and since it is a linear function we will not have a problem identifying using simple linear regression modelling. Let’s start with a simulation in which we will model the true function with \\(\\hat{f}_{1} = 4 + 2x\\) and \\(\\hat{f}_{2} = 1 + 2x\\). set.seed(123) ## set the seed of R‘s random number generator # simulate function f(x) = 3 + 2x f &lt;- function(x){ 3 + 2 * x } # generate vector X x &lt;- seq(0, 10, by = 0.05) # the error term coming from N(mean = 0, variance = 4) e &lt;- rnorm(length(x), mean = 0, sd = 2) # simulate the response vector Y y &lt;- f(x) + e # plot the simulated data plot(x, y, cex = 0.75, pch = 16, main = &quot;Simulation: 1&quot;) abline(3, 2, col =&quot;gray&quot;, lwd = 2, lty = 1) # model fitted to simulated data f_hat_1&lt;- function(x){ 4 + 2 * x } f_hat_2 &lt;- function(x){ 1 + 2 * x } y_bar = mean(y) # average value of the response variable y f_hat_3 &lt;- function(x){ y_bar } # add the line representing the fitted model abline(1, 2, col = &quot;red&quot;, lwd = 2, lty = 2) abline(4, 2, col = &quot;blue&quot;, lwd = 2, lty = 1) abline(y_bar, 0, col = &quot;darkgreen&quot;, lwd = 2, lty = 3) legend(7.5, 10, legend=c(&quot;f_hat_1&quot;, &quot;f_hat_2&quot;, &quot;f_hat_3&quot;, &quot;f&quot;), col = c(&quot;blue&quot;, &quot;red&quot;, &quot;darkgreen&quot;, &quot;gray&quot;), lwd = c(2, 2, 2, 2), lty = c(1:3, 1), text.font = 4, bg = &#39;lightyellow&#39;) Observing the graph, we notice that \\(\\hat{f}_1\\) and \\(\\hat{f}_2\\), depicted in blue and red lines respectively, follow the data nicely, but are also systematically over (in the case of \\(\\hat{f}_1\\) and under (in the case of \\(\\hat{f}_2\\)) estimating the values. In the simple model \\(\\hat{f}_3\\), the line represents the value \\(\\bar{y}\\), which cuts the data in half. As we mentioned earlier, knowing the true function \\(f\\) and the distribution of \\(\\epsilon\\) we can calculate: - the MSE using the simulated data and the estimated model, - the model’s bias and variance which will allow for the calculation of the “theoretical” MSE. This will allow for more detailed illustration about the information contained in the model’s bias and variance. # calculate MSE from data MSE_data1 = mean((y - f_hat_1(x))^2) MSE_data2 = mean((y - f_hat_2(x))^2) MSE_data3 = mean((y - f_hat_3(x))^2) # model bias bias_1 = mean(f_hat_1(x) - f(x)) bias_2 = mean(f_hat_2(x) - f(x)) bias_3 = mean(f_hat_3(x) - f(x)) # model variance var_1 = var(f(x) - f_hat_1(x)) var_2 = var(f(x) - f_hat_2(x)) var_3 = var(f(x) - f_hat_3(x)) # calculate &#39;theoretical&#39; MSE MSE_1 = bias_1^2 + var_1 + 2^2 MSE_2 = bias_2^2 + var_2 + 2^2 MSE_3 = bias_3^2 + var_3 + 2^2 for (i in 1:1){ cat (c(&quot;==============================================&quot;,&quot;\\n&quot;)) cat (c(&quot;=============== f_hat_1 ================&quot;,&quot;\\n&quot;)) cat(c(&quot;MSE_data1 = &quot;, round(MSE_data1, 2), sep = &#39;\\n&#39;)) cat(c(&quot;bias_1 = &quot;, bias_1, sep = &#39;\\n&#39; )) cat(c(&quot;variance_1 = &quot;, round(var_1, 2), sep = &#39;\\n&#39; )) cat(c(&quot;MSE_1 = 4 + bias_1^2 + variance_1 = &quot;, MSE_1, sep = &#39;\\n&#39; )) cat(c(&quot;==============================================&quot;,&quot;\\n&quot;)) cat(c(&quot;=============== f_hat_2 ================&quot;,&quot;\\n&quot;)) cat(c(&quot;MSE_data2 = &quot;, round(MSE_data2, 2), sep = &#39;\\n&#39;)) cat(c(&quot;bias_2 = &quot;, bias_2, sep = &#39;\\n&#39; )) cat(c(&quot;variance_2 = &quot;, round(var_2, 2), sep = &#39;\\n&#39; )) cat(c(&quot;MSE_2 = 4 + bias_2^2 + variance_2 = &quot;, MSE_2, sep = &#39;\\n&#39; )) cat(c(&quot;==============================================&quot;,&quot;\\n&quot;)) cat(c(&quot;=============== f_hat_3 ================&quot;,&quot;\\n&quot;)) cat(c(&quot;average y = &quot;, round(y_bar, 2), sep = &#39;\\n&#39;)) cat(c(&quot;MSE_data3 = &quot;, round(MSE_data3, 2), sep = &#39;\\n&#39;)) cat(c(&quot;bias_3 = &quot;, round(bias_3, 2), sep = &#39;\\n&#39; )) cat(c(&quot;variance_3 = &quot;, round(var_3, 2), sep = &#39;\\n&#39; )) cat(c(&quot;MSE_3 = 4 + bias_3^2 + variance_3 = &quot;, round(MSE_3, 2), sep = &#39;\\n&#39; )) cat(c(&quot;==============================================&quot;,&quot;\\n&quot;)) } ## ============================================== ## =============== f_hat_1 ================ ## MSE_data1 = 4.61 ## bias_1 = 1 ## variance_1 = 0 ## MSE_1 = 4 + bias_1^2 + variance_1 = 5 ## ============================================== ## =============== f_hat_2 ================ ## MSE_data2 = 7.64 ## bias_2 = -2 ## variance_2 = 0 ## MSE_2 = 4 + bias_2^2 + variance_2 = 8 ## ============================================== ## =============== f_hat_3 ================ ## average y = 13 ## MSE_data3 = 36.7 ## bias_3 = 0 ## variance_3 = 33.84 ## MSE_3 = 4 + bias_3^2 + variance_3 = 37.84 ## ============================================== \\(\\hat{f}_1\\) has a positive bias because it is overestimating data points more often than it is underestimating, but as it does it so consistently in comparison to \\(f\\) that produces variance of zero. In contrast \\(\\hat{f}_2\\) has a negative bias as it is underestimating simulated data, but nonetheless it also does it consistently, resulting in zero variance with \\(f\\). Unlike in the previous two model estimates which follow the data points, \\(\\hat{f}_3\\) predicts the mean value of data, resulting in no bias since it evenly underestimates and overestimates \\(f(x)\\). However, the variation in prediction between \\(f\\) and \\(\\hat{f}_3\\) is obvious. Given that the true function \\(f\\) is linear, by applying simple regression modelling, we should be able to estimate it easily in R using the \\(lm()\\) function. # model fitted to simulated data f_hat_4&lt;- function(x){ lm(y~x) } # plot the simulated data plot(x, y, cex = 0.75, pch = 16, main = &quot;Simulation: 2&quot;) abline(3, 2, col =&quot;gray&quot;, lwd = 2, lty = 1) # add the line representing the fitted model abline(lm(y~x), col =&quot;red&quot;, lwd = 2, lty = 3) legend(7.5, 8, legend=c(&quot;f_hat_4&quot;, &quot;f&quot;), col = c(&quot;red&quot;, &quot;gray&quot;), lwd = c(2, 2), lty = c(3, 1), text.font = 4, bg = &#39;lightyellow&#39;) Since the true function \\(f\\) is a linear model it is not surprising that \\(\\hat{f}_4\\) can learn it, resulting in zero values of the model’s bias and variance. # calculate MSE from data MSE_data4 = mean((y - predict(f_hat_4(x)))^2) # model bias bias_4 = mean(predict(f_hat_4(x)) - f(x)) # model variance var_4 = var(f(x) - predict(f_hat_4(x))) # calculate &#39;theoretical&#39; MSE MSE_4 = bias_4^2 + var_4 + 2^2 for (i in 1:1){ cat (c(&quot;==============================================&quot;,&quot;\\n&quot;)) cat (c(&quot;=============== f_hat_4 ================&quot;,&quot;\\n&quot;)) cat(c(&quot;MSE_data4 = &quot;, round(MSE_data4, 2), sep = &#39;\\n&#39;)) cat(c(&quot;bias_4 = &quot;, round(bias_4, 2), sep = &#39;\\n&#39; )) cat(c(&quot;variance_4 = &quot;, round(var_4, 2), sep = &#39;\\n&#39; )) cat(c(&quot;MSE_4 = 4 + bias_4^2 + variance_4 = &quot;, round(MSE_4, 2), sep = &#39;\\n&#39; )) cat (c(&quot;==============================================&quot;,&quot;\\n&quot;)) } ## ============================================== ## =============== f_hat_4 ================ ## MSE_data4 = 3.62 ## bias_4 = 0 ## variance_4 = 0 ## MSE_4 = 4 + bias_4^2 + variance_4 = 4 ## ============================================== We realise that the \\(MSE\\) is more than just a simple error measurement. It is a tool that informs and educates the modeller about the performance of the model being used in the analysis of a problem. It is packed with information that when unwrapped can provide a greater insight into not just the fitted model, but the nature of the problem and its data. \\(^*\\) The Expectation of a Random Variable is the sum of its values weighted by their probability. For example: What is the average toss of a fair six-sided die? If the random variable is the top face of a tossed, fair six-sided die, then the probability of die landing on \\(X\\) is: \\[f(x) = \\frac{1}{6}\\] for \\(x = 1, 2,... 6\\). Therefore, the average toss, i.e. the expected value of \\(X\\) is: \\[E(X) = 1(\\frac{1}{6}) + 2(\\frac{1}{6}) + 3(\\frac{1}{6}) + 4(\\frac{1}{6}) + 5(\\frac{1}{6}) + 6(\\frac{1}{6}) = 3.5\\] Of course, we do not expect to get a fraction when tossing a die, i.e. we do not expect the toss to be 3.5, but rather an integer number between 1 to 6. So, what the expected value is really saying is what is the expected average of a large number of tosses will be. If we toss a fair, six-side die hundreds of times and calculate the average of the tosses we will not get the exact 3.5 figure, but we will expect it to be close to 3.5. This is a theoretical average, not the exact one that is realised. "],["practical-1.html", "Practical 1", " Practical 1 For the tasks below, you will require the College dataset from the core textbook (James et. al 2021). Click here to download the file: College.csv . Remember to place your data file in a separate subfolder within your R project working directory. The College dataset contains statistics for a large number of US Colleges from the 1995 issue of US News and World Report. It is a data frame with 777 observations and 18 variables. The variables are: Private : Public/private indicator Apps : Number of applications received Accept : Number of applicants accepted Enroll : Number of new students enrolled Top10perc : New students from top 10% of high school class Top25perc : New students from top 25% of high school class F.Undergrad : Number of full-time undergraduates P.Undergrad : Number of part-time undergraduates Outstate : Out-of-state tuition Room.Board : Room and board costs Books : Estimated book costs Personal : Estimated personal spending PhD : Percent of faculty with Ph.D.’s Terminal : Percent of faculty with terminal degree S.F.Ratio: Student/faculty ratio perc.alumni : Percent of alumni who donate Expend : Instructional expenditure per student Grad.Rate : Graduation rate Task 1 Import the dataset in an object called college using the tidyverse read_csv() function. If you then have a look at the contents of the data object using View(), you will notice that the first column contains the names of all of the universities in the dataset. You will also notice that it has a strange name. Actually, these data should not be treated as a variable (column) since it is just a list of university names. Task 2 Keeping the list of names in the data object, transform this column such that the university names in the column become row names. Hint: use the column_to_rownames() function from dplyr. How would have your approach to this task differed if you would have imported the dataset using base R? Try it! Task 3 Produce summary statistics for all variables in the data object. Task 4 Create a scatterplot matrix of the first three numeric variables. Task 5 Produce side by side box plots of Outstate versus Private using base R. Did this work? Why? Task 6 Using the Top10perc variable, create a new categorical variable called Elite such that universities are divided into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%. Hint: use a combination of mutate() and if_else(). Task 7 Produce side by side box plots of the new Elite variable and Outstate. How would you produce a similar plot using base R? Task 8 Use base R to produce a multipanel plot that displays histograms of the following variables: Apps, perc.alumni, S.F.Ratio, Expend. Hint: use par(mfrow=c(2,2)) to set up a 2x2 panel. Try to adjust the specifications (e.g. breaks). Task 9 Using Accept and Apps, create a new variable that describes acceptance rate. Name this variable acceptance_rate. Hint: use mutate(). Task 10 Using the acceptance_rate variable, find out which university has the lowest acceptance rate. Hint: for a tidyverse approach, you can use slice_min(). Task 11 Using the acceptance_rate variable, find out which university has the highest acceptance rate. "],["practical-2.html", "Practical 2", " Practical 2 For the tasks below, you will require the Boston dataset. This dataset is part of the MASS R package. To access the dataset, load the MASS package (install the package first, if you have not done so previously). Task 1 Install and load the MASS package. You will also require tidyverse. Does R provide any message when loading MASS? Why does this matter? Task 2 Find out more about the Boston dataset variables by accessing the R Documentation. To explore the Boston dataset, simply type the name of the data object into the console or use View() What type of data structure is the Boston dataset? What are its dimensions? How many categorical and quantitative variables are there? Task 3 Find the class of all 14 variables. Hint: use sapply. Task 4 Using a tidyverse approach, calculate the mean, median, and standard deviation of all variables of class numeric. What is the mean pupil-teacher ratio? What is the median and mean per capita crime rate? Which value do you think is more suitable to describe per capita crime rate? Task 5 Using a base R approach, create a 2x2 multipanel plot of crim versus age, dis, rad, tax and ptratio. What can you say about the relationships between age, dis, rad, tax, and crim? Task 6 Using a base R approach, create and display histograms of crim, tax and ptratio in a 1x2 multipanel plot. Set the breaks argument to 25 . What do these histograms indicate? "],["answers.html", "Answers Practical 1 Practical 2", " Answers Practical 1 For the tasks below, you will require the College dataset from the core textbook (James et. al 2021). Click here to download the file: College.csv . Remember to place your data file in a separate subfolder within your R project working directory. This data file contains 18 variables for 777 different universities and colleges in the United States. The variables are: Private : Public/private indicator Apps : Number of applications received Accept : Number of applicants accepted Enroll : Number of new students enrolled Top10perc : New students from top 10% of high school class Top25perc : New students from top 25% of high school class F.Undergrad : Number of full-time undergraduates P.Undergrad : Number of part-time undergraduates Outstate : Out-of-state tuition Room.Board : Room and board costs Books : Estimated book costs Personal : Estimated personal spending PhD : Percent of faculty with Ph.D.’s Terminal : Percent of faculty with terminal degree S.F.Ratio: Student/faculty ratio perc.alumni : Percent of alumni who donate Expend : Instructional expenditure per student Grad.Rate : Graduation rate Task 1 Import the dataset in an object called college using the tidyverse read_csv() function. # Remember to load tidyverse first library(tidyverse) college &lt;- read_csv(&quot;data/College.csv&quot;) If you have a look at the contents of the data object using View(), you will notice that the first column contains the names of all of the universities in the dataset. You will also notice that it has a strange name. Actually, these data should not be treated as a variable (column) since it is just a list of university names. Task 2 Keeping the list of names in the data object, transform this column such that the university names in the column become row names. Hint: use the column_to_rownames() function from dplyr. college &lt;- college %&gt;% column_to_rownames(var = &quot;...1&quot;) How would have your approach to this task differed if you would have imported the dataset using base R? Try it! The data file could have instead been imported using read.csv(): college &lt;- read.csv(\"data/College.csv\") Using the base R approach, the first column containing the university names would have been named “X”, as shown below using View(). Now, how would be go about transforming the contents of the first column into row names? This would require two steps. First, we assign the column contents to rows names. rownames(college) &lt;- college[, 1] If you have another look at the data object, you will see that the rows have now been renamed using the university names in the “X” column, but the column is still part of the dataset. We therefore need to tell R to delete the column. college &lt;- college[, -1] Task 3 Produce summary statistics for all variables in the data object. summary(college) ## Private Apps Accept Enroll ## Length:777 Min. : 81 Min. : 72 Min. : 35 ## Class :character 1st Qu.: 776 1st Qu.: 604 1st Qu.: 242 ## Mode :character Median : 1558 Median : 1110 Median : 434 ## Mean : 3002 Mean : 2019 Mean : 780 ## 3rd Qu.: 3624 3rd Qu.: 2424 3rd Qu.: 902 ## Max. :48094 Max. :26330 Max. :6392 ## Top10perc Top25perc F.Undergrad P.Undergrad ## Min. : 1.00 Min. : 9.0 Min. : 139 Min. : 1.0 ## 1st Qu.:15.00 1st Qu.: 41.0 1st Qu.: 992 1st Qu.: 95.0 ## Median :23.00 Median : 54.0 Median : 1707 Median : 353.0 ## Mean :27.56 Mean : 55.8 Mean : 3700 Mean : 855.3 ## 3rd Qu.:35.00 3rd Qu.: 69.0 3rd Qu.: 4005 3rd Qu.: 967.0 ## Max. :96.00 Max. :100.0 Max. :31643 Max. :21836.0 ## Outstate Room.Board Books Personal ## Min. : 2340 Min. :1780 Min. : 96.0 Min. : 250 ## 1st Qu.: 7320 1st Qu.:3597 1st Qu.: 470.0 1st Qu.: 850 ## Median : 9990 Median :4200 Median : 500.0 Median :1200 ## Mean :10441 Mean :4358 Mean : 549.4 Mean :1341 ## 3rd Qu.:12925 3rd Qu.:5050 3rd Qu.: 600.0 3rd Qu.:1700 ## Max. :21700 Max. :8124 Max. :2340.0 Max. :6800 ## PhD Terminal S.F.Ratio perc.alumni ## Min. : 8.00 Min. : 24.0 Min. : 2.50 Min. : 0.00 ## 1st Qu.: 62.00 1st Qu.: 71.0 1st Qu.:11.50 1st Qu.:13.00 ## Median : 75.00 Median : 82.0 Median :13.60 Median :21.00 ## Mean : 72.66 Mean : 79.7 Mean :14.09 Mean :22.74 ## 3rd Qu.: 85.00 3rd Qu.: 92.0 3rd Qu.:16.50 3rd Qu.:31.00 ## Max. :103.00 Max. :100.0 Max. :39.80 Max. :64.00 ## Expend Grad.Rate ## Min. : 3186 Min. : 10.00 ## 1st Qu.: 6751 1st Qu.: 53.00 ## Median : 8377 Median : 65.00 ## Mean : 9660 Mean : 65.46 ## 3rd Qu.:10830 3rd Qu.: 78.00 ## Max. :56233 Max. :118.00 Task 4 Create a scatterplot matrix of the first three numeric variables. pairs(college[,2:4]) Task 5 Produce side by side box plots of Outstate versus Private using base R. plot(college$Private, college$Outstate, xlab = &quot;Private&quot;, ylab = &quot;Outstate&quot;) Did this work? Why? Using the plot() base R function to produce a box plot would produce an error since the Private variable is of class character. Most statistical functions will not work with character vectors. Error in plot.window(...) : need finite 'xlim' values In addition: Warning messages: 1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion 2: In min(x) : no non-missing arguments to min; returning Inf 3: In max(x) : no non-missing arguments to max; returning -Inf Creating a box plot with tidyverse would work. college %&gt;% ggplot(aes(x = Private, y = Outstate)) + geom_boxplot() However, it is important to note that if a variable is not of the right class, then this might have unintended consequences for example, when building models. In this case, the Private variable must be transformed into a factor. Task 6 Using the Top10perc variable, create a new categorical variable called Elite such that universities are divided into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%. Hint: use a combination of mutate() and if_else(). college &lt;- college %&gt;% mutate(Elite = if_else(Top10perc &gt; 50, &quot;Yes&quot;, &quot;No&quot;), Elite = as_factor(Elite)) #do not forget the factor transformation step (categorical variables are factors in R) Task 7 Produce side by side box plots of the new Elite variable and Outstate. college %&gt;% ggplot(aes(x = Elite, y = Outstate)) + geom_boxplot() How would you produce a similar plot using base R? plot(college$Elite, college$Outstate, xlab = \"Elite\", ylab = \"Outstate\") Task 8 Use base R to produce a multipanel plot that displays histograms of the following variables: Apps, perc.alumni, S.F.Ratio, Expend. Hint: use par(mfrow=c(2,2)) to set up a 2x2 panel. Try to adjust the specifications (e.g. breaks). # An example is shown below. Note that the purpose of the mfrow parameter is # to change the default way in which R displays plots. # Once applied, all plots you create later will also be displayed in a 2x2 grid. # To revert back, you need to enter par(mfrow=c(1,1)) into the console. par(mfrow=c(2,2)) hist(college$Apps) hist(college$perc.alumni, col=2) hist(college$S.F.Ratio, col=3, breaks=10) hist(college$Expend, breaks=100) Task 9 Using Accept and Apps, create a new variable that describes acceptance rate. Name this variable acceptance_rate. Hint: use mutate(). college &lt;- college %&gt;% mutate(acceptance_rate = Accept / Apps) Task 10 Using the acceptance_rate variable, find out which university has the lowest acceptance rate. Hint: for a tidyverse approach, you can use slice_min(). college %&gt;% slice_min(order_by = acceptance_rate, n = 1) ## Private Apps Accept Enroll Top10perc Top25perc ## Princeton University Yes 13218 2042 1153 90 98 ## F.Undergrad P.Undergrad Outstate Room.Board Books Personal ## Princeton University 4540 146 19900 5910 675 1575 ## PhD Terminal S.F.Ratio perc.alumni Expend Grad.Rate Elite ## Princeton University 91 96 8.4 54 28320 99 Yes ## acceptance_rate ## Princeton University 0.1544863 Task 11 Using the acceptance_rate variable, find out which university has the highest acceptance rate. college %&gt;% slice_max(order_by = acceptance_rate, n = 1) ## Private Apps Accept Enroll Top10perc Top25perc ## Emporia State University No 1256 1256 853 43 79 ## Mayville State University No 233 233 153 5 12 ## MidAmerica Nazarene College Yes 331 331 225 15 36 ## Southwest Baptist University Yes 1093 1093 642 12 32 ## University of Wisconsin-Superior No 910 910 342 14 53 ## Wayne State College No 1373 1373 724 6 21 ## F.Undergrad P.Undergrad Outstate Room.Board ## Emporia State University 3957 588 5401 3144 ## Mayville State University 658 58 4486 2516 ## MidAmerica Nazarene College 1100 166 6840 3720 ## Southwest Baptist University 1770 967 7070 2500 ## University of Wisconsin-Superior 1434 417 7032 2780 ## Wayne State College 2754 474 2700 2660 ## Books Personal PhD Terminal S.F.Ratio ## Emporia State University 450 1888 72 75 19.3 ## Mayville State University 600 1900 68 68 15.7 ## MidAmerica Nazarene College 1100 4913 33 33 15.4 ## Southwest Baptist University 400 1000 52 54 15.9 ## University of Wisconsin-Superior 550 1960 75 81 15.2 ## Wayne State College 540 1660 60 68 20.3 ## perc.alumni Expend Grad.Rate Elite ## Emporia State University 4 5527 50 No ## Mayville State University 11 6971 51 No ## MidAmerica Nazarene College 20 5524 49 No ## Southwest Baptist University 13 4718 71 No ## University of Wisconsin-Superior 15 6490 36 No ## Wayne State College 29 4550 52 No ## acceptance_rate ## Emporia State University 1 ## Mayville State University 1 ## MidAmerica Nazarene College 1 ## Southwest Baptist University 1 ## University of Wisconsin-Superior 1 ## Wayne State College 1 Practical 2 For the tasks below, you will require the Boston dataset. This dataset is part of the MASS R package. To access the dataset, load the MASS package (install the package first, if you have not done so previously). Task 1 Install and load the MASS package. You will also require tidyverse. # if you haven&#39;t already, install the MASS package before loading install.packages(&quot;MASS&quot;) library(MASS) library(tidyverse) Does R provide any message when loading MASS? Why does this matter? One important message that R provides when loading MASS is that this package masks the select() function from tidyverse. Attaching package: ‘MASS’ The following object is masked from ‘package:dplyr’: `select` When masking occurs, this means that both packages contain the same function. If you were to use the select() function, R will call the function from the MASS package, rather than from tidyverse (dplyr) package. This is because the MASS package is the one masking the function. If you intend to use the select() function as defined by the tidyverse package, it may not work as intended and/or you may be prompted by an error message such as: Error in select(...): unused argument (...) To avoid such issues, you must ‘call’ on the package from which you want R to use the masked function (e.g. dplyr::select()). This is why it is important to read through all warnings and messages provided in the console. Task 2 Find out more about the Boston dataset variables by accessing the R Documentation. ?Boston To explore the Boston dataset, simply type the name of the data object into the console or use View() What type of data structure is the Boston dataset? What are its dimensions? How many categorical and quantitative variables are there? The Boston dataset is a data frame with 506 rows (observations) and 14 columns (variables). There is one categorical variable (chas), and 13 quantitative variables. Task 3 Find the class of all 14 variables. Hint: use sapply. sapply(Boston, class) ## crim zn indus chas nox rm age dis ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;integer&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## rad tax ptratio black lstat medv ## &quot;integer&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; Task 4 Using a tidyverse approach, calculate the mean, median, and standard deviation of all variables of class numeric. Boston %&gt;% dplyr::select(dplyr::where(is.numeric)) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(Mean = mean(value, na.rm = TRUE), SD = sd(value, na.rm = TRUE), Median = median(value, na.rm = TRUE)) ## # A tibble: 14 × 4 ## name Mean SD Median ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 age 68.6 28.1 77.5 ## 2 black 357. 91.3 391. ## 3 chas 0.0692 0.254 0 ## 4 crim 3.61 8.60 0.257 ## 5 dis 3.80 2.11 3.21 ## 6 indus 11.1 6.86 9.69 ## 7 lstat 12.7 7.14 11.4 ## 8 medv 22.5 9.20 21.2 ## 9 nox 0.555 0.116 0.538 ## 10 ptratio 18.5 2.16 19.0 ## 11 rad 9.55 8.71 5 ## 12 rm 6.28 0.703 6.21 ## 13 tax 408. 169. 330 ## 14 zn 11.4 23.3 0 What is the mean pupil-teacher ratio? What is the median and mean per capita crime rate? Which value do you think is more suitable to describe per capita crime rate? The mean pupil-teacher ratio is about 19. The median crime rate is 0.257 whilst the mean is larger at 3.61. Given the difference between the median and the mean, a skewed distribution is expected, therefore, the median may be a more a suitable summary statistic to describe crime rate (a histogram would be needed). Task 5 Using a base R approach, create a 2x2 multipanel plot of crim versus age, dis, rad, tax and ptratio. par(mfrow = c(2,2)) plot(Boston$age, Boston$crim) plot(Boston$dis, Boston$crim) plot(Boston$rad, Boston$crim) plot(Boston$tax, Boston$crim) What can you say about the relationships between age, dis, rad, tax, and crim? As the age of the home increases (age), crime also increases. There is also higher crime around employment centers (dis). With very high index of accessibility to radial highways (rad), and tax rates (tax) there also appears to be high crime rates. Task 6 Using a base R approach, create and display histograms of crim, tax and ptratio in a 1x2 multipanel plot. Set the breaks argument to 25 . par(mfrow=c(1,2)) hist(Boston$crim, breaks=25) hist(Boston$tax, breaks=25) What do these histograms indicate? Most areas have low crime rates, but there is a rather long tail showing high crime rates (although the frequency seems to be very low). Given the degree of skew, the mean would not be a good measure of central tendency. With respect to tax rates, there appears to be a large divide between low taxation and high taxation, with the highest peak at around 670. Remember to revert back to single panel display. "],["overview-1.html", "Overview", " Overview Section 2: Linear Regression and Prediction This section is comprised of two demonstrations and two practicals. The two practicals will make use of exercises adapted from the core textbook for this course: James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd ed. New York: Springer. https://www.statlearning.com/ Learning Outcomes: explain the relevance of the intercept; appreciate the impact of noise on coefficient estimates; produce and interpret diagnostic plots with base R; apply non-linear transformations; compare model fit; compute and interpret confidence intervals. In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. Function Description Package lm() fit linear models base R predict() generic function for predictions from results of different models (e.g. predict.lm()) base R confint() compute confidence intervals base R plot() generic function for plotting base R legend() add legend (to plot()) arguments such as col, lty, and cex control colour, line type, and font size respectively base R abline() adding one or more straight lines to plot base R cor() computes correlation between variables base R rnorm() generates normal distribution base R poly() returns or evaluates polynomials base R par() set graphical parameters base R mfrow() par() parameter base R subset() return subset of a data object (vector, matrix, or dataframe) according to condition(s) base R anova() compute analysis of variance for base R rnorm() density, distribution function, quantile function and random generation for the normal distribution base R sqrt() compute square root base R "],["demonstration-1.html", "Demonstration 1 Simple Linear Models Without Intercept Simple Linear Models with Intercept", " Demonstration 1 Simple Linear Models Without Intercept Let’s investigate the t-statistic for the null hypothesis \\(H_{0}:\\beta = 0\\) in simple linear regression without an intercept. The equation for a model without the intercept would therefore be \\(Y = \\beta X\\). By excluding the intercept, the model is constrained to pass through the origin \\((0,0)\\), allowing the relationship between the response and predictor to be interpreted as proportional. In other words, the removal of the intercept forces the regression line to start at \\((0,0)\\), so when \\(x = 0\\), then \\(y = 0\\). Let’s first generate some data for a predictor \\(x\\) and a response \\(y\\). We select a seed value to ensure that we generate the same data every time. To generate values, we use the rnorm function to produce 100 data values drawn from a normal distribution (hence rnorm()). set.seed(1) x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100) Now that we generated our predictor and our response variable, let’s run a simple linear regression without an intercept using \\(y\\) as the response and \\(x\\) as a predictor. One way to do so is by adding \\(0\\) into the formula. fit &lt;- lm(y ~ x + 0) And now, let’s have a look at the results. coef(summary(fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## x 1.993876 0.1064767 18.72593 2.642197e-34 We can see a significant positive relationship between y and x. The coefficient estimate for \\(x\\) is \\(1.993876\\), and since the relationship between \\(x\\) and \\(y\\) is proportional, we interpret the estimate as the \\(y\\) values being predicted to be (a little below) twice the \\(x\\) values. But what happens if we swap \\(x\\) and \\(y\\) and run a model using \\(y\\) as the predictor and \\(x\\) as the response? fit2 &lt;- lm(x ~ y + 0) coef(summary(fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## y 0.3911145 0.02088625 18.72593 2.642197e-34 We again observe a significant positive relationship between \\(x\\) and \\(y\\), except that the \\(x\\) values are predicted to be (a little below) half the \\(y\\) values (since the coefficient estimate is \\(0.3911145\\)). Note also the t-statistic values for the two models. They are identical and of course so is the p-value (therefore, there is a significant relationships between \\(x\\) and \\(y\\)). Therefore, the results of the models of \\(y\\) onto \\(x\\) and \\(x\\) onto \\(y\\) indicate that the coefficients would be the inverse of each other (2 and 1/2) whilst the t-statistic values (and p-values) remain the same. Why are the t-statistic values identical? For each coefficient, the t statistic is calculated by dividing the coefficient estimate by its standard error. For example, for the fit2 model, we have a coefficient estimate of \\(0.3911145\\) and a standard error of \\(0.02088625\\) and so dividing \\(0.3911145\\) by \\(0.02088625\\) gives us \\(18.72593\\). You’ll also remember that the correlation coefficient between two variables is symmetric and so the correlation between \\(X\\) and \\(Y\\) is the same as for \\(Y\\) and \\(X\\). This is the reason why it is incorrect to state that “\\(X\\) causes a change in \\(Y\\)”. In a linear model, we are testing whether there is a linear association between \\(x\\) and \\(y\\) but not if X causes Y or Y causes X. Therefore, irrespective of whether we are regressing \\(y\\) onto \\(x\\) or \\(y\\) onto \\(y\\), the t-statistic is testing the same null hypothesis \\(H_{0} : \\beta = 0\\) (i.e. fundamentally, it is testing whether there is a linear correlation between \\(x\\) and \\(y\\)). So what exactly is the role of the intercept? As you already know, the intercept represents the value of \\(y\\) when \\(x = 0\\) which can be thought of as the initial value effect that exists independently of \\(x\\). This not only applies to the simple linear regression model but also to the multiple linear regression model (i.e. the intercept is the value of \\(y\\) when all predictors are zero). In other words, the intercept adjusts the starting point of regression line and allows for the line to shift up or down on the y-axis thus reflecting a “baseline” level of \\(y\\) that is not dependent on \\(x\\). With an intercept, the slope coefficient still tells us how much \\(Y\\) changes with a one-unit change in \\(x\\), but this change is relative to the value of \\(y\\) when \\(x = 0\\) and this is important when \\(x\\) can take a value of \\(0\\) that is meaningful to the model. Without the intercept, the line is forced to pass through the origin \\((0,0)\\), which may not be suitable unless the data naturally begin at zero (or there are some other theoretical or practical reasons which warrant the line passing through the origin). With an intercept, the regression line is no longer forced to pass through zero (and will only do so if the data naturally begin at zero). The intercept therefore allows for the regression line to better fit the data, particularly when the data do not actually begin at zero. In this way, the model can capture the average outcome when the predictor(s) is/are zero. Simple Linear Models with Intercept Do you think that the t-statistic will be the same for both regression of Y onto X and X onto Y if we were to include the intercept? We use the same data as before and run a regression with \\(y\\) as response and \\(x\\) as predictor and include the intercept. fit3 &lt;- lm(y ~ x) We then extract the model coefficients from the summary results of the model. coef(summary(fit3)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.03769261 0.09698729 -0.3886346 6.983896e-01 ## x 1.99893961 0.10772703 18.5555993 7.723851e-34 How does coefficient for fit3 compare to fit? How about the t-statistic value? coef(summary(fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## x 1.993876 0.1064767 18.72593 2.642197e-34 As you can see, the coefficient for the model with the intercept (fit3) is very similar to the coefficient for the model without the intercept (fit). The t-statistic is also very close (\\(18.72593\\) for the model without intercept and \\(18.5555993\\) for the model with the intercept). Now we run a regression with \\(x\\) as response and \\(y\\) as predictor. fit4 &lt;- lm(x ~ y) We then extract the model coefficients from the summary results of the model. coef(summary(fit4)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.03880394 0.04266144 0.9095787 3.652764e-01 ## y 0.38942451 0.02098690 18.5555993 7.723851e-34 How does the coefficient for fit4 compare to fit2? How about the t-statistic value? Are the t-statistic values different between the fit3 and fit4 models? coef(summary(fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## y 0.3911145 0.02088625 18.72593 2.642197e-34 The slope coefficient for the model with the intercept (\\(0.38942451\\)) is very similar to the coefficient for the model without the intercept (\\(0.3911145\\)) and so is the t-statistic. Also, as expected, the t-statistic value for the two models for which we included the intercept are identical. Therefore, irrespective of whether we include the intercept or not, the t-statistic value for the regression of \\(y\\) onto \\(x\\) will be identical to the t-statistic value for the regression of \\(x\\) onto \\(y\\). "],["demonstration-2.html", "Demonstration 2 Population Parameters and Estimated Coefficients What happens if we reduce noise? What happens if we increase noise? How does noise affect confidence intervals for the coefficients?", " Demonstration 2 Population Parameters and Estimated Coefficients Let’s explore the differences between population parameters and estimated coefficients. We will generate a “true” population dataset. We create a variable \\(X\\) with 100 observations drawn from a normal distribution. To be more specific about the characteristic of our variable \\(X\\), we will not only specify the total number of observations (100), but also the mean (0), and standard deviation (1). This will be our predictor. set.seed(1) x &lt;- rnorm(100, 0, 1) We now create another vector called eps containing 100 observations drawn from a normal distribution with a mean of zero and a variance of 0.25. This will be the our error term \\(\\epsilon\\). eps &lt;- rnorm(100, 0, sqrt(0.25)) Using \\(X\\) and \\(\\epsilon\\), we now generate a vector \\(Y\\) according to the following formula: \\(Y = -1 + 0.5X + \\epsilon\\). Essentially, we specify our intercept, the slope coefficient, the predictor variable and the error to obtain our response variable. y &lt;- -1 + 0.5 * x + eps The values \\(-1\\) and \\(0.5\\) represent the “true” population values for the intercept \\(\\beta_{0}\\) and slope \\(\\beta_{1}\\) respectively. Now we can create a scatterplot to observe the association between \\(X\\) and \\(Y\\). plot(x, y) The plot indicates a linear relationship between \\(X\\) and \\(Y\\). The relationship is clearly not perfectly linear due to noise. If we were to instead estimate the intercept and slope, to what degree do you think these estimated coefficients will differ from the true population values? Ok, so we have the variables we generated, so our predictor X and our response X and we run a regression model. fit &lt;- lm(y ~ x) summary(fit) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.93842 -0.30688 -0.06975 0.26970 1.17309 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.01885 0.04849 -21.010 &lt; 2e-16 *** ## x 0.49947 0.05386 9.273 4.58e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4814 on 98 degrees of freedom ## Multiple R-squared: 0.4674, Adjusted R-squared: 0.4619 ## F-statistic: 85.99 on 1 and 98 DF, p-value: 4.583e-15 The results of the model show an estimated slope coefficient (\\(\\hat{\\beta_{1}}\\)) for \\(x\\) of \\(0.49947\\). This is very close to the population value (\\(\\beta_{1}\\)) which is \\(0.5\\). We see a similar estimated value for the intercept (\\(\\hat{\\beta_{0}}\\)) which is \\(-1.01885\\), again very close to the true value for the intercept (\\(\\beta_{0}\\)) which is \\(-1\\). Therefore, if we were to plot the population regression line and the estimated regression line, we would see that the two are difficult to distinguish (given the similarity of the estimated and true values for the coefficients). plot(x, y) abline(fit) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2), cex = 0.72, ) What if we were to fit a polynomial regression model? Would there be any evidence that adding a quadratic term improves the model fit? To add a polynomial term of degree two, we can use the poly base R function directly in the code for the model. Since the F-test is not statistically significant, there is no evidence that adding a quadratic term improves the model fit. fit2 &lt;- lm(y ~ poly(x, 2)) anova(fit, fit2) ## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ poly(x, 2) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 98 22.709 ## 2 97 22.257 1 0.45163 1.9682 0.1638 What happens if we reduce noise? For our first model (fit), we specified a variance of \\(0.5\\) (standard deviation 0.25) for \\(\\epsilon\\) and we noted an \\(R^2\\) value of \\(0.4674\\). set.seed(1) x &lt;- rnorm(100, 0, 1) eps &lt;- rnorm(100, 0, sqrt(0.25)) y &lt;- -1 + 0.5 * x + eps fit &lt;- lm(y ~ x) summary(fit) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.93842 -0.30688 -0.06975 0.26970 1.17309 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.01885 0.04849 -21.010 &lt; 2e-16 *** ## x 0.49947 0.05386 9.273 4.58e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4814 on 98 degrees of freedom ## Multiple R-squared: 0.4674, Adjusted R-squared: 0.4619 ## F-statistic: 85.99 on 1 and 98 DF, p-value: 4.583e-15 Now, let’s observe what happens to the \\(R^2\\) value if we reduce noise from 0.25 to 0.05. We can do so directly when we generate data for our variable \\(y\\) without needing to create a new eps object. The results show that the \\(R^{2}\\) value for fit3 is much higher than the \\(R^{2}\\) value for fit. x &lt;- rnorm(100, 0, 1) y &lt;- -1 + 0.5 * x + rnorm(100, 0, sqrt(0.05)) fit3 &lt;- lm(y ~ x) summary(fit3) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.61308 -0.12553 -0.00391 0.15199 0.41332 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.98917 0.02216 -44.64 &lt;2e-16 *** ## x 0.52375 0.02152 24.33 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2215 on 98 degrees of freedom ## Multiple R-squared: 0.858, Adjusted R-squared: 0.8565 ## F-statistic: 592.1 on 1 and 98 DF, p-value: &lt; 2.2e-16 By plotting the data we can clearly see that the data points are less dispersed than before and therefore, the association between x and y appears more linear. We can also observe that the estimated regression line deviates slightly from the population regression line (particularly at lowest and highest values). plot(x, y) abline(fit3) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2), cex = 0.72 ) What happens if we increase noise? Using the same approach as before, we now increase the standard deviation to 1. Now, the \\(R^{2}\\) value for fit4 is much lower than that of either fit3 or fit. x &lt;- rnorm(100, 0, 1) y &lt;- -1 + 0.5 * x + rnorm(100, 0, 1) fit4 &lt;- lm(y ~ x) summary(fit4) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.51014 -0.60549 0.02065 0.70483 2.08980 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.04745 0.09676 -10.825 &lt; 2e-16 *** ## x 0.42505 0.08310 5.115 1.56e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9671 on 98 degrees of freedom ## Multiple R-squared: 0.2107, Adjusted R-squared: 0.2027 ## F-statistic: 26.16 on 1 and 98 DF, p-value: 1.56e-06 If we plot the data, we can observe that the data points are more dispersed and therefore, the estimated regression line deviates to a greater extent from the population regression line. plot(x, y) abline(fit4) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2), cex = 0.72, ) How does noise affect confidence intervals for the coefficients? The fit3 model is the model with the lowest amount of noise (standard deviation of 0.05), whilst fit4 is the model with the largest amount of noise (standard deviation of 1). The confidence interval for the coefficient for the model with the highest noise is the widest. The larger the amount of noise, the wider the interval and therefore, the less precise the coefficient estimates will be. Conversely, the narrowest interval is that of the model with the lowest noise which yielded the most precise estimates. confint(fit) ## 2.5 % 97.5 % ## (Intercept) -1.1150804 -0.9226122 ## x 0.3925794 0.6063602 confint(fit3) ## 2.5 % 97.5 % ## (Intercept) -1.033141 -0.9451916 ## x 0.481037 0.5664653 confint(fit4) ## 2.5 % 97.5 % ## (Intercept) -1.2394772 -0.8554276 ## x 0.2601391 0.5899632 "],["practical-1-2.html", "Practical 1", " Practical 1 For the tasks below, you will require the Auto dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Auto dataset loads automatically: library(ISLR2) The Auto dataset contains information such as engine horsepower, gas mileage, model year, and origin of car, for 392 vehicles. It is a dataframe with 392 observations and 9 variables. The variables are: mpg: miles per gallon cylinders: Number of cylinders between 4 and 8 displacement: Engine displacement (cu. inches) horsepower: Engine horsepower weight: Vehicle weight (lbs.) acceleration: Time to accelerate from 0 to 60 mph (sec.) year: Model year origin: Origin of car (1. American, 2. European, 3. Japanese) name: Vehicle name Task 1 Use the lm() function to perform simple linear regression with mpg as the response and horsepower as the predictor. Store the output in an object called fit. Task 2 Have a look at the results of the model. Is there a relationship between the predictor and the response? Task 3 What is the associated 95% confidence intervals for predicted miles per gallon associated with an engine horsepower of 98? Hint: use the predict() function. For confidence intervals, set the interval argument to confidence. Task 4 How about the prediction interval for the same value? Are the two intervals different? Why? Task 5 Using base R, plot the response and the predictor as well as the least squares regression line. Add suitable labels to the X and Y axes. Task 6 Use base R to produce diagnostic plots of the least squares regression fit. Display these in a 2X2 grid. Task 7 Subset the Auto dataset such that it excludes the name and origin variables and store this subsetted dataset in a new object called quant_vars. Task 8 Compute a correlation matrix of all variables. Did you use the Auto dataset or the quant_vars object? Why does it matter which data object you use? Task 9 Using the quant_vars object, perform multiple linear regression with miles per gallon as the response and all other variables as the predictors. Store the results in an object called fit2. Task 10 Have a look at the results of the multiple regression model. Is there a relationship between the predictors and the response? Which predictors appear to have a statistically significant relationship to the response? What does the coefficient for the year variable suggest? Task 11 Produce diagnostic plots of the multiple linear regression fit in a 2x2 grid. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage? Task 12 Fit separate linear regression models with interaction effect terms for: weight and horsepower, acceleration and horsepower, and cylinders and weight. Are any of the interaction terms statistically significant? Task 13 Using the Auto data object, apply transformations for the horsepower variable and plot the relationship between horsepower and mpg in a 2x2 grid. First plot: use the original variable; Second plot: apply log transform; Third plot: raise it to the power of two. Which of these transformations is most suitable? Task 14 Now run a multiple regression model with all variables as before but this time, apply a log transformation of the horsepower variable. Task 15 Have a look at the results. How do the results of model fit3 differ from those of model fit2? Task 16 Produce diagnostic plots for the fit3 object and display them in a 2x2 grid. How do the diagnostic plots differ? "],["practical-2-2.html", "Practical 2 Task 1 Task 2 Task 3 Task 4 Task 5", " Practical 2 For the tasks below, you will require the Carseats dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Carseats dataset loads automatically: library(ISLR2) Carseats is a simulated dataset comprising of sales of child car seats at 400 different stores. It is a datafrate with 400 observations and 11 variables. The variables are: Sales: Unit sales (thousands of dollars) at each location CompPrice: Price charged by competitor at each location Income: Community income level (thousands of dollars) Advertising: Local advertising budget for company at each location (thousands of dollars) Population: Population size in region (thousands of dollars) Price: Price company charges for car seats at each site ShelveLoc: Quality of the shelving location for the car seats at each site Age: Average age of the local population Education: Education level at each location Urban: Whether the store is in an urban or rural location US: Whether the store is in the US or not Task 1 Fit a multiple regression model to predict unit sales at each location based on price company charges for car seats, whether the store is in an urban or rural location, and whether the store is in the US or not. Task 2 Have a look at the results and interpret the coefficients. Which coefficients are statistically significant? What do they indicate? Task 3 Based on the conclusions you have drawn in Task 2, now fit a smaller model that only uses the predictors for which there is evidence of association with sales. Task 4 Compare the two models (fit and fit2). Which model is the better fit? Task 5 Produce diagnostic plots for fit2 and display these in a 2x2 grid. Is there evidence of outliers or high leverage observations in the fit2 model? "],["answers-1.html", "Answers Practical 1 Practical 2", " Answers Practical 1 For the tasks below, you will require the Auto dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Auto dataset loads automatically: library(ISLR2) Remember to install it first install.packages(\"ISLR2\") This data file (text format) contains 398 observations of 9 variables. The variables are: mpg: miles per gallon cylinders: Number of cylinders between 4 and 8 displacement: Engine displacement (cu. inches) horsepower: Engine horsepower weight: Vehicle weight (lbs.) acceleration: Time to accelerate from 0 to 60 mph (sec.) year: Model year origin: Origin of car (1. American, 2. European, 3. Japanese) name: Vehicle name Task 1 Use the lm() function to perform simple linear regression with mpg as the response and horsepower as the predictor. Store the output in an object called fit. fit &lt;- lm(mpg ~ horsepower, data = Auto) Task 2 Have a look at the results of the model. summary(fit) ## ## Call: ## lm(formula = mpg ~ horsepower, data = Auto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.5710 -3.2592 -0.3435 2.7630 16.9240 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.935861 0.717499 55.66 &lt;2e-16 *** ## horsepower -0.157845 0.006446 -24.49 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.906 on 390 degrees of freedom ## Multiple R-squared: 0.6059, Adjusted R-squared: 0.6049 ## F-statistic: 599.7 on 1 and 390 DF, p-value: &lt; 2.2e-16 Is there a relationship between the predictor and the response? The slope coefficient (-0.157845) is statistically significant (&lt;2e-16 ***). We can conclude that there is evidence to suggest a negative relationship between miles per gallon and engine horsepower. For a one-unit increase in engine horsepower, miles per gallon are reduced by 0.16. Task 3 What is the associated 95% confidence intervals for predicted miles per gallon associated with an engine horsepower of 98? Hint: use the predict() function. For confidence intervals, set the interval argument to confidence. predict(fit, data.frame(horsepower = 98), interval = &quot;confidence&quot;) ## fit lwr upr ## 1 24.46708 23.97308 24.96108 Task 4 How about the prediction interval for the same value? predict(fit, data.frame(horsepower = 98), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 24.46708 14.8094 34.12476 Are the two intervals different? Why? The prediction interval (lower limit 14.8094 and upper limit 34.12476) is wider (and therefore less precise) than the confidence interval (lower limit 23.97308 and upper limit 24.96108). The confidence interval measures the uncertainty around the estimate of the conditional mean whilst the prediction interval takes into account not only uncertainty but also the variability of the conditional distribution. Task 5 Using base R, plot the response and the predictor as well as the least squares regression line. Add suitable labels to the X and Y axes. plot(Auto$horsepower, Auto$mpg, xlab = &quot;horsepower&quot;, ylab = &quot;mpg&quot;) abline(fit) Task 6 Use base R to produce diagnostic plots of the least squares regression fit. Display these in a 2X2 grid. par(mfrow = c(2, 2)) plot(fit, cex = 0.2) Task 7 Subset the Auto dataset such that it excludes the name and origin variables and store this subsetted dataset in a new object called quant_vars. quant_vars &lt;- subset(Auto, select = -c(name, origin)) Task 8 Compute a correlation matrix of all variables. cor(quant_vars) ## mpg cylinders displacement horsepower weight ## mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 ## cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 ## displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 ## horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 ## weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 ## acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 ## year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 ## acceleration year ## mpg 0.4233285 0.5805410 ## cylinders -0.5046834 -0.3456474 ## displacement -0.5438005 -0.3698552 ## horsepower -0.6891955 -0.4163615 ## weight -0.4168392 -0.3091199 ## acceleration 1.0000000 0.2903161 ## year 0.2903161 1.0000000 Did you use the Auto dataset or the quant_vars object? Why does it matter which data object you use? To compute the correlation matrix using all variables of a data object, these variables must all be numeric. In the Auto data object, the name variable is coded as a factor. class(Auto$name) [1] \"factor\" Therefore, if you try to use the cor() function with Auto dataset without excluding the name variable, you will get an error. cor(Auto) Error in cor(Auto) : 'x' must be numeric. Also, whilst the origin variable is of class integer and will not pose a problem when you apply the cor() function, you’ll remember from the variable description list that this is a nominal variable with its categories numerically labelled. Compute the correlation matrix using quant_vars. Task 9 Using the quant_vars object, perform multiple linear regression with miles per gallon as the response and all other variables as the predictors. Store the results in an object called fit2. fit2 &lt;- lm(mpg ~ ., data = quant_vars) Task 10 Have a look at the results of the multiple regression model. summary(fit2) ## ## Call: ## lm(formula = mpg ~ ., data = quant_vars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.6927 -2.3864 -0.0801 2.0291 14.3607 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.454e+01 4.764e+00 -3.051 0.00244 ** ## cylinders -3.299e-01 3.321e-01 -0.993 0.32122 ## displacement 7.678e-03 7.358e-03 1.044 0.29733 ## horsepower -3.914e-04 1.384e-02 -0.028 0.97745 ## weight -6.795e-03 6.700e-04 -10.141 &lt; 2e-16 *** ## acceleration 8.527e-02 1.020e-01 0.836 0.40383 ## year 7.534e-01 5.262e-02 14.318 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.435 on 385 degrees of freedom ## Multiple R-squared: 0.8093, Adjusted R-squared: 0.8063 ## F-statistic: 272.2 on 6 and 385 DF, p-value: &lt; 2.2e-16 Is there a relationship between the predictors and the response? Which predictors appear to have a statistically significant relationship to the response? What does the coefficient for the year variable suggest? Two of the predictors are statistically significant: weight and year. The relationship between weight and mpg is negative which suggests that for a one pound increase in weight of vehicle, the number of miles per gallon the vehicle can travel decreases, whilst that of mpg and year is positive which suggests that the more recent the vehicle is, the higher the number of miles per gallon it can travel. Task 11 Produce diagnostic plots of the multiple linear regression fit in a 2x2 grid. par(mfrow = c(2, 2)) plot(fit2, cex = 0.2) Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage? One point has high leverage, the residuals also show a trend with fitted values. Task 12 Fit separate linear regression models with interaction effect terms for: weight and horsepower, acceleration and horsepower, and cylinders and weight. summary(lm(mpg ~ . + weight:horsepower, data = quant_vars)) summary(lm(mpg ~ . + acceleration:horsepower, data = quant_vars)) summary(lm(mpg ~ . + cylinders:weight, data = quant_vars)) Are any of the interaction terms statistically significant? For each model, the interaction term is statistically significant. Task 13 Using the Auto data object, apply transformations for the horsepower variable and plot the relationship between horsepower and mpg in a 2x2 grid. First plot: use the original variable; Second plot: apply log transform; Third plot: raise it to the power of two. par(mfrow = c(2, 2)) plot(Auto$horsepower, Auto$mpg, cex = 0.2) plot(log(Auto$horsepower), Auto$mpg, cex = 0.2) plot(Auto$horsepower ^ 2, Auto$mpg, cex = 0.2) Which of these transformations is most suitable? The relationship between horsepower and miles per gallon is clearly non-linear (plot 1). The log transform seems to address this best. Task 14 Now run a multiple regression model with all variables as before but this time, apply a log transformation of the horsepower variable. quant_vars$horsepower &lt;- log(quant_vars$horsepower) fit3 &lt;- lm(mpg ~ ., data = quant_vars) Task 15 Have a look at the results. summary(fit3) ## ## Call: ## lm(formula = mpg ~ ., data = quant_vars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.6778 -2.0080 -0.3142 1.9262 14.0979 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.1713000 8.9291383 3.267 0.00118 ** ## cylinders -0.3563199 0.3181815 -1.120 0.26347 ## displacement 0.0088277 0.0068866 1.282 0.20066 ## horsepower -8.7568129 1.5958761 -5.487 7.42e-08 *** ## weight -0.0044304 0.0007213 -6.142 2.03e-09 *** ## acceleration -0.3317439 0.1077476 -3.079 0.00223 ** ## year 0.6979715 0.0503916 13.851 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.308 on 385 degrees of freedom ## Multiple R-squared: 0.8231, Adjusted R-squared: 0.8203 ## F-statistic: 298.5 on 6 and 385 DF, p-value: &lt; 2.2e-16 How do the results of model fit3 differ from those of model fit2? The fit2 model results showed that only two predictors were statistically significant: weight and year. The fit3 model has two additional predictors that are statistically significant: acceleration as well as horsepower.Also, the coefficient values can now be interpreted more easily. Task 16 Produce diagnostic plots for the fit3 object and display them in a 2x2 grid. par(mfrow = c(2, 2)) plot(fit3, cex = 0.2) How do the diagnostic plots differ? A log transformation of horsepower appears to give a more linear relationship with mpg but this difference does not seem to be substantial. Practical 2 For the tasks below, you will require the Carseats dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Carseats dataset loads automatically: library(ISLR2) This dataframe object contains a simulated dataset of sales of child car seats at 400 different stores. The 9 variables are: Sales: Unit sales (thousands of dollars) at each location CompPrice: Price charged by competitor at each location Income: Community income level (thousands of dollars) Advertising: Local advertising budget for company at each location (thousands of dollars) Population: Population size in region (thousands of dollars) Price: Price company charges for car seats at each site ShelveLoc: Quality of the shelving location for the car seats at each site Age: Average age of the local population Education: Education level at each location Urban: Whether the store is in an urban or rural location US: Whether the store is in the US or not Task 1 Fit a multiple regression model to predict unit sales at each location based on price company charges for car seats, whether the store is in an urban or rural location, and whether the store is in the US or not. fit &lt;- lm(Sales ~ Price + Urban + US, data = Carseats) Task 2 Have a look at the results and interpret the coefficients. summary(fit) ## ## Call: ## lm(formula = Sales ~ Price + Urban + US, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9206 -1.6220 -0.0564 1.5786 7.0581 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.043469 0.651012 20.036 &lt; 2e-16 *** ## Price -0.054459 0.005242 -10.389 &lt; 2e-16 *** ## UrbanYes -0.021916 0.271650 -0.081 0.936 ## USYes 1.200573 0.259042 4.635 4.86e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.472 on 396 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2335 ## F-statistic: 41.52 on 3 and 396 DF, p-value: &lt; 2.2e-16 Which coefficients are statistically significant? What do they indicate? The null hypothesis for the slope being zero is rejected for the Price and US variables. The coefficient for Price is statistically significant; since it is negative, as price increases by a thousand dollars (i.e. one unit increase), the sales of child decrease by about 0.05. The slope for the US variable is also statistically significant but it is positive. Also, this is a binary factor variable coded as Yes and No (No is the reference category). Therefore, sales of child car seats are higher by 1.2 for car seat products that are produced in the US than for car seat products not produced in the US. Task 3 Based on the conclusions you have drawn in Task 2, now fit a smaller model that only uses the predictors for which there is evidence of association with sales. fit2 &lt;- lm(Sales ~ Price + US, data = Carseats) Task 4 Compare the two models (fit and fit2). anova(fit, fit2) ## Analysis of Variance Table ## ## Model 1: Sales ~ Price + Urban + US ## Model 2: Sales ~ Price + US ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 396 2420.8 ## 2 397 2420.9 -1 -0.03979 0.0065 0.9357 Which model is the better fit? They have similar r-squared values, and the fit model (containing the extra variable Urban) is non-significantly better. Task 5 Produce diagnostic plots for fit2 and display these in a 2x2 grid. par(mfrow = c(2, 2)) plot(fit2, cex = 0.2) Is there evidence of outliers or high leverage observations in the fit2 model? Yes, there is evidence of outliers and high leverage observations for fit2. "],["overview-2.html", "Overview", " Overview Section 3: Classification This section is comprised of one, expanded demonstration which is adapted from the core textbook for this course: James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd ed. New York: Springer. https://www.statlearning.com/ Learning Outcomes: appreciate the difference between classic modelling and supervised learning; compute and plot a correlation matrix; compute and interpret a confusion matrix and overall prediction accuracy; perform a variety of classification methods and appreciate their advantages and limitations. In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. Function Description Package str() display structure of object base R cor() compute correlation base R corrplot() plot correlation matrix arguments such as type and diag control the structure of the plot corrplot glm() fit a generalised linear model arguments such as family control the distribution base R predict() generic function for predictions from results of models arguments such as type specify the type of predictions required base R diag() extract/replace diagonal of a matrix base R sum() compute the sum of values base R lda() perform LDA MASS table() build contingency table base R qda() perform QDA MASS knn() perform KNN classification arguments such as k control number of neighbours class naiveBayes() apply the Naive Bayes classifier e1071 "],["demonstration-classification-problems.html", "Demonstration: Classification Problems Dataset and Variables Correlation Matrix and Plot “Classic” Logistic Regression Logistic Regression in Statistical Learning Linear Discriminant Analysis Quadratic Discriminant Analysis \\(K\\)-nearest neighbours Naive Bayes", " Demonstration: Classification Problems In this demonstration, you will learn how to address classification problems using logistic regression, discriminant analysis, KNN, and Naive Bayes. You will need the Weekly dataset, part of the ISRL2 package. By loading the package, the Weekly dataset loads automatically. In addition to the ISLR2 package, will also require the following: library(MASS) library(class) library(tidyverse) library(corrplot) library(ISLR2) library(e1071) Dataset and Variables The Weekly dataset contains weekly percentage returns for the S&amp;P 500 stock index between 1990 and 2010. It is a dataframe with 1098 observations and 9 variables. The variables are: Year: year observation was recorded Lag1: Percentage return for previous week Lag2: Percentage return for 2 weeks previous Lag3: Percentage return for 3 weeks previous Lag4: Percentage return for 4 weeks previous Lag5: Percentage return for 5 weeks previous Volume: Volume of shares traded (average number of daily shares traded in billions) Today: percentage return for current week Direction: whether the market had a positive (up) or negative (down) return on a given week. In this demonstration, the goal is to predict whether the market had a positive (up) or negative (down) return on a given week. Therefore, Direction will be our response variable. Before we consider the model, let’s first explore our dataset. By exploring the structure of the dataframe, we find out that all variables are numeric, with the exception of the Direction variable. str(Weekly) ## &#39;data.frame&#39;: 1089 obs. of 9 variables: ## $ Year : num 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ... ## $ Lag1 : num 0.816 -0.27 -2.576 3.514 0.712 ... ## $ Lag2 : num 1.572 0.816 -0.27 -2.576 3.514 ... ## $ Lag3 : num -3.936 1.572 0.816 -0.27 -2.576 ... ## $ Lag4 : num -0.229 -3.936 1.572 0.816 -0.27 ... ## $ Lag5 : num -3.484 -0.229 -3.936 1.572 0.816 ... ## $ Volume : num 0.155 0.149 0.16 0.162 0.154 ... ## $ Today : num -0.27 -2.576 3.514 0.712 1.178 ... ## $ Direction: Factor w/ 2 levels &quot;Down&quot;,&quot;Up&quot;: 1 1 2 2 2 1 2 2 2 1 ... Correlation Matrix and Plot Let’s now produce a correlation plot between all pairs of numeric variables in the dataset. Using the base R cor() function, we exclude the 9th variable (which is a factor) and compute the correlation among all pairs of numeric variables. cor(Weekly[, -9]) ## Year Lag1 Lag2 Lag3 Lag4 ## Year 1.00000000 -0.032289274 -0.03339001 -0.03000649 -0.031127923 ## Lag1 -0.03228927 1.000000000 -0.07485305 0.05863568 -0.071273876 ## Lag2 -0.03339001 -0.074853051 1.00000000 -0.07572091 0.058381535 ## Lag3 -0.03000649 0.058635682 -0.07572091 1.00000000 -0.075395865 ## Lag4 -0.03112792 -0.071273876 0.05838153 -0.07539587 1.000000000 ## Lag5 -0.03051910 -0.008183096 -0.07249948 0.06065717 -0.075675027 ## Volume 0.84194162 -0.064951313 -0.08551314 -0.06928771 -0.061074617 ## Today -0.03245989 -0.075031842 0.05916672 -0.07124364 -0.007825873 ## Lag5 Volume Today ## Year -0.030519101 0.84194162 -0.032459894 ## Lag1 -0.008183096 -0.06495131 -0.075031842 ## Lag2 -0.072499482 -0.08551314 0.059166717 ## Lag3 0.060657175 -0.06928771 -0.071243639 ## Lag4 -0.075675027 -0.06107462 -0.007825873 ## Lag5 1.000000000 -0.05851741 0.011012698 ## Volume -0.058517414 1.00000000 -0.033077783 ## Today 0.011012698 -0.03307778 1.000000000 We store the computed correlation matrix in a new object which we will then use to create a correlation matrix plot with the corrplot() function from the corrplot package. cor_matrix &lt;- cor(Weekly[, -9]) corrplot(cor_matrix) By using the default arguments, we obtain the correlation matrix in full, with each circle representing the correlation between each variable. The size of the circle represents the magnitude of the correlation, whilst the shade corresponds to both strength and direction of the correlation. As the legend illustrates, a perfect negative correlation (-1) is represented by dark red and a perfect positive correlation (+1) in dark blue. As you already know, the correlation matrix is symmetric around its diagonal. The diagonal area represents the correlation of each variable with itself, and therefore, this corresponds to a perfect correlation (dark blue). To facilitate interpretation, particularly when we are dealing with a large number of variables, we can set the diag argument to FALSE to remove the correlation of each variable with itself from the plot. Because of its symmetric property, we can also display just half of the square since the parts on either side of the diagonal are mirror images. We can achieve this using the type argument. We can either choose to display the area above the diagonal or the area below the diagonal, as I did below. There are many other options if you want to further customise your correlation plot such using a different visualisation method of the direction and strength of the correlation using the method argument. Have a look at the documentaion of the corrplot function using ?. corrplot(cor_matrix, type = &quot;lower&quot;, diag = FALSE) Now, the correlation plot only displays the correlations between the 8 numeric variables. We observe a strong positive correlation between volume of daily shares traded and the year the observation was recorded (dark blue). The correlations between other variables are quite weak but notably, we see that Lag1 and Lag3, Lag 2 and Lag4, Lag3 and Lag5, and Today and Lag2 are positively correlated with one another (albeit weakly). Other variables also appear weakly negatively correlated, such as Lag1 and Lag2. Ok, so what was the purpose of computing the correlation matrix? You’ll remember that multicollinearity is an issue in model building which can lead to inflated variances of the estimated coefficients. As a result of the shared variance between two highly correlated predictors, our ability to adequately evaluate the effect of the predictors on the outcome will be affected (e.g. increased risk of overfitting). One way to deal with multicollinearity is to eliminate one of the highly correlated predictors. “Classic” Logistic Regression Now let’s build our logistic regression model the classic way. Given the high correlation between Year and Volume, we have to reflect on how we want to address this. Assuming we have knowledge of important factors that can predict market movement (Direction), we decide to drop the Year variable rather than Volume* since the latter measures average number of daily shares traded (in billions). You may remember that in R, these models are built using the base R glm function within which the family argument must be set to binomial. Note that in this dataset, the Direction variable is already a factor so there is no need to perform any recoding/transformations but remember to ALWAYS explore your data in detail before you build any model. fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial) summary(fit) ## ## Call: ## glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + ## Volume, family = binomial, data = Weekly) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6949 -1.2565 0.9913 1.0849 1.4579 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.26686 0.08593 3.106 0.0019 ** ## Lag1 -0.04127 0.02641 -1.563 0.1181 ## Lag2 0.05844 0.02686 2.175 0.0296 * ## Lag3 -0.01606 0.02666 -0.602 0.5469 ## Lag4 -0.02779 0.02646 -1.050 0.2937 ## Lag5 -0.01447 0.02638 -0.549 0.5833 ## Volume -0.02274 0.03690 -0.616 0.5377 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1496.2 on 1088 degrees of freedom ## Residual deviance: 1486.4 on 1082 degrees of freedom ## AIC: 1500.4 ## ## Number of Fisher Scoring iterations: 4 The results show that only Lag2 is significant at an alpha level of 0.05. Ok, so let’s see now how well our model predicts whether the market had a positive or negative return in a given week. Confusion Matrix Now, let’s assess the effectiveness of our model by comparing the actual values with those predicted by the model. We can do so using a confusion matrix which compares the model predictions with the true values. Using predict(fit, type = \"response\"), we generate predictions from our model (fit) on the scale of the response variable (type = \"response\"). In this case, our response variable is measured on a probability scale. We choose the standard threshold of 0.5 such that we label an observation as belonging to the Up category if its posterior probability is above 0.5 or as Down if the posterior probability is below 0.5. Hence, in this context, the &gt; 0.5 argument transforms the predicted probabilities into a binary form such that predictions greater than 0.5 are labelled TRUE (so representing upward market movement), whilst the rest are labelled FALSE (representing downward market movement). pred &lt;- predict(fit, type = &quot;response&quot;) &gt; 0.5 Now that we have the frequencies of the TRUE and FALSE instances, let’s build our two-way confusion matrix using the base R table() function. The ifelse function nested inside table() converts the logical TRUE and FALSE values in the pred object intro descriptive labels to facilitate interpretation; so, if pred is TRUE (&gt; 0.5), it becomes labelled as Up (pred), whilst it is is FALSE, it is labelled as \"Down (pred)\". To also include the actual values of market movement from the dataset, we also need to add Weekly$Directionas our argument. Finally to ‘force’ R to display the conf_matrix object we just created, we can place the entire code in single parentheses. (conf_matrix &lt;- table(ifelse(pred, &quot;Up (pred)&quot;, &quot;Down (pred)&quot;), Weekly$Direction)) ## ## Down Up ## Down (pred) 54 48 ## Up (pred) 430 557 Now let’s interpret the results. The results in diagonal represent correct predictions of market movement whilst those in the off-diagonal represent misclassified observations. We can see that our model incorrectly classified 430 instances of market movement as upward movement when in fact they represented downward movement and 48 instances as downward movement when in fact they represented upward movement. Overall, our logistic regression model correctly predicts upwards movements well but it performs poorly at predicting downward movements. We can also compute the overall fraction of correct predictions by dividing the number of correct predictions by total number of predictions. We therefore divide the sum of the diagonal values in our confusion matrix (numerator) by the sum of all elements of the matrix (denominator). We extract the diagonal values from the matrix using the base R diag() function. sum(diag(conf_matrix)) / sum(conf_matrix) ## [1] 0.5610652 The overall fraction of correct predictions is 0.561 (so our model makes correct predictions about 56.1% of the time). Right, so does that mean that this model will make correct predictions 56% of the time on a new, unseen dataset? You already know the answer :)! We used our entire dataset to fit our model. This means that we cannot say anything about how our model will perform on a different dataset and we no longer have any ‘unseen’ data left to test this out. Logistic Regression in Statistical Learning Now, let’s consider logistic regression as applied in statistical learning. We will again consider a basic fixed split. In this example, we will fit our model using data from 1990 up to 2008 and set the data from 2009 and 2010 aside; this will be our test dataset. This is easily achieved by creating a vector of logical values from the data according to our Year criterion. train &lt;- Weekly$Year &lt; 2009 In our previous model, we observed that Lag2 was the only statistically significant predictor. To exemplify how we can use logistic regression in statistical learning, we will build a simple simple with only one predictor. The approach to building the model is the same as the one with which you are already familiar. The exception, of course, is that we will only use part of the dataset to build our model (which in this case is referred to as training the model). To subset our dataset to only include data from years previous to 2009, we use the logical vector train we just created. fit_log_SL &lt;- glm(Direction ~ Lag2, data = Weekly[train, ], family = binomial) Now let’s generate predictions; the function and the overall structure of the code is the same as discussed earlier in the demonstration. The exception is that we used the trained model fit_log_SL to make predictions on the test dataset (Weekly[!train, ]). Using !, we tell R to not include the training data when generating predictions. pred &lt;- predict(fit_log_SL, Weekly[!train, ], type = &quot;response&quot;) &gt; 0.5 Now let’s compute the confusion matrix such that we can compare our predictions on the test data (pred) against the actual values in our dataset (Weekly[!train, ]$Direction)). (t &lt;- table(ifelse(pred, &quot;Up (pred)&quot;, &quot;Down (pred)&quot;), Weekly[!train, ]$Direction)) ## ## Down Up ## Down (pred) 9 5 ## Up (pred) 34 56 If we then compute the overall fraction of correct predictions for the test data we can see that this is higher than the value we obtained using the classical approach (\\(0.561\\)). sum(diag(t)) / sum(t) ## [1] 0.625 Linear Discriminant Analysis How well would linear discriminant analysis address our binary classification problem? In R, LDA can be performed using the lda() function from the MASS package. The basic structure of the function is similar to the other models you have built. fit_lda &lt;- lda(Direction ~ Lag2, data = Weekly[train, ]) The output is, of course, different. prior probabilities of groups: these tells us the way in which the two classes are distributed in our training data (i.e. 44.8 % of the observations correspond to downward market movement whilst 55.2% to upward market movement). group means: the average of our single predictor Lag2 within each class, and are used by LDA as an estimate of \\(μ_{k}\\). coefficient(s) of linear discriminants: tells us how our predictor(s) influence the score that is used to classify the observations into one of the two categories. Here, the coefficient is positive 0.44 and so this indicates that higher values for Lag2 will make the modelmore likely classify an observation as belonging to the Up class; also, the larger the absolute value of the coefficient, the stronger the influence on the model. fit_lda ## Call: ## lda(Direction ~ Lag2, data = Weekly[train, ]) ## ## Prior probabilities of groups: ## Down Up ## 0.4477157 0.5522843 ## ## Group means: ## Lag2 ## Down -0.03568254 ## Up 0.26036581 ## ## Coefficients of linear discriminants: ## LD1 ## Lag2 0.4414162 Now let’s consider what the predict() function does when applied in the context of LDA. result_lda &lt;- predict(fit_lda, Weekly[!train, ]) The output will contain three components: class, posterior, and x, each of which can be accessed using $. The class component is a factor that contains the predictions for market movement (up/down). result_lda$class ## [1] Up Up Down Down Up Up Up Down Down Down Down Up Up Up Up ## [16] Up Up Up Up Up Down Up Up Up Up Up Up Up Up Up ## [31] Up Up Up Up Up Up Up Up Up Up Up Up Up Up Down ## [46] Up Up Up Up Up Up Up Up Up Up Up Down Up Up Up ## [61] Up Up Up Up Up Up Up Up Up Up Up Down Up Down Up ## [76] Up Up Up Down Down Up Up Up Up Up Down Up Up Up Up ## [91] Up Up Up Up Up Up Up Up Up Up Up Up Up Up ## Levels: Down Up The posterior component is matrix that contains the posterior probability that the corresponding observation belongs to a given class. result_lda$posterior ## Down Up ## 986 0.4736555 0.5263445 ## 987 0.3558617 0.6441383 ## 988 0.5132860 0.4867140 ## 989 0.5142948 0.4857052 ## 990 0.4799727 0.5200273 ## 991 0.4597586 0.5402414 ## 992 0.3771117 0.6228883 ## 993 0.5184724 0.4815276 ## 994 0.5480397 0.4519603 ## 995 0.5146118 0.4853882 ## 996 0.5504246 0.4495754 ## 997 0.3055404 0.6944596 ## 998 0.4268160 0.5731840 ## 999 0.3637275 0.6362725 ## 1000 0.4034316 0.5965684 ## 1001 0.4256310 0.5743690 ## 1002 0.4277053 0.5722947 ## 1003 0.4548626 0.5451374 ## 1004 0.4308002 0.5691998 ## 1005 0.3674066 0.6325934 ## 1006 0.5210641 0.4789359 ## 1007 0.4426627 0.5573373 ## 1008 0.3983332 0.6016668 ## 1009 0.4170520 0.5829480 ## 1010 0.4400457 0.5599543 ## 1011 0.4872186 0.5127814 ## 1012 0.4529323 0.5470677 ## 1013 0.4844231 0.5155769 ## 1014 0.4769786 0.5230214 ## 1015 0.3531293 0.6468707 ## 1016 0.3912903 0.6087097 ## 1017 0.4373753 0.5626247 ## 1018 0.4163510 0.5836490 ## 1019 0.4583549 0.5416451 ## 1020 0.4182305 0.5817695 ## 1021 0.4454253 0.5545747 ## 1022 0.4667580 0.5332420 ## 1023 0.4126831 0.5873169 ## 1024 0.4146279 0.5853721 ## 1025 0.4814414 0.5185586 ## 1026 0.4756405 0.5243595 ## 1027 0.3860819 0.6139181 ## 1028 0.4278606 0.5721394 ## 1029 0.4599449 0.5400551 ## 1030 0.5071309 0.4928691 ## 1031 0.4042648 0.5957352 ## 1032 0.4173045 0.5826955 ## 1033 0.4520606 0.5479394 ## 1034 0.4491759 0.5508241 ## 1035 0.4304467 0.5695533 ## 1036 0.4487621 0.5512379 ## 1037 0.4544049 0.5455951 ## 1038 0.4184691 0.5815309 ## 1039 0.4637729 0.5362271 ## 1040 0.4114393 0.5885607 ## 1041 0.4605038 0.5394962 ## 1042 0.5053429 0.4946571 ## 1043 0.4728071 0.5271929 ## 1044 0.4595437 0.5404563 ## 1045 0.4368785 0.5631215 ## 1046 0.4051682 0.5948318 ## 1047 0.4553490 0.5446510 ## 1048 0.4056270 0.5943730 ## 1049 0.4352188 0.5647812 ## 1050 0.4370488 0.5629512 ## 1051 0.4410978 0.5589022 ## 1052 0.4352756 0.5647244 ## 1053 0.4296973 0.5703027 ## 1054 0.4520034 0.5479966 ## 1055 0.4194240 0.5805760 ## 1056 0.4853885 0.5146115 ## 1057 0.5411727 0.4588273 ## 1058 0.4177113 0.5822887 ## 1059 0.5100863 0.4899137 ## 1060 0.4470646 0.5529354 ## 1061 0.4816287 0.5183713 ## 1062 0.4138300 0.5861700 ## 1063 0.4157203 0.5842797 ## 1064 0.5017234 0.4982766 ## 1065 0.5216975 0.4783025 ## 1066 0.3738247 0.6261753 ## 1067 0.4666863 0.5333137 ## 1068 0.3993705 0.6006295 ## 1069 0.4506892 0.5493108 ## 1070 0.4235170 0.5764830 ## 1071 0.5036414 0.4963586 ## 1072 0.4593288 0.5406712 ## 1073 0.4587988 0.5412012 ## 1074 0.3965787 0.6034213 ## 1075 0.4428192 0.5571808 ## 1076 0.4287787 0.5712213 ## 1077 0.4202670 0.5797330 ## 1078 0.4523464 0.5476536 ## 1079 0.4258989 0.5741011 ## 1080 0.4358286 0.5641714 ## 1081 0.4409698 0.5590302 ## 1082 0.4491046 0.5508954 ## 1083 0.3986650 0.6013350 ## 1084 0.4804910 0.5195090 ## 1085 0.4487050 0.5512950 ## 1086 0.4616361 0.5383639 ## 1087 0.4074084 0.5925916 ## 1088 0.4311115 0.5688885 ## 1089 0.4452828 0.5547172 The x component contains the linear discriminants. result_lda$x ## LD1 ## 986 -0.80594669 ## 987 2.92755168 ## 988 -2.01984129 ## 989 -2.05074043 ## 990 -0.99972841 ## 991 -0.37865579 ## 992 2.22702414 ## 993 -2.17875113 ## 994 -3.08806854 ## 995 -2.06045158 ## 996 -3.16178505 ## 997 4.66982149 ## 998 0.64322275 ## 999 2.66623327 ## 1000 1.38038783 ## 1001 0.68030171 ## 1002 0.61541353 ## 1003 -0.22769145 ## 1004 0.51874338 ## 1005 2.54484381 ## 1006 -2.25820605 ## 1007 0.14971942 ## 1008 1.54282900 ## 1009 0.94956560 ## 1010 0.23094000 ## 1011 -1.22176077 ## 1012 -0.16810026 ## 1013 -1.13612602 ## 1014 -0.90791384 ## 1015 3.01892483 ## 1016 1.76839269 ## 1017 0.31392625 ## 1018 0.97163642 ## 1019 -0.33539700 ## 1020 0.91248664 ## 1021 0.06408467 ## 1022 -0.59406691 ## 1023 1.08728746 ## 1024 1.02593061 ## 1025 -1.04475287 ## 1026 -0.86686213 ## 1027 1.93613085 ## 1028 0.61055795 ## 1029 -0.38439421 ## 1030 -1.83135657 ## 1031 1.35390286 ## 1032 0.94162011 ## 1033 -0.14117387 ## 1034 -0.05200779 ## 1035 0.52977878 ## 1036 -0.03920672 ## 1037 -0.21356613 ## 1038 0.90498257 ## 1039 -0.50225234 ## 1040 1.12657351 ## 1041 -0.40160944 ## 1042 -1.77662096 ## 1043 -0.77990314 ## 1044 -0.37203455 ## 1045 0.32937582 ## 1046 1.32521081 ## 1047 -0.24269960 ## 1048 1.31064407 ## 1049 0.38102152 ## 1050 0.32407882 ## 1051 0.19827520 ## 1052 0.37925585 ## 1053 0.55317384 ## 1054 -0.13940820 ## 1055 0.87496626 ## 1056 -1.16570091 ## 1057 -2.87618875 ## 1058 0.92881904 ## 1059 -1.92184689 ## 1060 0.01332181 ## 1061 -1.05049128 ## 1062 1.05109133 ## 1063 0.99150015 ## 1064 -1.66582548 ## 1065 -2.27762836 ## 1066 2.33428828 ## 1067 -0.59185983 ## 1068 1.50972278 ## 1069 -0.09879791 ## 1070 0.74651414 ## 1071 -1.72453384 ## 1072 -0.36541331 ## 1073 -0.34908091 ## 1074 1.59888886 ## 1075 0.14486384 ## 1076 0.58186590 ## 1077 0.84848129 ## 1078 -0.15000219 ## 1079 0.67191480 ## 1080 0.36204062 ## 1081 0.20224795 ## 1082 -0.04980071 ## 1083 1.53223501 ## 1084 -1.01561940 ## 1085 -0.03744106 ## 1086 -0.43648132 ## 1087 1.25414279 ## 1088 0.50903222 ## 1089 0.06849883 To obtain our predictions, we can simply extract the class element. Alternatively, if we want to directly extract just the predictions from the class element, we can use the predict function as we did earlier in the demonstration. #either pred_lda &lt;- result_lda$class #or pred_lda &lt;- predict(fit_lda, Weekly[!train, ], type = &quot;response&quot;)$class Now let’s compute the confusion matrix for our LDA classifier such that we can compare our predictions on the test data against the actual values in our dataset. (t &lt;- table(pred_lda, Weekly[!train, ]$Direction)) ## ## pred_lda Down Up ## Down 9 5 ## Up 34 56 And now the fraction of correct predictions which, we can see is identical to that obtained for logistic regression. sum(diag(t)) / sum(t) ## [1] 0.625 Quadratic Discriminant Analysis Let’s now consider how quadratic discriminant analysis would address our binary classification problem. The code syntax is identical to that for linear discriminant analysis and the qda function is also part of the MASS package. fit_qda &lt;- qda(Direction ~ Lag2, data = Weekly[train, ]) In terms of prior probabilities and group means, the output is identical to that of linear discriminant analysis. However, the output does not include the coefficients of the linear discriminants for obvious reasons. fit_qda ## Call: ## qda(Direction ~ Lag2, data = Weekly[train, ]) ## ## Prior probabilities of groups: ## Down Up ## 0.4477157 0.5522843 ## ## Group means: ## Lag2 ## Down -0.03568254 ## Up 0.26036581 The prediction function works in the same way as for LDA, except that it will produce only two elements (class, and posterior); again, the x element will not be included since we are dealing with a quadratic function. pred_qda &lt;- predict(fit_qda, Weekly[!train, ], type = &quot;response&quot;)$class The confusion matrix is computed in the same way. (t &lt;- table(pred_qda, Weekly[!train, ]$Direction)) ## ## pred_qda Down Up ## Down 0 0 ## Up 43 61 The fraction of correct predictions is lower than that for logistic regression and for LDA. We therefore conclude that in this context, QDA does not perform well in comparison to the previous two approaches. sum(diag(t)) / sum(t) ## [1] 0.5865385 \\(K\\)-nearest neighbours Earlier in the course, we covered K-nearest neighbour classification. Let’s now explore how this approach is used in R and how it performs in the context of our market movement problem. To implement KNN in R, the most commonly used package is class. Note that it is possible to be confronted with the following error when loading the package: Error: (converted from warning) package ‘class’ was built under R version ... This will occur if you are using an older version of R than that under which the package was built. The best option is to update RStudio. If that is not possible (e.g. due to system requirements), then another option is to suppress it using suppressWarnings(library(class)) in your console. This should allow you to use the functions from the package. In R, we build our model using the knn() function from the class package. This function works differently to those we have covered so far for linear and logistic regression, and for LDA and QDA. This is because the knn() function both fits the model AND generates predictions. There are four arguments required: argument 1: predictors in our training data, argument 2: predictors in our test data, argument 3: outcome variable in our training data, argument 4: the value for \\(k\\); note the function specifies 1 nearest neighbour by default but I added it here for illustration purposes (this value needs to be added only when using a value other than 1). Now, you may wonder why I also included the drop = FALSE argument when I subsetting the dataset. fit_knn &lt;- knn(Weekly[train, &quot;Lag2&quot;, drop = FALSE], Weekly[!train, &quot;Lag2&quot;, drop = FALSE], Weekly$Direction[train], k = 1 ) Before we proceed to interpret the results, let’s see what output we would produce if we subsetted our dataset such that we extract our predictor from the training data without the drop = FALSE argument. This looks like a vector, right? Weekly[train, &quot;Lag2&quot;] ## [1] 1.572 0.816 -0.270 -2.576 3.514 0.712 1.178 -1.372 0.807 ## [10] 0.041 1.253 -2.678 -1.793 2.820 4.022 0.750 -0.017 2.420 ## [19] -1.225 1.171 -2.061 0.729 0.112 2.480 -1.552 -2.259 -2.428 ## [28] -2.708 -2.292 -4.978 3.547 0.260 -2.032 -1.739 -1.693 1.781 ## [37] -3.682 4.150 -2.487 2.343 0.606 1.077 -0.637 2.260 1.716 ## [46] -0.284 1.508 -0.913 -2.349 -1.798 5.393 1.156 2.077 4.751 ## [55] 2.702 -0.924 1.318 1.209 -0.363 -1.635 2.106 0.037 1.343 ## [64] 0.999 -1.348 0.470 -1.329 -0.892 1.370 3.269 -2.668 0.754 ## [73] -1.188 -1.745 0.787 1.649 1.044 -0.856 1.641 -0.015 -0.398 ## [82] 2.228 0.320 -1.601 -1.416 1.129 -0.521 -1.205 0.052 2.897 ## [91] -2.115 1.853 0.401 -2.614 -1.694 -0.245 1.034 1.417 0.668 ## [100] 5.018 3.169 -1.011 0.906 -0.807 -1.613 0.565 0.338 -0.255 ## [109] 0.309 -2.001 0.346 1.345 -1.896 -0.483 0.682 2.906 -1.687 ## [118] 0.858 0.853 -1.433 0.958 0.321 -0.450 -0.900 -1.486 -0.054 ## [127] 2.062 0.692 0.241 -0.967 3.064 -1.256 0.246 -1.205 -0.002 ## [136] 0.540 0.599 0.798 -2.029 -0.936 -1.903 2.253 0.576 1.106 ## [145] -0.263 1.161 0.999 0.823 0.442 0.387 1.741 -0.342 -0.923 ## [154] -1.529 1.888 -0.238 0.612 2.313 -0.969 -2.330 2.110 0.616 ## [163] 0.834 0.078 -0.533 -1.427 0.102 1.607 -2.653 0.723 0.482 ## [172] -0.622 1.429 0.976 -0.029 -0.622 -0.800 0.884 -0.393 0.509 ## [181] -0.527 0.303 0.230 0.123 0.325 1.337 0.960 0.174 0.082 ## [190] -0.626 -0.262 0.798 -0.210 1.996 -1.327 0.984 -1.766 1.266 ## [199] -0.599 0.099 0.395 -0.207 0.528 0.214 -0.199 0.740 1.066 ## [208] -0.040 0.838 -1.857 0.079 -0.530 -0.346 -0.285 0.366 0.990 ## [217] -2.225 -3.216 0.298 -0.206 0.325 0.733 -0.685 -0.822 2.427 ## [226] 0.530 0.612 -0.317 -0.048 -3.414 0.768 0.751 1.025 -0.231 ## [235] 1.137 -0.255 1.061 0.377 2.183 -0.593 -0.597 0.643 -2.445 ## [244] 0.661 -1.645 3.076 -0.897 1.910 -2.425 0.015 -0.190 -1.989 ## [253] 0.223 -1.399 2.649 0.224 -0.122 0.307 1.148 -0.255 1.207 ## [262] 1.756 0.587 0.106 1.274 -0.551 0.855 1.215 1.100 -0.052 ## [271] 1.140 0.555 -0.145 1.223 1.051 1.044 -1.210 0.859 1.692 ## [280] -0.858 2.252 1.830 -0.902 2.133 0.633 -1.120 1.682 -0.709 ## [289] -0.685 0.739 0.159 0.668 1.568 1.863 -0.278 0.461 -0.329 ## [298] 0.345 0.506 -1.321 1.875 0.364 1.240 -0.017 1.168 1.730 ## [307] -0.185 -0.712 0.650 0.127 -2.416 1.665 1.600 2.288 3.229 ## [316] -1.278 1.713 -2.232 -1.687 1.252 1.433 -0.787 1.605 -2.920 ## [325] 1.313 1.301 -1.810 1.630 2.579 1.435 -1.384 0.626 -1.108 ## [334] 0.149 0.568 -1.967 -1.711 -1.154 -0.443 4.181 -0.059 0.470 ## [343] 0.274 -2.255 0.566 3.791 0.954 -0.122 2.225 -0.114 1.450 ## [352] -1.393 0.407 3.844 0.930 1.506 1.107 -2.301 -1.482 2.776 ## [361] 1.058 -1.158 1.533 2.195 -0.728 2.030 0.432 2.396 -0.830 ## [370] -1.366 1.789 -1.466 -1.144 -1.303 -2.065 -2.672 3.889 -0.127 ## [379] 6.219 1.453 0.603 2.083 0.148 1.147 4.110 0.608 -1.268 ## [388] 3.338 -0.026 -0.151 2.566 0.889 -1.436 -3.506 2.523 -2.606 ## [397] 3.289 -0.553 2.879 -0.557 2.096 0.202 -2.360 -0.267 -2.869 ## [406] 1.409 0.091 3.742 -0.798 2.972 -3.090 -0.693 -1.090 4.120 ## [415] -4.856 3.646 -0.408 2.369 3.283 0.754 1.384 1.463 0.605 ## [424] 1.224 2.859 -0.338 2.488 -1.072 1.085 -1.320 1.182 -1.147 ## [433] 0.053 0.157 -1.770 2.112 -1.348 0.165 2.957 1.167 1.562 ## [442] 1.926 -3.872 -1.765 -2.786 -2.451 1.740 -5.004 -5.184 3.611 ## [451] 1.093 2.417 -4.034 -1.816 7.317 1.349 2.615 3.854 -1.340 ## [460] 3.361 2.473 -1.308 -0.874 1.849 3.219 0.241 3.731 -2.496 ## [469] -1.453 4.444 -3.145 -0.748 0.739 -0.072 2.999 1.499 0.363 ## [478] -1.269 0.851 4.223 -2.177 2.870 -1.597 0.735 -0.535 -0.561 ## [487] -2.139 1.990 -2.569 3.803 -2.050 5.771 0.867 1.105 -4.359 ## [496] -2.080 -2.140 2.106 0.673 0.872 0.665 -0.411 -1.201 -4.348 ## [505] 0.427 4.148 -6.632 4.348 4.708 0.536 1.885 1.858 -0.378 ## [514] 1.177 -1.134 0.282 2.626 0.748 -1.891 1.643 -1.624 -5.634 ## [523] 4.721 -2.615 -2.958 -0.946 5.686 -1.001 4.975 4.301 -1.891 ## [532] 1.186 -10.538 5.748 1.247 -1.363 -0.815 -0.986 -2.056 7.202 ## [541] -1.375 0.515 -1.569 0.910 1.671 2.102 -1.973 -4.074 3.031 ## [550] 0.609 1.351 0.987 0.951 -1.727 -1.920 -1.166 -0.843 -1.916 ## [559] -2.471 1.656 -1.242 3.415 -4.255 0.127 -1.897 -1.978 4.156 ## [568] -4.215 -0.473 1.097 -1.661 1.556 1.819 0.924 -0.404 -2.572 ## [577] -1.006 -4.277 -0.938 -0.062 -6.720 -0.930 1.799 -2.749 4.880 ## [586] 5.026 0.810 1.082 -1.653 3.716 -1.089 -1.348 0.340 -4.000 ## [595] 0.905 -0.079 -2.760 2.107 -0.397 -0.415 0.707 -1.992 -2.369 ## [604] 1.976 -4.334 -4.217 -11.050 7.780 2.924 1.892 -1.664 2.900 ## [613] -1.576 3.045 1.637 1.027 -0.947 1.655 -3.041 1.941 1.409 ## [622] 0.990 -2.295 -1.573 0.506 -0.978 -2.315 0.726 -1.299 3.848 ## [631] 2.874 0.159 -1.497 -0.114 -2.149 -1.044 1.275 -4.342 -0.269 ## [640] -1.718 4.891 -2.058 -1.539 -3.712 -1.972 -1.800 0.069 -0.080 ## [649] -6.839 -7.992 0.600 1.337 5.137 2.215 1.302 -2.635 -2.418 ## [658] -0.460 -4.992 -2.132 -3.238 4.339 5.874 1.499 0.369 -0.690 ## [667] 1.687 2.277 0.619 -2.572 -2.494 0.706 -2.273 3.791 2.089 ## [676] -2.780 -4.478 -0.662 -3.040 0.627 1.591 -0.828 -1.458 0.528 ## [685] 7.503 -3.605 1.778 -1.200 2.911 0.585 3.479 0.358 1.167 ## [694] -1.173 3.254 2.508 0.086 0.716 -1.955 0.971 1.262 -0.483 ## [703] 0.540 -1.855 -0.261 1.338 0.241 1.505 1.327 -0.270 1.735 ## [712] -3.807 3.310 0.797 0.121 -1.002 2.119 0.238 -0.272 -1.435 ## [721] 2.214 0.312 1.191 1.352 0.664 1.149 1.207 1.602 0.151 ## [730] -0.913 1.028 0.267 -0.148 0.073 1.041 -3.137 -0.963 -0.155 ## [739] 3.046 -0.218 -0.413 0.528 -2.920 -0.777 -0.273 -0.195 2.480 ## [748] 0.162 1.245 -0.128 -0.052 -0.798 -1.117 -1.026 -1.379 1.429 ## [757] -3.426 0.078 3.151 0.858 0.529 0.924 0.412 -1.634 1.927 ## [766] -0.827 -1.242 -1.124 3.145 3.183 1.544 -1.168 1.052 0.720 ## [775] -0.266 0.522 1.334 0.148 -2.123 -0.141 -1.406 0.299 2.704 ## [784] 0.189 -0.308 0.814 0.887 -1.803 -0.869 -1.532 0.128 0.706 ## [793] -3.266 0.831 0.411 1.253 -1.477 3.053 0.799 -0.230 0.175 ## [802] 1.573 -2.086 0.241 1.458 1.325 0.469 0.041 -0.629 0.324 ## [811] -0.868 -1.198 1.072 1.926 -0.288 -1.827 1.112 -2.678 -0.780 ## [820] -0.588 1.595 1.813 1.195 1.097 1.601 -0.250 -0.451 0.631 ## [829] 0.106 -1.606 2.977 0.168 -2.029 1.762 -1.534 0.234 1.598 ## [838] 0.170 -0.171 -0.451 2.016 -0.329 -0.620 0.049 -0.492 1.719 ## [847] -0.051 1.156 -2.604 -1.875 1.036 0.630 -2.788 -0.061 -0.563 ## [856] 2.065 -0.372 -2.314 0.331 3.085 0.063 -0.986 2.807 -0.554 ## [865] 1.229 -0.922 1.597 -0.370 1.603 1.029 1.188 0.218 0.639 ## [874] -0.947 1.217 1.470 -0.018 -0.303 0.940 1.224 -1.144 0.534 ## [883] -0.606 1.491 -0.016 -0.582 1.843 -0.713 1.216 -0.299 -4.412 ## [892] 1.130 -1.133 3.544 -1.062 1.612 0.630 2.168 0.655 0.773 ## [901] 0.015 1.122 -0.461 1.360 -1.866 1.674 -1.980 0.053 1.802 ## [910] 1.441 -1.185 -4.899 -1.775 1.436 -0.530 2.312 -0.364 -1.387 ## [919] 2.112 2.796 0.066 2.020 0.270 -3.917 2.309 -1.669 -3.706 ## [928] 0.347 -1.237 2.807 1.588 -2.440 1.125 -0.402 -4.522 -0.752 ## [937] -5.412 0.409 4.871 -4.596 1.405 0.231 -1.661 -2.800 -0.404 ## [946] 3.212 -1.075 4.195 -2.742 4.314 0.540 1.149 -1.812 2.670 ## [955] -3.467 1.777 -2.835 -0.048 -3.096 -3.001 -1.211 -1.854 1.710 ## [964] -0.232 0.203 2.857 0.145 -0.462 -0.725 -3.159 0.756 0.270 ## [973] -3.331 -9.399 -18.195 4.596 -6.781 10.491 -3.898 -6.198 -8.389 ## [982] 12.026 -2.251 0.418 0.926 If you wrap the code within the class function you can indeed confirm that the output is a vector. class(Weekly[train, &quot;Lag2&quot;]) ## [1] &quot;numeric&quot; The knn() requires that the training and test data be specified as either a matrix or a dataframe. If we set the drop argument to FALSE, then we are essentially telling R NOT to delete the dimensions of our object when we are subsetting it such that it keeps the row numbers, thereby producing a dataframe. class(Weekly[train, &quot;Lag2&quot;, drop = FALSE]) ## [1] &quot;data.frame&quot; You must pay close attention to the requirements and specifications for the functions with which you build your models. Often, you will be prompted by error messages in the console. For example, the knn() function expects either a matrix or a dataframe. If you do not set drop = FALSE you will not be able to proceed: Error in knn(Weekly[train, \"Lag2\"], Weekly[!train, \"Lag2\"], Weekly$Direction[train], : dims of 'test' and 'train' differ Other times, the error may be not so severe as to impede the function from working, but incorrectly structured data or improperly coded variables will lead to the function producing invalid results. Remember to carefully explore the arguments using the Help tab (e.g. ?knn). Now let’s return to our results and produce a confusion matrix. (t &lt;- table(fit_knn, Weekly[!train, ]$Direction)) ## ## fit_knn Down Up ## Down 21 29 ## Up 22 32 Our overall fraction of correct predictions is 0.5. Therefore, the KNN classifier (\\(k = 1\\)) performs the worst out of all other classifiers we have explored so far (but only slightly worse than QDA). sum(diag(t)) / sum(t) ## [1] 0.5096154 But before we move on to our next classifier, let’s consider other values for \\(k\\) for illustration purposes. To ensure consistent results, we also set the seed (to 1 in this case). We fit KNN for up to \\(k = 30\\) by using the base R sapply to apply the knn() function to every integer from 1 to 30 and to then calculate the overall fraction of correct prediction. set.seed(1) knn_k &lt;- sapply(1:30, function(k) { fit &lt;- knn(Weekly[train, &quot;Lag2&quot;, drop = FALSE], Weekly[!train, &quot;Lag2&quot;, drop = FALSE], Weekly$Direction[train], k = k ) mean(fit == Weekly[!train, ]$Direction) }) We can then create a plot to observe at what value for k the overall fraction of correct predictions is highest. This fraction stabilises at a value for \\(k\\) somewhere between \\(k = 10\\) and \\(k = 15\\). plot(1:30, knn_k, type = &quot;o&quot;, xlab = &quot;k&quot;, ylab = &quot;Fraction correct&quot;) We can find this out directly by asking R the index of the first time a maximum value among all other values appears. Our classifier appears to perform best when \\(k = 12\\). (k &lt;- which.max(knn_k)) ## [1] 12 Now let’s re-evaluate our KNN classifier on the test data using `\\(k = 12\\) and compute the confusion matrix. fit_knn &lt;- knn(Weekly[train, &quot;Lag2&quot;, drop = FALSE], Weekly[!train, &quot;Lag2&quot;, drop = FALSE], Weekly$Direction[train], k = 12 ) table(fit_knn , Weekly[!train, ]$Direction) ## ## fit_knn Down Up ## Down 18 18 ## Up 25 43 Now, the overall fraction of correct predictions is higher than it was for \\(k = 1\\) but this fraction still does not outperform logistic regression and LDA. mean(fit_knn == Weekly[!train, ]$Direction) ## [1] 0.5865385 Naive Bayes Finally, let’s evaluate the performance of Naive Bayes and conclude which approach performs best for our market movement classification problem. A useful package for Naive Bayes is e1071. Like the class package, do note that you may be prompted with a similar error when loading the package (this can also be addressed by either updating RStudio or suppressing the warning). fit_NBayes &lt;- naiveBayes(Direction ~ Lag2, data = Weekly, subset = train) Before generating the predictions, let’s explore the output of the fit. There are two important components: - A-priori probabilities: i.e. prior probabilities (distribution of the classes for the response variable) - Conditional probabilities: parameters of the model for the predictor by class. For a numeric variable (as is our predictor in this case), the parameters shown are the mean [,1] and standard deviation [,2] for the predictor values in each class; for a categorical variable, these would be conditional probabilities for the predictor in each class. The a priori probabilities can be extracted by specifying fit_NBayes$apriori and the conditional probabilities can be extracted using fit_NBayes$tables. fit_NBayes ## ## Naive Bayes Classifier for Discrete Predictors ## ## Call: ## naiveBayes.default(x = X, y = Y, laplace = laplace) ## ## A-priori probabilities: ## Y ## Down Up ## 0.4477157 0.5522843 ## ## Conditional probabilities: ## Lag2 ## Y [,1] [,2] ## Down -0.03568254 2.199504 ## Up 0.26036581 2.317485 Now let’s predict market movement. pred_NBayes &lt;- predict(fit_NBayes, Weekly[!train, ], type = &quot;class&quot;) And finally, generate our confusion matrix. (t &lt;- table(pred_NBayes, Weekly[!train, ]$Direction)) ## ## pred_NBayes Down Up ## Down 0 0 ## Up 43 61 Our overall fraction of correct predictions is \\(0.5865385\\). Naive Bayes performs slightly better than KNN with \\(k = 1\\) (\\(0.5\\)) and the same as QDA (\\(0.5865385\\)). sum(diag(t)) / sum(t) ## [1] 0.5865385 Based on the approaches we have implemented in this demonstration, logistic regression and LDA perform best. "],["overview-3.html", "Overview", " Overview Section 4: Resampling Methods This section is comprised two demonstrations adapted from the core textbook for this course: James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd ed.New York: Springer. https://www.statlearning.com/ Learning Outcomes: appreciate the importance of resampling methods in gauging model accuracy and performance; appreciate the differences between cross-validation and bootstrapping; apply cross-validation and bootstrapping in R and interpret the output. In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. Function Description Package sample() obtain a random sample with or without replacement base R lm() fit linear model base R mean() compute the arithmetic mean base R poly() apply polynomials base R glm() fit generalised linear model base R cv.glm() calculate estimates for LOOCV (default) or K-fold CV (by specifying K) boot boot() generate bootstrap estimates boot "],["demonstration-1-cross-validation.html", "Demonstration 1: Cross-validation Data and Variables The Validation Set Approach Leave-One-Out Cross-Validation \\(k\\)-Fold Cross-Validation", " Demonstration 1: Cross-validation So far, we have established the importance of training and test data and you now should have a robust understanding of the differences between training and test error rates in relation to model performance. However, several challenges remain and these include: the size of the training set and whether it is sufficient to adequately train our model; the size of the test set and whether it is sufficient to adequately evaluate our model; the process of testing our model on just a single test set; the nature of the training set. We are dealing with a gap here: the gap between the need to reliably measure model performance and the process of splitting the data we have available. In this demonstration, you will learn more about how cross-validation provides us with ways to address this gap. Data and Variables In this demonstration, we will make use the Auto dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Auto dataset loads automatically. You should already be familiar with the Auto dataset but below is a reminder of the variables it contains: mpg: miles per gallon cylinders: Number of cylinders between 4 and 8 displacement: Engine displacement (cu. inches) horsepower: Engine horsepower weight: Vehicle weight (lbs.) acceleration: Time to accelerate from 0 to 60 mph (sec.) year: Model year origin: Origin of car (1. American, 2. European, 3. Japanese) name: Vehicle name In addition to ISRL2, you will also require the boot package which first needs to be installed. This package is required for both LOOCV and \\(k\\)-fold CV. library(ISLR2) library(boot) #don&#39;t forget to install it first The Validation Set Approach Let’s first consider the validation set approach. The Auto dataset has a total of 392 observations and the goal is to randomly split these observations using a 50/50 ratio. We can perform this randomised split using the base R sample() function. The first argument within sample() is 392. This is the total number of observations available to us from the Auto dataset. The second argument is 196. This represents the number of observations we want to select from the total we have available which is 392. Therefore, the sample() function will return a vector of 196 unique integers which represents a subset of 196 indices from a total of 392. Note: by default, the sample() function conducts the sampling without replacement. training_data &lt;- sample(392, 196) training_data ## [1] 45 280 62 222 341 23 127 147 294 139 271 269 16 201 230 375 388 198 ## [19] 85 74 352 174 164 22 153 236 76 372 232 369 101 90 138 202 360 6 ## [37] 135 377 208 146 256 184 362 131 32 197 149 30 99 350 373 105 119 148 ## [55] 9 263 327 179 140 359 93 348 363 262 223 241 277 114 50 210 215 300 ## [73] 81 217 310 87 134 206 72 233 53 297 366 27 177 5 356 235 77 229 ## [91] 29 390 144 104 88 296 89 14 80 169 261 301 349 323 303 259 51 33 ## [109] 108 247 186 200 35 42 364 273 86 143 209 249 178 116 267 151 181 61 ## [127] 160 287 226 231 221 65 268 304 211 237 161 155 357 298 71 260 328 243 ## [145] 248 340 251 73 392 79 47 157 337 257 167 57 162 8 92 180 333 264 ## [163] 387 312 384 270 220 141 175 213 136 214 63 374 288 191 2 293 282 290 ## [181] 183 128 52 279 385 286 100 84 335 318 391 193 165 355 60 283 Now we need to tell R to fit a linear regression model using only the observations corresponding to the training set. We can do so by using the subset argument which tells R to only select the 196 observations that are indexed at the specific positions as defined by the training_data object. In this way, the model will be fitted using only the observations from Auto dataset that are defined by this vector of indices. fit &lt;- lm(mpg ~ horsepower, data = Auto, subset = training_data) Let’s now calculate the Mean Squared Error for the test dataset. We obtain the predictions using thepredict() function with which you are already familiar. mean((Auto$mpg - predict(fit, Auto))[-training_data]^2) ## [1] 27.0118 Let’s try applying some transformations to our predictor horsepower using the poly() function and then calculate the test MSE values to observe how the MSE values change. We first fit a second-degree polynomial regression model: lm.fit2 &lt;- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = training_data) Then calculate the test MSE: mean((Auto$mpg - predict(lm.fit2, Auto))[-training_data]^2) ## [1] 24.05804 Now we fit a third-degree polynomial regression model lm.fit3 &lt;- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = training_data) Then calculate the test MSE: mean((Auto$mpg - predict(lm.fit3, Auto))[-training_data]^2) ## [1] 24.60815 What would happen if we choose a different training dataset instead? We can try this out by setting the seed to 2, for example. set.seed(2) training_data2 &lt;- sample(392, 196) We then run a new set of models: a linear regression model, a second degree polynomial model, and a third degree polynomial model and then calculate the test MSE values for each of the three models. linear regression model and corresponding test MSE: lm.fit &lt;- lm(mpg ~ horsepower, data = Auto, subset = training_data2) mean((Auto$mpg - predict(lm.fit, Auto))[-training_data2]^2) ## [1] 25.72651 quadratic regression model and corresponding test MSE: lm.fit2 &lt;- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = training_data2) mean((Auto$mpg - predict(lm.fit2, Auto))[-training_data2]^2) ## [1] 20.43036 cubic regression model and corresponding test MSE: lm.fit3 &lt;- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = training_data2) mean((Auto$mpg - predict(lm.fit3, Auto))[-training_data2]^2) ## [1] 20.38533 The results obtained using the second training dataset are consistent with the findings from the first training dataset whereby a model that predicts mpg using a quadratic function of horsepower performs better than a model that involves only a linear function of horsepower. Also, there is little evidence in favor of a model that uses a cubic function of horsepower. Overall, the main take-away point is that if we were to choose different training sets, we will obtain somewhat different errors on the validation set. This is an important point of reflection: if we implement the fixed split approach directly such that we fit our model once on our single training set and then immediately evaluate the predictive accuracy of that model once on our single test set, Nevertheless, the validation set approach still does not solve this challenge. Leave-One-Out Cross-Validation Now, let’s consider LOOCV. In R, one way to compute the LOOCV estimate is by using functions from the boot package. This package nevertheless requires that the models are built using the glm() function, including linear models. The glm() function will produce identical results to lm() when fitting a linear model. You do not need to specify the family argument as you did for logistic regression since it is already set by default to gaussian. Fitting a linear model with glm() is the same as with lm(). glm.fit &lt;- glm(mpg ~ horsepower, data = Auto) We then perform LOOCV using the cv.glm() function from the boot package. cv.err &lt;- cv.glm(Auto, glm.fit) The output includes a list of several components. The component of relevance for LOOCV is delta. cv.err$delta ## [1] 24.23151 24.23114 The delta component therefore provides us with the results of the cross-validation. The first value is the raw cross-validation estimate of prediction error whilst the second value is the adjusted cross-validation estimate that compensates for the bias introduced by not using leave-one-out cross-validation. In our case, the two values are identical to two decimal places and so our cross-validation estimate for the test error is approximately \\(24.23\\). We can repeat this procedure for increasingly complex polynomial fits. Instead of writing separate code for each model fit, we can automate the process using the for() function that initiates a for loop which iteratively fits polynomial regressions for polynomials of order \\(i=1\\) to \\(i=10\\), computes the associated cross-validation error, and stores it in the \\(i\\)th element of the vector cv.error. cv.error &lt;- rep(0, 10) for (i in 1:10) { glm.fit &lt;- glm(mpg ~ poly(horsepower, i), data = Auto) cv.error[i] &lt;- cv.glm(Auto, glm.fit)$delta[1] } The output shows us the estimated test MSE for the linear model and polynomials up to the 10th degree. cv.error ## [1] 24.23151 19.24821 19.33498 19.42443 19.03321 18.97864 18.83305 18.96115 ## [9] 19.06863 19.49093 We can see a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials. \\(k\\)-Fold Cross-Validation Finally, let’s consider \\(k\\)-fold cross-Validation. The same function (cv.glm()) can be used by setting the value of K. By default, the value of K is equal to the number of observations in data which therefore gives us LOOCV. In this example, we will use \\(k=10\\), a common choice for \\(k\\). We will again perform the same approach of increasingly complex polynomial fits as we did for LOOCV. The code is identical to the one we used to LOOCV except that of course, we specified K = 10. set.seed(17) cv.error.10 &lt;- rep(0, 10) for (i in 1:10) { glm.fit &lt;- glm(mpg ~ poly(horsepower, i), data = Auto) cv.error.10[i] &lt;- cv.glm(Auto, glm.fit, K = 10)$delta[1] } The output shows us a similar pattern to the estimated test MSE for the linear model and polynomials up to the 10th degree: we see a sharp drop between the linear and quadratic fits but once again no improvement with higher order polynomials. We can therefore conclude that a quadratic fit is suitable. cv.error.10 ## [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666 ## [9] 18.87013 20.95520 We saw earlier that when we LOOCV, the two values were essentially the same. In the case of \\(k\\)-fold CV, these values differ slightly but they are still very similar to each other. cv.glm(Auto, glm.fit, K = 10)$delta ## [1] 19.71557 19.60616 "],["demonstration-2-bootstrapping.html", "Demonstration 2: Bootstrapping Data and Variables Estimating the Accuracy of a Statistic of Interest Estimating the Accuracy of a Linear Regression Model", " Demonstration 2: Bootstrapping How can we implement bootstrapping in R? Follow the steps outlined in this demonstration to build basic coding skills required to perform bootstrapping. Data and Variables In this demonstration, we will make use the Portfolio and Auto package from the core textbook (James et. al 2021). These datasets are part of the ISRL2 package. Portfolio is a dataframe contains 100 observations specifically designed to illustrate the concept of bootstrapping and the way in which it is applied in R. The dataframe contains only two variables: X: returns for Asset X, and Y: returns for Asset Y. In addition to ISRL2, you will also require the boot package. library(ISLR2) library(boot) #you should have already installed this for Demo 1 Estimating the Accuracy of a Statistic of Interest One of the advantages of bootstrapping as a resampling method is that it can be applied in almost all situations and it is quite simple to perform it with R. The first step is to create a function that computes our statistic of interest. This function should take as input the \\((X,Y)\\) data as well as a vector indicating which observations should be used to estimate \\(\\alpha\\). The function then outputs the estimate for \\(\\alpha\\) based on the selected observations. We will call our function alpha.fn(), alpha.fn &lt;- function(data, index) { X &lt;- data$X[index] Y &lt;- data$Y[index] (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y)) } For example, the following command tells R to estimate \\(\\alpha\\) using all \\(100\\) observations from our dataset. alpha.fn(Portfolio, 1:100) ## [1] 0.5758321 We now use the sample() function to randomly select \\(100\\) observations from the range \\(1\\) to \\(100\\), with replacement. This is equivalent to constructing one new bootstrap dataset and recomputing \\(\\hat{\\alpha}\\) based on the new data set. We can perform this command many, many times, recording all of the corresponding estimates for \\(\\alpha\\), and computing the resulting standard deviation. set.seed(7) alpha.fn(Portfolio, sample(100, 100, replace = T)) ## [1] 0.5385326 However, the boot() function automates this approach. For example, we can tell R to repeat this command 1000 times and we obtain \\(R=1,000\\) bootstrap estimates for \\(\\alpha\\). The final output shows that using the original data,\\(\\hat{\\alpha}=0.5758\\), and that the bootstrap estimate for \\({\\rm SE}(\\hat{\\alpha})\\) is \\(0.0897\\). boot(Portfolio, alpha.fn, R = 1000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Portfolio, statistic = alpha.fn, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 0.5758321 0.0007959475 0.08969074 Estimating the Accuracy of a Linear Regression Model The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. In this example, we will assess the variability of the estimates for \\(\\beta_0\\) and \\(\\beta_1\\), the intercept and slope terms for a simple linear regression model that uses horsepower to predict mpg in the Auto data set. We will compare the estimates obtained using the bootstrap to those obtained using the formulas for \\({\\rm SE}(\\hat{\\beta}_0)\\) and \\({\\rm SE}(\\hat{\\beta}_1)\\). We first create a simple function, boot.fn(), which takes in the Auto data set as well as a set of indices for the observations, and returns the intercept and slope estimates for the linear regression model. We then apply this function to the full set of \\(392\\) observations in order to compute the estimates of \\(\\beta_0\\) and \\(\\beta_1\\) on the entire data set. Note that we do not need the { and } at the beginning and end of the function because it is only one line long. boot.fn &lt;- function(data, index) coef(lm(mpg ~ horsepower, data = data, subset = index)) boot.fn(Auto, 1:392) ## (Intercept) horsepower ## 39.9358610 -0.1578447 The boot.fn() function can also be used in order to create bootstrap estimates for the intercept and slope terms by randomly sampling from among the observations with replacement (as we did in the previous example with the `Portfolio dataset). We can see slight differences for the coefficient estimates each time we repeat this procedure. set.seed(1) boot.fn(Auto, sample(392, 392, replace = T)) ## (Intercept) horsepower ## 40.3404517 -0.1634868 boot.fn(Auto, sample(392, 392, replace = T)) ## (Intercept) horsepower ## 40.1186906 -0.1577063 Now, we use the boot() function to compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms. boot(Auto, boot.fn, 1000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Auto, statistic = boot.fn, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 39.9358610 0.0544513229 0.841289790 ## t2* -0.1578447 -0.0006170901 0.007343073 The results show that the bootstrap estimate for \\({\\rm SE}(\\hat{\\beta}_0)\\) is \\(0.84\\), and that the bootstrap estimate for \\({\\rm SE}(\\hat{\\beta}_1)\\) is \\(0.0073\\). How different are those estimates from those provided by fitting the model? Let’s now compute the standard errors for the regression coefficients in a linear model (we use summary and then extract the coefficients using $coef) summary(lm(mpg ~ horsepower, data = Auto))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.9358610 0.717498656 55.65984 1.220362e-187 ## horsepower -0.1578447 0.006445501 -24.48914 7.031989e-81 The standard error estimates for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) obtained from fitting the model using lm() are \\(0.717\\) for the intercept and \\(0.0064\\) for the slope. Interestingly, these are somewhat different from the estimates obtained using the bootstrap. Does this indicate a problem with the bootstrap? In fact, it suggests the opposite. Reflect on the formulae we covered earlier in the course and the assumptions on which these formulae rely. For example, they depend on the unknown parameter \\(\\sigma^2\\) (the noise variance) and so we estimate \\(\\sigma^2\\) using the RSS. Now although the formulas for the standard errors do not rely on the linear model being correct, the estimate for \\(\\sigma^2\\) does. If we create a scatterplot and examine the relationship between mpg and horsepower, we can see that there is a non-linear relationship and so the residuals from a linear fit will be inflated, and so will \\(\\hat{\\sigma}^2\\). plot(Auto$mpg, Auto$horsepower) Secondly, the standard formulas assume (somewhat unrealistically) that the \\(x_i\\) are fixed, and all the variability comes from the variation in the errors \\(\\epsilon_i\\). The bootstrap approach does not rely on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) than is the summary() function. Given the non-linear association betwen the two variables, let’s now compute the bootstrap standard error estimates and the standard linear regression estimates that result from fitting a quadratic model to the data. boot.fn &lt;- function(data, index) coef( lm(mpg ~ horsepower + I(horsepower^2), data = data, subset = index) ) set.seed(1) boot(Auto, boot.fn, 1000) ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = Auto, statistic = boot.fn, R = 1000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 56.900099702 3.511640e-02 2.0300222526 ## t2* -0.466189630 -7.080834e-04 0.0324241984 ## t3* 0.001230536 2.840324e-06 0.0001172164 Since this model provides a good fit to the data, there is now a better correspondence between the bootstrap estimates and the standard estimates of \\({\\rm SE}(\\hat{\\beta}_0)\\), \\({\\rm SE}(\\hat{\\beta}_1)\\) and \\({\\rm SE}(\\hat{\\beta}_2)\\). summary(lm(mpg ~ horsepower + I(horsepower^2), data = Auto))$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 56.900099702 1.8004268063 31.60367 1.740911e-109 ## horsepower -0.466189630 0.0311246171 -14.97816 2.289429e-40 ## I(horsepower^2) 0.001230536 0.0001220759 10.08009 2.196340e-21 This is an R Markdown Notebook. When you execute code within the notebook, the results appear beneath the code. Try executing this chunk by clicking the Run button within the chunk or by placing your cursor inside it and pressing Ctrl+Shift+Enter. plot(cars) Add a new chunk by clicking the Insert Chunk button on the toolbar or by pressing Ctrl+Alt+I. When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the Preview button or press Ctrl+Shift+K to preview the HTML file). The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike Knit, Preview does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed. "],["overview-4.html", "Overview 0.1 Learning Outcomes:", " Overview Section 5: Data Science in Practice 0.1 Learning Outcomes: In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. "],["practical-1-academic-salary.html", "Practical 1: Academic Salary Part I Part II", " Practical 1: Academic Salary This practical is based on a demonstration created by Dr. Tatjana Kecojevic, Lecturer in Social Statistics. For the tasks below, you will require the Salaries dataset. This dataset is part of the carData R package. To access the dataset, load the carData package (make sure to first install the package). You will also require the GGally package; please make sure to install it. Salaries is a data frame with 397 observations. This dataset consists of nine-month academic salary for Assistant Professors, Associate Professors and Professors in a college in the U.S to monitor salary differences between male and female faculty members. The data are from 2008-09. There are six variables: Variable Name Variable Description rank a factor with levels = AssocProf, AsstProf, Prof discipline a factor with levels A = theoretical departments) or B = applied departments yrs.since.phd years since PhD yrs.service years of service sex a factor with levels Female and Male salary nine-month salary, in dollars. Let’s first load the packages: library(carData) library(tidyverse) library(GGally) Once you load the carData package, the Salaries dataset will be ‘loaded’ too and can be accessed without needing to assign it to a separate object. head(Salaries) ## rank discipline yrs.since.phd yrs.service sex salary ## 1 Prof B 19 18 Male 139750 ## 2 Prof B 20 16 Male 173200 ## 3 AsstProf B 4 3 Male 79750 ## 4 Prof B 45 39 Male 115000 ## 5 Prof B 40 41 Male 141500 ## 6 AssocProf B 6 6 Male 97000 As usual, we can access variables within the dataset by indexing them. Salaries$salary ## [1] 139750 173200 79750 115000 141500 97000 175000 147765 119250 129000 ## [11] 119800 79800 77700 78000 104800 117150 101000 103450 124750 137000 ## [21] 89565 102580 93904 113068 74830 106294 134885 82379 77000 118223 ## [31] 132261 79916 117256 80225 80225 77000 155750 86373 125196 100938 ## [41] 146500 93418 101299 231545 94384 114778 98193 151768 140096 70768 ## [51] 126621 108875 74692 106639 103760 83900 117704 90215 100135 75044 ## [61] 90304 75243 109785 103613 68404 100522 101000 99418 111512 91412 ## [71] 126320 146856 100131 92391 113398 73266 150480 193000 86100 84240 ## [81] 150743 135585 144640 88825 122960 132825 152708 88400 172272 107008 ## [91] 97032 105128 105631 166024 123683 84000 95611 129676 102235 106689 ## [101] 133217 126933 153303 127512 83850 113543 82099 82600 81500 131205 ## [111] 112429 82100 72500 104279 105000 120806 148500 117515 72500 73500 ## [121] 115313 124309 97262 62884 96614 78162 155500 72500 113278 73000 ## [131] 83001 76840 77500 72500 168635 136000 108262 105668 73877 152664 ## [141] 100102 81500 106608 89942 112696 119015 92000 156938 144651 95079 ## [151] 128148 92000 111168 103994 92000 118971 113341 88000 95408 137167 ## [161] 89516 176500 98510 89942 88795 105890 167284 130664 101210 181257 ## [171] 91227 151575 93164 134185 105000 111751 95436 100944 147349 92000 ## [181] 142467 141136 100000 150000 101000 134000 103750 107500 106300 153750 ## [191] 180000 133700 122100 86250 90000 113600 92700 92000 189409 114500 ## [201] 92700 119700 160400 152500 165000 96545 162200 120000 91300 163200 ## [211] 91000 111350 128400 126200 118700 145350 146000 105350 109650 119500 ## [221] 170000 145200 107150 129600 87800 122400 63900 70000 88175 133900 ## [231] 91000 73300 148750 117555 69700 81700 114000 63100 77202 96200 ## [241] 69200 122875 102600 108200 84273 90450 91100 101100 128800 204000 ## [251] 109000 102000 132000 77500 116450 83000 140300 74000 73800 92550 ## [261] 88600 107550 121200 126000 99000 134800 143940 104350 89650 103700 ## [271] 143250 194800 73000 74000 78500 93000 107200 163200 107100 100600 ## [281] 136500 103600 57800 155865 88650 81800 115800 85000 150500 74000 ## [291] 174500 168500 183800 104800 107300 97150 126300 148800 72300 70700 ## [301] 88600 127100 170500 105260 144050 111350 74500 122500 74000 166800 ## [311] 92050 108100 94350 100351 146800 84716 71065 67559 134550 135027 ## [321] 104428 95642 126431 161101 162221 84500 124714 151650 99247 134778 ## [331] 192253 116518 105450 145098 104542 151445 98053 145000 128464 137317 ## [341] 106231 124312 114596 162150 150376 107986 142023 128250 80139 144309 ## [351] 186960 93519 142500 138000 83600 145028 88709 107309 109954 78785 ## [361] 121946 109646 138771 81285 205500 101036 115435 108413 131950 134690 ## [371] 78182 110515 109707 136660 103275 103649 74856 77081 150680 104121 ## [381] 75996 172505 86895 105000 125192 114330 139219 109305 119450 186023 ## [391] 166605 151292 103106 150564 101738 95329 81035 However, if we want to access variables within the dataset without needing to index them we can use the base R attach() function. attach(Salaries) So now, we can call on the variables from the dataset directly. salary ## [1] 139750 173200 79750 115000 141500 97000 175000 147765 119250 129000 ## [11] 119800 79800 77700 78000 104800 117150 101000 103450 124750 137000 ## [21] 89565 102580 93904 113068 74830 106294 134885 82379 77000 118223 ## [31] 132261 79916 117256 80225 80225 77000 155750 86373 125196 100938 ## [41] 146500 93418 101299 231545 94384 114778 98193 151768 140096 70768 ## [51] 126621 108875 74692 106639 103760 83900 117704 90215 100135 75044 ## [61] 90304 75243 109785 103613 68404 100522 101000 99418 111512 91412 ## [71] 126320 146856 100131 92391 113398 73266 150480 193000 86100 84240 ## [81] 150743 135585 144640 88825 122960 132825 152708 88400 172272 107008 ## [91] 97032 105128 105631 166024 123683 84000 95611 129676 102235 106689 ## [101] 133217 126933 153303 127512 83850 113543 82099 82600 81500 131205 ## [111] 112429 82100 72500 104279 105000 120806 148500 117515 72500 73500 ## [121] 115313 124309 97262 62884 96614 78162 155500 72500 113278 73000 ## [131] 83001 76840 77500 72500 168635 136000 108262 105668 73877 152664 ## [141] 100102 81500 106608 89942 112696 119015 92000 156938 144651 95079 ## [151] 128148 92000 111168 103994 92000 118971 113341 88000 95408 137167 ## [161] 89516 176500 98510 89942 88795 105890 167284 130664 101210 181257 ## [171] 91227 151575 93164 134185 105000 111751 95436 100944 147349 92000 ## [181] 142467 141136 100000 150000 101000 134000 103750 107500 106300 153750 ## [191] 180000 133700 122100 86250 90000 113600 92700 92000 189409 114500 ## [201] 92700 119700 160400 152500 165000 96545 162200 120000 91300 163200 ## [211] 91000 111350 128400 126200 118700 145350 146000 105350 109650 119500 ## [221] 170000 145200 107150 129600 87800 122400 63900 70000 88175 133900 ## [231] 91000 73300 148750 117555 69700 81700 114000 63100 77202 96200 ## [241] 69200 122875 102600 108200 84273 90450 91100 101100 128800 204000 ## [251] 109000 102000 132000 77500 116450 83000 140300 74000 73800 92550 ## [261] 88600 107550 121200 126000 99000 134800 143940 104350 89650 103700 ## [271] 143250 194800 73000 74000 78500 93000 107200 163200 107100 100600 ## [281] 136500 103600 57800 155865 88650 81800 115800 85000 150500 74000 ## [291] 174500 168500 183800 104800 107300 97150 126300 148800 72300 70700 ## [301] 88600 127100 170500 105260 144050 111350 74500 122500 74000 166800 ## [311] 92050 108100 94350 100351 146800 84716 71065 67559 134550 135027 ## [321] 104428 95642 126431 161101 162221 84500 124714 151650 99247 134778 ## [331] 192253 116518 105450 145098 104542 151445 98053 145000 128464 137317 ## [341] 106231 124312 114596 162150 150376 107986 142023 128250 80139 144309 ## [351] 186960 93519 142500 138000 83600 145028 88709 107309 109954 78785 ## [361] 121946 109646 138771 81285 205500 101036 115435 108413 131950 134690 ## [371] 78182 110515 109707 136660 103275 103649 74856 77081 150680 104121 ## [381] 75996 172505 86895 105000 125192 114330 139219 109305 119450 186023 ## [391] 166605 151292 103106 150564 101738 95329 81035 Part I Exploring the data glimpse(Salaries) ## Rows: 397 ## Columns: 6 ## $ rank &lt;fct&gt; Prof, Prof, AsstProf, Prof, Prof, AssocProf, Prof, Prof,… ## $ discipline &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, A, A,… ## $ yrs.since.phd &lt;int&gt; 19, 20, 4, 45, 40, 6, 30, 45, 21, 18, 12, 7, 1, 2, 20, 1… ## $ yrs.service &lt;int&gt; 18, 16, 3, 39, 41, 6, 23, 45, 20, 18, 8, 2, 1, 0, 18, 3,… ## $ sex &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Fe… ## $ salary &lt;int&gt; 139750, 173200, 79750, 115000, 141500, 97000, 175000, 14… We can see that rank, discipline, and sex are already coded as factors. The variables yrs.since.phd and yrs.service are coded as integers. Our viewpoint states a belief that more years in service will cause higher salary. Let us focus on the mechanics of fitting the model. First we will examine the impact of each individual variable to see if our view point is correct. We start off with salary vs yrs.since.phd. summary(Salaries) ## rank discipline yrs.since.phd yrs.service sex ## AsstProf : 67 A:181 Min. : 1.00 Min. : 0.00 Female: 39 ## AssocProf: 64 B:216 1st Qu.:12.00 1st Qu.: 7.00 Male :358 ## Prof :266 Median :21.00 Median :16.00 ## Mean :22.31 Mean :17.61 ## 3rd Qu.:32.00 3rd Qu.:27.00 ## Max. :56.00 Max. :60.00 ## salary ## Min. : 57800 ## 1st Qu.: 91000 ## Median :107300 ## Mean :113706 ## 3rd Qu.:134185 ## Max. :231545 Both explanatory variables, yrs.since.phd and yrs.service have mean and median values that are close to each other. However, the mean and median for the salary variable are quite different. We can better visualise this using boxplots. boxplot(Salaries[,3:4], col = c(&#39;brown1&#39;, &#39;steelblue&#39;), main = &quot;Distribution&quot;) means &lt;- sapply(Salaries[,3:4], mean) points(means, col = &quot;gray&quot;, pch = 22, lwd = 7) boxplot(salary, col = c(&#39;chartreuse4&#39;), main = &quot;Distributions&quot;) means &lt;- sapply(salary, mean) points(means, col = &quot;gray&quot;, pch = 22, lwd = 7) What do the box plots indicate? Salary and Years since PhD Let’s consider the relationship between yrs.since.phd and salary using a scatterplot onto which we add a line of best fit. Note that since we ‘attached’ the dataset, we can call on the variables without need to index or specify the dataset by name. plot(salary ~ yrs.since.phd, cex =.6, main = &quot;The Relationship between Nine-month Salary and Years since PhD&quot;, xlab = &quot;Years since PhD&quot;, ylab = &quot;Nine-month Salary (dollars)&quot;) model1 &lt;- lm(salary ~ yrs.since.phd) abline(model1, lty = 2, col = 2) summary(model1) ## ## Call: ## lm(formula = salary ~ yrs.since.phd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -84171 -19432 -2858 16086 102383 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 91718.7 2765.8 33.162 &lt;2e-16 *** ## yrs.since.phd 985.3 107.4 9.177 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 27530 on 395 degrees of freedom ## Multiple R-squared: 0.1758, Adjusted R-squared: 0.1737 ## F-statistic: 84.23 on 1 and 395 DF, p-value: &lt; 2.2e-16 What do the results indicate? Salary and Years of Service Let’s find out more about the relationship between nine-month salary and years of service. plot(salary ~ yrs.service, cex =.6, main = &quot;The Relationship between Nine-month Salary and Years of Service&quot;, xlab = &quot;Years of Service&quot;, ylab = &quot;Nine-month Salary (dollars)&quot;) model2 &lt;- lm(salary ~ yrs.service) abline(model1, lty = 2, col = 2) summary(model2) ## ## Call: ## lm(formula = salary ~ yrs.service) ## ## Residuals: ## Min 1Q Median 3Q Max ## -81933 -20511 -3776 16417 101947 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 99974.7 2416.6 41.37 &lt; 2e-16 *** ## yrs.service 779.6 110.4 7.06 7.53e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28580 on 395 degrees of freedom ## Multiple R-squared: 0.1121, Adjusted R-squared: 0.1098 ## F-statistic: 49.85 on 1 and 395 DF, p-value: 7.529e-12 What do the plot and model results indicate? The Model Let’s consider both variables (years of service and years since PhD) and whether these help explain salary. We define our multiple linear regression model as: \\[y = b_0 + b_1x_1 + b_2x_2 + e\\] mr_model &lt;- lm(salary ~ yrs.since.phd + yrs.service) summary(mr_model) ## ## Call: ## lm(formula = salary ~ yrs.since.phd + yrs.service) ## ## Residuals: ## Min 1Q Median 3Q Max ## -79735 -19823 -2617 15149 106149 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 89912.2 2843.6 31.620 &lt; 2e-16 *** ## yrs.since.phd 1562.9 256.8 6.086 2.75e-09 *** ## yrs.service -629.1 254.5 -2.472 0.0138 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 27360 on 394 degrees of freedom ## Multiple R-squared: 0.1883, Adjusted R-squared: 0.1842 ## F-statistic: 45.71 on 2 and 394 DF, p-value: &lt; 2.2e-16 Test a): Does the fitted model make sense? Do the estimated coefficients have the correct sign? The estimated model of best fit is: \\(salary = 89912.2 + 1562.9yrs.since.phd − 629.1yrs.service\\) We notice that when put together with the variable yrs.since.phd, the yrs.service changes sign, which is not in line with our previously drawn conclusion and the viewpoint. This is the result of collinearity, which you already know happens when two predictors are correlated with one another. (Multi)collinearity can be identified when: a regression coefficient \\(x_i\\) is not significant even though, theoretically, it should be highly correlated with the response variable \\(y\\); by adding or deleting an \\(x_i\\) variable, the regression coefficients change dramatically; we get a negative regression coefficient when the response should increase along with \\(x_i\\), or we get a positive regression coefficient when the response should decrease as \\(x_i\\) increases; the explanatory variables have high pairwise correlations. Removing one of the correlated explanatory variables usually doesn’t drastically reduce the \\(R^2/R^2adj\\). With this model, using yrs.since.phd and yrs.service variables we have managed to explain just over 18% of variation in the variable salary. Test b): Overall, is the model a good fit? \\(R^2adj\\) is 18.42%, putting this model on the weaker side. However let us go through the formal procedure and set the hypothesis below. The null hypothesis of will be tested using the F-test: \\(H_0:R^2=0\\) (that is, the set of explanatory variables are insignificant, or in other words: useless) \\(H_1:R^2&gt;0\\) (that is, at least one explanatory variable is significant, or in other words: important) The decision rule is: if \\(F_{calc} &lt; F_{crit} =&gt; H_0\\) if \\(F_{calc} &gt; F_{crit} =&gt; H_1\\) Examining the sample evidence we get that \\(F_{calc} = 45.71\\). The value for \\(F_{crit}\\) can be found in the statistical tables for \\(df1 = 2\\) and \\(df2 = 394\\). qf(0.95, 2, 394) ## [1] 3.018626 Since \\(F_{crit} = 3.02 &lt; F_{calc} =&gt; H_1\\), this implies that this is a valid model. As pointed out earlier, this formal test involves a rather weak alternative hypothesis, which says only that \\(R^2\\) is significantly bigger than 0. With \\(R^2\\) of around 18% we can conclude that this is a useful model worthy of further investigation. Test c): Individually, are the explanatory variables important? Stage two of our model validation procedure is to examine the importance of any one single explanatory variable used in the fitted model. We have pointed out that just because a set of variables is important does not necessarily mean that each individual variable is contributing towards explaining the behaviour of \\(Y\\). We will conduct a set of t-tests to check the validity of each variable one at a time. \\(b_1\\): previously we concluded that the relationship between \\(x_1\\) and \\(y\\) is positive (in the fitted model parameter \\(b_1\\) is positive). Consequently, we will use one tail t-test to assess the importance of \\(x_1\\) in the model. \\(H_0:b_1 = 0\\) (explanatory variable \\(i\\) is not important) \\(H_1:b_1 &gt; 0\\) (explanatory variable \\(i\\) has a positive influence) whereby: If \\(t_{calc} &lt; t_{crit} =&gt; H_0\\) If \\(t_{calc} &gt; t_{crit} =&gt; H_1\\) qt(0.95, 394) ## [1] 1.64873 \\(t_{calc} = 6.09 &gt; t_{crit} = 1.65 =&gt; H_1\\), which implies that we need to keep x1 in the model. \\(b_2\\): previously we concluded that the relationship between \\(x_2\\) and y is a positive relationship, but the model is suggesting that it is negative. We will stick to our belief and test if the coefficient should be positive: \\(H_0:b_2 = 0\\) (explanatory variable \\(i\\) is not important) \\(H_1:b_2 &gt; 0\\) (explanatory variable \\(i\\) has a positive influence) whereby: If \\(t_{calc} &lt; t_{crit} =&gt; H_0\\) If \\(t_{calc} &gt; t_{crit} =&gt; H_1\\) qt(0.95, 394) ## [1] 1.64873 \\(t_{calc} = −2.47 &lt; t_{crit} = 1.65 =&gt; H_0\\) therefore, the variable should be removed from the model. The increase in the explain variation of around 1% is negligible in comparison to the best one factor model \\(salary = f(yrs.since.phd) + e\\). Hence, we will put forward the model \\(salary = 91719 + 985yrs.since.phd\\) as our best fitted model. Alternatively you could test for the coefficient not being equal to zero and make a conclusion for yourself if this would be a sensible thing to do. In this example, we have adopted a ‘standard’ regression approach that assumes modelling a relationship between quantitative response and only quantitative predictors. However, often when building multiple regression models, we do not want to be limited to just quantitative predictors. Part II Now let’s expand our multiple linear regression model with two quantitative variables to a model that also includes categorical variables. # if you are starting a fresh R session, don&#39;t forget to: # load the package library(carData) # attach the dataset attach(Salaries) In many datasets, categorical (attribute) variables are usually encoded numerically and are accompanied by information about the levels of the variable saved in the levels attribute. Let’s consider the sex variable. attributes(sex) ## $levels ## [1] &quot;Female&quot; &quot;Male&quot; ## ## $class ## [1] &quot;factor&quot; This variable is already coded as a factor with two levels, Female and Male (which you should already know from earlier in the demonstration). Now, what if we want to transform a variable of class factor into one of class integer? unclass(sex) ## [1] 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 1 2 ## [38] 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 2 ## [75] 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 ## [112] 2 2 2 1 2 2 2 2 1 2 2 2 1 2 2 2 1 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [149] 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 ## [186] 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 ## [223] 2 2 2 2 2 2 2 2 1 1 2 1 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2 2 2 ## [260] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [297] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 1 ## [334] 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 2 2 2 ## [371] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## attr(,&quot;levels&quot;) ## [1] &quot;Female&quot; &quot;Male&quot; We can easily do so with the unclass() function which removes the attributes of a factor variable and transforms the levels into numeric values. However, when using factor variable in a linear regression model, it would make no sense to treat it as a quantitative explanatory variable. In the context of linear modelling we need to code each category to represent factor levels. Two-level attribute variables are very easy to code. We simply create an indicator or dummy variable that takes on two possible dummy numerical values. Consider the sex variable. We can code this using a dummy variable \\(d\\): \\[ d = \\begin{cases} 0, &amp; \\text{if female} \\\\ 1, &amp; \\text{if male} \\end{cases} \\]  This is the default coding used in R. A zero value is assigned to the level which is first alphabetically, unless it is changed by using the releveld() function for example, or by specifying the levels of the factor variable specifically. So, for a simple regression model predicting nine-month salary using one categorical variable: \\[salary = b_0 + b_1sex + e\\] the model is specified as follows: \\[salary_i = b_0 + b_1 sex_i + e_i = \\begin{cases} b_0 + b_1 \\times 1 + e_i = b_0 + b_1 + e_i, &amp; \\text{if the person is male} \\\\ b_0 + b_1 \\times 0 + e_i = b_0 + e_i, &amp; \\text{if the person is female} \\end{cases}\\] where \\(b_0\\) can be interpreted as the average nine-month salary for females, and \\(b_0 + b_1\\) as the nine-month average salary for males. The value of \\(b_1\\) represents the average difference in nine-month salary between females and males. We can conclude that dealing with an attribute variable with two levels in a linear model is straightforward. In this case, a dummy variable indicates whether an observation has a particular characteristic: yes/no. We can observe it as a ‘switch’ in a model, as this dummy variable can only assume the values \\(0\\) and \\(1\\), where \\(0\\) indicates the absence of the effect, and \\(1\\) indicates the presence. The values 0/1 can be seen as off/on. The way in which R codes dummy variables is controlled by the contrasts option: options(&quot;contrasts&quot;) ## $contrasts ## unordered ordered ## &quot;contr.treatment&quot; &quot;contr.poly&quot; The output points out the conversion of the factor into an appropriate set of contrasts. In particular, the first one: for unordered factors, and the second one: the ordered factors. The former is applicable in our context. To explicitly identify the coding of the factor, i.e. dummy variable used by R, we can use the contrasts() function. contrasts(sex) ## Male ## Female 0 ## Male 1 Note that applied contr.treatment conversion takes only the value \\(0\\) or \\(1\\) and that for an attribute variable with \\(k\\) levels it will create \\(k-1\\) dummy variables. There are many different ways of coding attribute variables besides the dummy variable approach explained here. All of these different approaches lead to equivalent model fits. What differs are the coefficients (i.e. model parameters as they require different interpretations, arranged to measure particular contrasts). This 0/1 coding implemented in R’s default contr.treatment contrast offers straightforward interpretation of the associated parameter in the model, which often is not the case when implementing other contrasts. Interpreting coefficients of attribute variables In the case of measured predictors, we are comfortable with the interpretation of the linear model coefficient as a slope, which tells us what a unit increase in the response variable is (i.e. outcome per unit increase in the explanatory variable). This is not necessarily the right interpretation for attribute predictors. Let’s consider average nine-month salary values for males and females separately. Salaries %&gt;% select(salary, sex) %&gt;% group_by(sex) %&gt;% summarise(mean=mean(salary)) ## # A tibble: 2 × 2 ## sex mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 Female 101002. ## 2 Male 115090. If we obtain the mean salary for each sex group we will find that for female professors the average salary is $ $101,002$ and for male professors the average is $ $115,090$. That is, a difference of \\(\\$14,088\\). If we now look at the parameters of the regression model for salary vs sex where females are coded as zero and males as one, we get exactly the same information, implying that the coefficient is the estimated difference in average between the two groups. lm(salary ~ sex) ## ## Call: ## lm(formula = salary ~ sex) ## ## Coefficients: ## (Intercept) sexMale ## 101002 14088 Fitting a Multivariate Regression Model In Part I, we explored the extent to which variation in the response variable salary is associated with variation in years since PhD and years in service. Now, we extend the model to also include sex, discipline and rank. The overall goals of any model we construct is that it should contain enough to explain relations in the data and at the same time be simple enough to understand, explain to others, and use. For convenience we will adopt the following notation: \\(y\\): salary \\(x_1\\): yrs.since.phd \\(x_2\\): yrs.service \\(x_3\\): discipline \\(x_4\\): sex \\(x_5\\): rank Next, we need to specify the model that embodies our mechanistic understanding of the factors involved and the way that they are related to the response variable. It would make sense to expect that all of the available x variables may impact the behaviour of y, thus the model we wish to build should reflect our viewpoint, i.e. \\(y=f(x_1,x_2,x_3,x_4,x_5)\\): \\[y=b_0 + b_1x_1 + b_2x_2 + b_3x_3 + b_4x_4 + b_5x_5 + e\\] Our viewpoint states a belief that all explanatory variables have a positive impact on the response. For example, more years in service will cause a higher salary. Our objective now is to determine the values of the parameters in the model that lead to the best fit of the model to the data. That is, we are not only trying to estimate the parameters of the model, but we are also seeking the minimal adequate model to describe the data. The best model is the model that produces the least unexplained variation following the principle of parsimony rather than complexity. That is the model should have as few parameters as possible, subject to the constraint that the parameters in the model should all be statistically significant. For regression modelling in R we use the lm() function, that fits a linear model assuming normal errors and constant variance. We specify the model by a formula that uses arithmetic operators which enable different functionalities from their ordinary ones. But, before we dive into statistical modelling of the given data, we need to take a first step and conduct the most fundamental task of data analysis procedure: Get to Know Our Data. ggpairs(Salaries) What information can you extract from this visualisation? Fitting the Model There are no fixed rules when fitting linear models, but there are adopted standards that have proven to work well in practice. We start off by fitting a maximal model then we carry on simplifying it by removing non-significant explanatory variables. This needs to be done with caution, making sure that the simplifications make good scientific sense, and do not lead to significant reductions in explanatory power. Although this should be the adopted strategy for fitting a model, it is not a guarantee to finding all the important structures in a complex data frame. We can summarise our model building procedure algorithm as follows: Fit the maximal model that includes all the variables. Then, assess the overall significance of the model by checking how big the \\(R^2/\\overline{R}^2\\) is. If statistically significant, carry on with the model fitting procedure, otherwise stop (F-test). Remove the least significant terms one at a time. Then, check the \\(t_calculated\\) for the variables values and perform a one tail or two tail t-test depending on your prior view. If the deletion causes an insignificant increase in \\(\\overline{R}^2\\), leave that term out of the model. Keep removing terms from the model until the model contains nothing but significant terms. Let’s build the model. Now, if we plan to use all variables in a dataset, there is no need to write the names of each individual predictor. Instead, we can use a full stop which tell R to include all other variables in the data object that do not already appear in the formula. model_1 &lt;- lm(salary ~ ., data = Salaries) summary(model_1) ## ## Call: ## lm(formula = salary ~ ., data = Salaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -65248 -13211 -1775 10384 99592 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 65955.2 4588.6 14.374 &lt; 2e-16 *** ## rankAssocProf 12907.6 4145.3 3.114 0.00198 ** ## rankProf 45066.0 4237.5 10.635 &lt; 2e-16 *** ## disciplineB 14417.6 2342.9 6.154 1.88e-09 *** ## yrs.since.phd 535.1 241.0 2.220 0.02698 * ## yrs.service -489.5 211.9 -2.310 0.02143 * ## sexMale 4783.5 3858.7 1.240 0.21584 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 22540 on 390 degrees of freedom ## Multiple R-squared: 0.4547, Adjusted R-squared: 0.4463 ## F-statistic: 54.2 on 6 and 390 DF, p-value: &lt; 2.2e-16 Overall, is the model a good fit? How big is the \\(R^2/\\overline{R}^2\\)? Individually, are the explanatory variables important? What steps are required given the results of the model? What is the structure of the final fitted model and how should the results be interpreted? "],["practical-2-foreign-direct-investment-fdi-study.html", "Practical 2: Foreign Direct Investment (FDI) Study", " Practical 2: Foreign Direct Investment (FDI) Study This practical has been developed by Dr. Tatjana Kecojevic, Lecturer in Social Statistics. For the tasks below, you will require the FDI dataset. Click here to download the file: FDI.csv . Remember to place your data file in a separate subfolder within your R project working directory. A business consultancy firm is compiling a major report about globalisation. One aspect of this study concerns the determinants of FDI undertaken by multi-national enterprises. Relevant information from a sample of 60 multi-national companies that undertook significant investment in overseas projects was made available as follows: Variable Name Variable Description FDI Value of FDI undertaken, in £ millions, by investing company | GDP_Cap GDP per capita, £000s, in the country receiving the investment Gr_rate The economic growth rate, in %-terms, in the country receiving the investment ROC The average return on capital invested, in %-terms, in the country receiving the investment Stable The political stability of the country receiving the investment as measured by the number of changes in government over the past 25 years Infra Infrastructure facilities (eg transport, communications) in the country receiving the investment Coded: 1 = basic infrastructure 2 = good infrastructure Trade The openness to trade of the country receiving the investment Coded: 1 = trade tightly controlled 2 = some restrictions on trade 3 = free trade This is a multiple regression type of the problem; FDI is the key response variable as this study concerns the determinants of FDI undertaken by multi-national enterprises. The model is defined as \\(Y = b_0 + b_1x_1 + b_2x_2 + ... + b_kx_k + e\\), for the general \\(k\\) explanatory variable model and where e is also known as the error term \\(e ∼ N(0,\\sigma^2)\\), with the error term from a normal distribution with a mean of \\(0\\), and a variance of \\(\\sigma^2\\). Based on prior knowledge, we make the assumption that GDP_Cap, Gr_rate, ROC, Infra, and Trade have positive relationships with FDI, whilst Stable has a negative relationship with FDI. We will use our best fit model to predict FDI for the following information: country X receiving the investment has GDP per capita of 11.1 and Gr_rate per capita of 3.05; The average return on capital invested is 20.5%; There were 11 changes of government over the past 25 years and country X has good infrastructure with some restrictions on trade. First, let’s load the required packages: library(tidyverse) # you should have already installed this package as part of the previous Demonstration library(GGally) Let’s import the data into R. mydataq1 &lt;- read.csv(&quot;data/FDI.csv&quot;, header = T) Now let’s get a glimpse of the data. As you know, there are many ways to do that, such as, for example, using the tidyverse glimpse function. This is quite a handy function because it also tells us more about the class of each variable. We can see that although Infra and Trade categorical, these are coded as integers. glimpse(mydataq1) ## Rows: 60 ## Columns: 7 ## $ FDI &lt;dbl&gt; 184.00, 187.00, 186.00, 192.00, 188.00, 190.00, 193.00, 194.00… ## $ GDP_Cap &lt;dbl&gt; 4.4, 6.3, 5.3, 5.9, 9.4, 7.6, 8.7, 6.0, 8.4, 10.1, 8.0, 6.9, 7… ## $ Gr_rate &lt;dbl&gt; 2.54, 4.06, 3.79, 3.38, 1.54, 2.25, 3.01, 2.13, 2.18, 3.33, 2.… ## $ ROC &lt;dbl&gt; 6.7, 9.3, 7.1, 3.9, 6.3, 9.3, 6.3, 9.7, 5.6, 17.1, 9.1, 15.2, … ## $ Stable &lt;int&gt; 9, 8, 11, 11, 8, 9, 9, 11, 12, 12, 8, 7, 12, 9, 8, 5, 9, 7, 11… ## $ Infra &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ Trade &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3,… We therefore need to transform them into factors. mydataq1 &lt;- mydataq1 %&gt;% mutate(Infra = as_factor(Infra), Trade = as_factor(Trade) ) We can then explore all variables in the dataset as pairs using a matrix of plots. Among many interesting features, we can note quite strong correlations among pairs of variables which suggest the presence of multicollinearity: GDP_Cap and ROC, Infra and GDP_Cap, Infra and ROC, and Infra and Gr_rate. GGally::ggpairs(mydataq1) Ok, so our initial model is: \\(FDI = b_0 + b_1GDP\\_Cap + b_2Gr\\_rate + b_3ROC – b_4Stable + b_5Infra + b_6Trade + e\\) where Infra and Stable are dummy variables. We can have a look at how these two dummy variables are used in the model by using the contrasts() function from base R. The Infra variable is coded as 1 = basic infrastructure and 2 = good infrastructure. Since this is a binary variable, there will be one reference category and a single dummy variable. contrasts(mydataq1$Infra) ## 2 ## 1 0 ## 2 1 The Trade variable is coded as 1 = trade tightly and 2 = some restrictions on trade and 2 = some restrictions on trade. Since this is variable with three categories, there will be one reference category and two dummy variables. contrasts(mydataq1$Trade) ## 2 3 ## 1 0 0 ## 2 1 0 ## 3 0 1 Let’s now fit our multiple regression model. m1 &lt;- lm(FDI ~ GDP_Cap + Gr_rate + ROC + Stable + Infra + Trade, data = mydataq1) And explore the results. summary(m1) ## ## Call: ## lm(formula = FDI ~ GDP_Cap + Gr_rate + ROC + Stable + Infra + ## Trade, data = mydataq1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.8594 -1.2595 -0.0808 1.4183 8.7210 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 189.472086 2.690418 70.425 &lt; 2e-16 *** ## GDP_Cap 0.938713 0.236206 3.974 0.000219 *** ## Gr_rate -0.089122 0.498508 -0.179 0.858807 ## ROC 0.003144 0.088466 0.036 0.971782 ## Stable -0.539889 0.172155 -3.136 0.002816 ** ## Infra2 -0.169110 1.493552 -0.113 0.910287 ## Trade2 4.875539 0.891476 5.469 1.31e-06 *** ## Trade3 5.890833 1.179891 4.993 7.07e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.734 on 52 degrees of freedom ## Multiple R-squared: 0.7481, Adjusted R-squared: 0.7142 ## F-statistic: 22.06 on 7 and 52 DF, p-value: 1.646e-13 What do the results indicate? How do they compare with our initial assumptions? How do you proceed with the analysis? "],["demonstration-propublicas-analysis-of-the-compas-tool.html", "Demonstration: ProPublica’s Analysis of the COMPAS Tool Notes on the data Setup Preprocess data Exploratory analysis Bias in COMPAS Predictive Accuracy", " Demonstration: ProPublica’s Analysis of the COMPAS Tool In this demonstration, you will explore the analysis conducted by ProPublica on the COMPAS Recidivism Algorithm. Notes on the data Please ensure that your first complete the two readings indicated in the learning materials for this section. Salient points are highlighted below; see the full description from ProPublica for additional details. Goal: We looked at more than 10,000 criminal defendants in Broward County, Florida, and compared their predicted recidivism rates with the rate that actually occurred over a two-year period. COMPAS tool input (data subjects): When most defendants are booked in jail, they respond to a COMPAS questionnaire. Their answers are fed into the COMPAS software to generate several scores including predictions of Risk of Recidivism and Risk of Violent Recidivism. How COMPAS input was acquired by ProPublica: Through a public records request, ProPublica obtained two years worth of COMPAS scores from the Broward County Sheriff’s Office in Florida. We received data for all 18,610 people who were scored in 2013 and 2014. COMPAS tool output: Each pretrial defendant received at least three COMPAS scores: “Risk of Recidivism,” “Risk of Violence” and “Risk of Failure to Appear. \\[...\\] COMPAS scores for each defendant ranged from 1 to 10, with ten being the highest risk. Scores 1 to 4 were labeled by COMPAS as”Low;” 5 to 7 were labeled “Medium;” and 8 to 10 were labeled “High.” Data integration (record linkage): Starting with the database of COMPAS scores, we built a profile of each person’s criminal history, both before and after they were scored. We collected public criminal records from the Broward County Clerk’s Office website through April 1, 2016. On average, defendants in our dataset were not incarcerated for 622.87 days (sd: 329.19). We matched the criminal records to the COMPAS records using a person’s first and last names and date of birth. This is the same technique used in the Broward County COMPAS validation study conducted by researchers at Florida State University in 2010. We downloaded around 80,000 criminal records from the Broward County Clerk’s Office website. What is recidivism? Northpointe defined recidivism as “a finger-printable arrest involving a charge and a filing for any uniform crime reporting (UCR) code.” We interpreted that to mean a criminal offense that resulted in a jail booking and took place after the crime for which the person was COMPAS scored. \\[...\\] For most of our analysis, we defined recidivism as a new arrest within two years. Setup Before proceeding through the tasks, ensure that you install the glue package. Load packages library(tidyverse) library(glue) Load data url &lt;- &quot;https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv&quot; compas_raw &lt;- read_csv( file = url, show_col_types = FALSE ) dim(compas_raw) ## [1] 7214 53 head(compas_raw) ## # A tibble: 6 × 53 ## id name first last compas_screening_date sex dob age age_cat ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 miguel… migu… hern… 2013-08-14 Male 1947-04-18 69 Greate… ## 2 3 kevon … kevon dixon 2013-01-27 Male 1982-01-22 34 25 - 45 ## 3 4 ed phi… ed philo 2013-04-14 Male 1991-05-14 24 Less t… ## 4 5 marcu … marcu brown 2013-01-13 Male 1993-01-21 23 Less t… ## 5 6 bouthy… bout… pier… 2013-03-26 Male 1973-01-22 43 25 - 45 ## 6 7 marsha… mars… miles 2013-11-30 Male 1971-08-22 44 25 - 45 ## # ℹ 44 more variables: race &lt;chr&gt;, juv_fel_count &lt;dbl&gt;, ## # decile_score...12 &lt;dbl&gt;, juv_misd_count &lt;dbl&gt;, juv_other_count &lt;dbl&gt;, ## # priors_count...15 &lt;dbl&gt;, days_b_screening_arrest &lt;dbl&gt;, c_jail_in &lt;dttm&gt;, ## # c_jail_out &lt;dttm&gt;, c_case_number &lt;chr&gt;, c_offense_date &lt;date&gt;, ## # c_arrest_date &lt;date&gt;, c_days_from_compas &lt;dbl&gt;, c_charge_degree &lt;chr&gt;, ## # c_charge_desc &lt;chr&gt;, is_recid &lt;dbl&gt;, r_case_number &lt;chr&gt;, ## # r_charge_degree &lt;chr&gt;, r_days_from_arrest &lt;dbl&gt;, r_offense_date &lt;date&gt;, … Inspect data For convenience, here is a table of variable definitions: Variable Description age Age of the defendant age_cat Age category. It can be &lt; 25, 25-45, &gt;45 sex Sex of the defendant. It is either “Male” or “Female” race Race of the defendant. It can be “African-American”, “Caucasian”, “Hispanic”, “Asian”, or “Other” c_charge_degree Charge. Either “M” for misdemeanor, “F” for felony, or “O” (not causing jail time) priors_count Count of prior crimes committed by the defendant days_b_screening_arrest Days between the arrest and COMPAS screening decile_score The COMPAS score estimated by the system. It is between 0-10 score_text Decile score. It can be “Low” (1-4), “Medium” (5-7), or “High” (8-10) is_recid Indicates if the defendant recidivated. It can be 0, 1, or -1 two_year_recid Indicates if the defendant recidivated within two years of COMPAS assessment c_jail_in Date the defendant was in jail c_jail_out Date when the defendant was released from jail Plot the distribution of age, race, and sex in the imported data (compas_raw): compas_raw |&gt; ggplot(aes(age)) + geom_histogram(binwidth = 1) compas_raw |&gt; ggplot(aes(race)) + geom_bar() compas_raw |&gt; ggplot(aes(sex)) + geom_bar() Preprocess data ProPublica implemented a few pre-processing steps. First, they generated a subset of the data with a few variables of interest. Here, we select even fewer variables, keeping only those that we will use in this notebook. We also relabel the race column. cols &lt;- c(&quot;id&quot;, &quot;age&quot;, &quot;c_charge_degree&quot;, &quot;race&quot;, &quot;age_cat&quot;, &quot;score_text&quot;, &quot;sex&quot;, &quot;priors_count...15&quot;, &quot;days_b_screening_arrest&quot;, &quot;decile_score...12&quot;, &quot;is_recid&quot;, &quot;two_year_recid&quot;) compas_selected &lt;- compas_raw |&gt; select( all_of(cols) ) |&gt; rename( priors_count = priors_count...15, decile_score = decile_score...12 ) |&gt; mutate( race = case_when( race == &quot;African-American&quot; ~ &quot;Black&quot;, race == &quot;Caucasian&quot; ~ &quot;White&quot;, TRUE ~ race ) ) head(compas_selected) ## # A tibble: 6 × 12 ## id age c_charge_degree race age_cat score_text sex priors_count ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 69 F Other Greater than … Low Male 0 ## 2 3 34 F Black 25 - 45 Low Male 0 ## 3 4 24 F Black Less than 25 Low Male 4 ## 4 5 23 F Black Less than 25 High Male 1 ## 5 6 43 F Other 25 - 45 Low Male 2 ## 6 7 44 M Other 25 - 45 Low Male 0 ## # ℹ 4 more variables: days_b_screening_arrest &lt;dbl&gt;, decile_score &lt;dbl&gt;, ## # is_recid &lt;dbl&gt;, two_year_recid &lt;dbl&gt; glimpse(compas_selected) ## Rows: 7,214 ## Columns: 12 ## $ id &lt;dbl&gt; 1, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 16, 18… ## $ age &lt;dbl&gt; 69, 34, 24, 23, 43, 44, 41, 43, 39, 21, 27, 23… ## $ c_charge_degree &lt;chr&gt; &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;… ## $ race &lt;chr&gt; &quot;Other&quot;, &quot;Black&quot;, &quot;Black&quot;, &quot;Black&quot;, &quot;Other&quot;, &quot;… ## $ age_cat &lt;chr&gt; &quot;Greater than 45&quot;, &quot;25 - 45&quot;, &quot;Less than 25&quot;, … ## $ score_text &lt;chr&gt; &quot;Low&quot;, &quot;Low&quot;, &quot;Low&quot;, &quot;High&quot;, &quot;Low&quot;, &quot;Low&quot;, &quot;Me… ## $ sex &lt;chr&gt; &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;… ## $ priors_count &lt;dbl&gt; 0, 0, 4, 1, 2, 0, 14, 3, 0, 1, 0, 3, 0, 0, 1, … ## $ days_b_screening_arrest &lt;dbl&gt; -1, -1, -1, NA, NA, 0, -1, -1, -1, 428, -1, 0,… ## $ decile_score &lt;dbl&gt; 1, 3, 4, 8, 1, 1, 6, 4, 1, 3, 4, 6, 1, 4, 1, 3… ## $ is_recid &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1… ## $ two_year_recid &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1… Take a moment to get a feel for the variables and structure of the data. ProPublica filtered the above data by removing rows where: The COMPAS score is missing. The charge date of the defendant’s COMPAS-scored crime was not within 30 days from the date of arrest. ProPublica assumed that the offense may not be correct in these cases. The recividist flag is “-1”. In such cases, ProPublica could not find a COMPAS record at all. The charge is “O”. These are ordinary traffic offenses and do not result in jail time. We implement these conditions here: compas &lt;- compas_selected |&gt; filter( score_text != &quot;N/A&quot;, days_b_screening_arrest &lt;= 30, days_b_screening_arrest &gt;= -30, is_recid != -1, c_charge_degree != &quot;O&quot; ) Note that ProPublica only included people who had recidivated within two years or had at least two years outside a correctional facility. This pre-processing step is “baked in” to the data that we imported from GitHub in this notebook. Check the dimensions (i.e. the number of variables and observations) of the imported (compas_raw) and preprocessed (compas) data: glue(&quot;Imported data: {nrow(compas_raw)}, {ncol(compas_raw)}&quot;) ## Imported data: 7214, 53 glue(&quot;Data after selecting variables: {nrow(compas_selected)}, {ncol(compas_selected)}&quot;) ## Data after selecting variables: 7214, 12 glue(&quot;Data after filtering observations: {nrow(compas)}, {ncol(compas)}&quot;) ## Data after filtering observations: 6172, 12 Take the additional step of making sure that the decile score (discussed below) is numeric: compas &lt;- compas |&gt; mutate(decile_score = as.numeric(decile_score)) Inspect data again Re-inspect salient variables in the data after the preprocessing steps. Plot the distribution of age, race, and sex in the preprocessed data (compas) and compare these distributions to the imported data (compas_raw): compas_compare &lt;- bind_rows( mutate(compas, source = &quot;raw&quot;), mutate(compas_raw, source = &quot;preprocessed&quot;), ) compas_compare |&gt; ggplot(aes(age, fill = source)) + geom_histogram(alpha = 0.5, position = &quot;identity&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. compas_compare |&gt; ggplot(aes(race, fill = source)) + geom_bar(alpha = 0.5, position = &quot;identity&quot;) compas_compare |&gt; ggplot(aes(sex, fill = source)) + geom_bar(alpha = 0.5, position = &quot;identity&quot;) Observe that we are iterating through the data analysis: import, inspect &amp; profile, preprocess, and profile again. Generate a crosstab summarizing the number of observations by race and sex: table(compas$race, compas$sex) ## ## Female Male ## Asian 2 29 ## Black 549 2626 ## Hispanic 82 427 ## Native American 2 9 ## Other 58 285 ## White 482 1621 Exploratory analysis Let’s turn our focus to the primary variable of interest: the COMPAS recidivism score. In this exploratory analysis, we are interested in the variable named decile_score. The ProPublica analysis notes: “Judges are often presented with two sets of scores from the COMPAS system: one that classifies people into high, medium or low risk, and a corresponding decile score.” Plot the distribution of decile_score for males and for females. To what extent do these distributions differ? # plot decile score by sex compas |&gt; ggplot(aes(x = decile_score)) + geom_histogram( aes(y = stat(density * width)), binwidth = 1, color = &quot;white&quot; ) + scale_x_continuous(breaks = 1:10) + scale_y_continuous(&quot;proportion&quot;) + facet_wrap(~ sex) ## Warning: `stat(density * width)` was deprecated in ggplot2 3.4.0. ## ℹ Please use `after_stat(density * width)` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. What about race? # plot decile score by race compas |&gt; filter(race %in% c(&quot;Black&quot;, &quot;White&quot;)) |&gt; ggplot(aes(x = decile_score)) + geom_histogram( aes(y = after_stat(density * width)), binwidth = 1, color = &quot;white&quot; ) + scale_x_continuous(breaks = 1:10) + scale_y_continuous(&quot;proportion&quot;) + facet_wrap(~ race)  Exercise Summarise the difference between the distribution of decile scores for Black defendants and White defendants in this text cell: Your answer here Risk labels Plot the distribution of COMPAS-assigned “risk labels” (the variable is named score_text) for Black defendants and White defendants: # plot risk labels by race compas |&gt; mutate( score_text = factor( score_text, levels = c(&quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;) ) ) |&gt; filter(race %in% c(&quot;Black&quot;, &quot;White&quot;)) |&gt; ggplot(aes(x = score_text, group = race, fill = race)) + geom_bar(aes(y = after_stat(prop)), position = &quot;dodge&quot;) + scale_y_continuous(&quot;proportion&quot;) Bias in COMPAS ProPublica focused on racial bias in the COMPAS algorithm. In general terms, ProPublica analyzed (i) how the risk scores vary by race and (ii) the extent to which the risk labels assigned to defendants matches up with their observed recidivism and how this varies by race. We will (approximately) reproduce this analysis below. Preprocess data for logistic regression ProPublica used a logistic regression model to analyze variation in the risk scores by race. In their analysis, they considered a “medium” and “high” risk score to be “high”, and “low” to be low. We will prepare the data accordingly, with low = 0 and high = 1: compas &lt;- compas |&gt; mutate(score_binary = ifelse(score_text == &quot;Low&quot;, 0, 1)) table(compas$score_text, compas$score_binary) ## ## 0 1 ## High 0 1144 ## Low 3421 0 ## Medium 0 1607 Estimate the logistic regression model model &lt;- glm( score_binary ~ priors_count + two_year_recid + c_charge_degree + age_cat + race + sex, data = compas, family = binomial(link = &quot;logit&quot;) ) summary(model) ## ## Call: ## glm(formula = score_binary ~ priors_count + two_year_recid + ## c_charge_degree + age_cat + race + sex, family = binomial(link = &quot;logit&quot;), ## data = compas) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.9966 -0.7919 -0.3303 0.8121 2.6024 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.55869 0.48236 -3.231 0.00123 ** ## priors_count 0.26895 0.01110 24.221 &lt; 2e-16 *** ## two_year_recid 0.68586 0.06402 10.713 &lt; 2e-16 *** ## c_charge_degreeM -0.31124 0.06655 -4.677 2.91e-06 *** ## age_catGreater than 45 -1.35563 0.09908 -13.682 &lt; 2e-16 *** ## age_catLess than 25 1.30839 0.07593 17.232 &lt; 2e-16 *** ## raceBlack 0.73162 0.47708 1.534 0.12514 ## raceHispanic -0.17398 0.48897 -0.356 0.72199 ## raceNative American 1.64862 0.89972 1.832 0.06689 . ## raceOther -0.57193 0.49897 -1.146 0.25170 ## raceWhite 0.25441 0.47821 0.532 0.59472 ## sexMale -0.22127 0.07951 -2.783 0.00539 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 8483.3 on 6171 degrees of freedom ## Residual deviance: 6168.4 on 6160 degrees of freedom ## AIC: 6192.4 ## ## Number of Fisher Scoring iterations: 5 Interpret estimates Take a moment to read through the model summary. One way to interpret the estimates is by calculating odds ratios. To calculate odds ratios, we take the exponential of the coefficients. For example, taking the exponential of the coefficient for defendants aged less than 25 (\\(\\beta_{age&lt;25}\\) = 1.30839) will return the odds of score_text taking the value “high” for those aged under 25 relative to those aged 25-45. Calculate this odds ratio here: exp(1.30839) ## [1] 3.700212 In words, the odds that COMPAS labeled a defendant as “high risk” of recidivism is 3.7 times greater for someone aged 25-45. Next, calculate the odds ratio for all of the coefficients in the model: exp(coef(model)) ## (Intercept) priors_count two_year_recid ## 0.2104123 1.3085835 1.9854836 ## c_charge_degreeM age_catGreater than 45 age_catLess than 25 ## 0.7325374 0.2577840 3.7002128 ## raceBlack raceHispanic raceNative American ## 2.0784485 0.8403136 5.1998120 ## raceOther raceWhite sexMale ## 0.5644338 1.2897066 0.8015029 Take a moment to read through these coefficients. What is the reference category for each variable? (e.g. For females, the reference category is male.) Think in terms of comparisons, for example: A person with a value of \\[     \\] on variable \\[     \\] is \\[     \\] times more likely to be labeled high risk compared to a person with a value of \\[     \\] on variable \\[     \\] In the female example above, this could be stated: “A person with a value of female on variable sex is 1.25 times more likely to be labeled high risk compared to a person with a value of male on variable sex” Of course, we should be more straightforward when writing up results. “A person with a value of male on variable sex” is rather verbose; “males” will suffice. Interpreting model estimates in straightforward terms is an underrated skill.  Exercise Summarise the odds associated with the race variable. Your answer here Predictive Accuracy In terms of fairness, ProPublica focused on the predictive accuracy of the COMPAS algorithm. In this case, predictive accuracy refers to the concordance between a person’s recidivism and the label assigned to that person by the COMPAS algorithm. For instance, how often did COMPAS predict that a person was at “high risk” of recidivism and that person in fact recidivated within two years? We can think of this in terms of a 2x2 table: Did not recidivate Recidivated Labeled high risk A B Labeled low risk C D ProPublica reported A and D for black defendants and white defendants, separately.  Exercise What are generic terms for A and D? Why focus on A and D? Your answer here ProPublica used a somewhat different data set to calculate the predictive accuracy of COMPAS. In this section we will use the compas data we preprocessed above for brevity. Note therefore that the numbers we calculate below will not match those reported by ProPublica. Let’s generate a crosstab of the variable denoting recidivism within two years (is_recid) and the binary score variable (score_binary): table(compas$is_recid, compas$score_binary) ## ## 0 1 ## 0 2248 934 ## 1 1173 1817 Based on this crosstab, input the number of true positives, false positives, true negatives, and false negatives: true_positive = 1817 false_positive = 934 true_negative = 2248 false_negative = 1173 You can calculate the false positive rate by taking FP / (FP + TN), where FP is the number of false positives and TN is the number of true negatives. Calculate the false positive rate: glue( &quot;All defendants, false positive rate: {(false_positive / (false_positive + true_negative) * 100)}&quot; ) ## All defendants, false positive rate: ## 29.3526084223759 Now calculate the false negative rate: (hint, replace the terms in the false positive rate formula in the previous text cell) glue( &quot;All defendants, false negative rate: {(false_negative / (false_negative + true_positive) * 100)}&quot; ) ## All defendants, false negative rate: ## 39.2307692307692 How do the false positive and false negative rates vary by race? Calculate the false positive rate and false negative rate for White defendants: rates &lt;- compas |&gt; filter(race %in% c(&quot;Black&quot;, &quot;White&quot;)) |&gt; group_by(race) |&gt; count(score_binary, is_recid) # white defendants w_tp = 430 w_fp = 266 w_tn = 963 w_fn = 444 glue(&quot;White defendants, false positive rate: {(w_fp / (w_fp + w_tn) * 100)}&quot;) ## White defendants, false positive rate: 21.6436126932465 glue(&quot;White defendants, false negative rate: {(w_fn / (w_fn + w_tp) * 100)}&quot;) ## White defendants, false negative rate: 50.8009153318078 Lastly, calculate the false positive rate and false negative rate for Black defendants: b_tp = 1248 b_fp = 581 b_tn = 821 b_fn = 525 glue(&quot;Black defendants, false positive rate: {(b_fp / (b_fp + b_tn) * 100)}&quot;) ## Black defendants, false positive rate: 41.4407988587732 glue(&quot;Black defendants, false negative rate: {(b_fn / (b_fn + b_tp) * 100)}&quot;) ## Black defendants, false negative rate: 29.6108291032149 Take a moment to review and compare the false positive rates and false negative rates above.  Exercise Reflect on the false positive and false negative rates for Black and White defendants. What do these rates suggest about the COMPAS algorithm’s and whether it should be used to assist sentencing decisions? Your answer here "],["answers-2.html", "Answers Practical 1 Practical 2", " Answers Practical 1 This practical is based on a demonstration created by Dr. Tatjana Kecojevic, Lecturer in Social Statistics. For the tasks below, you will require the Salaries dataset. This dataset is part of the carData R package. To access the dataset, load the carData package (make sure to first install the package). You will also require the GGally package; please make sure to install it. Salaries is a data frame with 397 observations. This dataset consists of nine-month academic salary for Assistant Professors, Associate Professors and Professors in a college in the U.S to monitor salary differences between male and female faculty members. The data are from 2008-09. There are six variables: Variable Name Variable Description rank a factor with levels = AssocProf, AsstProf, Prof discipline a factor with levels A = theoretical departments) or B = applied departments yrs.since.phd years since PhD yrs.service years of service sex a factor with levels Female and Male salary nine-month salary, in dollars. Let’s first load the packages: library(carData) library(tidyverse) library(GGally) Once you load the carData package, the Salaries dataset will be ‘loaded’ too and can be accessed without needing to assign it to a separate object. head(Salaries) ## rank discipline yrs.since.phd yrs.service sex salary ## 1 Prof B 19 18 Male 139750 ## 2 Prof B 20 16 Male 173200 ## 3 AsstProf B 4 3 Male 79750 ## 4 Prof B 45 39 Male 115000 ## 5 Prof B 40 41 Male 141500 ## 6 AssocProf B 6 6 Male 97000 As usual, we can access variables within the dataset by indexing them. Salaries$salary ## [1] 139750 173200 79750 115000 141500 97000 175000 147765 119250 129000 ## [11] 119800 79800 77700 78000 104800 117150 101000 103450 124750 137000 ## [21] 89565 102580 93904 113068 74830 106294 134885 82379 77000 118223 ## [31] 132261 79916 117256 80225 80225 77000 155750 86373 125196 100938 ## [41] 146500 93418 101299 231545 94384 114778 98193 151768 140096 70768 ## [51] 126621 108875 74692 106639 103760 83900 117704 90215 100135 75044 ## [61] 90304 75243 109785 103613 68404 100522 101000 99418 111512 91412 ## [71] 126320 146856 100131 92391 113398 73266 150480 193000 86100 84240 ## [81] 150743 135585 144640 88825 122960 132825 152708 88400 172272 107008 ## [91] 97032 105128 105631 166024 123683 84000 95611 129676 102235 106689 ## [101] 133217 126933 153303 127512 83850 113543 82099 82600 81500 131205 ## [111] 112429 82100 72500 104279 105000 120806 148500 117515 72500 73500 ## [121] 115313 124309 97262 62884 96614 78162 155500 72500 113278 73000 ## [131] 83001 76840 77500 72500 168635 136000 108262 105668 73877 152664 ## [141] 100102 81500 106608 89942 112696 119015 92000 156938 144651 95079 ## [151] 128148 92000 111168 103994 92000 118971 113341 88000 95408 137167 ## [161] 89516 176500 98510 89942 88795 105890 167284 130664 101210 181257 ## [171] 91227 151575 93164 134185 105000 111751 95436 100944 147349 92000 ## [181] 142467 141136 100000 150000 101000 134000 103750 107500 106300 153750 ## [191] 180000 133700 122100 86250 90000 113600 92700 92000 189409 114500 ## [201] 92700 119700 160400 152500 165000 96545 162200 120000 91300 163200 ## [211] 91000 111350 128400 126200 118700 145350 146000 105350 109650 119500 ## [221] 170000 145200 107150 129600 87800 122400 63900 70000 88175 133900 ## [231] 91000 73300 148750 117555 69700 81700 114000 63100 77202 96200 ## [241] 69200 122875 102600 108200 84273 90450 91100 101100 128800 204000 ## [251] 109000 102000 132000 77500 116450 83000 140300 74000 73800 92550 ## [261] 88600 107550 121200 126000 99000 134800 143940 104350 89650 103700 ## [271] 143250 194800 73000 74000 78500 93000 107200 163200 107100 100600 ## [281] 136500 103600 57800 155865 88650 81800 115800 85000 150500 74000 ## [291] 174500 168500 183800 104800 107300 97150 126300 148800 72300 70700 ## [301] 88600 127100 170500 105260 144050 111350 74500 122500 74000 166800 ## [311] 92050 108100 94350 100351 146800 84716 71065 67559 134550 135027 ## [321] 104428 95642 126431 161101 162221 84500 124714 151650 99247 134778 ## [331] 192253 116518 105450 145098 104542 151445 98053 145000 128464 137317 ## [341] 106231 124312 114596 162150 150376 107986 142023 128250 80139 144309 ## [351] 186960 93519 142500 138000 83600 145028 88709 107309 109954 78785 ## [361] 121946 109646 138771 81285 205500 101036 115435 108413 131950 134690 ## [371] 78182 110515 109707 136660 103275 103649 74856 77081 150680 104121 ## [381] 75996 172505 86895 105000 125192 114330 139219 109305 119450 186023 ## [391] 166605 151292 103106 150564 101738 95329 81035 However, if we want to access variables within the dataset without needing to index them we can use the base R attach() function. attach(Salaries) So now, we can call on the variables from the dataset directly. salary ## [1] 139750 173200 79750 115000 141500 97000 175000 147765 119250 129000 ## [11] 119800 79800 77700 78000 104800 117150 101000 103450 124750 137000 ## [21] 89565 102580 93904 113068 74830 106294 134885 82379 77000 118223 ## [31] 132261 79916 117256 80225 80225 77000 155750 86373 125196 100938 ## [41] 146500 93418 101299 231545 94384 114778 98193 151768 140096 70768 ## [51] 126621 108875 74692 106639 103760 83900 117704 90215 100135 75044 ## [61] 90304 75243 109785 103613 68404 100522 101000 99418 111512 91412 ## [71] 126320 146856 100131 92391 113398 73266 150480 193000 86100 84240 ## [81] 150743 135585 144640 88825 122960 132825 152708 88400 172272 107008 ## [91] 97032 105128 105631 166024 123683 84000 95611 129676 102235 106689 ## [101] 133217 126933 153303 127512 83850 113543 82099 82600 81500 131205 ## [111] 112429 82100 72500 104279 105000 120806 148500 117515 72500 73500 ## [121] 115313 124309 97262 62884 96614 78162 155500 72500 113278 73000 ## [131] 83001 76840 77500 72500 168635 136000 108262 105668 73877 152664 ## [141] 100102 81500 106608 89942 112696 119015 92000 156938 144651 95079 ## [151] 128148 92000 111168 103994 92000 118971 113341 88000 95408 137167 ## [161] 89516 176500 98510 89942 88795 105890 167284 130664 101210 181257 ## [171] 91227 151575 93164 134185 105000 111751 95436 100944 147349 92000 ## [181] 142467 141136 100000 150000 101000 134000 103750 107500 106300 153750 ## [191] 180000 133700 122100 86250 90000 113600 92700 92000 189409 114500 ## [201] 92700 119700 160400 152500 165000 96545 162200 120000 91300 163200 ## [211] 91000 111350 128400 126200 118700 145350 146000 105350 109650 119500 ## [221] 170000 145200 107150 129600 87800 122400 63900 70000 88175 133900 ## [231] 91000 73300 148750 117555 69700 81700 114000 63100 77202 96200 ## [241] 69200 122875 102600 108200 84273 90450 91100 101100 128800 204000 ## [251] 109000 102000 132000 77500 116450 83000 140300 74000 73800 92550 ## [261] 88600 107550 121200 126000 99000 134800 143940 104350 89650 103700 ## [271] 143250 194800 73000 74000 78500 93000 107200 163200 107100 100600 ## [281] 136500 103600 57800 155865 88650 81800 115800 85000 150500 74000 ## [291] 174500 168500 183800 104800 107300 97150 126300 148800 72300 70700 ## [301] 88600 127100 170500 105260 144050 111350 74500 122500 74000 166800 ## [311] 92050 108100 94350 100351 146800 84716 71065 67559 134550 135027 ## [321] 104428 95642 126431 161101 162221 84500 124714 151650 99247 134778 ## [331] 192253 116518 105450 145098 104542 151445 98053 145000 128464 137317 ## [341] 106231 124312 114596 162150 150376 107986 142023 128250 80139 144309 ## [351] 186960 93519 142500 138000 83600 145028 88709 107309 109954 78785 ## [361] 121946 109646 138771 81285 205500 101036 115435 108413 131950 134690 ## [371] 78182 110515 109707 136660 103275 103649 74856 77081 150680 104121 ## [381] 75996 172505 86895 105000 125192 114330 139219 109305 119450 186023 ## [391] 166605 151292 103106 150564 101738 95329 81035 Part I Exploring the data Let’s begin by exploring the dataset. glimpse(Salaries) ## Rows: 397 ## Columns: 6 ## $ rank &lt;fct&gt; Prof, Prof, AsstProf, Prof, Prof, AssocProf, Prof, Prof,… ## $ discipline &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, A, A,… ## $ yrs.since.phd &lt;int&gt; 19, 20, 4, 45, 40, 6, 30, 45, 21, 18, 12, 7, 1, 2, 20, 1… ## $ yrs.service &lt;int&gt; 18, 16, 3, 39, 41, 6, 23, 45, 20, 18, 8, 2, 1, 0, 18, 3,… ## $ sex &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Fe… ## $ salary &lt;int&gt; 139750, 173200, 79750, 115000, 141500, 97000, 175000, 14… We can see that rank, discipline, and sex are already coded as factors. The variables yrs.since.phd and yrs.service are coded as integers. Our viewpoint states a belief that more years in service will cause higher salary. Let us focus on the mechanics of fitting the model. First we will examine the impact of each individual variable to see if our view point is correct. We start off with salary vs yrs.since.phd. summary(Salaries) ## rank discipline yrs.since.phd yrs.service sex ## AsstProf : 67 A:181 Min. : 1.00 Min. : 0.00 Female: 39 ## AssocProf: 64 B:216 1st Qu.:12.00 1st Qu.: 7.00 Male :358 ## Prof :266 Median :21.00 Median :16.00 ## Mean :22.31 Mean :17.61 ## 3rd Qu.:32.00 3rd Qu.:27.00 ## Max. :56.00 Max. :60.00 ## salary ## Min. : 57800 ## 1st Qu.: 91000 ## Median :107300 ## Mean :113706 ## 3rd Qu.:134185 ## Max. :231545 Both explanatory variables, yrs.since.phd and yrs.service have mean and median values that are close to each other. However, the mean and median for the salary variable are quite different. We can better visualise this using boxplots. boxplot(Salaries[,3:4], col = c(&#39;brown1&#39;, &#39;steelblue&#39;), main = &quot;Distribution&quot;) means &lt;- sapply(Salaries[,3:4], mean) points(means, col = &quot;gray&quot;, pch = 22, lwd = 7) boxplot(salary, col = c(&#39;chartreuse4&#39;), main = &quot;Distributions&quot;) means &lt;- sapply(salary, mean) points(means, col = &quot;gray&quot;, pch = 22, lwd = 7) What do the box plots indicate? We notice that a number of observations are identified as the outliers that are pulling the mean away from the median. Salary and Years since PhD Let’s now consider the relationship between yrs.since.phd and salary using a scatterplot onto which we add a line of best fit. Note that since we ‘attached’ the dataset, we can call on the variables without need to index or specify the dataset by name. plot(salary ~ yrs.since.phd, cex =.6, main = &quot;The Relationship between Nine-month Salary and Years since PhD&quot;, xlab = &quot;Years since PhD&quot;, ylab = &quot;Nine-month Salary (dollars)&quot;) model1 &lt;- lm(salary ~ yrs.since.phd) abline(model1, lty = 2, col = 2) summary(model1) ## ## Call: ## lm(formula = salary ~ yrs.since.phd) ## ## Residuals: ## Min 1Q Median 3Q Max ## -84171 -19432 -2858 16086 102383 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 91718.7 2765.8 33.162 &lt;2e-16 *** ## yrs.since.phd 985.3 107.4 9.177 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 27530 on 395 degrees of freedom ## Multiple R-squared: 0.1758, Adjusted R-squared: 0.1737 ## F-statistic: 84.23 on 1 and 395 DF, p-value: &lt; 2.2e-16 What do the results indicate? The results show that there is a positive relationship between the nine-month salary and years since PhD completion. The relationship is on a weak side, with only 17.60% of variability in the response variable salary being explained by the predictor yrs.since.phd. Salary and Years of Service Let’s find out more about the relationship between nine-month salary and years of service. plot(salary ~ yrs.service, cex =.6, main = &quot;The Relationship between Nine-month Salary and Years of Service&quot;, xlab = &quot;Years of Service&quot;, ylab = &quot;Nine-month Salary (dollars)&quot;) model2 &lt;- lm(salary ~ yrs.service) abline(model1, lty = 2, col = 2) summary(model2) ## ## Call: ## lm(formula = salary ~ yrs.service) ## ## Residuals: ## Min 1Q Median 3Q Max ## -81933 -20511 -3776 16417 101947 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 99974.7 2416.6 41.37 &lt; 2e-16 *** ## yrs.service 779.6 110.4 7.06 7.53e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 28580 on 395 degrees of freedom ## Multiple R-squared: 0.1121, Adjusted R-squared: 0.1098 ## F-statistic: 49.85 on 1 and 395 DF, p-value: 7.529e-12 What do the plot and model results indicate? The plot confirms our viewpoint and again we have a positive relationship between salary and years of service. This variable explains around 11 % of variability in the response variable. Individually, the two variables do not seem to explain much of the variability in the response. The Model Let’s consider both variables (years of service and years since PhD) and whether these help explain salary. We define our multiple linear regression model as: \\[y = b_0 + b_1x_1 + b_2x_2 + e\\] mr_model &lt;- lm(salary ~ yrs.since.phd + yrs.service) summary(mr_model) ## ## Call: ## lm(formula = salary ~ yrs.since.phd + yrs.service) ## ## Residuals: ## Min 1Q Median 3Q Max ## -79735 -19823 -2617 15149 106149 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 89912.2 2843.6 31.620 &lt; 2e-16 *** ## yrs.since.phd 1562.9 256.8 6.086 2.75e-09 *** ## yrs.service -629.1 254.5 -2.472 0.0138 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 27360 on 394 degrees of freedom ## Multiple R-squared: 0.1883, Adjusted R-squared: 0.1842 ## F-statistic: 45.71 on 2 and 394 DF, p-value: &lt; 2.2e-16 Test a): Does the fitted model make sense? Do the estimated coefficients have the correct sign? The estimated model of best fit is: \\(salary = 89912.2 + 1562.9yrs.since.phd − 629.1yrs.service\\) We notice that when put together with the variable yrs.since.phd, the yrs.service changes sign, which is not in line with our previously drawn conclusion and the viewpoint. This is the result of collinearity, which you already know happens when two predictors are correlated with one another. (Multi)collinearity can be identified when: a regression coefficient \\(x_i\\) is not significant even though, theoretically, it should be highly correlated with the response variable \\(y\\); by adding or deleting an \\(x_i\\) variable, the regression coefficients change dramatically; we get a negative regression coefficient when the response should increase along with \\(x_i\\), or we get a positive regression coefficient when the response should decrease as \\(x_i\\) increases; the explanatory variables have high pairwise correlations. Removing one of the correlated explanatory variables usually doesn’t drastically reduce the \\(R^2/R^2adj\\). With this model, using yrs.since.phd and yrs.service variables we have managed to explain just over 18% of variation in the variable salary. Test b): Overall, is the model a good fit? \\(R^2adj\\) is 18.42%, putting this model on the weaker side. However let us go through the formal procedure and set the hypothesis below. The null hypothesis of will be tested using the F-test: \\(H_0:R^2=0\\) (that is, the set of explanatory variables are insignificant, or in other words: useless) \\(H_1:R^2&gt;0\\) (that is, at least one explanatory variable is significant, or in other words: important) The decision rule is: if \\(F_{calc} &lt; F_{crit} =&gt; H_0\\) if \\(F_{calc} &gt; F_{crit} =&gt; H_1\\) Examining the sample evidence we get that \\(F_{calc} = 45.71\\). The value for \\(F_{crit}\\) can be found in the statistical tables for \\(df1 = 2\\) and \\(df2 = 394\\). qf(0.95, 2, 394) ## [1] 3.018626 Since \\(F_{crit} = 3.02 &lt; F_{calc} =&gt; H_1\\), this implies that this is a valid model. As pointed out earlier, this formal test involves a rather weak alternative hypothesis, which says only that \\(R^2\\) is significantly bigger than 0. With \\(R^2\\) of around 18% we can conclude that this is a useful model worthy of further investigation. Test c): Individually, are the explanatory variables important? Stage two of our model validation procedure is to examine the importance of any one single explanatory variable used in the fitted model. We have pointed out that just because a set of variables is important does not necessarily mean that each individual variable is contributing towards explaining the behaviour of \\(Y\\). We will conduct a set of t-tests to check the validity of each variable one at a time. \\(b_1\\): previously we concluded that the relationship between \\(x_1\\) and \\(y\\) is positive (in the fitted model parameter \\(b_1\\) is positive). Consequently, we will use one tail t-test to assess the importance of \\(x_1\\) in the model. \\(H_0:b_1 = 0\\) (explanatory variable \\(i\\) is not important) \\(H_1:b_1 &gt; 0\\) (explanatory variable \\(i\\) has a positive influence) whereby: If \\(t_{calc} &lt; t_{crit} =&gt; H_0\\) If \\(t_{calc} &gt; t_{crit} =&gt; H_1\\) qt(0.95, 394) ## [1] 1.64873 \\(t_{calc} = 6.09 &gt; t_{crit} = 1.65 =&gt; H_1\\), which implies that we need to keep x1 in the model. \\(b_2\\): previously we concluded that the relationship between \\(x_2\\) and y is a positive relationship, but the model is suggesting that it is negative. We will stick to our belief and test if the coefficient should be positive: \\(H_0:b_2 = 0\\) (explanatory variable \\(i\\) is not important) \\(H_1:b_2 &gt; 0\\) (explanatory variable \\(i\\) has a positive influence) whereby: If \\(t_{calc} &lt; t_{crit} =&gt; H_0\\) If \\(t_{calc} &gt; t_{crit} =&gt; H_1\\) qt(0.95, 394) ## [1] 1.64873 \\(t_{calc} = −2.47 &lt; t_{crit} = 1.65 =&gt; H_0\\) therefore, the variable should be removed from the model. The increase in the explain variation of around 1% is negligible in comparison to the best one factor model \\(salary = f(yrs.since.phd) + e\\). Hence, we will put forward the model \\(salary = 91719 + 985yrs.since.phd\\) as our best fitted model. Alternatively you could test for the coefficient not being equal to zero and make a conclusion for yourself if this would be a sensible thing to do. In this example, we have adopted a ‘standard’ regression approach that assumes modelling a relationship between quantitative response and only quantitative predictors. However, often when building multiple regression models, we do not want to be limited to just quantitative predictors. Part II Now let’s expand our multiple linear regression model with two quantitative variables to a model that also includes categorical variables. # if you are starting a fresh R session, don&#39;t forget to: # load the package library(carData) # attach the dataset attach(Salaries) ## The following objects are masked from Salaries (pos = 3): ## ## discipline, rank, salary, sex, yrs.service, yrs.since.phd In many datasets, categorical (attribute) variables are usually encoded numerically and are accompanied by information about the levels of the variable saved in the levels attribute. Let’s consider the sex variable. attributes(sex) ## $levels ## [1] &quot;Female&quot; &quot;Male&quot; ## ## $class ## [1] &quot;factor&quot; This variable is already coded as a factor with two levels, Female and Male (which you should already know from earlier in the demonstration). Now, what if we want to transform a variable of class factor into one of class integer? unclass(sex) ## [1] 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 1 2 ## [38] 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 2 ## [75] 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 ## [112] 2 2 2 1 2 2 2 2 1 2 2 2 1 2 2 2 1 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [149] 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 ## [186] 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 ## [223] 2 2 2 2 2 2 2 2 1 1 2 1 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2 2 2 ## [260] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [297] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 1 ## [334] 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 2 2 2 ## [371] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## attr(,&quot;levels&quot;) ## [1] &quot;Female&quot; &quot;Male&quot; We can easily do so with the unclass() function which removes the attributes of a factor variable and transforms the levels into numeric values. However, when using factor variable in a linear regression model, it would make no sense to treat it as a quantitative explanatory variable. In the context of linear modelling we need to code each category to represent factor levels. Two-level attribute variables are very easy to code. We simply create an indicator or dummy variable that takes on two possible dummy numerical values. Consider the sex variable. We can code this using a dummy variable \\(d\\): \\[ d = \\begin{cases} 0, &amp; \\text{if female} \\\\ 1, &amp; \\text{if male} \\end{cases} \\]  This is the default coding used in R. A zero value is assigned to the level which is first alphabetically, unless it is changed by using the releveld() function for example, or by specifying the levels of the factor variable specifically. So, for a simple regression model predicting nine-month salary using one categorical variable: \\[salary = b_0 + b_1sex + e\\] the model is specified as follows: \\[salary_i = b_0 + b_1 sex_i + e_i = \\begin{cases} b_0 + b_1 \\times 1 + e_i = b_0 + b_1 + e_i, &amp; \\text{if the person is male} \\\\ b_0 + b_1 \\times 0 + e_i = b_0 + e_i, &amp; \\text{if the person is female} \\end{cases}\\] where \\(b_0\\) can be interpreted as the average nine-month salary for females, and \\(b_0 + b_1\\) as the nine-month average salary for males. The value of \\(b_1\\) represents the average difference in nine-month salary between females and males. We can conclude that dealing with an attribute variable with two levels in a linear model is straightforward. In this case, a dummy variable indicates whether an observation has a particular characteristic: yes/no. We can observe it as a ‘switch’ in a model, as this dummy variable can only assume the values \\(0\\) and \\(1\\), where \\(0\\) indicates the absence of the effect, and \\(1\\) indicates the presence. The values 0/1 can be seen as off/on. The way in which R codes dummy variables is controlled by the contrasts option: options(&quot;contrasts&quot;) ## $contrasts ## unordered ordered ## &quot;contr.treatment&quot; &quot;contr.poly&quot; The output points out the conversion of the factor into an appropriate set of contrasts. In particular, the first one: for unordered factors, and the second one: the ordered factors. The former is applicable in our context. To explicitly identify the coding of the factor, i.e. dummy variable used by R, we can use the contrasts() function. contrasts(sex) ## Male ## Female 0 ## Male 1 Note that applied contr.treatment conversion takes only the value \\(0\\) or \\(1\\) and that for an attribute variable with \\(k\\) levels it will create \\(k-1\\) dummy variables. There are many different ways of coding attribute variables besides the dummy variable approach explained here. All of these different approaches lead to equivalent model fits. What differs are the coefficients (i.e. model parameters as they require different interpretations, arranged to measure particular contrasts). This 0/1 coding implemented in R’s default contr.treatment contrast offers straightforward interpretation of the associated parameter in the model, which often is not the case when implementing other contrasts. Interpreting coefficients of attribute variables In the case of measured predictors, we are comfortable with the interpretation of the linear model coefficient as a slope, which tells us what a unit increase in the response variable is (i.e. outcome per unit increase in the explanatory variable). This is not necessarily the right interpretation for attribute predictors. Let’s consider average nine-month salary values for males and females separately. Salaries %&gt;% select(salary, sex) %&gt;% group_by(sex) %&gt;% summarise(mean=mean(salary)) ## # A tibble: 2 × 2 ## sex mean ## &lt;fct&gt; &lt;dbl&gt; ## 1 Female 101002. ## 2 Male 115090. If we obtain the mean salary for each sex group we will find that for female professors the average salary is $ $101,002$ and for male professors the average is $ $115,090$. That is, a difference of \\(\\$14,088\\). If we now look at the parameters of the regression model for salary vs sex where females are coded as zero and males as one, we get exactly the same information, implying that the coefficient is the estimated difference in average between the two groups. lm(salary ~ sex) ## ## Call: ## lm(formula = salary ~ sex) ## ## Coefficients: ## (Intercept) sexMale ## 101002 14088 Fitting a Multivariate Regression Model In Part I, we explored the extent to which variation in the response variable salary is associated with variation in years since PhD and years in service. Now, we extend the model to also include sex, discipline and rank. The overall goals of any model we construct is that it should contain enough to explain relations in the data and at the same time be simple enough to understand, explain to others, and use. For convenience we will adopt the following notation: \\(y\\): salary \\(x_1\\): yrs.since.phd \\(x_2\\): yrs.service \\(x_3\\): discipline \\(x_4\\): sex \\(x_5\\): rank Next, we need to specify the model that embodies our mechanistic understanding of the factors involved and the way that they are related to the response variable. It would make sense to expect that all of the available x variables may impact the behaviour of y, thus the model we wish to build should reflect our viewpoint, i.e. \\(y=f(x_1,x_2,x_3,x_4,x_5)\\): \\[y=b_0 + b_1x_1 + b_2x_2 + b_3x_3 + b_4x_4 + b_5x_5 + e\\] Our viewpoint states a belief that all explanatory variables have a positive impact on the response. For example, more years in service will cause a higher salary. Our objective now is to determine the values of the parameters in the model that lead to the best fit of the model to the data. That is, we are not only trying to estimate the parameters of the model, but we are also seeking the minimal adequate model to describe the data. The best model is the model that produces the least unexplained variation following the principle of parsimony rather than complexity. That is the model should have as few parameters as possible, subject to the constraint that the parameters in the model should all be statistically significant. For regression modelling in R we use the lm() function, that fits a linear model assuming normal errors and constant variance. We specify the model by a formula that uses arithmetic operators which enable different functionalities from their ordinary ones. But, before we dive into statistical modelling of the given data, we need to take a first step and conduct the most fundamental task of data analysis procedure: Get to Know Our Data. ggpairs(Salaries) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. What information can you extract from this visualisation? This is an information rich visualisation that includes pairwise relationships of all the variables we want to consider for our model. By focusing on the last column of the plots, we can notice influence from all explanatory variables onto the response, except maybe for discipline and sex. We also notice unbalanced representation of the groups for the variables rank and sex, but for the purpose of our practice in fitting a multi-factor model this isn’t too problematic. We need to be especially concerned with the extent of correlations between the explanatory variables, and what is of particular interest to us is the high multicollinearity between rank, yrs.since.phd and yrs.service, which happens when the variables are highly linearly related. As a consequence, we will need to keep an eye on the significance of using all of these variables in the model. Fitting the Model There are no fixed rules when fitting linear models, but there are adopted standards that have proven to work well in practice. We start off by fitting a maximal model then we carry on simplifying it by removing non-significant explanatory variables. This needs to be done with caution, making sure that the simplifications make good scientific sense, and do not lead to significant reductions in explanatory power. Although this should be the adopted strategy for fitting a model, it is not a guarantee to finding all the important structures in a complex data frame. We can summarise our model building procedure algorithm as follows: Fit the maximal model that includes all the variables. Then, assess the overall significance of the model by checking how big the \\(R^2/\\overline{R}^2\\) is. If statistically significant, carry on with the model fitting procedure, otherwise stop (F-test). Remove the least significant terms one at a time. Then, check the \\(t_calculated\\) for the variables values and perform a one tail or two tail t-test depending on your prior view. If the deletion causes an insignificant increase in \\(\\overline{R}^2\\), leave that term out of the model. Keep removing terms from the model until the model contains nothing but significant terms. Let’s build the model. Now, if we plan to use all variables in a dataset, there is no need to write the names of each individual predictor. Instead, we can use a full stop which tell R to include all other variables in the data object that do not already appear in the formula. model_1 &lt;- lm(salary ~ ., data = Salaries) summary(model_1) ## ## Call: ## lm(formula = salary ~ ., data = Salaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -65248 -13211 -1775 10384 99592 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 65955.2 4588.6 14.374 &lt; 2e-16 *** ## rankAssocProf 12907.6 4145.3 3.114 0.00198 ** ## rankProf 45066.0 4237.5 10.635 &lt; 2e-16 *** ## disciplineB 14417.6 2342.9 6.154 1.88e-09 *** ## yrs.since.phd 535.1 241.0 2.220 0.02698 * ## yrs.service -489.5 211.9 -2.310 0.02143 * ## sexMale 4783.5 3858.7 1.240 0.21584 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 22540 on 390 degrees of freedom ## Multiple R-squared: 0.4547, Adjusted R-squared: 0.4463 ## F-statistic: 54.2 on 6 and 390 DF, p-value: &lt; 2.2e-16 Overall, is the model a good fit? How big is the \\(R^2/\\overline{R}^2\\)? The \\(R^2 = 45.47%\\) and the \\(\\overline{R}^2= 44.63%\\) are well above the value of zero allowing us to accept this as a valid model without having to formally test it to assess its statistical significance. It manages to explain almost half of the variability in the response variable salary. Individually, are the explanatory variables important? What steps are required given the results of the model? We identify the sex variable as clearly not significant, which is in line with the conclusion we could draw from the boxplot in the pairwise comparison plot for salary vs. sex. We will remove it to begin the process of model simplification and remove the least significant term. We therefore re-fit (or ‘update’) the model without the sex variable. model_2 &lt;- update(model_1,~. - sex) summary(model_2) ## ## Call: ## lm(formula = salary ~ rank + discipline + yrs.since.phd + yrs.service, ## data = Salaries) ## ## Residuals: ## Min 1Q Median 3Q Max ## -65244 -13498 -1455 9638 99682 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 69869.0 3332.1 20.968 &lt; 2e-16 *** ## rankAssocProf 12831.5 4147.7 3.094 0.00212 ** ## rankProf 45287.7 4236.7 10.689 &lt; 2e-16 *** ## disciplineB 14505.2 2343.4 6.190 1.52e-09 *** ## yrs.since.phd 534.6 241.2 2.217 0.02720 * ## yrs.service -476.7 211.8 -2.250 0.02497 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 22550 on 391 degrees of freedom ## Multiple R-squared: 0.4525, Adjusted R-squared: 0.4455 ## F-statistic: 64.64 on 5 and 391 DF, p-value: &lt; 2.2e-16 We note a slight reduction in \\(\\overline{R}^2\\) from \\(44.63%\\) to \\(44.55%\\) which we can regard as an insignificant decrease. The next step is to check the coefficients and assess for the effect of the remaining variables. We identify yrs.since.phd and yrs.service as the least influential in explaining the variability of salary. To illustrate how to formally assess their effect, we will conduct the t-test for the yrs.since.phd variable: \\(H_0:b_{ysp} = 0\\) \\(H_1:b_{ysp} &gt; 0\\) Therefore: If \\(t_{calc} &lt; t_{crit} =&gt; H_0\\) If \\(t_{calc} &gt; t_{crit} =&gt; H_1\\) qt(0.95, 391) ## [1] 1.64876 As \\(t_{calc} = 2.217 &gt; t_{crit} = 1.64876 =&gt; H1\\), we will keep the remaining variable and stop with the model simplification and focus on its interpretation. Specify the final fitted model. The structure of our final fitted model is: \\[y=b_0 + b_1x_1 + b_2x_2 + b_3x_3 + b_4x_4 + e\\] where: \\(y\\): salary \\(x_1\\): yrs.since.phd \\(x_2\\): yrs.service \\(x_3\\): discipline \\(x_4\\): rank We can take a closer look at the coefficients of our fitted model: coef(model_2) ## (Intercept) rankAssocProf rankProf disciplineB yrs.since.phd ## 69869.0110 12831.5375 45287.6890 14505.1514 534.6313 ## yrs.service ## -476.7179 Examining the output we realise that R has created three dummy variables for the variable rank: \\[ dr_1 = \\begin{cases} 1 &amp; \\text{rank is AsstProf} \\\\ 0 &amp; \\text{for rank is not AsstProf} \\end{cases} \\] \\[ dr_2 = \\begin{cases} 1 &amp; \\text{rank is AssocProf} \\\\ 0 &amp; \\text{rank is not AssocProf} \\end{cases} \\] \\[ dr_3 = \\begin{cases} 1 &amp; \\text{rank is Prof} \\\\ 0 &amp; \\text{rank is not Prof} \\end{cases} \\] Therefore, R has chosen to use the model: \\[y = b_0 + b_1dr_2 + b_2dr_3 + b_3d_1 + b_4x_1 + b_5x_2 + e\\] where: - \\(y\\) is salary - \\(x_1\\) is yrs.since.phd - \\(x_2\\) is yrs.service - \\(dr_2\\) and \\(dr_3\\) are the dummy variables defined above for the purpose of coding variable rank - \\(d_1\\) is a dummy variable used in the coding of variable discipline as explained earlier Note that R doesn’t need to use \\(dr_1\\) to create three models; it only needs two dummy variables since it is using \\(dr_1\\) as a reference level, also known as the base line. This subsequently allows R to create three models relating to the rank variable: AsstProf: \\(y = b_0 + b_3d_1 + b_4x_1 + b_5x_2 + e\\) AssocProf: \\(y = (b_0 + b_1) + b_3d_1 + b_4x_1 + b_5x_2 + e\\) Prof: \\(y = (b_0 + b_2) + b_3d_1 + b_4x_1 + b_5x_2 + e\\) telling us that: \\(b_0\\) is the average salary for an Assistant Professor who works in a ‘theoretical’ department and \\(b_0 + b_3\\) the average salary for an Assistant Professor who works in an ‘applied’ department. \\((b_0 + b_1)\\) is the average salary for an Associate Professor who works in a ‘theoretical’ department and \\((b_0 + b_1) + b_3\\) the average salary for an Associate Professor who works in an ‘applied’ department. \\((b_0 + b_2)\\) is the average salary for a Professor who works in a ‘theoretical’ department and \\((b_0 + b_2) + b_3\\) the average salary for a Professor who works in an ‘applied’ department. Interpret the results Learning this we can make an interpretation of our final fitted model as follows: For every year since PhD (yrs.since.phd) on average salary (salary) will go up by $534.63 assuming the rest of the variables are fixed in the model. For every year in service (yrs.service) on average salary (salary) will go down by $476.72 assuming the rest of the variables are fixed in the model. The average salary of an Assistant Professor (rank: AsstProf) who works in a “theoretical” department is \\(\\$69,869.01\\) and who works in an “applied” department is \\(\\$84,374.16\\); this can vary for the number of years in service and since PhD. The average salary of an Associate Professor (rank: AssocProf) who works in a ‘theoretical’ department is \\(\\$82,700.55\\), and one who works in an ‘applied’ department is \\(\\$97,205.70\\); this can vary for the number of years in service and since PhD. The average salary of a Professor (rank: Prof) who works in a ‘theoretical’ department is \\(\\$115,156.70\\), and who works in an ‘applied’ department is \\(\\$129,661.90\\); this can vary for the number of years in service and since PhD. This model explains around 45% of the variability in the response variable salary. Adding ~ 0 to the lm() formula enables R to suppress the intercept. Note that if we remove the intercept, then we can directly obtain all “three intercepts” without a base level to fit the final fitted model: model_2_1 &lt;- lm(salary ~ 0 + rank + discipline + yrs.since.phd + yrs.service) summary(model_2_1) ## ## Call: ## lm(formula = salary ~ 0 + rank + discipline + yrs.since.phd + ## yrs.service) ## ## Residuals: ## Min 1Q Median 3Q Max ## -65244 -13498 -1455 9638 99682 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## rankAsstProf 69869.0 3332.1 20.968 &lt; 2e-16 *** ## rankAssocProf 82700.5 3916.7 21.115 &lt; 2e-16 *** ## rankProf 115156.7 4350.9 26.467 &lt; 2e-16 *** ## disciplineB 14505.2 2343.4 6.190 1.52e-09 *** ## yrs.since.phd 534.6 241.2 2.217 0.0272 * ## yrs.service -476.7 211.8 -2.250 0.0250 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 22550 on 391 degrees of freedom ## Multiple R-squared: 0.9638, Adjusted R-squared: 0.9633 ## F-statistic: 1736 on 6 and 391 DF, p-value: &lt; 2.2e-16 Practical 2 This practical has been developed by Dr. Tatjana Kecojevic, Lecturer in Social Statistics. For the tasks below, you will require the FDI dataset. Click here to download the file: FDI.csv . Remember to place your data file in a separate subfolder within your R project working directory. A business consultancy firm is compiling a major report about globalisation. One aspect of this study concerns the determinants of FDI undertaken by multi-national enterprises. Relevant information from a sample of 60 multi-national companies that undertook significant investment in overseas projects was made available as follows: Variable Name Variable Description FDI Value of FDI undertaken, in £ millions, by investing company | GDP_Cap GDP per capita, £000s, in the country receiving the investment Gr_rate The economic growth rate, in %-terms, in the country receiving the investment ROC The average return on capital invested, in %-terms, in the country receiving the investment Stable The political stability of the country receiving the investment as measured by the number of changes in government over the past 25 years Infra Infrastructure facilities (eg transport, communications) in the country receiving the investment Coded: 1 = basic infrastructure 2 = good infrastructure Trade The openness to trade of the country receiving the investment Coded: 1 = trade tightly controlled 2 = some restrictions on trade 3 = free trade This is a multiple regression type of the problem; FDI is the key response variable as this study concerns the determinants of FDI undertaken by multi-national enterprises. The model is defined as \\(Y = b_0 + b_1x_1 + b_2x_2 + ... + b_kx_k + e\\), for the general \\(k\\) explanatory variable model and where e is also known as the error term \\(e ∼ N(0,\\sigma^2)\\), with the error term from a normal distribution with a mean of \\(0\\), and a variance of \\(\\sigma^2\\). Based on prior knowledge, we make the assumption that GDP_Cap, Gr_rate, ROC, Infra, and Trade have positive relationships with FDI, whilst Stable has a negative relationship with FDI. We will use our best fit model to predict FDI for the following information: country X receiving the investment has GDP per capita of 11.1 and Gr_rate per capita of 3.05; The average return on capital invested is 20.5%; There were 11 changes of government over the past 25 years and country X has good infrastructure with some restrictions on trade. First, let’s load the required packages: library(tidyverse) # you should have already installed this package as part of the previous Demonstration library(GGally) Let’s import the data into R. mydataq1 &lt;- read.csv(&quot;data/FDI.csv&quot;, header = T) Now let’s get a glimpse of the data. As you know, there are many ways to do that, such as, for example, using the tidyverse glimpse function. This is quite a handy function because it also tells us more about the class of each variable. We can see that although Infra and Trade categorical, these are coded as integers. glimpse(mydataq1) ## Rows: 60 ## Columns: 7 ## $ FDI &lt;dbl&gt; 184.00, 187.00, 186.00, 192.00, 188.00, 190.00, 193.00, 194.00… ## $ GDP_Cap &lt;dbl&gt; 4.4, 6.3, 5.3, 5.9, 9.4, 7.6, 8.7, 6.0, 8.4, 10.1, 8.0, 6.9, 7… ## $ Gr_rate &lt;dbl&gt; 2.54, 4.06, 3.79, 3.38, 1.54, 2.25, 3.01, 2.13, 2.18, 3.33, 2.… ## $ ROC &lt;dbl&gt; 6.7, 9.3, 7.1, 3.9, 6.3, 9.3, 6.3, 9.7, 5.6, 17.1, 9.1, 15.2, … ## $ Stable &lt;int&gt; 9, 8, 11, 11, 8, 9, 9, 11, 12, 12, 8, 7, 12, 9, 8, 5, 9, 7, 11… ## $ Infra &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ Trade &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3,… We therefore need to transform them into factors. mydataq1 &lt;- mydataq1 %&gt;% mutate(Infra = as_factor(Infra), Trade = as_factor(Trade) ) We can then explore all variables in the dataset as pairs using a matrix of plots. Among many interesting features, we can note quite strong correlations among pairs of variables which suggest the presence of multicollinearity: GDP_Cap and ROC, Infra and GDP_Cap, Infra and ROC, and Infra and Gr_rate. GGally::ggpairs(mydataq1) Ok, so our initial model is: \\(FDI = b_0 + b_1GDP\\_Cap + b_2Gr\\_rate + b_3ROC – b_4Stable + b_5Infra + b_6Trade + e\\) where Infra and Stable are dummy variables. We can have a look at how these two dummy variables are used in the model by using the contrasts() function from base R. The Infra variable is coded as 1 = basic infrastructure and 2 = good infrastructure. Since this is a binary variable, there will be one reference category and a single dummy variable. contrasts(mydataq1$Infra) ## 2 ## 1 0 ## 2 1 The Trade variable is coded as 1 = trade tightly and 2 = some restrictions on trade and 2 = some restrictions on trade. Since this is variable with three categories, there will be one reference category and two dummy variables. contrasts(mydataq1$Trade) ## 2 3 ## 1 0 0 ## 2 1 0 ## 3 0 1 Let’s now fit our multiple regression model. m1 &lt;- lm(FDI ~ GDP_Cap + Gr_rate + ROC + Stable + Infra + Trade, data = mydataq1) And explore the results. summary(m1) ## ## Call: ## lm(formula = FDI ~ GDP_Cap + Gr_rate + ROC + Stable + Infra + ## Trade, data = mydataq1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.8594 -1.2595 -0.0808 1.4183 8.7210 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 189.472086 2.690418 70.425 &lt; 2e-16 *** ## GDP_Cap 0.938713 0.236206 3.974 0.000219 *** ## Gr_rate -0.089122 0.498508 -0.179 0.858807 ## ROC 0.003144 0.088466 0.036 0.971782 ## Stable -0.539889 0.172155 -3.136 0.002816 ** ## Infra2 -0.169110 1.493552 -0.113 0.910287 ## Trade2 4.875539 0.891476 5.469 1.31e-06 *** ## Trade3 5.890833 1.179891 4.993 7.07e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.734 on 52 degrees of freedom ## Multiple R-squared: 0.7481, Adjusted R-squared: 0.7142 ## F-statistic: 22.06 on 7 and 52 DF, p-value: 1.646e-13 What do the results indicate? How do they compare with our initial assumptions? How do you proceed with the analysis? The results provide us with several pieces of important information. We initially assumed that the relationship between Gr_rate and FDI and Infra and FDI are positive. However, we can see that the estimated coefficients are negative. Also, there are several variables that are not statistically significant. The ROC variable has the highest p-value. Overall, the model appears to be a good fit given that 74.81% of variability is being explained. Also, make a note of the adjusted r-squared value which is 71.42 % (as the name implies, this measure adjusts the r-squared value according to the number of predictors in the model). Ok, so given the evidence of multicollinearity from earlier and given the lack of statistical significance, we can proceed to remove the variable with the largest p-value (so the ROC) variable and refit the model. m2 &lt;- lm(FDI ~ GDP_Cap + Gr_rate + Stable + Infra + Trade, data = mydataq1) # or m2 &lt;- update(m1,~. - ROC, data = mydataq1) Now, we see that the explained variability is the same as in model 1 (74.81%). However, the adjusted R-squared has increased slightly (from 71.4% to about 72%). summary(m2) ## ## Call: ## lm(formula = FDI ~ GDP_Cap + Gr_rate + Stable + Infra + Trade, ## data = mydataq1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.8633 -1.2684 -0.0897 1.4174 8.7346 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 189.4790 2.6579 71.289 &lt; 2e-16 *** ## GDP_Cap 0.9409 0.2258 4.167 0.000114 *** ## Gr_rate -0.0846 0.4774 -0.177 0.860031 ## Stable -0.5413 0.1663 -3.255 0.001976 ** ## Infra2 -0.1358 1.1519 -0.118 0.906598 ## Trade2 4.8805 0.8723 5.595 7.92e-07 *** ## Trade3 5.9116 1.0151 5.824 3.45e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.708 on 53 degrees of freedom ## Multiple R-squared: 0.7481, Adjusted R-squared: 0.7196 ## F-statistic: 26.23 on 6 and 53 DF, p-value: 3.05e-14 We can observe that the Infra variable has the largest p-value and, as before with the ROC variable, we remove it and refit the model by removing the least significant term. m3 &lt;- update(m2,~. - Infra, data = mydataq1) The r-squared value has decreased slightly from 74.81% to 74.8. But, again, we see that the adjusted R-squared increased from about 72% in model 2 to about 72.5%. summary(m3) ## ## Call: ## lm(formula = FDI ~ GDP_Cap + Gr_rate + Stable + Trade, data = mydataq1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.8072 -1.3012 -0.0538 1.3545 8.7185 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 189.63356 2.29103 82.772 &lt; 2e-16 *** ## GDP_Cap 0.92068 0.14544 6.331 5.01e-08 *** ## Gr_rate -0.09192 0.46906 -0.196 0.84537 ## Stable -0.54240 0.16445 -3.298 0.00173 ** ## Trade2 4.89108 0.85969 5.689 5.34e-07 *** ## Trade3 5.95001 0.95263 6.246 6.86e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.683 on 54 degrees of freedom ## Multiple R-squared: 0.748, Adjusted R-squared: 0.7247 ## F-statistic: 32.06 on 5 and 54 DF, p-value: 5.104e-15 The Gr_rate variable has the largest p-value and, as before, we remove it and refit the model by removing the least significant term. m4 &lt;- update(m3,~. - Gr_rate, data = mydataq1) Finally, we obtain a model where all coefficients are statistically significant (although the Stable is significant at an \\(\\alpha\\) level of 0.05). We see that the r-squared value has again decreased slightly to 74.79% but the explained variability is highest for this model, about 73% (the adjusted r-squared penalises the addition of predictors that are non-significant; since we removed these, the value increased). Overall, the explained variability is still about 75% (the decrease across the models was extremely small). summary(m4) ## ## Call: ## lm(formula = FDI ~ GDP_Cap + Stable + Trade, data = mydataq1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.661 -1.300 -0.035 1.317 8.626 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 189.4274 2.0173 93.903 &lt; 2e-16 *** ## GDP_Cap 0.9109 0.1355 6.723 1.07e-08 *** ## Stable -0.5411 0.1629 -3.322 0.00159 ** ## Trade2 4.8837 0.8513 5.737 4.27e-07 *** ## Trade3 5.9368 0.9419 6.303 5.19e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.66 on 55 degrees of freedom ## Multiple R-squared: 0.7479, Adjusted R-squared: 0.7295 ## F-statistic: 40.78 on 4 and 55 DF, p-value: 7.573e-16 Now, we can specify the model as: \\[ \\begin{align*} FDI &amp;= 189.4274 + 0.9109 \\cdot GDP\\_Cap - 0.5411 \\cdot stable + 0.0000 \\cdot Trade1 \\\\ &amp;\\phantom{= 189.4274 + 0.9109 \\cdot GDP\\_Cap - 0.5411 \\cdot stable} + 4.8837 \\cdot Trade2 \\\\ &amp;\\phantom{= 189.4274 + 0.9109 \\cdot GDP\\_Cap - 0.5411 \\cdot stable} + 5.9368 \\cdot Trade3 \\end{align*} \\] Finally, we use our best fit model to predict FDI for the following information: country X receiving the investment has GDP per capita of 11.1 and Gr_rate per capita of 3.05; The average return on capital invested is 20.5%; There were 11 changes of government over the past 25 years and country X has good infrastructure with some restrictions on trade. \\(FDI = 189.4274 + 0.9109*11.1 - 0.5411*11 + 4.8837\\) 189.4274 + 0.9109*11.1 - 0.5411*11 + 4.8837 ## [1] 198.47 Given that our final model (model 4) explains about 75% of the variability, we can conclude that our prediction of 198.47 is a fairly good one. "],["overview-5.html", "Overview", " Overview Section 6 In this section, we will cover the following functions: Learning Outcomes: "],["practical.html", "Practical", " Practical "],["overview-6.html", "Overview", " Overview Section 7: Unsupervised Learning This section is comprised two demonstrations (accompanied by tasks at the end of each) developed by Dr. George Wood, Lecturer in Social Statistics. Learning Outcomes: preprocess the data for k-means clustering; perform k-means clustering in R and interpret the results; evaluate k-means clustering performance; visualise clusters; decide number of clusters; perform PCA analysis and interpret the results; plot principal components; compute correlation between observed y and fitted y; compute cumulative variance explained compute percentage of variance explained by each principal component; In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. Function Description Package kmeans() perform k-means clustering on a data matrix base R (stats) fviz_cluster() visualise clustering results factoextra map_dbl() apply a function to each element of a vector purrr fviz_nbclust() determining and visualising the optimal number of clusters factoextra head() returns the first part of a vector, matrix, table, data frame or function base R (utils) glimpse() obtain a glimpse of your data dplyr gglot() create a new ggplot ggplot2 geom_point() create scatterplot ggplot2 scale_color_discrete() use a discrete colour scale ggplot2 scale_x_continuous(), scale_y_continuous() position scales for continuous data ggplot2 geom_hline(), geom_vline() add horizontal or vertical lines, respectively ggplot2 geom_text() adding text geoms (labelling plots) ggplot2 facet_wrap() wrap a 1D sequence of panels into 2D ggplot2 coord_equal() fixed scale coordinate system ggplot2 geom_smooth() smoothed conditional means (addressed overplotting) ggplot2 theme() modify components of themes ggplot2 drop_na() drop rows containing missing values tidyr select() keep or drop columns by name and type dplyr mutate() create, modify, delete columns dplyr table() build a contingency table  base R rownames(), colnames() retrieve or set row or column names of a matrix-like object. base R clean_names() clean names of object (usually data.frame) janitor scale() scaling and centering matrix-like objects base R pivot_longer() pivot data from wide to long tidyr PCA() computation of weighted or unweighted principal component analysis of a matrix of interval-scale data easyCODA summary() produce result summaries base R lm() fit linear models stats cor() compute correlation matrix stats t() matrix transpose base round() round numbers base "],["demonstration-1-k-means-clustering-in-r.html", "Demonstration 1: K-means Clustering in R Loading the necessary packages Profile the Palmer Penguins dataset Plot Preprocess the data for clustering Perform k-means clustering Visualise the clusters  TASKS", " Demonstration 1: K-means Clustering in R In this practical, you will learn how to implement k-means clustering using the Palmer Penguins dataset. You can read more about this data here: Palmer Penguins Dataset The task in this practical is to cluster penguins based on their bill length and flipper length. We will then assess whether the predicted clusters map onto the actual species of penguin in the data. You can view this as an exercise in whether bill length and flipper length is sufficient to differentiate between species of penguin. (Artwork by @allison_horst) You should modify and experiment with the below code. There is also a brief exercise for you complete at the end of the walk-through. You will be required to install two new packages first: palmerpenguins and factoextra. Loading the necessary packages library(palmerpenguins) library(tidyverse) library(factoextra) Profile the Palmer Penguins dataset head(penguins) ## # A tibble: 6 × 8 ## species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g ## &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 Adelie Torgersen 39.1 18.7 181 3750 ## 2 Adelie Torgersen 39.5 17.4 186 3800 ## 3 Adelie Torgersen 40.3 18 195 3250 ## 4 Adelie Torgersen NA NA NA NA ## 5 Adelie Torgersen 36.7 19.3 193 3450 ## 6 Adelie Torgersen 39.3 20.6 190 3650 ## # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt; glimpse(penguins) ## Rows: 344 ## Columns: 8 ## $ species &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel… ## $ island &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse… ## $ bill_length_mm &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, … ## $ bill_depth_mm &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, … ## $ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186… ## $ body_mass_g &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, … ## $ sex &lt;fct&gt; male, female, female, NA, female, male, female, male… ## $ year &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007… Plot penguins |&gt; ggplot(aes(bill_length_mm, flipper_length_mm, color = species)) + geom_point() ## Warning: Removed 2 rows containing missing values or values outside the scale range ## (`geom_point()`). Preprocess the data for clustering pengs &lt;- penguins |&gt; drop_na() Perform k-means clustering k2 &lt;- pengs |&gt; select(bill_length_mm, flipper_length_mm) |&gt; kmeans(centers = 3, nstart = 25) Let’s add the predicted cluster to the pengs data frame: pengs &lt;- pengs |&gt; mutate(cluster = k2$cluster) Visualise the clusters fviz_cluster( k2, data = pengs |&gt; select(bill_length_mm, flipper_length_mm) ) pengs |&gt; ggplot(aes(x = bill_length_mm, y = flipper_length_mm, color = factor(cluster), shape = species)) + geom_point() + scale_color_discrete(&quot;cluster&quot;) To what extent do the clusters overlap with species? pred &lt;- table(pengs$species, k2$cluster) pred ## ## 1 2 3 ## Adelie 38 2 106 ## Chinstrap 54 5 9 ## Gentoo 2 117 0 overlap &lt;- sum(diag(pred)) differs &lt;- sum(pred[upper.tri(pred)], pred[lower.tri(pred)]) # proportion of penguins &quot;correctly&quot; classified according to species: overlap / sum(overlap, differs) ## [1] 0.1291291 How can we visualise the cluster-species overlap? pengs |&gt; mutate( overlap = case_when( species == &quot;Adelie&quot; &amp; cluster == 1 ~ &quot;overlap&quot;, species == &quot;Chinstrap&quot; &amp; cluster == 2 ~ &quot;overlap&quot;, species == &quot;Gentoo&quot; &amp; cluster == 3 ~ &quot;overlap&quot;, TRUE ~ &quot;differs&quot; ) ) |&gt; ggplot(aes(x = bill_length_mm, y = flipper_length_mm, color = overlap, shape = species)) + geom_point()  TASKS TASK 1: How well did k-means clustering perform? Suppose you randomly picked Adelie, Chinstrap, or Gentoo for each penguin. What proportion of penguins would you “correctly” classify? Your code here Consider how this compares to the performance of the k-means clustering algorithm. TASK 2: How many clusters should we use? In the above example, we used three clusters because there are three species of penguin in our data. However, in practice, we may not know this “ground-truth” information. That is, we may not know how many species are of penguin are represented in the data. Additionally, we may wish to cluster the data based on other criteria, such as minimizing the intra-cluster variation. Recall that unsupervised learning is useful for detecting patterns in the data. One method we can use to determine the optimal number of clusters is the elbow method. This method involves plotting the within-cluster sum of squares (WSS; also known as within-cluster variation or intra-cluster variation) for a range values of k (recall that k is the number of clusters). We then look for the location of the “bend” in the in the plot, i.e., the elbow. Below, we use the elbow method to determine the optimal number of clusters of penguins using the body_mass_g and bill_length_mm features. # preprocess data pengs_mass_length &lt;- pengs |&gt; select(body_mass_g, bill_length_mm) # compute total within-cluster sum of square wss &lt;- function(k) { kmeans( pengs_mass_length, centers = k, nstart = 10 )$tot.withinss } # Compute and plot WSS for k = 1 to k = 15 k_values &lt;- 1:10 # plot the WSS values against k wss_values &lt;- map_dbl(k_values, wss) tibble( k_values, wss_values ) |&gt; ggplot(aes(k_values, wss_values)) + geom_line() + geom_point() + scale_x_continuous(&quot;Number of clusters, k&quot;, breaks = unique(k_values)) + scale_y_continuous(&quot;Total within-clusters sum of squares&quot;) The results here suggest that the optimal number of clusters is 3 (which neatly aligns with the number of penguin species in the data). Although, 4 also looks like a good choice. The elbow method is useful, but it isn’t always clear where the elbow lies, which often simply reflects the reality of the data. The elbow method is implemented in fviz_nbclust() function from the factoextra package: fviz_nbclust(pengs_mass_length, kmeans, method = &quot;wss&quot;) TASK 3: Clustering cars Cluster the mtcars data using kmeans(). head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Use the mpg and hp features to group the cars into three clusters. Your code here How well do your predicted clusters map onto the cyl feature in the mtcars data? (Note that the cyl feature (cylinders) has three values: 4, 6, 8). Your code here Next, find the optimal number of clusters in the data using the elbow method. You should start by using the mpg and hp features, although feel free to experiment with using other features. Your code here "],["demonstration-2-principal-component-analysis-in-r.html", "Demonstration 2: Principal Component Analysis in R Load packages Preprocess the world happiness report data Profile the happiness data Plot happiness score against feature Run PCA using five features Correlations of happiness score with PC1 and PC2 Plot first two principal components Regress happiness on the five indicators Visualise happiness against principal components Regress happiness on first principal component  TASK", " Demonstration 2: Principal Component Analysis in R For the tasks below, you will require the World Happiness Report (2021) dataset. Click here to download the file: world_happiness_report_2021.csv . Remember to place your data file in a separate subfolder within your R project working directory. Prior to beginning the practical, you will be required install two packages easyCODA and janitor. Load packages library(easyCODA) library(dplyr) library(tidyr) library(readr) library(janitor) library(ggplot2) Preprocess the world happiness report data happy &lt;- read_csv( file = &quot;data/world_happiness_report_2021.csv&quot;, show_col_types = FALSE ) |&gt; clean_names() happy_standardized &lt;- happy |&gt; select( social = social_support, life = healthy_life_expectancy, choices = freedom_to_make_life_choices, generosity, corruption = perceptions_of_corruption ) |&gt; scale() rownames(happy_standardized) &lt;- happy$country_name Profile the happiness data head(happy) ## # A tibble: 6 × 20 ## country_name regional_indicator ladder_score standard_error_of_ladder_score ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Finland Western Europe 7.84 0.032 ## 2 Denmark Western Europe 7.62 0.035 ## 3 Switzerland Western Europe 7.57 0.036 ## 4 Iceland Western Europe 7.55 0.059 ## 5 Netherlands Western Europe 7.46 0.027 ## 6 Norway Western Europe 7.39 0.035 ## # ℹ 16 more variables: upperwhisker &lt;dbl&gt;, lowerwhisker &lt;dbl&gt;, ## # logged_gdp_per_capita &lt;dbl&gt;, social_support &lt;dbl&gt;, ## # healthy_life_expectancy &lt;dbl&gt;, freedom_to_make_life_choices &lt;dbl&gt;, ## # generosity &lt;dbl&gt;, perceptions_of_corruption &lt;dbl&gt;, ## # ladder_score_in_dystopia &lt;dbl&gt;, explained_by_log_gdp_per_capita &lt;dbl&gt;, ## # explained_by_social_support &lt;dbl&gt;, ## # explained_by_healthy_life_expectancy &lt;dbl&gt;, … Plot happiness score against feature happy |&gt; select( ladder = ladder_score, social = social_support, life = healthy_life_expectancy, choices = freedom_to_make_life_choices, generosity, corruption = perceptions_of_corruption ) |&gt; pivot_longer(-ladder, names_to = &quot;feature&quot;, values_to = &quot;value&quot;) |&gt; ggplot(aes(value, ladder)) + geom_point(alpha = 0.4) + scale_x_continuous(&quot;Feature value&quot;) + scale_y_continuous(&quot;Cantrill Ladder happiness score&quot;) + facet_wrap(~ feature, scales = &quot;free_x&quot;) Run PCA using five features happy_pca &lt;- PCA(happy_standardized, weight = FALSE) summary(happy_pca) ## ## Principal inertias (eigenvalues): ## ## dim value % cum% scree plot ## 1 0.466493 47.0 47.0 ************ ## 2 0.243482 24.5 71.5 ****** ## 3 0.139560 14.1 85.5 **** ## 4 0.095018 9.6 95.1 ** ## 5 0.048735 4.9 100.0 * ## -------- ----- ## Total: 0.993289 100.0 ## ## ## Rows: ## name mass qlt inr k=1 cor ctr k=2 cor ctr ## 1 | Fnln | 7 707 19 | -1378 683 27 | -257 24 2 | ## 2 | Dnmr | 7 808 19 | -1403 699 28 | -553 109 8 | ## 3 | Swtz | 7 860 14 | -1281 789 24 | -384 71 4 | ## 4 | Icln | 7 817 10 | -1021 738 15 | -334 79 3 | ## 5 | Nthr | 7 929 13 | -1149 667 19 | -719 261 14 | ## 6 | Nrwy | 7 918 17 | -1366 763 27 | -615 155 10 | ## 7 | Swdn | 7 871 16 | -1303 699 24 | -647 172 12 | ## 8 | Lxmb | 7 834 9 | -1030 803 15 | -201 30 1 | ## 9 | NwZl | 7 890 17 | -1322 689 25 | -713 201 14 | ## 10 | Austr | 7 973 8 | -1022 919 15 | -247 54 2 | ## 11 | Astrl | 7 958 11 | -1104 765 18 | -554 193 8 | ## 12 | Isrl | 7 615 4 | -569 563 5 | 173 52 1 | ## 13 | Grmn | 7 888 6 | -882 842 11 | -206 46 1 | ## 14 | Cand | 7 951 10 | -1099 824 17 | -433 128 5 | ## 15 | Irln | 7 867 10 | -1070 750 16 | -422 116 5 | ## 16 | CstR | 7 695 5 | -602 495 5 | 383 200 4 | ## 17 | UntK | 7 856 10 | -914 548 12 | -685 308 13 | ## 18 | CzcR | 7 885 6 | -493 261 3 | 763 624 16 | ## 19 | UntS | 7 665 2 | -456 572 3 | -184 93 1 | ## 20 | Blgm | 7 864 4 | -522 483 4 | 463 381 6 | ## 21 | Frnc | 7 890 6 | -812 717 9 | 399 173 4 | ## 22 | Bhrn | 7 717 3 | -533 573 4 | -267 144 2 | ## 23 | Malt | 7 873 6 | -847 759 10 | -328 114 3 | ## 24 | TwPC | 7 883 2 | -338 506 2 | 292 377 2 | ## 25 | UnAE | 7 919 4 | -555 578 4 | -427 342 5 | ## 26 | SdAr | 7 707 3 | -431 485 3 | 291 222 2 | ## 27 | Span | 7 762 5 | -531 418 4 | 482 345 6 | ## 28 | Itly | 7 552 5 | -140 27 0 | 613 525 10 | ## 29 | Slvn | 7 747 6 | -754 606 8 | 364 142 4 | ## 30 | Gtml | 7 139 2 | -175 138 0 | -16 1 0 | ## 31 | Urgy | 7 893 4 | -721 870 7 | 117 23 0 | ## 32 | Sngp | 7 710 25 | -1535 644 34 | -491 66 7 | ## 33 | Kosv | 7 307 7 | 62 4 0 | -544 303 8 | ## 34 | Slvk | 7 762 4 | -187 56 1 | 664 706 12 | ## 35 | Brzl | 7 864 1 | -199 344 1 | 244 520 2 | ## 36 | Mexc | 7 646 2 | -240 179 1 | 388 467 4 | ## 37 | Jamc | 7 534 4 | -270 140 1 | 453 394 6 | ## 38 | Lthn | 7 941 4 | -233 85 1 | 736 855 15 | ## 39 | Cypr | 7 214 3 | -138 42 0 | 277 171 2 | ## 40 | Estn | 7 824 6 | -829 815 10 | 85 8 0 | ## 41 | Panm | 7 777 4 | -382 243 2 | 566 534 9 | ## 42 | Uzbk | 7 918 13 | -775 320 9 | -1059 598 31 | ## 43 | Chil | 7 542 2 | -133 61 0 | 373 481 4 | ## 44 | Plnd | 7 959 3 | -442 445 3 | 475 514 6 | ## 45 | Kzkh | 7 584 3 | -411 457 2 | 217 127 1 | ## 46 | Romn | 7 684 5 | -32 1 0 | 699 682 13 | ## 47 | Kuwt | 7 615 1 | -272 395 1 | 203 220 1 | ## 48 | Serb | 7 304 1 | -127 87 0 | 201 217 1 | ## 49 | ElSl | 7 112 2 | -170 102 0 | 55 11 0 | ## 50 | Mrts | 7 601 2 | -343 450 2 | 199 151 1 | ## 51 | Latv | 7 823 4 | -96 18 0 | 650 806 12 | ## 52 | Clmb | 7 733 2 | -162 90 0 | 435 643 5 | ## 53 | Hngr | 7 893 5 | -170 41 0 | 775 852 17 | ## 54 | Thln | 7 283 8 | -262 56 1 | -526 227 8 | ## 55 | Ncrg | 7 982 1 | -350 909 2 | -99 73 0 | ## 56 | Japn | 7 820 7 | -618 351 5 | 715 469 14 | ## 57 | Argn | 7 916 3 | -296 171 1 | 618 745 11 | ## 58 | Prtg | 7 772 7 | -467 199 3 | 791 573 17 | ## 59 | Hndr | 7 227 1 | -130 79 0 | -177 148 1 | ## 60 | Crot | 7 726 5 | -171 41 0 | 703 685 14 | ## 61 | Phlp | 7 75 2 | -154 69 0 | 49 7 0 | ## 62 | SthK | 7 279 4 | -65 7 0 | 409 272 5 | ## 63 | Peru | 7 722 3 | -64 10 0 | 535 711 8 | ## 64 | BsnH | 7 44 4 | 132 29 0 | 97 16 0 | ## 65 | Moldv | 7 446 2 | 6 0 0 | 369 446 4 | ## 66 | Ecdr | 7 600 2 | -146 73 0 | 392 527 4 | ## 67 | Kyrg | 7 137 5 | -250 81 1 | -209 56 1 | ## 68 | Grec | 7 741 11 | 199 24 1 | 1088 717 33 | ## 69 | Bolv | 7 104 2 | -8 0 0 | 153 104 1 | ## 70 | Mngl | 7 28 4 | 129 27 0 | -11 0 0 | ## 71 | Prgy | 7 137 3 | -216 125 1 | 65 11 0 | ## 72 | Mntn | 7 419 2 | 16 1 0 | 320 418 3 | ## 73 | DmnR | 7 614 2 | -270 326 1 | 254 288 2 | ## 74 | NrtC | 7 476 3 | -445 474 3 | -23 1 0 | ## 75 | Blrs | 7 407 5 | -66 6 0 | 552 401 8 | ## 76 | Russ | 7 717 2 | 81 18 0 | 508 699 7 | ## 77 | HKSA | 7 333 10 | -645 293 6 | -237 39 2 | ## 78 | Tjks | 7 404 2 | -314 375 1 | -86 28 0 | ## 79 | Vtnm | 7 432 3 | -415 350 2 | 201 82 1 | ## 80 | Liby | 7 120 1 | 57 29 0 | 99 91 0 | ## 81 | Mlys | 7 263 3 | -179 73 0 | -288 190 2 | ## 82 | Indn | 7 576 20 | 90 3 0 | -1310 574 47 | ## 83 | CngB | 7 812 6 | 832 809 10 | -53 3 0 | ## 84 | Chin | 7 485 3 | -360 291 2 | 294 194 2 | ## 85 | IvrC | 7 840 10 | 1074 781 17 | -295 59 2 | ## 86 | Armn | 7 334 2 | -205 137 1 | 245 197 2 | ## 87 | Nepl | 7 774 2 | 134 66 0 | -441 708 5 | ## 88 | Blgr | 7 597 4 | -118 26 0 | 556 572 9 | ## 89 | Mldvs | 7 516 3 | -444 473 3 | 135 43 1 | ## 90 | Azrb | 7 262 5 | -327 152 2 | 278 110 2 | ## 91 | Cmrn | 7 905 6 | 881 858 11 | -206 47 1 | ## 92 | Sngl | 7 964 3 | 672 958 6 | 54 6 0 | ## 93 | Albn | 7 216 3 | 275 161 1 | 160 55 1 | ## 94 | NrtM | 7 289 2 | 252 254 1 | 93 35 0 | ## 95 | Ghan | 7 781 4 | 546 481 4 | -431 300 5 | ## 96 | Niger | 7 738 7 | 721 506 7 | -488 232 7 | ## 97 | Trkm | 7 202 10 | -268 49 1 | -474 153 6 | ## 98 | Gamb | 7 869 17 | 837 280 10 | -1213 589 41 | ## 99 | Benn | 7 653 14 | 1070 540 16 | -490 113 7 | ## 100 | Laos | 7 738 5 | 109 17 0 | -705 721 14 | ## 101 | Bngl | 7 120 2 | 51 7 0 | -201 113 1 | ## 102 | Guin | 7 991 8 | 964 801 13 | -468 189 6 | ## 103 | SthA | 7 460 3 | 415 361 2 | 218 99 1 | ## 104 | Trky | 7 563 6 | 371 154 2 | 605 409 10 | ## 105 | Pkst | 7 965 5 | 764 748 8 | -412 217 5 | ## 106 | Mrcc | 7 334 10 | 590 239 5 | 372 95 4 | ## 107 | Vnzl | 7 746 5 | 279 97 1 | 721 649 14 | ## 108 | Gerg | 7 211 5 | 272 94 1 | 303 117 3 | ## 109 | Algr | 7 394 10 | 624 252 6 | 468 141 6 | ## 110 | Ukrn | 7 355 3 | 170 73 0 | 333 282 3 | ## 111 | Iraq | 7 921 5 | 766 824 8 | 262 96 2 | ## 112 | Gabn | 7 851 4 | 492 424 3 | 493 427 7 | ## 113 | BrkF | 7 906 7 | 912 857 12 | -217 49 1 | ## 114 | Cmbd | 7 117 4 | -3 0 0 | -269 117 2 | ## 115 | Mzmb | 7 557 5 | 313 134 1 | -555 423 8 | ## 116 | Nigeri | 7 773 9 | 962 731 13 | -232 42 1 | ## 117 | Mali | 7 855 7 | 955 852 13 | -56 3 0 | ## 118 | Iran | 7 417 8 | 522 231 4 | -468 185 6 | ## 119 | Ugnd | 7 755 5 | 687 641 7 | -289 114 2 | ## 120 | Libr | 7 942 4 | 743 874 8 | -207 68 1 | ## 121 | Keny | 7 795 8 | 547 251 4 | -805 544 18 | ## 122 | Tuns | 7 752 7 | 577 343 5 | 630 409 11 | ## 123 | Lbnn | 7 520 9 | 528 205 4 | 654 315 12 | ## 124 | Namb | 7 728 4 | 553 491 4 | 385 238 4 | ## 125 | PlsT | 7 859 4 | 438 330 3 | 554 529 8 | ## 126 | Mynm | 7 857 18 | 64 1 0 | -1526 855 64 | ## 127 | Jrdn | 7 476 2 | 74 20 0 | 357 457 4 | ## 128 | Chad | 7 943 17 | 1519 907 33 | -304 36 3 | ## 129 | SrLn | 7 53 2 | -79 24 0 | -86 29 0 | ## 130 | Swzl | 7 569 10 | 884 525 11 | 256 44 2 | ## 131 | Cmrs | 7 839 12 | 1211 806 21 | -247 33 2 | ## 132 | Egyp | 7 762 3 | 393 381 2 | 392 380 4 | ## 133 | Ethp | 7 923 2 | 440 624 3 | -305 299 3 | ## 134 | Mrtn | 7 610 8 | 789 529 9 | 307 80 3 | ## 135 | Mdgs | 7 797 9 | 1025 793 15 | 71 4 0 | ## 136 | Togo | 7 924 13 | 1272 873 23 | -308 51 3 | ## 137 | Zamb | 7 872 4 | 676 702 7 | -332 169 3 | ## 138 | SrrL | 7 965 11 | 1163 852 19 | -424 113 5 | ## 139 | Indi | 7 538 7 | 452 198 3 | -592 340 10 | ## 140 | Brnd | 7 724 18 | 1323 647 25 | -456 77 6 | ## 141 | Yemn | 7 693 7 | 697 476 7 | 471 217 6 | ## 142 | Tnzn | 7 950 6 | 275 82 1 | -894 868 22 | ## 143 | Hait | 7 876 26 | 1312 450 25 | -1275 426 45 | ## 144 | Malw | 7 721 10 | 869 535 11 | -514 187 7 | ## 145 | Lsth | 7 654 11 | 992 615 14 | 253 40 2 | ## 146 | Btsw | 7 436 5 | 281 116 1 | 466 320 6 | ## 147 | Rwnd | 7 430 22 | -55 1 0 | -1187 429 39 | ## 148 | Zmbb | 7 910 5 | 777 901 9 | 79 9 0 | ## 149 | Afgh | 7 890 37 | 2193 878 69 | 257 12 2 | ## ## Columns: ## name mass qlt inr k=1 cor ctr k=2 cor ctr ## 1 | socl | 200 767 200 | -822 680 290 | 294 87 71 | ## 2 | life | 200 816 200 | -860 744 317 | 268 72 59 | ## 3 | chcs | 200 664 200 | -761 583 248 | -284 81 66 | ## 4 | gnrs | 200 782 200 | 7 0 0 | -881 782 638 | ## 5 | crrp | 200 544 200 | 582 341 145 | 449 203 166 | What is the percentage of variance explained by each principal component? happy_pca$sv ^ 2 / sum(happy_pca$sv ^ 2) ## [1] 0.46964540 0.24512744 0.14050280 0.09566051 0.04906384 What is the cumulative variance explained for each variable? vexp &lt;- function(feature, df, pca) { reg_pc1 &lt;- lm(df[, feature] ~ pca$rowpcoord[, 1]) reg_pc1_pc2 &lt;- lm(df[, feature] ~ pca$rowpcoord[, 1] + pca$rowpcoord[, 2]) explained_pc1 &lt;- cor(predict(reg_pc1), df[, feature]) ^ 2 explained_pc1_pc2 &lt;- cor(predict(reg_pc1_pc2), df[, feature]) ^ 2 c(explained_pc1, explained_pc1_pc2) } var_explained &lt;- sapply(1:5, \\(x) vexp(x, happy_standardized, happy_pca)) colnames(var_explained) &lt;- colnames(happy_standardized) rownames(var_explained) &lt;- c(&quot;PC1&quot;, &quot;PC2&quot;) t(round(var_explained, 3)) ## PC1 PC2 ## social 0.680 0.767 ## life 0.744 0.816 ## choices 0.583 0.664 ## generosity 0.000 0.782 ## corruption 0.341 0.544 round(colMeans(t(var_explained)), 3) ## PC1 PC2 ## 0.470 0.715 How do we build a correlation matrix of all variables? round(cor(cbind(&quot;ladder&quot; = happy$ladder_score, happy_standardized)), 3) ## ladder social life choices generosity corruption ## ladder 1.000 0.757 0.768 0.608 -0.018 -0.421 ## social 0.757 1.000 0.723 0.483 -0.115 -0.203 ## life 0.768 0.723 1.000 0.461 -0.162 -0.364 ## choices 0.608 0.483 0.461 1.000 0.169 -0.401 ## generosity -0.018 -0.115 -0.162 0.169 1.000 -0.164 ## corruption -0.421 -0.203 -0.364 -0.401 -0.164 1.000 Correlations of happiness score with PC1 and PC2 row_principal &lt;- -happy_pca$rowpcoord[, 1:2] colnames(row_principal) &lt;- c(&quot;PC1&quot;, &quot;PC2&quot;) round(cor(cbind(&quot;ladder&quot; = happy$ladder_score, row_principal)), 3) ## ladder PC1 PC2 ## ladder 1.000 0.85 -0.067 ## PC1 0.850 1.00 0.000 ## PC2 -0.067 0.00 1.000 Plot first two principal components row_principal |&gt; as_tibble() |&gt; mutate( region = happy$regional_indicator, country = happy$country_name ) |&gt; ggplot(aes(PC1, -PC2, color = region, label = country)) + geom_hline(yintercept = 0, color = &quot;gray70&quot;, linetype = &quot;dashed&quot;) + geom_vline(xintercept = 0, color = &quot;gray70&quot;, linetype = &quot;dashed&quot;) + geom_text(size = 3, show.legend = FALSE) + coord_equal() Regress happiness on the five indicators happy_standardized_pc &lt;- happy_standardized |&gt; as_tibble() |&gt; mutate( ladder = happy$ladder_score, pc1 = happy_pca$rowpcoord[, 1], pc2 = happy_pca$rowpcoord[, 2], country = happy$country_name, region = happy$regional_indicator ) happiness_full &lt;- lm(ladder ~ social + life + choices + generosity + corruption, data = happy_standardized_pc) summary(happiness_full) ## ## Call: ## lm(formula = ladder ~ social + life + choices + generosity + ## corruption, data = happy_standardized_pc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.63771 -0.26591 0.02327 0.37120 1.37329 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.53284 0.04581 120.776 &lt; 2e-16 *** ## social 0.40177 0.06975 5.760 4.96e-08 *** ## life 0.38823 0.07217 5.380 2.98e-07 *** ## choices 0.21977 0.05783 3.801 0.000213 *** ## generosity 0.03033 0.04933 0.615 0.539620 ## corruption -0.13599 0.05301 -2.565 0.011343 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5592 on 143 degrees of freedom ## Multiple R-squared: 0.738, Adjusted R-squared: 0.7289 ## F-statistic: 80.57 on 5 and 143 DF, p-value: &lt; 2.2e-16 What is the correlation between observed y and fitted y? sqrt(summary(happiness_full)$r.squared) ## [1] 0.8590876 Visualise happiness against principal components happy_standardized_pc |&gt; pivot_longer( cols = c(pc1, pc2), names_to = &quot;PC&quot;, values_to = &quot;value&quot; ) |&gt; ggplot(aes(value, ladder)) + geom_point(aes(color = region)) + geom_smooth( method = &quot;lm&quot;, formula = y ~ x, linewidth = 0.5, color = &quot;grey20&quot;, se = FALSE ) + scale_x_continuous(&quot;PC Value&quot;) + scale_y_continuous(&quot;Cantrill Ladder Happiness Score&quot;) + facet_wrap(~ PC) + theme(text = element_text(size = 12)) Regress happiness on first principal component happiness_reduced &lt;- lm(formula = ladder ~ pc1, data = happy_standardized_pc) summary(happiness_reduced) ## ## Call: ## lm(formula = ladder ~ pc1, data = happy_standardized_pc) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.19102 -0.26848 0.06941 0.40025 1.20478 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 5.53284 0.04645 119.11 &lt;2e-16 *** ## pc1 -1.33258 0.06801 -19.59 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.567 on 147 degrees of freedom ## Multiple R-squared: 0.7231, Adjusted R-squared: 0.7212 ## F-statistic: 383.9 on 1 and 147 DF, p-value: &lt; 2.2e-16 sqrt(summary(happiness_reduced)$r.squared) ## [1] 0.8503647  TASK Now let’s consider PCA using six features. In the example of PCA above, we did not use logged_gdp_per_capita feature for the PCA analysis. Run PCA using the original five features (ladder, social, life, choices, corruption) and logged_gdp_per_capita. You may need to do some preprocessing first. Your code here Now regress happiness on the first principal component from your analysis. Your code here How does this compare to the previous analysis with five features? Consider here the R^2 value and the correlation between the observed and fitted values. Your code here "],["overview-7.html", "Overview", " Overview Section 8: Case Study and Formative Assessment This section is comprised of a demonstration on utilising ChatGPT for data science projects developed by Dr. Tatjana Kecojevic, Lecturer in Social Statistics. Leveraging ChatGPT for Enhanced Data Science Projects Engaging with ChatGPT during data science projects in R offers a versatile and innovative approach to problem-solving and project management. As an AI-powered assistant, ChatGPT efficiently addresses complex challenges at various stages of the project lifecycle, serving as a reliable resource for guidance on best practices, educational insights, and time-saving solutions. By providing instant responses and suggestions, ChatGPT streamlines data preprocessing, model selection, and result interpretation, enabling users to focus more on analysis and decision-making. ChatGPT’s comprehensive support spans data visualization, statistical analysis, machine learning model building, and beyond, making it an invaluable asset for both novice and experienced data scientists. Its innovative approach encourages experimentation and exploration, fostering creativity and pushing the boundaries of traditional data analysis methodologies. Integrating ChatGPT into data science projects in R not only enhances efficiency but also promotes continuous learning and innovation in the field. In this tutorial, we will learn how to utilise ChatGPT for an end-to-end data science project. We will use various prompts to create a project outline, write R code, conduct research, and debug applications. Additionally, we will provide tips on crafting effective ChatGPT prompts. The focus of this tutorial is a single AI project suitable for intermediate practitioners. This project builds on foundational skills and is designed to be both challenging and engaging, offering a practical way to enhance your abilities. You will learn how to handle datasets, build and train models, and evaluate their performance, providing a comprehensive understanding of the AI project lifecycle. For this, we will use the Spanish Wine Quality Dataset available from kaggle. Learning Outcomes: Utilise ChatGPT to guide data preprocessing and selection of machine learning algorithms and techniques; Interpret complex models using ChatGPT; Summarise findings, create code, and generate reports using ChatGPT; Write effective prompts. "],["demonstration.html", "Demonstration Data Set Project Planning: Setting the Stage for Success Data Preprocessing Exploratory Data Analysis (EDA) Model Selection and Model Training Model Evaluation Model Interpretation Report Conclusion Key Insights", " Demonstration Enhancing Data Science Projects with ChatGPT: A Case Study in R. Data Set The Spanish Wine Quality Dataset focuses on red wine variants from Spain, detailing various metrics related to their popularity and characteristics, and their influence on quality. It is suitable for classification or regression tasks, with the goal being to predict the quality or price of the wines. Note that the quality ratings range from nearly 5 to 4 points and are not evenly distributed. The dataset comprises 7,500 different types of Spanish red wines, each described by 11 features including price, rating, and flavor descriptors. The data was meticulously gathered through web scraping from diverse sources, such as specialised wine websites and supermarkets. Attribute Information winery: Name of the winery wine: Name of the wine year: Harvest year of the grapes rating: Average user rating (1-5) num_reviews: Number of user reviews country: Country of origin (Spain) region: Wine-producing region price: Price in euros (€) type: Wine variety body: Body score, indicating the richness and weight of the wine (1-5) acidity: Acidity score, indicating the wine’s tartness and refreshing quality (1-5) To start our AI-aided data science project, we’ll need to download the dataset named wines_SPA.csv. Save this dataset in a “data” folder within a new project directory called wines_SPA. Before we begin, let’s take an initial look at the dataset. Open the downloaded csv file to ensure it’s functional and contains the data we need. After that, we’ll set up an R project file and dive into our data science project. Exciting times ahead! Project Planning: Setting the Stage for Success Project planning is a critical phase where we assess available resources and goals to develop an optimal strategy. To initiate this process, visit chatGPT and start a new chat. Mention the availability of the Spanish Wine Quality dataset and ask ChatGPT to provide steps for building an end-to-end generic portfolio project. This step lays the groundwork for a successful project by defining clear objectives and identifying the necessary resources. Prompt: I have The Spanish Wine Quality Dataset, which centers on red wine variations from Spain. It provides detailed metrics regarding their popularity, characteristics, and how these factors influence quality. This dataset is ideal for classification or regression tasks, aiming to predict the quality or price of wines. It’s worth noting that the quality ratings range from nearly 5 to 4 points and are not evenly distributed. The dataset includes 7,500 different types of Spanish red wines, each described by 11 features such as price, rating, and flavor descriptors. Could you guide me through the necessary steps for this data science project using this dataset ChatGPT To be in line with our adopted practice, we will summarise ChatGPT’s suggestions into the following set of steps: Data Preprocessing + Feature Engineering Exploratory Data Analysis (EDA) + Feature Engineering if still needed Model Selection and Model Training Model Evaluation Model Interpretation Conclusion Data Preprocessing As we use ChatGPT to assist us in this project, some steps will be performed without its aid. These include loading the necessary packages, uploading data, and converting some character-type variables into appropriate factor types. ❗️ Please ensure that you have installed all necessary packages before proceeding with these steps. &gt; # Load packages &gt; # Note that you will need to install some of these packages &gt; &gt; library(ggplot2) # For creating visualisations. &gt; library(dplyr) # For wrangling data. &gt; library(tidyr) # For data tidying operations. &gt; library(reshape2) # For reshaping data frames. &gt; library(caret) # For data preprocessing tasks like scaling and imputing missing values and ML. &gt; library(glmnet) # For fitting Lasso and Ridge regression models. &gt; library(randomForest) # For fitting Random Forest models. &gt; library(rpart) # For fitting and visualising decision trees. &gt; library(rpart.plot) # For visualising decision trees created with the `rpart` package. &gt; library(splines) # For creating spline basis functions for use in generalised linear models (GLMs) &gt; &gt; #read data &gt; wine_data &lt;- read.csv(&#39;data/wines_SPA.csv&#39;, + header=TRUE, + na.strings=c(&quot;&quot;,&quot;NA&quot;), + stringsAsFactor = FALSE) &gt; glimpse(wine_data) ## Rows: 7,500 ## Columns: 11 ## $ winery &lt;chr&gt; &quot;Teso La Monja&quot;, &quot;Artadi&quot;, &quot;Vega Sicilia&quot;, &quot;Vega Sicilia&quot;,… ## $ wine &lt;chr&gt; &quot;Tinto&quot;, &quot;Vina El Pison&quot;, &quot;Unico&quot;, &quot;Unico&quot;, &quot;Unico&quot;, &quot;Unic… ## $ year &lt;chr&gt; &quot;2013&quot;, &quot;2018&quot;, &quot;2009&quot;, &quot;1999&quot;, &quot;1996&quot;, &quot;1998&quot;, &quot;2010&quot;, &quot;1… ## $ rating &lt;dbl&gt; 4.9, 4.9, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8… ## $ num_reviews &lt;int&gt; 58, 31, 1793, 1705, 1309, 1209, 1201, 926, 643, 630, 591, … ## $ country &lt;chr&gt; &quot;Espana&quot;, &quot;Espana&quot;, &quot;Espana&quot;, &quot;Espana&quot;, &quot;Espana&quot;, &quot;Espana&quot;… ## $ region &lt;chr&gt; &quot;Toro&quot;, &quot;Vino de Espana&quot;, &quot;Ribera del Duero&quot;, &quot;Ribera del … ## $ price &lt;dbl&gt; 995.0000, 313.5000, 324.9500, 692.9600, 778.0600, 490.0000… ## $ type &lt;chr&gt; &quot;Toro Red&quot;, &quot;Tempranillo&quot;, &quot;Ribera Del Duero Red&quot;, &quot;Ribera… ## $ body &lt;int&gt; 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5… ## $ acidity &lt;int&gt; 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3… Next, we will convert character types into factors and change the year variable to an integer type. We will also remove the country variable since all the wine is from Spain. &gt; wine_data [,1] &lt;- as.factor(wine_data[,1]) &gt; wine_data [,2] &lt;- as.factor(wine_data[,2]) &gt; wine_data [,3] &lt;- as.integer(wine_data[,3]) &gt; wine_data [,7] &lt;- as.factor(wine_data[,7]) &gt; wine_data [,9] &lt;- as.factor(wine_data[,9]) &gt; wine_data [,10] &lt;- as.factor(wine_data[,10]) &gt; wine_data [,11] &lt;- as.factor(wine_data[,11]) &gt; wine_data &lt;- wine_data[, !(names(wine_data) %in% c(&quot;country&quot;))] &gt; summary(wine_data) ## winery wine year rating ## Contino : 457 Reserva : 467 Min. :1910 Min. :4.200 ## Artadi : 261 Gran Reserva : 458 1st Qu.:2011 1st Qu.:4.200 ## La Rioja Alta : 254 Rioja Reserva: 240 Median :2015 Median :4.200 ## Sierra Cantabria: 237 El Viejo : 224 Mean :2013 Mean :4.255 ## Matarromera : 232 Corimbo I : 223 3rd Qu.:2017 3rd Qu.:4.200 ## Vina Pedrosa : 230 Mirto : 223 Max. :2021 Max. :4.900 ## (Other) :5829 (Other) :5665 NA&#39;s :290 ## num_reviews region price ## Min. : 25.0 Rioja :2440 Min. : 4.99 ## 1st Qu.: 389.0 Ribera del Duero:1413 1st Qu.: 18.90 ## Median : 404.0 Priorato : 686 Median : 28.53 ## Mean : 451.1 Toro : 300 Mean : 60.10 ## 3rd Qu.: 415.0 Vino de Espana : 263 3rd Qu.: 51.35 ## Max. :32624.0 Rias Baixas : 252 Max. :3119.08 ## (Other) :2146 ## type body acidity ## Rioja Red :2357 2 : 34 1 : 35 ## Ribera Del Duero Red:1407 3 : 553 2 : 268 ## Red : 864 4 :4120 3 :6028 ## Priorat Red : 674 5 :1624 NA&#39;s:1169 ## Toro Red : 296 NA&#39;s:1169 ## (Other) :1357 ## NA&#39;s : 545 &gt; dim(wine_data) ## [1] 7500 10 We have observed the presence of missing values in our dataset. Given the substantial size of our dataset, we can opt to remove rows that contain NA values. &gt; # contains only the rows where there are no NAs in the specified columns. &gt; wine_data &lt;- wine_data[complete.cases(wine_data[, c(&quot;body&quot;, &quot;acidity&quot;, &quot;type&quot;)]), ] &gt; # Handle missing values in the &#39;year&#39; variable &gt; wine_data$year[is.na(wine_data$year)] &lt;- median(wine_data$year, na.rm = TRUE) &gt; # Check the number of missing values after removal &gt; missing_values &lt;- colSums(is.na(wine_data)) &gt; print(missing_values) ## winery wine year rating num_reviews region ## 0 0 0 0 0 0 ## price type body acidity ## 0 0 0 0 &gt; # Summary of the dataset &gt; summary(wine_data) ## winery wine year rating ## Contino : 414 Reserva : 422 Min. :1910 Min. :4.20 ## Artadi : 239 Gran Reserva : 415 1st Qu.:2011 1st Qu.:4.20 ## La Rioja Alta : 228 Rioja Reserva : 218 Median :2015 Median :4.20 ## Sierra Cantabria: 215 Corimbo I : 202 Mean :2013 Mean :4.26 ## Vina Pedrosa : 207 El Viejo : 202 3rd Qu.:2017 3rd Qu.:4.20 ## Imperial : 206 Rioja Graciano: 202 Max. :2021 Max. :4.90 ## (Other) :4822 (Other) :4670 ## num_reviews region price ## Min. : 25.0 Rioja :2221 Min. : 4.99 ## 1st Qu.: 388.0 Ribera del Duero :1281 1st Qu.: 19.98 ## Median : 402.0 Priorato : 622 Median : 29.15 ## Mean : 444.1 Toro : 264 Mean : 65.71 ## 3rd Qu.: 415.0 Vino de Espana : 240 3rd Qu.: 60.95 ## Max. :32624.0 Jerez-Xeres-Sherry: 225 Max. :3119.08 ## (Other) :1478 ## type body acidity ## Rioja Red :2143 2: 34 1: 35 ## Ribera Del Duero Red:1278 3: 553 2: 268 ## Red : 787 4:4120 3:6028 ## Priorat Red : 620 5:1624 ## Tempranillo : 268 ## Toro Red : 261 ## (Other) : 974 &gt; # Structure of the dataset &gt; glimpse(wine_data) ## Rows: 6,331 ## Columns: 10 ## $ winery &lt;fct&gt; Teso La Monja, Artadi, Vega Sicilia, Vega Sicilia, Vega Si… ## $ wine &lt;fct&gt; Tinto, Vina El Pison, Unico, Unico, Unico, Unico, Unico, U… ## $ year &lt;dbl&gt; 2013, 2018, 2009, 1999, 1996, 1998, 2010, 1995, 2015, 2011… ## $ rating &lt;dbl&gt; 4.9, 4.9, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8… ## $ num_reviews &lt;int&gt; 58, 31, 1793, 1705, 1309, 1209, 1201, 926, 643, 630, 591, … ## $ region &lt;fct&gt; Toro, Vino de Espana, Ribera del Duero, Ribera del Duero, … ## $ price &lt;dbl&gt; 995.0000, 313.5000, 324.9500, 692.9600, 778.0600, 490.0000… ## $ type &lt;fct&gt; Toro Red, Tempranillo, Ribera Del Duero Red, Ribera Del Du… ## $ body &lt;fct&gt; 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5… ## $ acidity &lt;fct&gt; 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3… As we conclude the data preprocessing step, we have transformed character types into factors, changed the year variable to an integer type, and removed the country variable, considering that all wines are from Spain. Additionally, we have addressed missing values by removing rows containing NA values. With these preparations completed, we are now ready to proceed to the next step: Exploratory Data Analysis (EDA), where we will delve deeper into the dataset to gain insights and prepare for the model selection process. Exploratory Data Analysis (EDA) Exploratory Data Analysis (EDA) is a crucial step in the data science process that involves exploring and understanding the structure, patterns, and relationships in a dataset. By visually and statistically analysing the data, data scientists can uncover insights, identify patterns, and detect anomalies that can inform subsequent analysis and modelling decisions. In this section, we will perform EDA on the wine dataset to gain a deeper understanding of its features and prepare for the modelling phase. Through visualisations and summary statistics, we aim to uncover meaningful patterns and relationships in the data, which will guide our further analysis and modelling strategies. Let’s begin by presenting the dataset’s size and complexity, starting with the number of rows and columns. Additionally, we’ll compute summary statistics for numerical variables to gain insights into their distribution and range. This analysis can help us identify potential outliers and better understand the overall structure of the data. &gt; # Display the number of rows and columns &gt; cat(&quot;Number of rows:&quot;, nrow(wine_data), &quot;\\n&quot;) ## Number of rows: 6331 &gt; cat(&quot;Number of columns:&quot;, ncol(wine_data), &quot;\\n\\n&quot;) ## Number of columns: 10 &gt; # Statistical summary for numerical variables &gt; num_vars &lt;- sapply(wine_data, is.numeric) &gt; num_data &lt;- wine_data[, num_vars] &gt; summary(num_data) ## year rating num_reviews price ## Min. :1910 Min. :4.20 Min. : 25.0 Min. : 4.99 ## 1st Qu.:2011 1st Qu.:4.20 1st Qu.: 388.0 1st Qu.: 19.98 ## Median :2015 Median :4.20 Median : 402.0 Median : 29.15 ## Mean :2013 Mean :4.26 Mean : 444.1 Mean : 65.71 ## 3rd Qu.:2017 3rd Qu.:4.20 3rd Qu.: 415.0 3rd Qu.: 60.95 ## Max. :2021 Max. :4.90 Max. :32624.0 Max. :3119.08 From the summary statistics, we can draw the following conclusions: year: The wines in the dataset span a wide range of years, from 1910 to 2021, with the majority falling between 2011 and 2017. rating: The ratings for the wines range from 4.2 to 4.9, with an average rating of approximately 4.26. The ratings appear to be quite high, with the median also at 4.2. num_reviews: The number of reviews for the wines varies widely, with a minimum of 25 reviews and a maximum of 16505 reviews. The average number of reviews is approximately 440, indicating that some wines have received significant attention and feedback. price: The prices of the wines range from 6.26 to 3119.08, with an average price of approximately 67.40. However, the distribution of prices appears to be skewed, as the median price is 31.63, which is much lower than the mean. Now, we will utilise ChatGPT to aid us in conducting Exploratory Data Analysis (EDA) on the wine_data dataset. Through a combination of visualisations and statistical analyses, we aim to explore the distribution of key numerical variables such as rating, num_reviews, and price. Additionally, we will investigate the potential impact of categorical variables such as region, type and acidity on these key metrics. Prompt: Please provide an R code for creating visually appealing visualisations of key numerical variables such as rating, num_reviews, and price. Additionally, analyse the potential impact of categorical variables such as region, and type on these key metrics. The dataset has a structure of 6,331 rows and 10 columns, with variables including winery, wine, year, rating, num_reviews, region, price, type, body, and acidity. &gt; glimpse(wine_data) Rows: 6,331 Columns: 10 $ winery &lt;fct&gt; Teso La Monja, Artadi, Vega… $ wine &lt;fct&gt; Tinto, Vina El Pison, Unico… $ year &lt;dbl&gt; 2013, 2018, 2009, 1999, 199… $ rating &lt;dbl&gt; 4.9, 4.9, 4.8, 4.8, 4.8, 4.… $ num_reviews &lt;int&gt; 58, 31, 1793, 1705, 1309, 1… $ region &lt;fct&gt; Toro, Vino de Espana, Riber… $ price &lt;dbl&gt; 995.00, 313.50, 324.95, 692… $ type &lt;fct&gt; Toro Red, Tempranillo, Ribe… $ body &lt;fct&gt; 5, 4, 5, 5, 5, 5, 5, 5, 5, … $ acidity &lt;fct&gt; 3, 2, 3, 3, 3, 3, 3, 3, 3, … ❗️ Make sure to include the glimpse() output in your prompt question to give an overview of your data’s structure! ChatGPT &gt; # Create visualisations &gt; &gt; # Histogram of Ratings &gt; ggplot(wine_data, aes(x = rating)) + + geom_histogram(binwidth = 0.1, fill = &quot;skyblue&quot;, color = &quot;black&quot;) + + labs(title = &quot;Distribution of Ratings&quot;, x = &quot;Rating&quot;, y = &quot;Frequency&quot;) &gt; # Histogram of Price &gt; ggplot(wine_data, aes(x = price)) + + geom_histogram(binwidth = 20, fill = &quot;skyblue&quot;, color = &quot;black&quot;) + + labs(title = &quot;Distribution of Wine Prices&quot;, x = &quot;Price&quot;, y = &quot;Frequency&quot;) &gt; # Scatterplot of Rating vs. Num_reviews &gt; ggplot(wine_data, aes(x = num_reviews, y = rating)) + + geom_point(color = &quot;skyblue&quot;) + + labs(title = &quot;Rating vs. Number of Reviews&quot;, x = &quot;Number of Reviews&quot;, y = &quot;Rating&quot;) &gt; # Barplot of Wine Types &gt; ggplot(wine_data, aes(x = type, fill = type)) + + geom_bar() + + labs(title = &quot;Distribution of Wine Types&quot;, x = &quot;Type&quot;, y = &quot;Count&quot;) + + theme(axis.text.x = element_text(angle = 45, hjust = 1), + legend.position = &quot;none&quot;) &gt; # Barplot of Regions &gt; ggplot(wine_data, aes(x = region, fill = region)) + + geom_bar() + + labs(title = &quot;Distribution of Regions&quot;, x = &quot;Region&quot;, y = &quot;Count&quot;) + + theme(axis.text.x = element_text(angle = 45, hjust = 1), + legend.position = &quot;none&quot;) The Barplot of Regions is challenging to interpret due to the numerous regions displayed. Since there are too many regions to effectively visualise, let’s seek assistance from ChatGPT for a solution. Prompt: Please filter and display only the top 10 regions for better clarity in this visualization. ggplot(wine_data, aes(x = region, fill = region)) + geom_bar() + labs(title = &quot;Distribution of Regions&quot;, x = &quot;Region&quot;, y = &quot;Count&quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = &quot;none&quot;) ❗️ Once more, please ensure that the code is included to enhance clarity ChatGPT We can enhance the ggplot by removing the legend, as the region names are already displayed below the bars. &gt; # Get the top 10 regions by frequency &gt; top_regions &lt;- wine_data %&gt;% + group_by(region) %&gt;% + summarize(count = n()) %&gt;% + arrange(desc(count)) %&gt;% + head(10) &gt; # Filter the dataset to include only the top 10 regions &gt; wine_data_top_regions &lt;- wine_data %&gt;% + filter(region %in% top_regions$region) &gt; # Barplot of Top 10 Regions &gt; ggplot(wine_data_top_regions, aes(x = region, fill = region)) + + geom_bar() + + labs(title = &quot;Top 10 Regions by Frequency&quot;, x = &quot;Region&quot;, y = &quot;Count&quot;) + + theme(axis.text.x = element_text(angle = 45, hjust = 1), + legend.position = &quot;none&quot;) Let’s assess the degree of association between the key variables. To do this, we’ll need to convert the variables ‘body’ and ‘acidity’. &gt; # Convert factors to numeric (if appropriate) &gt; wine_data$body &lt;- as.numeric(as.character(wine_data$body)) &gt; wine_data$acidity &lt;- as.numeric(as.character(wine_data$acidity)) &gt; &gt; # Compute the correlation matrix &gt; correlation_matrix &lt;- cor(wine_data[, c(&quot;year&quot;, &quot;rating&quot;, &quot;num_reviews&quot;, &quot;price&quot;, &quot;body&quot;, &quot;acidity&quot;)]) &gt; correlation_matrix ## year rating num_reviews price body ## year 1.00000000 -0.2957784867 0.0354872722 -0.38526005 -0.10277253 ## rating -0.29577849 1.0000000000 -0.0001653593 0.55209571 0.16303349 ## num_reviews 0.03548727 -0.0001653593 1.0000000000 -0.03393994 0.06710569 ## price -0.38526005 0.5520957084 -0.0339399360 1.00000000 0.15362352 ## body -0.10277253 0.1630334935 0.0671056895 0.15362352 1.00000000 ## acidity 0.14806938 -0.0945527010 0.0401375039 -0.03286992 -0.01795032 ## acidity ## year 0.14806938 ## rating -0.09455270 ## num_reviews 0.04013750 ## price -0.03286992 ## body -0.01795032 ## acidity 1.00000000 Let us ask chatGPT to visualise this outcome. Prompt: Please provide R code to create a heatmap visualising the correlation matrix shown below. Ensure the heatmap includes a gradient color scheme, correlation coefficients, axis labels, and a title. &gt; correlation_matrix year rating num_reviews price year 1.00000000 -0.2957784867 0.0354872722 -0.38526005 rating -0.29577849 1.0000000000 -0.0001653593 0.55209571 num_reviews 0.03548727 -0.0001653593 1.0000000000 -0.03393994 price -0.38526005 0.5520957084 -0.0339399360 1.00000000 body -0.10277253 0.1630334935 0.0671056895 0.15362352 acidity 0.14806938 -0.0945527010 0.0401375039 -0.03286992 body acidity year -0.10277253 0.14806938 rating 0.16303349 -0.09455270 num_reviews 0.06710569 0.04013750 price 0.15362352 -0.03286992 body 1.00000000 -0.01795032 acidity -0.01795032 1.00000000 ChatGPT &gt; # Create a heatmap of the correlation matrix with correlation coefficients &gt; ggplot(data = melt(correlation_matrix), aes(x=Var1, y=Var2, fill=value)) + + geom_tile() + + geom_text(aes(label = round(value, 2)), color = &quot;black&quot;, size = 3) + # Add text labels for correlation coefficients + scale_fill_gradient2(low=&quot;blue&quot;, mid=&quot;white&quot;, high=&quot;red&quot;, midpoint=0, limit=c(-1,1)) + + theme_minimal() + + labs(title=&quot;Correlation Heatmap&quot;, x=&quot;Variables&quot;, y=&quot;Variables&quot;) + + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) The correlation matrix offers valuable insights into the relationship between wine attributes and prices. It reveals that older wines tend to be less expensive, as seen in the negative correlation between year and price. Conversely, higher-rated wines are associated with higher prices, supported by the positive correlation between rating and price. Surprisingly, there is almost no relationship between the number of reviews or acidity and price, suggesting limited impact on pricing decisions. Additionally, wines with a higher body may command slightly higher prices, indicated by a weak positive correlation. Overall, the rating emerges as the strongest predictor of wine price, underscoring the significance of quality perception in pricing strategies. In conclusion, the EDA phase has shed light on the key variables influencing wine prices. Factors such as the year of production, wine rating, and body appear to be significant determinants. However, variables like the number of reviews and acidity show minimal impact. With these insights, we can now transition to the next phase: model selection and training. This step will involve choosing suitable models to predict wine prices based on the identified variables and training them on our dataset to make accurate price predictions. Model Selection and Model Training Model selection and training are pivotal stages in any data analysis project, especially when predicting intricate phenomena like wine prices. During this phase, we will leverage the insights gleaned from our exploratory data analysis (EDA) to select the most appropriate machine learning models for predicting wine prices. Our goal is to choose models that can adeptly capture the relationships between key variables identified in the EDA, thereby crafting robust and accurate models for predicting wine prices. This process will entail evaluating various regression models, including linear regression, lasso regression, ridge regression, regression tree, and Generalized Additive Model (GAM), to ascertain which one performs optimally for our dataset. Through the process of model selection and training, our aim is to develop a dependable pricing model that can facilitate stakeholders in making well-informed decisions within the wine industry. However, considering that certain attribute variables in wine_data comprise a large number of levels, we will exclude the fitting of the multiple regression model. Linear models assume a linear relationship between the predictors and the outcome variable, which can be challenging when dealing with numerous factor levels. In such instances, the model may attempt to fit each level individually, leading to overfitting and a lack of generalisation to new data. Furthermore, estimating coefficients for each factor level can be computationally intensive and may not be viable for sizable datasets. The interpretation of coefficients in a linear model with many factor levels can also be intricate, making it challenging to discern the impact of each level on the outcome variable. Therefore, alternative modeling techniques that can effectively handle factor variables with a large number of levels, such as tree-based models, may be more suitable in these scenarios. So, let us start by asking chatGPT to build for us those suggested models. Prompt: Perform machine learning to predict wine prices using lasso regression, ridge regression, regression tree, and Generalized Additive Model (GAM) to identify the most effective model for the wine_data dataset. &gt; glimpse(wine_data) Rows: 6,331 Columns: 10 $ winery &lt;fct&gt; Teso La Monja, Artadi, Vega… $ wine &lt;fct&gt; Tinto, Vina El Pison, Unico… $ year &lt;dbl&gt; 2013, 2018, 2009, 1999, 199… $ rating &lt;dbl&gt; 4.9, 4.9, 4.8, 4.8, 4.8, 4.… $ num_reviews &lt;int&gt; 58, 31, 1793, 1705, 1309, 1… $ region &lt;fct&gt; Toro, Vino de Espana, Riber… $ price &lt;dbl&gt; 995.00, 313.50, 324.95, 692… $ type &lt;fct&gt; Toro Red, Tempranillo, Ribe… $ body &lt;dbl&gt; 5, 4, 5, 5, 5, 5, 5, 5, 5, … $ acidity &lt;dbl&gt; 3, 2, 3, 3, 3, 3, 3, 3, 3, … ChatGPT &gt; # Set seed for reproducibility &gt; set.seed(123) &gt; &gt; # Split the data into training and testing sets &gt; train_indices &lt;- sample(1:nrow(wine_data), 0.7 * nrow(wine_data)) &gt; train_data &lt;- wine_data[train_indices, ] &gt; test_data &lt;- wine_data[-train_indices, ] &gt; &gt; # Fit lasso regression &gt; lasso_model &lt;- glmnet::glmnet(as.matrix(select(train_data, -price)), train_data$price, alpha = 1) &gt; &gt; # Fit ridge regression &gt; ridge_model &lt;- glmnet::glmnet(as.matrix(select(train_data, -price)), train_data$price, alpha = 0) &gt; &gt; # Fit regression tree &gt; tree_model &lt;- rpart::rpart(price ~ ., data = train_data) &gt; &gt; # Fit Generalized Additive Model (GAM) &gt; gam_model &lt;- mgcv::gam(price ~ bs(year, df = 3) + bs(rating, df = 3) + bs(num_reviews, df = 3) + bs(body, df = 3) + bs(acidity, df = 3), data = train_data) In this section, we fitted various models for predicting wine prices, including lasso regression, ridge regression, regression tree, and Generalized Additive Model (GAM). Each model offers unique advantages and complexities, making it crucial to select the most suitable one for our dataset. Following model selection and training, the next step is to evaluate the performance of these models to determine which one best predicts wine prices. This evaluation will provide insights into the model’s accuracy and reliability, guiding us in making informed decisions about the most effective model for our wine_data dataset. Model Evaluation In the Model Evaluation section, we assess the performance of various machine learning models in predicting wine prices for the wine_data dataset. By examining metrics such as RMSE and MAE, we aim to determine the most accurate and reliable model for our predictive tasks. This evaluation will provide valuable insights into the effectiveness of each model and guide us in selecting the best approach for predicting wine prices. We will ask chatGPT to generate the code for this task. Prompt: Please generate R code to evaluate the performance of the fitted models (lasso regression, ridge regression, regression tree, and Generalized Additive Model) on the test dataset. Compute and print the RMSE and MAE for each model in the code given to assess their predictive accuracy. # Set seed for reproducibility set.seed(123) # Split the data into training and testing sets train_indices &lt;- sample(1:nrow(wine_data), 0.7 * nrow(wine_data)) train_data &lt;- wine_data[train_indices, ] test_data &lt;- wine_data[-train_indices, ] # Fit lasso regression lasso_model &lt;- glmnet::glmnet(as.matrix(select(train_data, -price)), train_data$price, alpha = 1) # Fit ridge regression ridge_model &lt;- glmnet::glmnet(as.matrix(select(train_data, -price)), train_data$price, alpha = 0) # Fit regression tree tree_model &lt;- rpart::rpart(price ~ ., data = train_data) # Fit Generalized Additive Model (GAM) gam_model &lt;- mgcv::gam(price ~ bs(year, df = 3) + bs(rating, df = 3) + bs(num_reviews, df = 3) + bs(body, df = 3) + bs(acidity, df = 3), data = train_data) ChatGPT &gt; # Predictions for lasso regression &gt; lasso_pred &lt;- predict(lasso_model, newx = as.matrix(select(test_data, -price))) &gt; lasso_rmse &lt;- sqrt(mean((lasso_pred - test_data$price)^2)) &gt; lasso_mae &lt;- mean(abs(lasso_pred - test_data$price)) &gt; &gt; # Predictions for ridge regression &gt; ridge_pred &lt;- predict(ridge_model, newx = as.matrix(select(test_data, -price))) &gt; ridge_rmse &lt;- sqrt(mean((ridge_pred - test_data$price)^2)) &gt; ridge_mae &lt;- mean(abs(ridge_pred - test_data$price)) &gt; &gt; # Predictions for regression tree &gt; tree_pred &lt;- predict(tree_model, newdata = test_data) &gt; tree_rmse &lt;- sqrt(mean((tree_pred - test_data$price)^2)) &gt; tree_mae &lt;- mean(abs(tree_pred - test_data$price)) &gt; &gt; # Predictions for GAM &gt; gam_pred &lt;- predict(gam_model, newdata = test_data) &gt; gam_rmse &lt;- sqrt(mean((gam_pred - test_data$price)^2)) &gt; gam_mae &lt;- mean(abs(gam_pred - test_data$price)) &gt; &gt; # Print RMSE and MAE for each model &gt; cat(&quot;Lasso Regression RMSE:&quot;, lasso_rmse, &quot;\\n&quot;) ## Lasso Regression RMSE: 142.121 &gt; cat(&quot;Lasso Regression MAE:&quot;, lasso_mae, &quot;\\n&quot;) ## Lasso Regression MAE: 47.67765 &gt; cat(&quot;Ridge Regression RMSE:&quot;, ridge_rmse, &quot;\\n&quot;) ## Ridge Regression RMSE: 160.9951 &gt; cat(&quot;Ridge Regression MAE:&quot;, ridge_mae, &quot;\\n&quot;) ## Ridge Regression MAE: 52.27333 &gt; cat(&quot;Regression Tree RMSE:&quot;, tree_rmse, &quot;\\n&quot;) ## Regression Tree RMSE: 88.45144 &gt; cat(&quot;Regression Tree MAE:&quot;, tree_mae, &quot;\\n&quot;) ## Regression Tree MAE: 24.06545 &gt; cat(&quot;Generalized Additive Model RMSE:&quot;, gam_rmse, &quot;\\n&quot;) ## Generalized Additive Model RMSE: 123.112 &gt; cat(&quot;Generalized Additive Model MAE:&quot;, gam_mae, &quot;\\n&quot;) ## Generalized Additive Model MAE: 40.67011 We can consolidate the results and display them in a unified table for easier comparison. &gt; # Combine results into a table &gt; results &lt;- data.frame( + Model = c(&quot;Lasso Regression&quot;, &quot;Ridge Regression&quot;, &quot;Regression Tree&quot;, &quot;Generalized Additive Model&quot;), + RMSE = c(lasso_rmse, ridge_rmse, tree_rmse, gam_rmse), + MAE = c(lasso_mae, ridge_mae, tree_mae, gam_mae) + ) &gt; &gt; # Print the table &gt; print(results) ## Model RMSE MAE ## 1 Lasso Regression 142.12096 47.67765 ## 2 Ridge Regression 160.99508 52.27333 ## 3 Regression Tree 88.45144 24.06545 ## 4 Generalized Additive Model 123.11196 40.67011 Upon analysis, it becomes evident that the Regression Tree model outperforms other models in this dataset, particularly due to its ability to handle numerous features effectively. This conclusion is drawn from a comparative evaluation of the models, highlighting the Regression Tree’s superior performance in managing the complexity of the dataset’s features. Model Interpretation In the model interpretation section of this report, we delve into the insights gained from the trained machine learning models, aiming to understand the underlying factors that influence wine prices. By analysing the feature importance, decision rules, and predicted values of the models, we seek to unravel the complex relationships between wine characteristics and their market values. This section not only sheds light on the predictive performance of the models but also provides actionable insights for stakeholders in the wine industry, guiding them in making informed decisions related to pricing strategies and product development. We will begin by plotting the regression tree for the wine_data, providing a visual representation of the decision-making process of the model. You can interpret the tree by starting at the root node and following the branches to the leaf nodes, where the predictions are made. Each split in the tree represents a decision based on a feature in the dataset, and the leaf nodes represent the predicted wine prices. Let us visualise the tree. &gt; rpart.plot(tree_model, + box.palette = &quot;RdBu&quot;, + shadow.col = &quot;gray&quot;, + nn = TRUE, + type = 4, + fallen.leaves = TRUE, + branch.lty = 3, + branch.lwd = 2, + extra = 1, + cex = 0.6, # Reduce the size of nodes and text + faclen = 3) # Adjust the faclen value to increase space between nodes &gt; summary(tree_model) ## Call: ## rpart::rpart(formula = price ~ ., data = train_data) ## n= 4431 ## ## CP nsplit rel error xerror xstd ## 1 0.54735715 0 1.0000000 1.0006206 0.17076673 ## 2 0.08338496 1 0.4526429 0.5384705 0.10301520 ## 3 0.07689143 2 0.3692579 0.5090425 0.09812500 ## 4 0.03461715 3 0.2923665 0.4213455 0.08404526 ## 5 0.03452879 4 0.2577493 0.4161366 0.08524950 ## 6 0.01482093 5 0.2232205 0.3842724 0.08380711 ## 7 0.01208131 6 0.2083996 0.3696184 0.08374828 ## 8 0.01148682 7 0.1963183 0.3525272 0.08166889 ## 9 0.01000000 8 0.1848315 0.3686921 0.08631320 ## ## Variable importance ## wine winery region type year num_reviews ## 57 19 6 5 5 3 ## body rating ## 3 2 ## ## Node number 1: 4431 observations, complexity param=0.5473571 ## mean=63.84701, MSE=24260.23 ## left son=2 (4358 obs) right son=3 (73 obs) ## Primary splits: ## wine splits as --LLL--LLL--LLLLLLLLLLLL-LL--LLLL-LLL--LLLL--LLLLL-LLLLL-LLLLLLLL-LL-LLL-LLL--LLLLL--L--LL-L--LLLL---LL-LL--LLLLLL-LLLL-LLLLLLLLLL-L-LL-LLLL-L--LLLL-L-LLLLL-LLLLLLLLLLLL-LLLLLLLLLLLLLLLL---LLLLRL-LL-LL-LL-LLLL---LLLLLLLL-LL--L--LLLLLLLLLLL--LLL-LLL--LLLLLL--LLLLL----LL-LLL-LLL-L-L-LLLLLLLLL-LLL-LLLLLLL--LLLL--L-L-L-L-LLL-LLLL-LL-R-L--LLL-L-LLLLLLLL-LLLLL-L-L-LLL-LL-L-L---LLL--L-LLLL-L-R-LLLLLLLLL--LLLRLLL-L-LLLLLL-----L----L--LLLL-L--LLLLL-LLL--L--LLLL-LLLLLLLLL--LLL--LLLLL--LLLL--LL-LLLLLLLLL--LL-LL-LL-LLL--L--LL-L-L--L--LL--L--LL-LL-LLLLL-LLL-LLLLLLLL-L-L-LLL-LLL-LL-LLLL--LRL--LL-L-LLLL-L-LLLLL-LL-L--LLL-LLL--LLLLLLLLLLLL-LL-LLLLL-LLL-RLLL-LLLLLLL-L-LLLLLLLLLLLL---LLL---LLLL---L-L---LLLLLLLLLLL----LLLL---L-LLLLLLLLLLL-L---L-LL-LLLLL-LLLR-LLL-LLLL-LLLLLL--LLL--L-LLL-RRL-L-LL--LL-LLL---LL-L-L-L----LLLLLLLLL-LLLL-LLLLLLL-L-LLL-LL-LLLLLL, improve=0.5473571, (0 missing) ## winery splits as -LLLLLLLLLLL-LL-LLLLLLLLRL-LLLLLLLLLLL---L-LLL--L--LLLL---LL-LLRL-L--LLLL-L-LLLLL-LLLL-LL-LLLL-LLLLLLLLLLLLL---LL-LLLLLL-----L-L-LL-LLL-LLLLLL-LLL-LLRLLLLLLLLLLLLLL-LL-LLLR---LLLL-L--RLL-LL-LLLLL-L-L-LL-LL-LLLLLL-L-LL-LLLL--L-LL-L-LLLLL--L-LLLLLLLLL-L-LL-LLLLL-LLLLL-LLLLLL-L--L-LL-LLLLLL-LL--LLLLL-LLLL--LLL--LLLLLLLLL-LLLLLLLL-LLL-L-LLLL-R--LLLLLLLLLL-LLLL-LLLL-LLL---LLL-L-LLLLLLL-L--LLLLLLLLLLLL--L--LL-LLLLL----LL-RLLLRLLLL-LL-LLL-L-LLLL-LLLLLR--LLLLLL-LLLLLL-LLL-L-LLLLL-L-L, improve=0.3886168, (0 missing) ## rating &lt; 4.55 to the left, improve=0.3049241, (0 missing) ## year &lt; 2003.5 to the right, improve=0.1302212, (0 missing) ## num_reviews &lt; 325.5 to the right, improve=0.1113139, (0 missing) ## Surrogate splits: ## winery splits as -LLLLLLLLLLL-LL-LLLLLLLLLL-LLLLLLLLLLL---L-LLL--L--LLLL---LL-LLRL-L--LLLL-L-LLLLL-LLLL-LL-LLLL-LLLLLLLLLLLLL---LL-LLLLLL-----L-L-LL-LLL-LLLLLL-LLL-LLLLLLLLLLLLLLLLL-LL-LLLR---LLLL-L--LLL-LL-LLLLL-L-L-LL-LL-LLLLLL-L-LL-LLLL--L-LL-L-LLLLL--L-LLLLLLLLL-L-LL-LLLLL-LLLLL-LLLLLL-L--L-LL-LLLLLL-LL--LLLLL-LLLL--LLL--LLLLLLLLL-LLLLLLLL-LLL-L-LLLL-L--LLLLLLLLLL-LLLL-LLLL-LLL---LLL-L-LLLLLLL-L--LLLLLLLLLLLL--L--LL-LLLLL----LL-LLLLLLLLL-LL-LLL-L-LLLL-LLLLLR--LLLLLL-LLLLLL-LLL-L-LLLLL-L-L, agree=0.988, adj=0.274, (0 split) ## rating &lt; 4.75 to the left, agree=0.984, adj=0.014, (0 split) ## ## Node number 2: 4358 observations, complexity param=0.07689143 ## mean=48.93278, MSE=4646.262 ## left son=4 (4087 obs) right son=5 (271 obs) ## Primary splits: ## wine splits as --LLR--LLL--LLLLLLLLLLLL-LL--RLLL-RLL--LLLL--LLLLL-LLLLL-RLLLLLLL-RL-LLL-LLL--LRLLL--L--RL-L--LLLL---LL-LL--LLLLRR-LLLL-RLLLLRLLLL-L-RL-LLLR-L--LLLL-L-LLLLL-LLLLRLLRLLRL-LLLLLLRLLLLLRLLL---LLLL-L-LL-LL-LL-LLLR---LLLRLLLL-LR--L--LLLRLRRLLLL--LLL-RLL--LRRLLL--LLLLL----LL-LLL-LLL-L-L-LLLLLLLLL-LLL-LLLLLLL--LLLL--L-L-L-L-RLL-LLLL-LL---L--LLR-R-LRLLRLLL-LLLRL-L-R-RLL-LL-L-L---RLL--L-LLLL-L---LRLLLLLLL--RLL-LLL-L-LLLLLL-----L----L--LLLL-L--LRLLR-LRL--L--RLLL-LLLLLRRLL--LRL--RLLLL--RLLL--LL-LLRLLLLLL--LR-LL-LL-LLL--R--RL-L-L--L--LL--L--LL-RL-LLLLL-LLL-LLLLLLLL-L-L-LRL-LLL-RR-LLRL--L-L--LL-L-LLLL-L-RLLLL-LL-L--RLL-LLL--LLLLLLLLRLLL-LL-LLLLL-LLL--LRL-LLLLLLL-L-LLLLRLLRLLLR---LLL---LRLL---L-L---LLLLLLLLLLL----LLLL---L-LLLLLLRRRLR-L---L-LL-LLLLL-LLL--LLL-LLLL-RLLRLL--LLL--L-LLL---R-R-LR--LL-LRL---LL-L-L-L----LLLLLLLLL-LRLL-RRRLLLL-L-LLL-LL-LLLLLL, improve=0.4082101, (0 missing) ## winery splits as -LLLLLLLRLLL-LR-LLLLLLRLLL-LLRLLLLLLLL---L-LLL--L--LLLL---LL-LLLL-L--LLLL-L-LLLLR-LLLR-LL-LLLL-LLLLLLLLLLLRL---LL-LLLLLL-----L-L-LL-LRL-LLLLLL-LLL-LLRLLLLLLLLLLLLLL-LL-LLLR---LLLL-L--LRL-LL-LLLRL-L-L-LL-LL-LLLRLL-L-LL-LLLR--L-LL-L-LRLLL--L-LLLLLLLLL-L-LL-LLRLL-LLLLL-LLLLLL-L--L-LL-LLLLLL-LL--LLLLR-LLLL--LLR--RLRRLLLLL-LLLLLLLL-LLL-L-LLLL-R--LLLLRRLLLL-LLLL-LLLL-LLL---RLL-L-LLLLLLL-L--LLLLLLLLLLLL--L--RL-LLLLL----LL-RLRRRLLLL-RL-LLL-L-LLLR-LLLLLR--LLLLLL-RLLLLL-LLL-L-LLLLL-L-L, improve=0.2804499, (0 missing) ## rating &lt; 4.45 to the left, improve=0.2244645, (0 missing) ## num_reviews &lt; 386 to the right, improve=0.1585326, (0 missing) ## year &lt; 2010.5 to the right, improve=0.1194112, (0 missing) ## Surrogate splits: ## winery splits as -LLLLLRLRLLL-LL-LLLLLLRLLL-LLRLLLLLLLL---L-LLL--L--LLLL---LL-LLLL-L--LLLL-L-LLLLR-LRLR-LL-LLLL-LLLRLLLLLLLRL---LL-LLLLLL-----L-L-LL-LRL-LLLLLL-LLL-LRRLLRLLLLLLLLLLL-LL-LLLL---LLLL-L--LRL-LL-LLLLL-R-L-LL-RL-LLLLLL-L-LL-LLLR--L-LL-L-LLLLL--L-LLLLLLLLL-L-LL-LLLLL-LLLLL-LLLLLL-L--L-LL-LRLLLL-LL--LLLLR-LLLL--LLR--RLRRLLLLL-LLLLRLLL-LLL-L-LLLL-R--LLLLLRLLLL-LLLL-LLLL-LLL---RLL-L-LLLLLLR-L--LRLLLLLRLLLL--L--RL-LLLLL----LL-RLLRRLLLL-RL-LLL-R-LLLR-LLLLLR--LLLLLL-LLLLLL-LLL-L-RLLLL-L-L, agree=0.966, adj=0.450, (0 split) ## rating &lt; 4.55 to the left, agree=0.945, adj=0.111, (0 split) ## year &lt; 1990 to the right, agree=0.944, adj=0.103, (0 split) ## region splits as -LLLLLLLL-LLLLLLRLLR-LLRLLL--LL-LLLLL-L-RLRLRLLLL-LLLLL--L-LLLLLLLLLLLL--LLL, agree=0.941, adj=0.059, (0 split) ## ## Node number 3: 73 observations, complexity param=0.08338496 ## mean=954.2058, MSE=389169.5 ## left son=6 (50 obs) right son=7 (23 obs) ## Primary splits: ## wine splits as -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------L-----------------------------------------------------------------------------------------------------------------------------------------L--------------------------------------------------------R---------------R---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------R--------------------------------------------------------------L------------------------------------------------------------------------------------------------------L-----------------------------LL-------------------------------------------------------------------, improve=0.3155169, (0 missing) ## winery splits as ------------------------R----------------------------------L---L---------------------------------------L-------------------------------------------------------------------R-----------R-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------L------------------L-------------------------------, improve=0.3155169, (0 missing) ## region splits as --------R-----------------------------------L--------R---L-L---------L------, improve=0.2168240, (0 missing) ## type splits as --------LRRLL------L-, improve=0.2168240, (0 missing) ## num_reviews &lt; 109.5 to the right, improve=0.1796302, (0 missing) ## Surrogate splits: ## region splits as --------R-----------------------------------L--------R---L-L---------L------, agree=0.877, adj=0.609, (0 split) ## type splits as --------LRRLL------L-, agree=0.877, adj=0.609, (0 split) ## body &lt; 4.5 to the right, agree=0.836, adj=0.478, (0 split) ## num_reviews &lt; 67.5 to the right, agree=0.795, adj=0.348, (0 split) ## year &lt; 2014.5 to the left, agree=0.712, adj=0.087, (0 split) ## ## Node number 4: 4087 observations, complexity param=0.01482093 ## mean=37.71838, MSE=630.4267 ## left son=8 (2687 obs) right son=9 (1400 obs) ## Primary splits: ## wine splits as --LL---RRL--LLRRLLRLRLRL-LL---RRL--LL--LLLR--LLLRL-RLLLL--RLRRRRR--L-LLL-RRR--L-LLL--L---R-R--LLLL---RL-LL--LRRL---LLLL--LRLL-RRRL-L--L-RLR--L--RRLL-L-LRLLR-LLLL-RL-RR-L-RRLRRR-RRLLL-LLR---LLRR-L-LL-LL-LL-RRR----LLR-LRLL-L---L--LLL-L--RRLL--RLL--RL--R--RLR--RRLLR----LL-RLL-LRL-L-L-LRRRLRRLL-LLR-LLRRLLL--LLLL--R-R-L-L--LR-LLLR-LR---R--LR----R-LL-RLR-LLR-R-R----LL-LR-R-R----LR--R-LLLL-L---L-LRLRRLL---LR-LLL-L-RLRLLR-----R----R--LRRL-L--L-LL--R-R--L---RLL-LLLLL--LL--R-R---LLLR---RLR--LR-LR-RRLRLR--L--LR-LL-LLR------L-L-L--L--LL--L--RL--R-LLLLL-LRR-LRRRRLRL-R-L-L-L-LRR----RR-R--R-L--LR-R-RRRL-L--LLRL-LL-L---RL-RRL--LLRLRLRL-LLL-LL-LLRLL-RLL--R-L-LRLLRLL-L-RRLR-RL-RRR----LRL---R-RL---R-L---RRLLLRLRLRR----LLLR---L-LRLLLR---L--R---L-LL-LRLRL-RLL--RLR-LRLL--RR-RL--RLR--R-LLR-------R---RR-L-L---LL-R-L-L----RLLLLRLLL-R-RL----LLRR-L-LLL-RL-LLRRLR, improve=0.6183481, (0 missing) ## winery splits as -RLRRR-L-RLL-RR-RLLRLR-LRR-RL-LLRRRLRL---R-LLR--L--LRLR---LR-RRRL-L--LLRL-L-LLRLR-LLR--RL-RRLR-LLRRLRRRLLL-R---LL-RLRRRL-----R-L-RR-L-L-RLLLRL-RRL-R-RRLRLRLLRLRRRRR-LL-RLRR---LLRL-R--RRR-LL-LLLRR-L-L-LR--L-LRRRLL-R-RL-LLR---L-LL-L-RRLRL--L-LRRLRLRLR-R-LL-RRRLL-LLLLL-RLLLLL-L--L-LR-R-LLLL-RL--LRLR--RRLR--LR---LL--LRRRL-RLRL-LRR-LRR-L-LLLL----RLLLLRRRLL-RLLL-LLLL-LLL----LR-R-LRRRRLL-L--R-RRRRL-LLLR--R---R-LRLLR----RR--LR-LRRLL-RL-LRL---LLR--LRRLL---LRLRRR-RLRRLR-LLL-R--LLLL-R-L, improve=0.4686453, (0 missing) ## year &lt; 2014.5 to the right, improve=0.2151698, (0 missing) ## num_reviews &lt; 389.5 to the right, improve=0.1672325, (0 missing) ## region splits as -LLLLLRRL-RLRRLR-RLR-LL-LRL--LR-RRRRL-R-RR-LRRLRL-RLLRL--R-RRLLRLRLRRRL--LRL, improve=0.1543332, (0 missing) ## Surrogate splits: ## winery splits as -RLRLL-L-RLL-RR-RLLLLR-LRR-RL-LLRRLLRL---R-LLL--L--LRLL---LR-LRLL-L--LLRL-L-LLRLR-LLR--RL-LRLR-LLLRLRRRLLL-R---LL-RLRRLL-----L-L-RR-L-L-RLLLLL-RRL-R-LRLRLRLLRLRRRRR-LL-LLRR---LLRL-R--RLR-LL-LLLRR-L-L-LR--L-LRLRLL-R-RL-LLR---L-LL-R-LRLRL--L-LLRLRLRLL-R-LL-LRRLL-LLLLL-RLLLLL-L--L-LL-L-LLLL-LL--LLLR--RRLL--LR---LL--LRLRL-RLRL-LRR-LLR-L-LLLL----RLLLLRLRLL-RRLL-LLLL-LLL----LR-R-LRRRRLL-L--L-LRLLL-LLLR--R---R-LRLLR----LR--LR-LLRLL-LL-LLL---LLL--RRRLL---LRLRRL-RLRRLL-LLL-L--LLLL-L-L, agree=0.930, adj=0.794, (0 split) ## year &lt; 2011.5 to the right, agree=0.780, adj=0.359, (0 split) ## region splits as -LLLLLRLL-LLLRLL-RLR-LL-LLL--LL-RRLLL-R-LL-LRLLRL-LLLRL--L-LRLLRLLLLLLL--LRL, agree=0.710, adj=0.153, (0 split) ## type splits as LRRLLLLLLRLLLLLLLLRLL, agree=0.707, adj=0.145, (0 split) ## num_reviews &lt; 386 to the right, agree=0.698, adj=0.119, (0 split) ## ## Node number 5: 271 observations, complexity param=0.03461715 ## mean=218.0592, MSE=34709.41 ## left son=10 (257 obs) right son=11 (14 obs) ## Primary splits: ## winery splits as -L----L-L-----L-------L-L----L---LL------------------------L----------L---------L--L-L----L------LL-----L-L--------------------------L--------------LR--L------------------L------L-----L--------L--L-L----L---L-------L-----L----------L-L----------------------L-----------L-------------L-------------L-L------LL--L-LL---L------L------L--------L------LL-----L-L-------------L--------L--L-----L--L--L---------L--L--L--------L-LLRL----L------L---LL------L------L--L------------L--------, improve=0.39561390, (0 missing) ## type splits as --LLLL-LLLLLLL-LL-LR-, improve=0.16341780, (0 missing) ## region splits as ---L-L--L-------LL-L-L-L-L---L---L-L----L-L-LL----L--L---L-LLL---L---R---LL-, improve=0.16341780, (0 missing) ## rating &lt; 4.75 to the left, improve=0.13420570, (0 missing) ## year &lt; 2013.5 to the right, improve=0.09244459, (0 missing) ## Surrogate splits: ## wine splits as ----L------------------------L----R----------------------L--------L------------L--------L-----------------------LL------L----L-------L-----L---------------------L--L--L--------L-----L-------------------------L------R------L--------L-LL----------L-----LL------------------------------------------------------------------L------------------L-L--L--L-------L----L-L------------L----------------L---------L-------------------------------------L--L--L------L---------LL-----L---L------L----------L---------L------------L--L--------------------L--------------------------L------LL---L--------------------L-----------L----------------L-------------------L----------------L--L---L----------L-----------------------------------------LLL-L------------------------------L--L-----------------L-L--L------L---------------------------L---LLL--------------------, agree=0.959, adj=0.214, (0 split) ## region splits as ---L-L--L-------LL-L-L-L-L---L---L-L----L-L-LL----L--L---L-LLL---L---R---LL-, agree=0.956, adj=0.143, (0 split) ## type splits as --LLLL-LLLLLLL-LL-LR-, agree=0.956, adj=0.143, (0 split) ## ## Node number 6: 50 observations, complexity param=0.03452879 ## mean=716.5438, MSE=226049.2 ## left son=12 (23 obs) right son=13 (27 obs) ## Primary splits: ## year &lt; 2004.5 to the right, improve=0.32840130, (0 missing) ## num_reviews &lt; 309.5 to the right, improve=0.16080040, (0 missing) ## rating &lt; 4.65 to the left, improve=0.08287315, (0 missing) ## winery splits as -----------------------------------------------------------L---L---------------------------------------R-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------R------------------R-------------------------------, improve=0.04522092, (0 missing) ## wine splits as -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------L-----------------------------------------------------------------------------------------------------------------------------------------R-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------R------------------------------------------------------------------------------------------------------L-----------------------------RR-------------------------------------------------------------------, improve=0.04522092, (0 missing) ## Surrogate splits: ## wine splits as -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------L-----------------------------------------------------------------------------------------------------------------------------------------R-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------R------------------------------------------------------------------------------------------------------L-----------------------------RL-------------------------------------------------------------------, agree=0.76, adj=0.478, (0 split) ## winery splits as -----------------------------------------------------------L---L---------------------------------------R-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------R------------------R-------------------------------, agree=0.68, adj=0.304, (0 split) ## region splits as --------------------------------------------R------------R-L---------L------, agree=0.68, adj=0.304, (0 split) ## type splits as --------R--RL------L-, agree=0.68, adj=0.304, (0 split) ## num_reviews &lt; 124 to the right, agree=0.64, adj=0.217, (0 split) ## ## Node number 7: 23 observations, complexity param=0.01208131 ## mean=1470.862, MSE=354055.4 ## left son=14 (9 obs) right son=15 (14 obs) ## Primary splits: ## year &lt; 2011.5 to the right, improve=0.15948200, (0 missing) ## rating &lt; 4.65 to the right, improve=0.13463400, (0 missing) ## num_reviews &lt; 62.5 to the right, improve=0.12780900, (0 missing) ## body &lt; 4.5 to the right, improve=0.02788812, (0 missing) ## winery splits as ------------------------R--------------------------------------------------------------------------------------------------------------------------------------------------R-----------L--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------, improve=0.02788812, (0 missing) ## Surrogate splits: ## rating &lt; 4.75 to the right, agree=0.696, adj=0.222, (0 split) ## num_reviews &lt; 41 to the left, agree=0.696, adj=0.222, (0 split) ## winery splits as ------------------------R--------------------------------------------------------------------------------------------------------------------------------------------------L-----------R--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------, agree=0.652, adj=0.111, (0 split) ## wine splits as ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------R---------------L---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------R------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------, agree=0.652, adj=0.111, (0 split) ## region splits as --------L--------------------------------------------R---R------------------, agree=0.652, adj=0.111, (0 split) ## ## Node number 8: 2687 observations ## mean=23.46677, MSE=77.10961 ## ## Node number 9: 1400 observations ## mean=65.0713, MSE=554.3952 ## ## Node number 10: 257 observations, complexity param=0.01148682 ## mean=190.7092, MSE=11604.8 ## left son=20 (149 obs) right son=21 (108 obs) ## Primary splits: ## wine splits as ----R------------------------L---------------------------L--------R------------L--------L-----------------------LR------L----L-------R-----L---------------------R--L--R--------L-----L-------------------------L-------------L--------L-LR----------L-----LL------------------------------------------------------------------R------------------L-R--R--R-------R----R-R------------L----------------R---------L-------------------------------------R--R--L------R---------LL-----L---L------L----------L---------R------------L--R--------------------R--------------------------L------RL---R--------------------L-----------R----------------R-------------------R----------------L--L---R----------L-----------------------------------------LLL-R------------------------------L--R-----------------L-R--R------R---------------------------R---LRL--------------------, improve=0.41402420, (0 missing) ## winery splits as -L----L-R-----R-------L-R----R---RL------------------------R----------L---------R--L-R----L------RL-----L-R--------------------------L--------------L---L------------------R------R-----R--------R--R-R----L---R-------L-----L----------R-L----------------------R-----------R-------------L-------------L-R------LR--R-RR---R------L------R--------R------RR-----L-L-------------L--------L--R-----L--R--L---------R--L--L--------R-RR-L----R------L---LR------R------R--R------------L--------, improve=0.34110450, (0 missing) ## year &lt; 2016.5 to the right, improve=0.11247530, (0 missing) ## region splits as ---L-L--R-------LL-L-L-L-R---L---R-L----R-L-LL----L--L---L-LLR---L---L---LR-, improve=0.09828861, (0 missing) ## rating &lt; 4.55 to the left, improve=0.08983699, (0 missing) ## Surrogate splits: ## winery splits as -L----L-R-----L-------L-R----R---RL------------------------L----------L---------L--L-L----L------LL-----L-R--------------------------L--------------L---L------------------R------R-----R--------R--L-L----L---R-------L-----L----------R-L----------------------L-----------R-------------L-------------L-R------LR--L-LL---R------L------L--------R------RL-----L-L-------------L--------L--L-----L--L--L---------R--L--L--------R-RR-L----L------L---LL------R------L--R------------L--------, agree=0.899, adj=0.759, (0 split) ## region splits as ---L-L--R-------LL-R-L-L-R---L---R-L----R-L-LL----L--L---R-LLR---L---L---LR-, agree=0.658, adj=0.185, (0 split) ## rating &lt; 4.55 to the left, agree=0.650, adj=0.167, (0 split) ## year &lt; 2005.5 to the right, agree=0.638, adj=0.139, (0 split) ## type splits as --LRLR-LLLLRLR-LL-RL-, agree=0.626, adj=0.111, (0 split) ## ## Node number 11: 14 observations ## mean=720.1264, MSE=193040.9 ## ## Node number 12: 23 observations ## mean=421.34, MSE=56728.43 ## ## Node number 13: 27 observations ## mean=968.0137, MSE=232813.5 ## ## Node number 14: 9 observations ## mean=1174.492, MSE=45966.15 ## ## Node number 15: 14 observations ## mean=1661.386, MSE=459348.1 ## ## Node number 20: 149 observations ## mean=131.6958, MSE=5364.493 ## ## Node number 21: 108 observations ## mean=272.1258, MSE=8780.789 In our analysis, the decision tree model started with 4431 observations and successfully split them into child nodes based on certain criteria, resulting in a deviance of 107497100.0 and a predicted value of 63.84701 for the root node. The complexity of interpreting the tree is compounded by the large number of levels in some of the attribute variables, which makes it challenging to understand the model’s decision-making process using this tree. One particular challenge arises from the inclusion of a large number of wine names as features in the decision tree model. The sheer volume of features and their unique names can make interpretation difficult. However, by using feature importance, we can gain insight into which wine attributes, i.e. features are most influential in predicting the target variable. Features with higher importance values are more critical in the model’s decision-making process. Understanding feature importance is crucial in data science as it not only helps prioritise efforts but also enhances the transparency of the model. By identifying the most impactful factors, we can improve the interpretability of the model’s results and make our data science reports more informative and actionable. &gt; # Get feature importance &gt; importance &lt;- varImp(tree_model) &gt; &gt; # Print the feature importance &gt; print(importance) ## Overall ## body 0.02788812 ## num_reviews 0.90531862 ## rating 0.97093847 ## region 0.63286356 ## type 0.38024180 ## wine 2.34867729 ## winery 2.26305636 ## year 1.15760534 ## acidity 0.00000000 The feature importance scores provide insight into the relative importance of each feature in the model for predicting the target variable. Features with higher importance scores have more influence on accurate predictions. In this case, the wine feature has the highest importance score of 2.3487, indicating its significant impact on predictions. The winery feature follows closely behind with a score of 2.2631, followed by rating with a score of 0.9709, and year with a score of 1.1576. Conversely, features with lower importance scores, such as body and acidity, have less influence on predictions. We realise that interpreting a decision tree with a large number of features like wine names requires careful analysis and consideration of the unique characteristics of the dataset. Features such as wine names can lead to a high-dimensional feature space, making it challenging to visualise and comprehend the decision-making process of the model. It is important to understand the underlying relationships between features and the target variable to ensure the model’s predictions are reliable and actionable. Report Conclusion The proposed tree_model offers valuable insights into the dataset, highlighting key features like wine, winery, rating, and year that significantly influence predictions. However, the model’s complexity, particularly with a large number of levels in some attribute variables, poses challenges for interpretation. Future models could benefit from simplification techniques or alternative algorithms to improve interpretability. Additionally, while the feature importance analysis provides useful information, it would be beneficial to explore the interaction effects between features, which could further enhance the model’s predictive power. Furthermore, the high-dimensional feature space created by including a large number of wine names as features underscores the importance of feature selection techniques to identify the most relevant features for the model. Overall, future modelling efforts could focus on improving interpretability, exploring feature interactions, and refining feature selection processes to build more robust and insightful models. Key Insights In the case study of enhancing data science projects with ChatGPT using the wine_data dataset in R, several positives and negatives emerged. On the positive side, ChatGPT proved to be a useful tool in facilitating data exploration and preprocessing, contributing to the cleaning and preparation of datasets. Additionally, ChatGPT’s ability to summarise findings, create visualisations, and generate reports enhanced the communication of results and insights. However, there are some considerations to note. ChatGPT’s effectiveness may be limited by the quality and quantity of the data it is trained on, potentially leading to biased or inaccurate recommendations. Furthermore, while ChatGPT can assist in selecting machine learning algorithms, it may not always suggest the most optimal approach for every dataset. Therefore, it is essential to use ChatGPT as a supportive tool rather than relying solely on its suggestions, ensuring a balanced approach to data science project development. Despite these considerations, integrating ChatGPT into the data science workflow can bring valuable insights, streamline processes, and ultimately enhance the overall success of the project. "],["your-turn.html", " Your Turn!", "  Your Turn! Conduct a data science analysis using the Used Cars Dataset available from Kaggle and apply the ideas presented in this case study “Enhancing Data Science Projects with ChatGPT: A Case Study in R.” Begin by exploring and preprocessing the dataset with ChatGPT’s assistance, focusing on data cleaning and preparation. Utilise ChatGPT to guide the selection of machine learning algorithms and techniques based on the project’s objectives and dataset characteristics. During model development, use ChatGPT to interpret complex models, such as decision trees, by providing explanations and insights into the model’s predictions. Finally, leverage ChatGPT to communicate the results and insights effectively, whether through summarising findings, creating visualisations, or generating reports. Evaluate the impact of integrating ChatGPT into your workflow and note any positives and negatives that emerge from this approach. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
