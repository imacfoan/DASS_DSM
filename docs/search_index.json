[["index.html", "Data Science Modelling About", " Data Science Modelling Dr. Ioana Macoveciuc About Welcome to SOST70033 Data Science Modelling! This notebook will host the materials for all R practical tasks for this course unit. The notebook follows the same section-based structure as the learning materials on Blackboard. To access this notebook, you can bookmark it like any other website in your favourite web browser. For each section, you will have at least one practical to complete and each of these can be accessed by using the sidebar menu on the left hand side of your screen. Clicking on the headings in each section will expand the menu and each task can be accessed individually. The sidebar menu can be toggled on and off by clicking on the Toggle Sidebar button. Other customisation options include changing the font, font size, and the appearance. You also have a handy search button. This notebook is also designed to work well on tablet and mobile devices; to enhance your experience on these devices, it is recommended that you hide the sidebar menu and navigate across sections and subsections using the right and left arrows. The code, as well as the output and answers, have interactive buttons which can reveal or hide the content. One additional feature of the R code is that it can be copied and pasted directly in your R console or script by clicking on the following icon: 1: Before beginning, it is recommended that you create a RStudio project for this course and work through the exercises and tasks in each section using this project. 2: You should write and save your answers to the exercises and tasks in R scripts. You should have at least one R script for each course section. 3: The recommended approach for a ‘clean’ working directory is to place all the data files you plan to use in a separate folder (e.g. a folder called data) within your R project working directory. You should always use simple names that allow you easy access to the contents when you want to either explore the folder on your machine or specify the path to these folders in R. 4: The answers to all tasks are also provided in this notebook. To build a robust knowledge basis and adequately develop your practical programming skills, it is absolutely essential that you first attempt all tasks and exercises on your own before comparing your answers with those provided in this notebook. "],["overview.html", "Overview", " Overview Section 1: Introduction to Data Science - The Basics of Statistical Learning This section is comprised of two practicals, both of which will use exercises adapted from the core textbook for this course: James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd ed. New York: Springer. https://www.statlearning.com/ Learning Outcomes: indexing using base R; creating scatterplot matrices; creating new variables; transforming existing variables; using functionals; ‘calling’ on masked functions from specific packages; translating base R code to tidyverse and vice versa. Throughout these two practicals, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. You can access the R documentation in the Help tab using? (e.g. ?read.csv) Function Description Package read.csv() read csv files base R read_csv() read csv files tidyverse co l umn_to_rownames() convert column to row names tidyverse rownames() obtain names of rows base R summary() obtain summary statistics base R summarise() object summaries tidyverse (dplyr) group_by() group by one or more variables tidyverse (dplyr) pairs() produce a matrix of scatterplots base R plot() create a plot base R ggplot() generic function for creating a plot tidyverse (ggplot2) mutate() create, modify, and delete columns tidyverse (dplyr) if_else() condition-based function tidyverse (dplyr) as_factor() create factor using existing levels tidyverse (forcats) par() set graphical parameters base R mfrow() par() parameter base R slice_min() and slice_max() index rows by location (smallest and largest values of a variable respectively) tidyverse (dplyr) sapply() applying a function over list or vector base R select() keep or drop columns tidyverse (dplyr) note that this function is also available through the MASS package (we will not cover this in this section) pivot_longer() lengthen data tidyverse (tidyr) where() selection helper tidyverse median(), mean(), sd() median, mean, standard deviation base R "],["practical-1.html", "Practical 1", " Practical 1 For the tasks below, you will require the College dataset from the core textbook (James et. al 2021). Click here to download the file: College.csv . Remember to place your data file in a separate subfolder within your R project working directory. This data file contains 18 variables for 777 different universities and colleges in the United States. The variables are: Private : Public/private indicator Apps : Number of applications received Accept : Number of applicants accepted Enroll : Number of new students enrolled Top10perc : New students from top 10% of high school class Top25perc : New students from top 25% of high school class F.Undergrad : Number of full-time undergraduates P.Undergrad : Number of part-time undergraduates Outstate : Out-of-state tuition Room.Board : Room and board costs Books : Estimated book costs Personal : Estimated personal spending PhD : Percent of faculty with Ph.D.’s Terminal : Percent of faculty with terminal degree S.F.Ratio: Student/faculty ratio perc.alumni : Percent of alumni who donate Expend : Instructional expenditure per student Grad.Rate : Graduation rate Task 1 Import the dataset in an object called college using a suitable tidyverse function. Code # Remember to load tidyverse first library(tidyverse) college &lt;- read_csv(&quot;data/College.csv&quot;) If you have a look at the contents of the data object using View(), you will notice that the first column contains the names of all of the universities in the dataset. You will also notice that it has a strange name. Actually, these data should not be treated as a variable (column) since it is just a list of university names. Task 2 Keeping the list of names in the data object, transform this column such that the university names in the column become row names. Hint: use the column_to_rownames() function from dplyr. Code college &lt;- college %&gt;% column_to_rownames(var = &quot;...1&quot;) How would have your approach to this task differed if you would have imported the dataset using base R? Try it! The data file could have instead been imported using read.csv(): college &lt;- read.csv(\"data/College.csv\") Using the base R approach, the first column containing the university names would have been named “X”, as shown below using View(). Now, how would be go about transforming the contents of the first column into row names? This would require two steps. First, we assign the column contents to rows names. rownames(college) &lt;- college[, 1] If you have another look at the data object, you will see that the rows have now been renamed using the university names in the “X” column, but the column is still part of the dataset. We therefore need to tell R to delete the column. college &lt;- college[, -1] Task 3 Produce summary statistics for all variables in the data object. Code summary(college) ## Private Apps Accept Enroll ## Length:777 Min. : 81 Min. : 72 Min. : 35 ## Class :character 1st Qu.: 776 1st Qu.: 604 1st Qu.: 242 ## Mode :character Median : 1558 Median : 1110 Median : 434 ## Mean : 3002 Mean : 2019 Mean : 780 ## 3rd Qu.: 3624 3rd Qu.: 2424 3rd Qu.: 902 ## Max. :48094 Max. :26330 Max. :6392 ## Top10perc Top25perc F.Undergrad P.Undergrad ## Min. : 1.00 Min. : 9.0 Min. : 139 Min. : 1.0 ## 1st Qu.:15.00 1st Qu.: 41.0 1st Qu.: 992 1st Qu.: 95.0 ## Median :23.00 Median : 54.0 Median : 1707 Median : 353.0 ## Mean :27.56 Mean : 55.8 Mean : 3700 Mean : 855.3 ## 3rd Qu.:35.00 3rd Qu.: 69.0 3rd Qu.: 4005 3rd Qu.: 967.0 ## Max. :96.00 Max. :100.0 Max. :31643 Max. :21836.0 ## Outstate Room.Board Books Personal ## Min. : 2340 Min. :1780 Min. : 96.0 Min. : 250 ## 1st Qu.: 7320 1st Qu.:3597 1st Qu.: 470.0 1st Qu.: 850 ## Median : 9990 Median :4200 Median : 500.0 Median :1200 ## Mean :10441 Mean :4358 Mean : 549.4 Mean :1341 ## 3rd Qu.:12925 3rd Qu.:5050 3rd Qu.: 600.0 3rd Qu.:1700 ## Max. :21700 Max. :8124 Max. :2340.0 Max. :6800 ## PhD Terminal S.F.Ratio perc.alumni ## Min. : 8.00 Min. : 24.0 Min. : 2.50 Min. : 0.00 ## 1st Qu.: 62.00 1st Qu.: 71.0 1st Qu.:11.50 1st Qu.:13.00 ## Median : 75.00 Median : 82.0 Median :13.60 Median :21.00 ## Mean : 72.66 Mean : 79.7 Mean :14.09 Mean :22.74 ## 3rd Qu.: 85.00 3rd Qu.: 92.0 3rd Qu.:16.50 3rd Qu.:31.00 ## Max. :103.00 Max. :100.0 Max. :39.80 Max. :64.00 ## Expend Grad.Rate ## Min. : 3186 Min. : 10.00 ## 1st Qu.: 6751 1st Qu.: 53.00 ## Median : 8377 Median : 65.00 ## Mean : 9660 Mean : 65.46 ## 3rd Qu.:10830 3rd Qu.: 78.00 ## Max. :56233 Max. :118.00 Task 4 Create a scatterplot matrix of the first three numeric variables. Code pairs(college[,2:4]) Task 5 Produce side by side box plots of Outstate versus Private using base R. Code plot(college$Private, college$Outstate, xlab = &quot;Private&quot;, ylab = &quot;Outstate&quot;) Did this work? Why? Using the plot() base R function to produce a box plot would produce an error since the Private variable is of class character. Most statistical functions will not work with character vectors. Error in plot.window(...) : need finite 'xlim' values In addition: Warning messages: 1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion 2: In min(x) : no non-missing arguments to min; returning Inf 3: In max(x) : no non-missing arguments to max; returning -Inf Creating a box plot with tidyverse would work. college %&gt;% ggplot(aes(x = Private, y = Outstate)) + geom_boxplot() However, it is important to note that if a variable is not of the right class, then this might have unintended consequences for example, when building models. In this case, the Private variable must be transformed into a factor. Task 6 Using the Top10perc variable, create a new categorical variable called Elite such that universities are divided into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%. Hint: use a combination of mutate() and if_else(). Code college &lt;- college %&gt;% mutate(Elite = if_else(Top10perc &gt; 50, &quot;Yes&quot;, &quot;No&quot;), Elite = as_factor(Elite)) #do not forget the factor transformation step (categorical variables are factors in R) Task 7 Produce side by side box plots of the new Elite variable and Outstate. Code college %&gt;% ggplot(aes(x = Elite, y = Outstate)) + geom_boxplot() How would you produce a similar plot using base R? plot(college$Elite, college$Outstate, xlab = \"Elite\", ylab = \"Outstate\") Task 8 Use base R to produce a multipanel plot that displays histograms of the following variables: Apps, perc.alumni, S.F.Ratio, Expend. Hint: use par(mfrow=c(2,2)) to set up a 2x2 panel. Try to adjust the specifications (e.g. breaks). Code # An example is shown below. Note that the purpose of the mfrow parameter is to change the default way in which R displays plots which is in a single panel display. Once applied, all plots you create later will also be displayed in a 2x2 grid. To revert back, you need to enter par(mfrow=c(1,1)) into the console. par(mfrow=c(2,2)) hist(college$Apps) hist(college$perc.alumni, col=2) hist(college$S.F.Ratio, col=3, breaks=10) hist(college$Expend, breaks=100) Task 9 Using Accept and Apps, create a new variable that describes acceptance rate. Name this variable acceptance_rate. Hint: use mutate(). Code college &lt;- college %&gt;% mutate(acceptance_rate = Accept / Apps) Task 10 Using the acceptance_rate variable, find out which university has the lowest acceptance rate. Hint: for a tidyverse approach, you can use slice_min(). Code college %&gt;% slice_min(order_by = acceptance_rate, n = 1) ## Private Apps Accept Enroll Top10perc Top25perc ## Princeton University Yes 13218 2042 1153 90 98 ## F.Undergrad P.Undergrad Outstate Room.Board Books Personal ## Princeton University 4540 146 19900 5910 675 1575 ## PhD Terminal S.F.Ratio perc.alumni Expend Grad.Rate Elite ## Princeton University 91 96 8.4 54 28320 99 Yes ## acceptance_rate ## Princeton University 0.1544863 Task 11 Using the acceptance_rate variable, find out which university has the highest acceptance rate. Code college %&gt;% slice_max(order_by = acceptance_rate, n = 1) ## Private Apps Accept Enroll Top10perc Top25perc ## Emporia State University No 1256 1256 853 43 79 ## Mayville State University No 233 233 153 5 12 ## MidAmerica Nazarene College Yes 331 331 225 15 36 ## Southwest Baptist University Yes 1093 1093 642 12 32 ## University of Wisconsin-Superior No 910 910 342 14 53 ## Wayne State College No 1373 1373 724 6 21 ## F.Undergrad P.Undergrad Outstate Room.Board ## Emporia State University 3957 588 5401 3144 ## Mayville State University 658 58 4486 2516 ## MidAmerica Nazarene College 1100 166 6840 3720 ## Southwest Baptist University 1770 967 7070 2500 ## University of Wisconsin-Superior 1434 417 7032 2780 ## Wayne State College 2754 474 2700 2660 ## Books Personal PhD Terminal S.F.Ratio ## Emporia State University 450 1888 72 75 19.3 ## Mayville State University 600 1900 68 68 15.7 ## MidAmerica Nazarene College 1100 4913 33 33 15.4 ## Southwest Baptist University 400 1000 52 54 15.9 ## University of Wisconsin-Superior 550 1960 75 81 15.2 ## Wayne State College 540 1660 60 68 20.3 ## perc.alumni Expend Grad.Rate Elite ## Emporia State University 4 5527 50 No ## Mayville State University 11 6971 51 No ## MidAmerica Nazarene College 20 5524 49 No ## Southwest Baptist University 13 4718 71 No ## University of Wisconsin-Superior 15 6490 36 No ## Wayne State College 29 4550 52 No ## acceptance_rate ## Emporia State University 1 ## Mayville State University 1 ## MidAmerica Nazarene College 1 ## Southwest Baptist University 1 ## University of Wisconsin-Superior 1 ## Wayne State College 1 "],["practical-2.html", "Practical 2", " Practical 2 For the tasks below, you will require the Boston dataset. This dataset is part of the MASS R package. To access the dataset, load the MASS package (install the package first, if you have not done so previously). Task 1 Install and load the MASS package. You will also require tidyverse. Code # if you haven&#39;t already, install the MASS package before loading install.packages(&quot;MASS&quot;) library(MASS) library(tidyverse) Does R provide any message when loading MASS? Why does this matter? One important message that R provides when loading MASS is that this package masks the select() function from tidyverse. Attaching package: ‘MASS’ The following object is masked from ‘package:dplyr’: `select` When masking occurs, this means that both packages contain the same function. If you were to use the select() function, R will call the function from the MASS package, rather than from tidyverse (dplyr) package. This is because the MASS package is the one masking the function. If you intend to use the select() function as defined by the tidyverse package, it may not work as intended and/or you may be prompted by an error message such as: Error in select(...): unused argument (...) To avoid such issues, you must ‘call’ on the package from which you want R to use the masked function (e.g. dplyr::select()). This is why it is important to read through all warnings and messages provided in the console. Task 2 Find out more about the Boston dataset variables by accessing the R Documentation. Code ?Boston To explore the Boston dataset, simply type the name of the data object into the console or use View() What type of data structure is the Boston dataset? What are its dimensions? How many categorical and quantitative variables are there? The Boston dataset is a data frame with 506 rows (observations) and 14 columns (variables). There is one categorical variable (chas), and 13 quantitative variables. Task 3 Find the class of all 14 variables. Hint: use sapply. Code sapply(Boston, class) ## crim zn indus chas nox rm age dis ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;integer&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## rad tax ptratio black lstat medv ## &quot;integer&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; Task 4 Using a tidyverse approach, calculate the mean, median, and standard deviation of all variables of class numeric. Code Boston %&gt;% dplyr::select(dplyr::where(is.numeric)) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(Mean = mean(value, na.rm = TRUE), SD = sd(value, na.rm = TRUE), Median = median(value, na.rm = TRUE)) ## # A tibble: 14 × 4 ## name Mean SD Median ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 age 68.6 28.1 77.5 ## 2 black 357. 91.3 391. ## 3 chas 0.0692 0.254 0 ## 4 crim 3.61 8.60 0.257 ## 5 dis 3.80 2.11 3.21 ## 6 indus 11.1 6.86 9.69 ## 7 lstat 12.7 7.14 11.4 ## 8 medv 22.5 9.20 21.2 ## 9 nox 0.555 0.116 0.538 ## 10 ptratio 18.5 2.16 19.0 ## 11 rad 9.55 8.71 5 ## 12 rm 6.28 0.703 6.21 ## 13 tax 408. 169. 330 ## 14 zn 11.4 23.3 0 What is the mean pupil-teacher ratio? What is the median and mean per capita crime rate? Which value do you think is more suitable to describe per capita crime rate? The mean pupil-teacher ratio is about 19. The median crime rate is 0.257 whilst the mean is larger at 3.61. Given the difference between the median and the mean, a skewed distribution is expected, therefore, the median may be a more a suitable summary statistic to describe crime rate (a histogram would be needed) Task 5 Using a base R approach, create a 2x2 multipanel plot of crim versus age, dis, rad, tax and ptratio. Code par(mfrow = c(2,2)) plot(Boston$age, Boston$crim) plot(Boston$dis, Boston$crim) plot(Boston$rad, Boston$crim) plot(Boston$tax, Boston$crim) What can you say about the relationships between age, dis, rad, tax, and crim? As the age of the home increases (age), crime also increases. There is also higher crime around employment centers (dis). With very high index of accessibility to radial highways (rad), and tax rates (tax) there also appears to be high crime rates. Task 6 Using a base R approach, create and display histograms of crim, tax and ptratio in a 1x2 multipanel plot. Set the breaks argument to 25 . Code par(mfrow=c(1,2)) hist(Boston$crim, breaks=25) hist(Boston$tax, breaks=25) What do these histograms indicate? Most areas have low crime rates, but there is a rather long tail showing high crime rates (although the frequency seems to be very low). Given the degree of skew, the mean would not be a good measure of central tendency. With respect to tax rates, there appears to be a large divide between low taxation and high taxation, with the highest peak at around 670. Remember to revert back to single panel display. "],["overview-1.html", "Overview", " Overview Section 2: Linear Regression and Prediction The practical in this section will make use of exercises adapted from the core textbook for this course: James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd ed. New York: Springer. https://www.statlearning.com/ Learning Outcomes: - indexing using base R; You will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. Function Description Package lm() fit linear models base R predict() generic function for predictions from results of different models (e.g. predict.lm()) base R plot() generic function for plotting base R abline() adding one or more straight lines to plot base R cor() computes correlation between variables base R rnorm() generates normal distribution base R poly() returns or evaluates polynomials base R "],["practical-1-1.html", "Practical 1 Task 1 Task 2 Task 3 Task 4 Task 5 Task 6 Task 7 Task 8 Task 9 Task 10 Task 11 Task 11 Task 12 Task 13 Task 14 Task 15", " Practical 1 For the tasks below, you will require the Auto dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Auto dataset loads automatically: library(ISLR2) Remember to install it first install.packages(\"ISLR2\") This data file (text format) contains 398 observations of 9 variables. The variables are: mpg: miles per gallon cylinders: Number of cylinders between 4 and 8 displacement: Engine displacement (cu. inches) horsepower: Engine horsepower weight: Vehicle weight (lbs.) acceleration: Time to accelerate from 0 to 60 mph (sec.) year: Model year origin: Origin of car (1. American, 2. European, 3. Japanese) name: Vehicle name Task 1 Use the lm() function to perform simple linear regression with mpg as the response and horsepower as the predictor. Store the output in an object called fit. Code fit &lt;- lm(mpg ~ horsepower, data = Auto) Task 2 Have a look at the results of the model. Code summary(fit) ## ## Call: ## lm(formula = mpg ~ horsepower, data = Auto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.5710 -3.2592 -0.3435 2.7630 16.9240 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.935861 0.717499 55.66 &lt;2e-16 *** ## horsepower -0.157845 0.006446 -24.49 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.906 on 390 degrees of freedom ## Multiple R-squared: 0.6059, Adjusted R-squared: 0.6049 ## F-statistic: 599.7 on 1 and 390 DF, p-value: &lt; 2.2e-16 Is there a relationship between the predictor and the response? The slope coefficient (-0.157845) is statistically significant (&lt;2e-16 ***). We can conclude that there is evidence to suggest a negative relationship between miles per gallon and engine horsepower. For a one-unit increase in engine horsepower, miles per gallon are reduced by 0.16. Task 3 What is the associated 95% confidence intervals for predicted miles per gallon associated with an engine horsepower of 98? Hint: use the predict() function. For confidence intervals, set the interval argument to confidence. Code predict(fit, data.frame(horsepower = 98), interval = &quot;confidence&quot;) ## fit lwr upr ## 1 24.46708 23.97308 24.96108 Task 4 How about the prediction interval for the same value? Code predict(fit, data.frame(horsepower = 98), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 24.46708 14.8094 34.12476 Are the two intervals different? Why? The prediction interval (lower limit 14.8094 and upper limit 34.12476) is wider (and therefore less precise) than the confidence interval (lower limit 23.97308 and upper limit 24.96108). The confidence interval measures the uncertainty around the estimate of the conditional mean whilst the prediction interval takes into account not only uncertainty but also the variability of the conditional distribution. Task 5 Using base R, plot the response and the predictor as well as the least squares regression line. Add suitable labels to the X and Y axes. Code plot(Auto$horsepower, Auto$mpg, xlab = &quot;horsepower&quot;, ylab = &quot;mpg&quot;) abline(fit) Task 6 Use base R to produce diagnostic plots of the least squares regression fit. Display these in a 2X2 grid. Code par(mfrow = c(2, 2)) plot(fit, cex = 0.2) Task 7 Subset the Auto dataset such that it excludes the name and origin variables and store this subsetted dataset in a new object called quant_vars. Code quant_vars &lt;- subset(Auto, select = -c(name, origin)) Task 8 Compute a correlation matrix of all variables. Code cor(quant_vars) ## mpg cylinders displacement horsepower weight ## mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 ## cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 ## displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 ## horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 ## weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 ## acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 ## year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 ## acceleration year ## mpg 0.4233285 0.5805410 ## cylinders -0.5046834 -0.3456474 ## displacement -0.5438005 -0.3698552 ## horsepower -0.6891955 -0.4163615 ## weight -0.4168392 -0.3091199 ## acceleration 1.0000000 0.2903161 ## year 0.2903161 1.0000000 Did you use the Auto dataset or the quant_vars object? Why does it matter which data object you use? To compute the correlation matrix using all variables of a data object, these variables must all be numeric. In the Auto data object, the name variable is coded as a factor. class(Auto$name) [1] \"factor\" Therefore, if you try to use the cor() function with Auto dataset without excluding the name variable, you will get an error. cor(Auto) Error in cor(Auto) : 'x' must be numeric. Also, whilst the origin variable is of class integer and will not pose a problem when you apply the cor() function, you’ll remember from the variable description list that this is a nominal variable with its categories numerically labelled. Compute the correlation matrix using quant_vars. Task 9 Using the quant_vars object, perform multiple linear regression with miles per gallon as the response and all other variables as the predictors. Store the results in an object called fit2. Code fit2 &lt;- lm(mpg ~ ., data = quant_vars) Task 10 Have a look at the results of the multiple regression model. Code summary(fit2) ## ## Call: ## lm(formula = mpg ~ ., data = quant_vars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.6927 -2.3864 -0.0801 2.0291 14.3607 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.454e+01 4.764e+00 -3.051 0.00244 ** ## cylinders -3.299e-01 3.321e-01 -0.993 0.32122 ## displacement 7.678e-03 7.358e-03 1.044 0.29733 ## horsepower -3.914e-04 1.384e-02 -0.028 0.97745 ## weight -6.795e-03 6.700e-04 -10.141 &lt; 2e-16 *** ## acceleration 8.527e-02 1.020e-01 0.836 0.40383 ## year 7.534e-01 5.262e-02 14.318 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.435 on 385 degrees of freedom ## Multiple R-squared: 0.8093, Adjusted R-squared: 0.8063 ## F-statistic: 272.2 on 6 and 385 DF, p-value: &lt; 2.2e-16 Is there a relationship between the predictors and the response? Which predictors appear to have a statistically significant relationship to the response? What does the coefficient for the year variable suggest? Two of the predictors are statistically significant: weight and year. The relationship between weight and mpg is negative which suggests that for a one pound increase in weight of vehicle, the number of miles per gallon the vehicle can travel decreases, whilst that of mpg and year is positive which suggests that the more recent the vehicle is, the higher the number of miles per gallon it can travel. Task 11 Produce diagnostic plots of the multiple linear regression fit in a 2x2 grid. Code par(mfrow = c(2, 2)) plot(fit2, cex = 0.2) Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage? One point has high leverage, the residuals also show a trend with fitted values. Task 11 Fit separate linear regression models with interaction effect terms for: weight and horsepower, acceleration and horsepower, and cylinders and weight. Code summary(lm(mpg ~ . + weight:horsepower, data = quant_vars)) summary(lm(mpg ~ . + acceleration:horsepower, data = quant_vars)) summary(lm(mpg ~ . + cylinders:weight, data = quant_vars)) Are any of the interaction terms statistically significant? For each model, the interaction term is statistically significant. Task 12 Using the Auto data object, apply transformations for the horsepower variable and plot the relationship between horsepower and mpg in a 2x2 grid. First plot: use the original variable; Second plot: apply log transform; Third plot: raise it to the power of two. Code par(mfrow = c(2, 2)) plot(Auto$horsepower, Auto$mpg, cex = 0.2) plot(log(Auto$horsepower), Auto$mpg, cex = 0.2) plot(Auto$horsepower ^ 2, Auto$mpg, cex = 0.2) Which of these transformations is most suitable? The relationship between horsepower and miles per gallon is clearly non-linear (plot 1). The log transform seems to address this best. Task 13 Now run a multiple regression model with all variables as before but this time, apply a log transformation of the horsepower variable. Code quant_vars$horsepower &lt;- log(quant_vars$horsepower) fit3 &lt;- lm(mpg ~ ., data = quant_vars) Task 14 Have a look at the results. Code summary(fit3) ## ## Call: ## lm(formula = mpg ~ ., data = quant_vars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.6778 -2.0080 -0.3142 1.9262 14.0979 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.1713000 8.9291383 3.267 0.00118 ** ## cylinders -0.3563199 0.3181815 -1.120 0.26347 ## displacement 0.0088277 0.0068866 1.282 0.20066 ## horsepower -8.7568129 1.5958761 -5.487 7.42e-08 *** ## weight -0.0044304 0.0007213 -6.142 2.03e-09 *** ## acceleration -0.3317439 0.1077476 -3.079 0.00223 ** ## year 0.6979715 0.0503916 13.851 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.308 on 385 degrees of freedom ## Multiple R-squared: 0.8231, Adjusted R-squared: 0.8203 ## F-statistic: 298.5 on 6 and 385 DF, p-value: &lt; 2.2e-16 How do the results of model fit3 differ from those of model fit2? The fit2 model results showed that only two predictors were statistically significant: weight and year. The fit3 model has two additional predictors that are statistically significant: acceleration as well as horsepower.Also, the coefficient values can now be interpreted more easily. Task 15 Produce diagnostic plots for the fit3 object and display them in a 2x2 grid. Code par(mfrow = c(2, 2)) plot(fit3, cex = 0.2) How do the diagnostic plots differ? A log transformation of horsepower appears to give a more linear relationship with mpg but this difference does not seem to be substantial. "],["practical-2-1.html", "Practical 2 Task 1 Task 2 Task 3 Task 4 Task 5", " Practical 2 For the tasks below, you will require the Carseats dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Carseats dataset loads automatically: library(ISLR2) This dataframe object contains a simulated dataset of sales of child car seats at 400 different stores. The 9 variables are: Sales: Unit sales (thousands of dollars) at each location CompPrice: Price charged by competitor at each location Income: Community income level (thousands of dollars) Advertising: Local advertising budget for company at each location (thousands of dollars) Population: Population size in region (thousands of dollars) Price: Price company charges for car seats at each site ShelveLoc: Quality of the shelving location for the car seats at each site Age: Average age of the local population Education: Education level at each location Urban: Whether the store is in an urban or rural location US: Whether the store is in the US or not Task 1 Fit a multiple regression model to predict unit sales at each location based on price company charges for car seats, whether the store is in an urban or rural location, and whether the store is in the US or not. Code fit &lt;- lm(Sales ~ Price + Urban + US, data = Carseats) Task 2 Have a look at the results and interpret the coefficients. Code summary(fit) ## ## Call: ## lm(formula = Sales ~ Price + Urban + US, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9206 -1.6220 -0.0564 1.5786 7.0581 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.043469 0.651012 20.036 &lt; 2e-16 *** ## Price -0.054459 0.005242 -10.389 &lt; 2e-16 *** ## UrbanYes -0.021916 0.271650 -0.081 0.936 ## USYes 1.200573 0.259042 4.635 4.86e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.472 on 396 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2335 ## F-statistic: 41.52 on 3 and 396 DF, p-value: &lt; 2.2e-16 Which coefficients are statistically significant? What do they indicate? The null hypothesis for the slope being zero is rejected for the Price and US variables. The coefficient for Price is statistically significant; since it is negative, as price increases by a thousand dollars (i.e. one unit increase), the sales of child decrease by about 0.05. The slope for the US variable is also statistically significant but it is positive. Also, this is a binary factor variable coded as Yes and No (No is the reference category). Therefore, sales of child car seats are higher by 1.2 for car seat products that are produced in the US than for car seat products not produced in the US. Task 3 Based on the conclusions you have drawn in Task 2, now fit a smaller model that only uses the predictors for which there is evidence of association with sales. Code fit2 &lt;- lm(Sales ~ Price + US, data = Carseats) Task 4 Compare the two models (fit and fit2). Code anova(fit, fit2) ## Analysis of Variance Table ## ## Model 1: Sales ~ Price + Urban + US ## Model 2: Sales ~ Price + US ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 396 2420.8 ## 2 397 2420.9 -1 -0.03979 0.0065 0.9357 Which model is the better fit? They have similar r-squared values, and the fit model (containing the extra variable Urban) is non-significantly better. Task 5 Produce diagnostic plots for fit2 and display these in a 2x2 grid. Code par(mfrow = c(2, 2)) plot(fit2, cex = 0.2) Is there evidence of outliers or high leverage observations in the fit2 model? Yes, there is evidence of outliers and high leverage observations for fit2. "],["practical-3.html", "Practical 3 0.1 Part I - Simple Linear Models Without Intercept 0.2 Part II - Simple Linear Models with Intercept 0.3 Part III - Noise", " Practical 3 In this practical, you will run simulations to dive deeper into the principles behind linear models. 0.1 Part I - Simple Linear Models Without Intercept Let’s investigate the t-statistic for the null hypothesis \\(H_{0} : \\beta = 0\\) in simple linear regression without an intercept. The formula would therefore be \\(y = \\beta x\\). By excluding the intercept, the model is constrained to pass through the origin \\((0,0)\\), allowing the relationship between the response and predictor to be interpreted as proportional. In other words, the removal of the intercept forces the regression line to start at \\((0,0)\\), so when \\(X = 0\\), then \\(y = 0\\). Let’s first generate some data for a predictor x and a response y. We select a seed to ensure that we generate the same data every time. To generate values, we use the rnorm function to produce 100 data values drawn from a normal distribution (hence rnorm()). Code set.seed(1) x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100) Now that we generated our predictor and our response variable, let’s run a simple linear regression of y onto x, without an intercept. One way to do so is by adding 0 into the formula. Code fit &lt;- lm(y ~ x + 0) And now, let’s have a look at the results. Code coef(summary(fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## x 1.993876 0.1064767 18.72593 2.642197e-34 We can see a significant positive relationship between y and x. The coefficient estimate for x is 1.993876, and since the relationship betweeen x and y is proportional, we interpret the estimate as the y values being predicted to be (a little below) twice the x values. But what happens if we run a regression of x onto y? Code fit2 &lt;- lm(x ~ y + 0) coef(summary(fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## y 0.3911145 0.02088625 18.72593 2.642197e-34 We again observe a significant positive relationship between x and y, except that the x values are predicted to be (a little below) half the y values (since the coefficient estimate is 0.3911145). Note also the t-values for the two models. They are identical and of course so is the p-value (therefore, there is a significant relationships between x and y). Therefore, the results of the models of y onto x and x onto y indicate that the coefficients would be the inverse of each other (2 and 1/2) whilst the t-statistic values (and p-values) remain the same. Why are the t-statistic values identical? For each coefficient, the t statistic is calculated by dividing the coefficient estimate by its standard error. For example, for the fit2 model, we have a coefficient estimate of 0.3911145 and a standard error of 0.02088625 and so dividing 0.3911145 by 0.02088625 gives us 18.72593. You’ll also remember that the correlation coefficient between two variables is symmetric and so the correlation between X and Y is the same as for Y and X. This is the reason why it is incorrect to state that “X causes a change in Y”. In a linear model, we are testing whether there is a linear association between X and Y but not if X causes Y or Y causes X. Therefore, irrespective of whether we are regressing Y onto X or X onto Y, the t-statistic is testing the same null hypothesis \\(H_{0} : \\beta = 0\\) (i.e. fundamentally, it is testing whether there is a linear correlation between X and Y). So what exactly is the role of the intercept? As you already know, the intercept represents the value of y when x is 0 which can be thought of as the initial value effect that exists independently of x. This not only applies to the simple linear regression model but also to the multiple linear regression model (i.e. the intercept is the value of Y when all predictors are zero). In other words, the intercept adjusts the starting point of regression line and allows for the line to shift up or down on the y-axis thus reflecting a “baseline” level of y that is not dependent on x. With an intercept, the slope coefficient still tells us how much Y changes with a one-unit change in X, but this change is relative to the y value when x is zero and this is important when x can take a value of 0 that is meaningful to the model. Without the intercept, the line is forced to pass through the origin \\((0,0)\\), which may not be suitable unless the data naturally begin at zero (or there are some other theoretical or practical reasons which warrant the line passing through the origin). With an intercept, the regression line is no longer forced to pass through zero (and will only do so if the data naturally begin at zero). The intercept therefore allows for the regression line to better fit the data, particularly when the data do not actually begin at zero. In this way, the model can capture the average outcome when the predictor(s) is/are zero. 0.2 Part II - Simple Linear Models with Intercept Do you think that the t-statistic will be the same for both regression of Y onto X and X onto Y if we were to include the intercept? 0.2.1 Task 1 Use the same data as before and run a regression with Y as response and X as predictor. Store the results into an object called fit3. Code fit3 &lt;- lm(y ~ x) 0.2.2 Task 2 Extract the model coefficients from the summary results of the model. Code coef(summary(fit3)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.03769261 0.09698729 -0.3886346 6.983896e-01 ## x 1.99893961 0.10772703 18.5555993 7.723851e-34 How does coefficient for fit3 compare to fit? How about the t-statistic value? The coefficient for the model with the intercept is very similar to the coefficient for the model without the intercept. The t-statistic is also very close (18.72593 for the model without intercept and 18.5555993 for the model with the intercept). 0.2.3 Task 3 Now run a regression with X as response and Y as predictor. Store the results into an object called fit4. Code fit4 &lt;- lm(x ~ y) 0.2.4 Task 4 Extract the model coefficients from the summary results of the model. Code coef(summary(fit4)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.03880394 0.04266144 0.9095787 3.652764e-01 ## y 0.38942451 0.02098690 18.5555993 7.723851e-34 How does coefficient for fit4 compare to fit2? How about the t-statistic value? Are the t-statistic values different between the fit3 and fit4 models? The slope coefficient for the model with the intercept (0.38942451) is very similar to the coefficient for the model without the intercept (0.3911145) and so is the t-statistic. Also, as expected, the t-statistic value is identical to that of the fit3 model. Therefore, the t-statistic for simple regression of Y onto X is identical to the t-statistic for simple regression of X onto Y. 0.3 Part III - Noise How does variability affect the coefficients of a linear model? Let’s generate our “true” population data. We create a variable x with 100 observations drawn from a normal distribution. To be more specific about the characteristic of our variable x, we will not only specify the total number of observations (100), but also the mean (0), and standard deviation (1). This will be our predictor. Code set.seed(1) x_n &lt;- rnorm(100, 0, 1) Now let’s create another vector called eps containing 100 observations drawn from a normal distribution with a mean of zero and a variance of 0.25. This will be the error or epsilon. Code eps_n &lt;- rnorm(100, 0, sqrt(0.25)) using x_n and eps_n, we now generate a vector y_n according to the following formula: \\(Y = -1 + 0.5X + \\epsilon\\). Essentially, we specify our intercept, the slope coefficient, the predictor variable and the error to obtain our response variable. Code y_n &lt;- -1 + 0.5 * x_n + eps_n The values -1 and 0.5 represent the “true” coefficients for the intercept \\(\\beta_{0}\\) and slope \\(\\beta_{1}\\) respectively. Now we can create a scatterplot to observe the association between X and Y. Code plot(x_n, y_n) The plot indicates a linear relationship between X and Y. The relationship is clearly not perfectly linear due to noise. If we were to estimate these population parameters, to what degree do you think the estimated coefficients will differ from these true population parameters? Ok, so we have the variables we generated, so our predictor x_n and our response y_n and we run a regression model. Code fit5 &lt;- lm(y_n ~ x_n) summary(fit5) ## ## Call: ## lm(formula = y_n ~ x_n) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.93842 -0.30688 -0.06975 0.26970 1.17309 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.01885 0.04849 -21.010 &lt; 2e-16 *** ## x_n 0.49947 0.05386 9.273 4.58e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4814 on 98 degrees of freedom ## Multiple R-squared: 0.4674, Adjusted R-squared: 0.4619 ## F-statistic: 85.99 on 1 and 98 DF, p-value: 4.583e-15 The results of the model show an estimated slope coefficient (\\(\\hat{\\beta_{1}}\\)) for x_n of 0.49947. This is very close to the population value (\\(\\beta_{1}\\)) which is 0.5! We see a similar estimated value for the intercept (\\(\\hat{\\beta_{0}}\\)) which is -1.01885, again very close to the true value for the intercept (\\(\\beta_{0}\\)) which is -1! Therefore, if we were to plot the true regression line and the estimated regression line, we would see that the two are difficult to distinguish (given the similarity of the estimated and true values for the coefficients). Code plot(x_n, y_n) abline(fit5) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2), cex = 0.72, ) What if we were to fit a polynomial regression model? Would there be any evidence that adding a quadratic term improves the model fit? To add a polynomial term of degree two, we can use the poly base R function directly in the code for the model. Code fit6 &lt;- lm(y_n ~ poly(x_n, 2)) anova(fit6, fit5) ## Analysis of Variance Table ## ## Model 1: y_n ~ poly(x_n, 2) ## Model 2: y_n ~ x_n ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 97 22.257 ## 2 98 22.709 -1 -0.45163 1.9682 0.1638 Since the F-test is not statistically significant, there is no evidence that adding a quadratic term improves the model fit. Ok, so what do you think would happen if we were to reduce the noise in the data? Let’s generate new data but reduce the variance from 0.25 to 0.05. We keep the predictor X the same. Code set.seed(1) x_n2 &lt;- rnorm(100, 0, 1) But we reduce the error (so we reduce eps from 0.25 to 0.05). Code eps_n2 &lt;- rnorm(100, 0, sqrt(0.05)) using x_n2 and eps_n2, we again now generate a vector y_n2 according to the same formula as before: \\(Y = -1 + 0.5X + \\epsilon\\). Code y_n2 &lt;- -1 + 0.5 * x_n2 + eps_n2 We now build a new model called fit7 using the new data. Code fit7 &lt;- lm(y_n2 ~ x_n2) summary(fit7) ## ## Call: ## lm(formula = y_n2 ~ x_n2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.41967 -0.13724 -0.03119 0.12061 0.52462 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.00843 0.02169 -46.50 &lt;2e-16 *** ## x_n2 0.49976 0.02409 20.75 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2153 on 98 degrees of freedom ## Multiple R-squared: 0.8145, Adjusted R-squared: 0.8127 ## F-statistic: 430.4 on 1 and 98 DF, p-value: &lt; 2.2e-16 If we compare the results of fit7 with fit5, we can observe that the \\(R^{2}\\) value for fit7 is 0.8145, much higher than the \\(R^{2}\\) value for fit5 which is 0.4674. By plotting the data we can clearly see the reduced variability. Code plot(x_n2, y_n2) abline(fit7) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2), cex = 0.72 ) Ok, so what if we were to increase the noise in the data? "],["overview-2.html", "Overview", " Overview Section 3: This section is comprised ofXXXXXXXXXXXXXXXXpracticals, both of which will use exercises adapted from the core textbook for this course: James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd ed. New York: Springer. https://www.statlearning.com/ In this section, we will cover the following functions: Learning Outcomes: Throughout these two practicals, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. You can access the R documentation in the Help tab using ? (e.g. ?read.csv) Function Description Package read.csv() read csv files base R read_csv() read csv files tidyverse co lumn_to_rownames() convert column to row names tidyverse rownames() obtain names of rows base R summary() obtain summary statistics base R summarise() object summaries tidyverse (dplyr) group_by() group by one or more variables tidyverse (dplyr) pairs() produce a matrix of scatterplots base R plot() create a plot base R ggplot() generic function for creating a plot tidyverse (ggplot2) mutate() create, modify, and delete columns tidyverse (dplyr) if_else() condition-based function tidyverse (dplyr) as_factor() create factor using existing levels tidyverse (forcats) par() set graphical parameters base R mfrow() par() parameter base R slice_min() and slice_max() index rows by location (smallest and largest values of a variable respectively) tidyverse (dplyr) sapply() applying a function over list or vector base R select() keep or drop columns tidyverse (dplyr) note that this function is also available through the MASS package (we will not cover this in this section) pivot_longer() lengthen data tidyverse (tidyr) where() selection helper tidyverse median(), mean(), sd() median, mean, standard deviation base R "],["practical.html", "Practical", " Practical "],["overview-3.html", "Overview", " Overview Section 4: In this section, we will cover the following functions: Learning Outcomes: "],["practical-4.html", "Practical", " Practical "],["overview-4.html", "Overview", " Overview Section 5 In this section, we will cover the following functions: Learning Outcomes: "],["practical-5.html", "Practical", " Practical "],["overview-5.html", "Overview", " Overview Section 6 In this section, we will cover the following functions: Learning Outcomes: "],["practical-6.html", "Practical", " Practical "],["overview-6.html", "Overview", " Overview Section 7 In this section, we will cover the following functions: Learning Outcomes: "],["practical-7.html", "Practical", " Practical "],["overview-7.html", "Overview", " Overview Section 8 In this section, we will cover the following functions: Learning Outcomes: "],["practical-8.html", "Practical", " Practical "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
