[["index.html", "Data Science Modelling About", " Data Science Modelling Dr. Ioana Macoveciuc About Welcome to SOST70033 Data Science Modelling! This notebook will host the materials for all R practicals and demonstrations for this course unit. The notebook follows the same section-based structure as the learning materials on Blackboard. To access this notebook, you can bookmark it like any other website in your favourite web browser. For each section, you will have at least one practical and/or demonstration to complete and each of these can be accessed by using the sidebar menu on the left hand side of your screen. Clicking on the headings in each section will expand the menu and each task can be accessed individually. The sidebar menu can be toggled on and off by clicking on the Toggle Sidebar button. Other customisation options include changing the font, font size, and the appearance. You also have a handy search button. This notebook is also designed to work well on tablet and mobile devices; to enhance your experience on these devices, it is recommended that you hide the sidebar menu and navigate across sections and subsections using the right and left arrows. The code, as well as the output and answers are provided for the practicals at the end of each section. The R code can be copied and pasted directly in your R console or script by clicking on the following icon: 1: Before beginning, it is recommended that you create a RStudio project for this course and work through the exercises and tasks in each section using this project. 2: You should write and save your answers to the exercises and tasks in R scripts. You should have at least one R script for each course section. 3: The recommended approach for a ‘clean’ working directory is to place all the data files you plan to use in a separate folder (e.g. a folder called data) within your R project working directory. You should always use simple names that allow you easy access to the contents when you want to either explore the folder on your machine or specify the path to these folders in R. 4: To build a robust knowledge basis and adequately develop your practical programming skills, it is absolutely essential that you first attempt all practical tasks and exercises on your own before comparing your answers with those provided in this notebook. You must also follow along in all demonstrations that contain code on your own machines (simply reading through the text and code will NOT be sufficient to develop a robust understanding of the method and its implementation in R); note that you may also need to reflect on and answer questions throughout a demonstration as well. "],["overview.html", "Overview", " Overview Section 1: Introduction to Data Science - The Basics of Statistical Learning This section is comprised of a demonstration and two practicals. The two practicals will make use of exercises adapted from the core textbook for this course: James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd ed. New York: Springer. https://www.statlearning.com/ Learning Outcomes: appreciate the importance of the mean squared error; indexing using base R; creating scatterplot matrices; creating new variables; transforming existing variables; using functionals; ‘calling’ on masked functions from specific packages; translating base R code to tidyverse and vice versa. In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. You can access the R documentation in the Help tab using? (e.g. ?read.csv) Function Description Package read.csv() read csv files base R read_csv() read csv files tidyverse co l umn_to_rownames() convert column to row names tidyverse rownames() obtain names of rows base R summary() obtain summary statistics base R summarise() object summaries tidyverse (dplyr) group_by() group by one or more variables tidyverse (dplyr) pairs() produce a matrix of scatterplots base R plot() create a plot base R ggplot() generic function for creating a plot tidyverse (ggplot2) mutate() create, modify, and delete columns tidyverse (dplyr) if_else() condition-based function tidyverse (dplyr) as_factor() create factor using existing levels tidyverse (forcats) par() set graphical parameters base R mfrow() par() parameter base R slice_min() and slice_max() index rows by location (smallest and largest values of a variable respectively) tidyverse (dplyr) sapply() applying a function over list or vector base R select() keep or drop columns tidyverse (dplyr) note that this function is also available through the MASS package (we will not cover this in this section) pivot_longer() lengthen data tidyverse (tidyr) where() selection helper tidyverse median(), mean(), sd() median, mean, standard deviation base R "],["demonstration-a-more-in-depth-consideration-of-model-accuracy.html", "Demonstration: A more in-depth consideration of model accuracy The Simulation", " Demonstration: A more in-depth consideration of model accuracy This demonstration was developed by Dr. Tatjana Kecojevic, Lecturer in Social Statistics. So we discussed earlier that statistical modeling allows us to explain and dig deeper into understanding the nature of a process of a phenomenon of interest and that we can mathematically describe this phenomenon as an unknown function \\(f\\). In its related data, we annotate the information that can explain the problem’s behaviour as \\(x\\) (this could be a single value or a vector or matrix of values or something more complex) and the results of its behaviour as \\(y\\) (this could also be a single value or something more complex). Mathematically we present this as: \\[y_i = f(x_i)\\] And of course, let’s not forget about the variation of the \\(f\\)’s behaviour: \\[y_i = f(x_i) + \\epsilon_i\\] The standard approach for the error/noise is to adopt the following distribution structure \\(\\epsilon \\sim N(0,\\sigma)\\), meaning that the \\(\\epsilon\\) value is considered to be normally distributed with a mean of \\(0\\) and standard deviation \\(\\sigma\\). Remember that this implies that the negative and positive impacts from the noise are considered equally likely, and that small errors are much more likely than extreme ones. To evaluate the performance of a statistical model on a given data set, we “observe” the discrepancies between the predicted response values for given observations obtained by the chosen statistical model (\\(\\hat{f}(x_i)\\)) and the true response values for these observations (\\(y_i\\)). As I already mentioned, the most commonly used performance measure for regression problems is the mean squared error (MSE): \\[MSE = \\frac{1}{n} \\sum^{n}_{i=1}(y_i - \\hat{f}(x_i))^2\\] The MSE above is computed using the training data, used to fit the model, and as such it would be more correct to refer to it as the training MSE. But we have already discussed that we are not really interested how well the model “works” on the training data, ie. \\(\\hat{f}(x_i) \\approx y_i\\). We are more interested in the accuracy of the predictions \\(\\hat{f}(x_0)\\) that are obtained when we apply the model to previously unseen test data \\((x_0, y_0)\\), ie. \\(\\hat{f}(x_0) \\approx y_0\\). In other words, we want to chose the model with the lowest \\(test\\) \\(MSE\\) and to do so we need a large enough number of observations in the test data to calculate the mean square prediction error for the test observations (\\(x_0\\), \\(y_0\\)), to which we can refer to as the \\(test\\) \\(MSE\\). \\[mean(y_0 - \\hat{f}(x_0))^2\\] In statistics nothing is black and white. In other words, nothing is straightforward and there are many considerations one needs to take into account when applying statistical modelling. The same applies in this situation. We need to realise that when a given model yields a small training MSE but a large test MSE, we are said to be overfitting the data. The statistical model is too ‘preoccupied’ to find patterns in the training data and consequently is modelling the patterns that are caused by random effects, rather than by true features of the unknown function \\(f\\). When the model overfits the training data, the test MSE will be large because the modelled features that the model identifies in the training data just do not exist in the test data. Saying that, regardless of overfitting occurring or not, we expect the training MSE to be smaller than the test MSE, as most of the statistical models either directly or indirectly seek to minimise the training MSE. We need to be aware that the chosen model needs to be flexible and not rigid and glued to the training data. MSE is simple to calculate and yet, despite its simplicity, it can provide us with a vital insight into modelling. It consists of two intrinsic components that can provide greater enlightenment about how the model works: variance: degree to which \\(\\hat{f}\\) would differ if we estimated it using a different training dataset (ideally, it should not vary greatly). bias: average difference between the estimator \\(\\hat{y}\\) and true value \\(y\\). Mathematically we write bias as: \\[E[\\hat{y} – y]\\] As it is not squared difference, it can be either positive or negative. Positive or negative bias implies that the model is over or under “predicting”, while the value of zero would indicate that the model is likely to predict too much as it is to predict too little. The latter implies that the model can be completely wrong in its prediction and still provide us with the bias of zero. This implies that bias on its own provides little information about how correct the model is in its prediction. Remember that \\(y = f(x) + \\epsilon\\) and therefore \\(\\hat{f}\\) is not directly approximating \\(f\\). \\(\\hat{f}\\) models \\(y\\) that includes the noise. It can be challenging and in some cases even impossible to meaningfully capture the behaviour of \\(f\\) itself when the noise term is very large. We have discussed earlier that we assess model accuracy using MSE which is calculated by: obtaining the error (i.e. discrepancy between \\(\\hat{f}(x_i)\\) and \\(y_i\\)) squaring this value (making negative into the positive same, and greater error gets more severe penalty) then averaging these results The mean of the squared error is the same as the expectation\\(^*\\) of our squared error so we can go ahead and simplify this a slightly: \\[MSE=E[(y-\\hat{f}(x))^2]\\] Now, we can break this further and write it as: \\[MSE = E[(f(x)+ \\epsilon - \\hat{f}(x))^2]\\] Knowing that computing the expectation of adding two random variables is the same as computing the expectation of each random variable and then adding them up: \\[E[X+Y]=E[X] +E[Y]\\] and recalling that \\(\\sigma^2\\) represent the variance of \\(\\epsilon\\), where the variance is calculated as: \\[E[X^2]-E[X]^2,\\] and therefore: \\[Var(\\epsilon) = \\sigma^2 = E[\\epsilon^2] - E[\\epsilon]^2,\\] with \\(\\epsilon \\sim N(0,\\sigma)\\) \\[E[\\epsilon]^2=\\mu^2=0^2=0\\] we get: \\[E[\\epsilon^2] = \\sigma^2\\] This helps us to rearranging MSE further and calculate it as: \\[MSE=σ^2+E[−2f(x)\\hat{f}(x)+f(x)^2+\\hat{f}(x)^2]\\] where \\(\\sigma^2\\) is the variance of the noise (i.e. \\(\\epsilon\\)). Therefore, the variance of the noise in data is an irreducible part of the MSE. Regardless of how good the model is, it can never reduce the MSE to being less than the variance related to the noise (i.e. error). This error represents the lack of information in data used to adequately explain everything that can be known about the phenomena being modelled. We should not look at it as a nuisance, as it can often guide us to further explore the problem and look into other factors that might be related to it. Knowing that: \\[Var(X) = E[X^2] - E[X]^2\\] we can apply further transformation and break MSE into: \\[MSE = \\sigma^{2}+Var[f(x)-\\hat{f}(x)]+E[f(x)-\\hat{f}(x)]^2\\] The term \\(Var[f(x)-\\hat{f}(x)]\\) is the variance in the model predictions from the true output values and the last term \\(E[f(x)-\\hat{f}(x)]^2\\) is just the bias squared. We mentioned earlier that that unlike variance, bias can be positive or negative, so we square this value in order to make sure it is always positive. With this in mind, we realise that MSE consists of: model variance model bias and irreducible error \\[\\text{Mean Squared Error}=\\text{Model Variance} + \\text{Model Bias}^2 + \\text{Irreducible Error}\\] We come to the conclusion that in order to minimise the expected test error, we need to select a statistical model that simultaneously achieves low variance and low bias. Note that in practice we will never know what the variance \\(\\sigma^2\\) of the error \\(\\epsilon\\) is, and therefore we will not be able to determine the variance and the bias of the model. However, since \\(\\sigma^2\\) is constant, we have to decrease either bias or variance to improve the model. Testing the model using the test data and observing its bias and variance can help us address some important issues, allowing us to reason with the model. If the model fails to find the \\(f\\) in data and is systematically over or under predicting, this will indicate underfitting and it will be reflected through high bias. However, high variance when working with test data indicates the issue of overfitting. What happens is that the model has learnt the training data really well and is too close to the data, so much so that it starts to mistake the \\(f(x) + \\epsilon\\) for true \\(f(x)\\). The Simulation To better understand these concepts, let us run a small simulation study. We will: simulate a function \\(f\\) apply the error, i.e. noise sampled from a distribution with a known variance To make it very simple and illustrative we will use a linear function \\(f(x) = 3 + 2x\\) to simulate response \\(y\\) with the error \\(e\\thicksim N(\\mu =0, \\sigma^2 =4)\\), where \\(x\\) is going to be a sequence of numbers between \\(0\\) and \\(10\\) in steps of \\(0.1\\). We will examine the simulations for the models that over and under estimate the true \\(f\\), and since it is a linear function we will not have a problem identifying using simple linear regression modelling. Let’s start with a simulation in which we will model the true function with \\(\\hat{f}_{1} = 4 + 2x\\) and \\(\\hat{f}_{2} = 1 + 2x\\). set.seed(123) ## set the seed of R‘s random number generator # simulate function f(x) = 3 + 2x f &lt;- function(x){ 3 + 2 * x } # generate vector X x &lt;- seq(0, 10, by = 0.05) # the error term coming from N(mean = 0, variance = 4) e &lt;- rnorm(length(x), mean = 0, sd = 2) # simulate the response vector Y y &lt;- f(x) + e # plot the simulated data plot(x, y, cex = 0.75, pch = 16, main = &quot;Simulation: 1&quot;) abline(3, 2, col =&quot;gray&quot;, lwd = 2, lty = 1) # model fitted to simulated data f_hat_1&lt;- function(x){ 4 + 2 * x } f_hat_2 &lt;- function(x){ 1 + 2 * x } y_bar = mean(y) # average value of the response variable y f_hat_3 &lt;- function(x){ y_bar } # add the line representing the fitted model abline(1, 2, col = &quot;red&quot;, lwd = 2, lty = 2) abline(4, 2, col = &quot;blue&quot;, lwd = 2, lty = 1) abline(y_bar, 0, col = &quot;darkgreen&quot;, lwd = 2, lty = 3) legend(7.5, 10, legend=c(&quot;f_hat_1&quot;, &quot;f_hat_2&quot;, &quot;f_hat_3&quot;, &quot;f&quot;), col = c(&quot;blue&quot;, &quot;red&quot;, &quot;darkgreen&quot;, &quot;gray&quot;), lwd = c(2, 2, 2, 2), lty = c(1:3, 1), text.font = 4, bg = &#39;lightyellow&#39;) Observing the graph, we notice that \\(\\hat{f}_1\\) and \\(\\hat{f}_2\\), depicted in blue and red lines respectively, follow the data nicely, but are also systematically over (in the case of \\(\\hat{f}_1\\) and under (in the case of \\(\\hat{f}_2\\)) estimating the values. In the simple model \\(\\hat{f}_3\\), the line represents the value \\(\\bar{y}\\), which cuts the data in half. As we mentioned earlier, knowing the true function \\(f\\) and the distribution of \\(\\epsilon\\) we can calculate: - the MSE using the simulated data and the estimated model, - the model’s bias and variance which will allow for the calculation of the “theoretical” MSE. This will allow for more detailed illustration about the information contained in the model’s bias and variance. # calculate MSE from data MSE_data1 = mean((y - f_hat_1(x))^2) MSE_data2 = mean((y - f_hat_2(x))^2) MSE_data3 = mean((y - f_hat_3(x))^2) # model bias bias_1 = mean(f_hat_1(x) - f(x)) bias_2 = mean(f_hat_2(x) - f(x)) bias_3 = mean(f_hat_3(x) - f(x)) # model variance var_1 = var(f(x) - f_hat_1(x)) var_2 = var(f(x) - f_hat_2(x)) var_3 = var(f(x) - f_hat_3(x)) # calculate &#39;theoretical&#39; MSE MSE_1 = bias_1^2 + var_1 + 2^2 MSE_2 = bias_2^2 + var_2 + 2^2 MSE_3 = bias_3^2 + var_3 + 2^2 for (i in 1:1){ cat (c(&quot;==============================================&quot;,&quot;\\n&quot;)) cat (c(&quot;=============== f_hat_1 ================&quot;,&quot;\\n&quot;)) cat(c(&quot;MSE_data1 = &quot;, round(MSE_data1, 2), sep = &#39;\\n&#39;)) cat(c(&quot;bias_1 = &quot;, bias_1, sep = &#39;\\n&#39; )) cat(c(&quot;variance_1 = &quot;, round(var_1, 2), sep = &#39;\\n&#39; )) cat(c(&quot;MSE_1 = 4 + bias_1^2 + variance_1 = &quot;, MSE_1, sep = &#39;\\n&#39; )) cat(c(&quot;==============================================&quot;,&quot;\\n&quot;)) cat(c(&quot;=============== f_hat_2 ================&quot;,&quot;\\n&quot;)) cat(c(&quot;MSE_data2 = &quot;, round(MSE_data2, 2), sep = &#39;\\n&#39;)) cat(c(&quot;bias_2 = &quot;, bias_2, sep = &#39;\\n&#39; )) cat(c(&quot;variance_2 = &quot;, round(var_2, 2), sep = &#39;\\n&#39; )) cat(c(&quot;MSE_2 = 4 + bias_2^2 + variance_2 = &quot;, MSE_2, sep = &#39;\\n&#39; )) cat(c(&quot;==============================================&quot;,&quot;\\n&quot;)) cat(c(&quot;=============== f_hat_3 ================&quot;,&quot;\\n&quot;)) cat(c(&quot;average y = &quot;, round(y_bar, 2), sep = &#39;\\n&#39;)) cat(c(&quot;MSE_data3 = &quot;, round(MSE_data3, 2), sep = &#39;\\n&#39;)) cat(c(&quot;bias_3 = &quot;, round(bias_3, 2), sep = &#39;\\n&#39; )) cat(c(&quot;variance_3 = &quot;, round(var_3, 2), sep = &#39;\\n&#39; )) cat(c(&quot;MSE_3 = 4 + bias_3^2 + variance_3 = &quot;, round(MSE_3, 2), sep = &#39;\\n&#39; )) cat(c(&quot;==============================================&quot;,&quot;\\n&quot;)) } ## ============================================== ## =============== f_hat_1 ================ ## MSE_data1 = 4.61 ## bias_1 = 1 ## variance_1 = 0 ## MSE_1 = 4 + bias_1^2 + variance_1 = 5 ## ============================================== ## =============== f_hat_2 ================ ## MSE_data2 = 7.64 ## bias_2 = -2 ## variance_2 = 0 ## MSE_2 = 4 + bias_2^2 + variance_2 = 8 ## ============================================== ## =============== f_hat_3 ================ ## average y = 13 ## MSE_data3 = 36.7 ## bias_3 = 0 ## variance_3 = 33.84 ## MSE_3 = 4 + bias_3^2 + variance_3 = 37.84 ## ============================================== \\(\\hat{f}_1\\) has a positive bias because it is overestimating data points more often than it is underestimating, but as it does it so consistently in comparison to \\(f\\) that produces variance of zero. In contrast \\(\\hat{f}_2\\) has a negative bias as it is underestimating simulated data, but nonetheless it also does it consistently, resulting in zero variance with \\(f\\). Unlike in the previous two model estimates which follow the data points, \\(\\hat{f}_3\\) predicts the mean value of data, resulting in no bias since it evenly underestimates and overestimates \\(f(x)\\). However, the variation in prediction between \\(f\\) and \\(\\hat{f}_3\\) is obvious. Given that the true function \\(f\\) is linear, by applying simple regression modelling, we should be able to estimate it easily in R using the \\(lm()\\) function. # model fitted to simulated data f_hat_4&lt;- function(x){ lm(y~x) } # plot the simulated data plot(x, y, cex = 0.75, pch = 16, main = &quot;Simulation: 2&quot;) abline(3, 2, col =&quot;gray&quot;, lwd = 2, lty = 1) # add the line representing the fitted model abline(lm(y~x), col =&quot;red&quot;, lwd = 2, lty = 3) legend(7.5, 8, legend=c(&quot;f_hat_4&quot;, &quot;f&quot;), col = c(&quot;red&quot;, &quot;gray&quot;), lwd = c(2, 2), lty = c(3, 1), text.font = 4, bg = &#39;lightyellow&#39;) Since the true function \\(f\\) is a linear model it is not surprising that \\(\\hat{f}_4\\) can learn it, resulting in zero values of the model’s bias and variance. # calculate MSE from data MSE_data4 = mean((y - predict(f_hat_4(x)))^2) # model bias bias_4 = mean(predict(f_hat_4(x)) - f(x)) # model variance var_4 = var(f(x) - predict(f_hat_4(x))) # calculate &#39;theoretical&#39; MSE MSE_4 = bias_4^2 + var_4 + 2^2 for (i in 1:1){ cat (c(&quot;==============================================&quot;,&quot;\\n&quot;)) cat (c(&quot;=============== f_hat_4 ================&quot;,&quot;\\n&quot;)) cat(c(&quot;MSE_data4 = &quot;, round(MSE_data4, 2), sep = &#39;\\n&#39;)) cat(c(&quot;bias_4 = &quot;, round(bias_4, 2), sep = &#39;\\n&#39; )) cat(c(&quot;variance_4 = &quot;, round(var_4, 2), sep = &#39;\\n&#39; )) cat(c(&quot;MSE_4 = 4 + bias_4^2 + variance_4 = &quot;, round(MSE_4, 2), sep = &#39;\\n&#39; )) cat (c(&quot;==============================================&quot;,&quot;\\n&quot;)) } ## ============================================== ## =============== f_hat_4 ================ ## MSE_data4 = 3.62 ## bias_4 = 0 ## variance_4 = 0 ## MSE_4 = 4 + bias_4^2 + variance_4 = 4 ## ============================================== We realise that the \\(MSE\\) is more than just a simple error measurement. It is a tool that informs and educates the modeller about the performance of the model being used in the analysis of a problem. It is packed with information that when unwrapped can provide a greater insight into not just the fitted model, but the nature of the problem and its data. \\(^*\\) The Expectation of a Random Variable is the sum of its values weighted by their probability. For example: What is the average toss of a fair six-sided die? If the random variable is the top face of a tossed, fair six-sided die, then the probability of die landing on \\(X\\) is: \\[f(x) = \\frac{1}{6}\\] for \\(x = 1, 2,... 6\\). Therefore, the average toss, i.e. the expected value of \\(X\\) is: \\[E(X) = 1(\\frac{1}{6}) + 2(\\frac{1}{6}) + 3(\\frac{1}{6}) + 4(\\frac{1}{6}) + 5(\\frac{1}{6}) + 6(\\frac{1}{6}) = 3.5\\] Of course, we do not expect to get a fraction when tossing a die, i.e. we do not expect the toss to be 3.5, but rather an integer number between 1 to 6. So, what the expected value is really saying is what is the expected average of a large number of tosses will be. If we toss a fair, six-side die hundreds of times and calculate the average of the tosses we will not get the exact 3.5 figure, but we will expect it to be close to 3.5. This is a theoretical average, not the exact one that is realised. "],["practical-1.html", "Practical 1", " Practical 1 For the tasks below, you will require the College dataset from the core textbook (James et. al 2021). Click here to download the file: College.csv . Remember to place your data file in a separate subfolder within your R project working directory. The College dataset contains statistics for a large number of US Colleges from the 1995 issue of US News and World Report. It is a data frame with 777 observations and 18 variables. The variables are: Private : Public/private indicator Apps : Number of applications received Accept : Number of applicants accepted Enroll : Number of new students enrolled Top10perc : New students from top 10% of high school class Top25perc : New students from top 25% of high school class F.Undergrad : Number of full-time undergraduates P.Undergrad : Number of part-time undergraduates Outstate : Out-of-state tuition Room.Board : Room and board costs Books : Estimated book costs Personal : Estimated personal spending PhD : Percent of faculty with Ph.D.’s Terminal : Percent of faculty with terminal degree S.F.Ratio: Student/faculty ratio perc.alumni : Percent of alumni who donate Expend : Instructional expenditure per student Grad.Rate : Graduation rate Task 1 Import the dataset in an object called college using the tidyverse read_csv() function. If you then have a look at the contents of the data object using View(), you will notice that the first column contains the names of all of the universities in the dataset. You will also notice that it has a strange name. Actually, these data should not be treated as a variable (column) since it is just a list of university names. Task 2 Keeping the list of names in the data object, transform this column such that the university names in the column become row names. Hint: use the column_to_rownames() function from dplyr. How would have your approach to this task differed if you would have imported the dataset using base R? Try it! Task 3 Produce summary statistics for all variables in the data object. Task 4 Create a scatterplot matrix of the first three numeric variables. Task 5 Produce side by side box plots of Outstate versus Private using base R. Did this work? Why? Task 6 Using the Top10perc variable, create a new categorical variable called Elite such that universities are divided into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%. Hint: use a combination of mutate() and if_else(). Task 7 Produce side by side box plots of the new Elite variable and Outstate. How would you produce a similar plot using base R? Task 8 Use base R to produce a multipanel plot that displays histograms of the following variables: Apps, perc.alumni, S.F.Ratio, Expend. Hint: use par(mfrow=c(2,2)) to set up a 2x2 panel. Try to adjust the specifications (e.g. breaks). Task 9 Using Accept and Apps, create a new variable that describes acceptance rate. Name this variable acceptance_rate. Hint: use mutate(). Task 10 Using the acceptance_rate variable, find out which university has the lowest acceptance rate. Hint: for a tidyverse approach, you can use slice_min(). Task 11 Using the acceptance_rate variable, find out which university has the highest acceptance rate. "],["practical-2.html", "Practical 2", " Practical 2 For the tasks below, you will require the Boston dataset. This dataset is part of the MASS R package. To access the dataset, load the MASS package (install the package first, if you have not done so previously). Task 1 Install and load the MASS package. You will also require tidyverse. Does R provide any message when loading MASS? Why does this matter? Task 2 Find out more about the Boston dataset variables by accessing the R Documentation. To explore the Boston dataset, simply type the name of the data object into the console or use View() What type of data structure is the Boston dataset? What are its dimensions? How many categorical and quantitative variables are there? Task 3 Find the class of all 14 variables. Hint: use sapply. Task 4 Using a tidyverse approach, calculate the mean, median, and standard deviation of all variables of class numeric. What is the mean pupil-teacher ratio? What is the median and mean per capita crime rate? Which value do you think is more suitable to describe per capita crime rate? Task 5 Using a base R approach, create a 2x2 multipanel plot of crim versus age, dis, rad, tax and ptratio. What can you say about the relationships between age, dis, rad, tax, and crim? Task 6 Using a base R approach, create and display histograms of crim, tax and ptratio in a 1x2 multipanel plot. Set the breaks argument to 25 . What do these histograms indicate? "],["answers.html", "Answers Practical 1 Practical 2", " Answers Practical 1 For the tasks below, you will require the College dataset from the core textbook (James et. al 2021). Click here to download the file: College.csv . Remember to place your data file in a separate subfolder within your R project working directory. This data file contains 18 variables for 777 different universities and colleges in the United States. The variables are: Private : Public/private indicator Apps : Number of applications received Accept : Number of applicants accepted Enroll : Number of new students enrolled Top10perc : New students from top 10% of high school class Top25perc : New students from top 25% of high school class F.Undergrad : Number of full-time undergraduates P.Undergrad : Number of part-time undergraduates Outstate : Out-of-state tuition Room.Board : Room and board costs Books : Estimated book costs Personal : Estimated personal spending PhD : Percent of faculty with Ph.D.’s Terminal : Percent of faculty with terminal degree S.F.Ratio: Student/faculty ratio perc.alumni : Percent of alumni who donate Expend : Instructional expenditure per student Grad.Rate : Graduation rate Task 1 Import the dataset in an object called college using the tidyverse read_csv() function. # Remember to load tidyverse first library(tidyverse) college &lt;- read_csv(&quot;data/College.csv&quot;) If you have a look at the contents of the data object using View(), you will notice that the first column contains the names of all of the universities in the dataset. You will also notice that it has a strange name. Actually, these data should not be treated as a variable (column) since it is just a list of university names. Task 2 Keeping the list of names in the data object, transform this column such that the university names in the column become row names. Hint: use the column_to_rownames() function from dplyr. college &lt;- college %&gt;% column_to_rownames(var = &quot;...1&quot;) How would have your approach to this task differed if you would have imported the dataset using base R? Try it! The data file could have instead been imported using read.csv(): college &lt;- read.csv(\"data/College.csv\") Using the base R approach, the first column containing the university names would have been named “X”, as shown below using View(). Now, how would be go about transforming the contents of the first column into row names? This would require two steps. First, we assign the column contents to rows names. rownames(college) &lt;- college[, 1] If you have another look at the data object, you will see that the rows have now been renamed using the university names in the “X” column, but the column is still part of the dataset. We therefore need to tell R to delete the column. college &lt;- college[, -1] Task 3 Produce summary statistics for all variables in the data object. summary(college) ## Private Apps Accept Enroll ## Length:777 Min. : 81 Min. : 72 Min. : 35 ## Class :character 1st Qu.: 776 1st Qu.: 604 1st Qu.: 242 ## Mode :character Median : 1558 Median : 1110 Median : 434 ## Mean : 3002 Mean : 2019 Mean : 780 ## 3rd Qu.: 3624 3rd Qu.: 2424 3rd Qu.: 902 ## Max. :48094 Max. :26330 Max. :6392 ## Top10perc Top25perc F.Undergrad P.Undergrad ## Min. : 1.00 Min. : 9.0 Min. : 139 Min. : 1.0 ## 1st Qu.:15.00 1st Qu.: 41.0 1st Qu.: 992 1st Qu.: 95.0 ## Median :23.00 Median : 54.0 Median : 1707 Median : 353.0 ## Mean :27.56 Mean : 55.8 Mean : 3700 Mean : 855.3 ## 3rd Qu.:35.00 3rd Qu.: 69.0 3rd Qu.: 4005 3rd Qu.: 967.0 ## Max. :96.00 Max. :100.0 Max. :31643 Max. :21836.0 ## Outstate Room.Board Books Personal ## Min. : 2340 Min. :1780 Min. : 96.0 Min. : 250 ## 1st Qu.: 7320 1st Qu.:3597 1st Qu.: 470.0 1st Qu.: 850 ## Median : 9990 Median :4200 Median : 500.0 Median :1200 ## Mean :10441 Mean :4358 Mean : 549.4 Mean :1341 ## 3rd Qu.:12925 3rd Qu.:5050 3rd Qu.: 600.0 3rd Qu.:1700 ## Max. :21700 Max. :8124 Max. :2340.0 Max. :6800 ## PhD Terminal S.F.Ratio perc.alumni ## Min. : 8.00 Min. : 24.0 Min. : 2.50 Min. : 0.00 ## 1st Qu.: 62.00 1st Qu.: 71.0 1st Qu.:11.50 1st Qu.:13.00 ## Median : 75.00 Median : 82.0 Median :13.60 Median :21.00 ## Mean : 72.66 Mean : 79.7 Mean :14.09 Mean :22.74 ## 3rd Qu.: 85.00 3rd Qu.: 92.0 3rd Qu.:16.50 3rd Qu.:31.00 ## Max. :103.00 Max. :100.0 Max. :39.80 Max. :64.00 ## Expend Grad.Rate ## Min. : 3186 Min. : 10.00 ## 1st Qu.: 6751 1st Qu.: 53.00 ## Median : 8377 Median : 65.00 ## Mean : 9660 Mean : 65.46 ## 3rd Qu.:10830 3rd Qu.: 78.00 ## Max. :56233 Max. :118.00 Task 4 Create a scatterplot matrix of the first three numeric variables. pairs(college[,2:4]) Task 5 Produce side by side box plots of Outstate versus Private using base R. plot(college$Private, college$Outstate, xlab = &quot;Private&quot;, ylab = &quot;Outstate&quot;) Did this work? Why? Using the plot() base R function to produce a box plot would produce an error since the Private variable is of class character. Most statistical functions will not work with character vectors. Error in plot.window(...) : need finite 'xlim' values In addition: Warning messages: 1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion 2: In min(x) : no non-missing arguments to min; returning Inf 3: In max(x) : no non-missing arguments to max; returning -Inf Creating a box plot with tidyverse would work. college %&gt;% ggplot(aes(x = Private, y = Outstate)) + geom_boxplot() However, it is important to note that if a variable is not of the right class, then this might have unintended consequences for example, when building models. In this case, the Private variable must be transformed into a factor. Task 6 Using the Top10perc variable, create a new categorical variable called Elite such that universities are divided into two groups based on whether or not the proportion of students coming from the top 10% of their high school classes exceeds 50%. Hint: use a combination of mutate() and if_else(). college &lt;- college %&gt;% mutate(Elite = if_else(Top10perc &gt; 50, &quot;Yes&quot;, &quot;No&quot;), Elite = as_factor(Elite)) #do not forget the factor transformation step (categorical variables are factors in R) Task 7 Produce side by side box plots of the new Elite variable and Outstate. college %&gt;% ggplot(aes(x = Elite, y = Outstate)) + geom_boxplot() How would you produce a similar plot using base R? plot(college$Elite, college$Outstate, xlab = \"Elite\", ylab = \"Outstate\") Task 8 Use base R to produce a multipanel plot that displays histograms of the following variables: Apps, perc.alumni, S.F.Ratio, Expend. Hint: use par(mfrow=c(2,2)) to set up a 2x2 panel. Try to adjust the specifications (e.g. breaks). # An example is shown below. Note that the purpose of the mfrow parameter is # to change the default way in which R displays plots. # Once applied, all plots you create later will also be displayed in a 2x2 grid. # To revert back, you need to enter par(mfrow=c(1,1)) into the console. par(mfrow=c(2,2)) hist(college$Apps) hist(college$perc.alumni, col=2) hist(college$S.F.Ratio, col=3, breaks=10) hist(college$Expend, breaks=100) Task 9 Using Accept and Apps, create a new variable that describes acceptance rate. Name this variable acceptance_rate. Hint: use mutate(). college &lt;- college %&gt;% mutate(acceptance_rate = Accept / Apps) Task 10 Using the acceptance_rate variable, find out which university has the lowest acceptance rate. Hint: for a tidyverse approach, you can use slice_min(). college %&gt;% slice_min(order_by = acceptance_rate, n = 1) ## Private Apps Accept Enroll Top10perc Top25perc ## Princeton University Yes 13218 2042 1153 90 98 ## F.Undergrad P.Undergrad Outstate Room.Board Books Personal ## Princeton University 4540 146 19900 5910 675 1575 ## PhD Terminal S.F.Ratio perc.alumni Expend Grad.Rate Elite ## Princeton University 91 96 8.4 54 28320 99 Yes ## acceptance_rate ## Princeton University 0.1544863 Task 11 Using the acceptance_rate variable, find out which university has the highest acceptance rate. college %&gt;% slice_max(order_by = acceptance_rate, n = 1) ## Private Apps Accept Enroll Top10perc Top25perc ## Emporia State University No 1256 1256 853 43 79 ## Mayville State University No 233 233 153 5 12 ## MidAmerica Nazarene College Yes 331 331 225 15 36 ## Southwest Baptist University Yes 1093 1093 642 12 32 ## University of Wisconsin-Superior No 910 910 342 14 53 ## Wayne State College No 1373 1373 724 6 21 ## F.Undergrad P.Undergrad Outstate Room.Board ## Emporia State University 3957 588 5401 3144 ## Mayville State University 658 58 4486 2516 ## MidAmerica Nazarene College 1100 166 6840 3720 ## Southwest Baptist University 1770 967 7070 2500 ## University of Wisconsin-Superior 1434 417 7032 2780 ## Wayne State College 2754 474 2700 2660 ## Books Personal PhD Terminal S.F.Ratio ## Emporia State University 450 1888 72 75 19.3 ## Mayville State University 600 1900 68 68 15.7 ## MidAmerica Nazarene College 1100 4913 33 33 15.4 ## Southwest Baptist University 400 1000 52 54 15.9 ## University of Wisconsin-Superior 550 1960 75 81 15.2 ## Wayne State College 540 1660 60 68 20.3 ## perc.alumni Expend Grad.Rate Elite ## Emporia State University 4 5527 50 No ## Mayville State University 11 6971 51 No ## MidAmerica Nazarene College 20 5524 49 No ## Southwest Baptist University 13 4718 71 No ## University of Wisconsin-Superior 15 6490 36 No ## Wayne State College 29 4550 52 No ## acceptance_rate ## Emporia State University 1 ## Mayville State University 1 ## MidAmerica Nazarene College 1 ## Southwest Baptist University 1 ## University of Wisconsin-Superior 1 ## Wayne State College 1 Practical 2 For the tasks below, you will require the Boston dataset. This dataset is part of the MASS R package. To access the dataset, load the MASS package (install the package first, if you have not done so previously). Task 1 Install and load the MASS package. You will also require tidyverse. # if you haven&#39;t already, install the MASS package before loading install.packages(&quot;MASS&quot;) library(MASS) library(tidyverse) Does R provide any message when loading MASS? Why does this matter? One important message that R provides when loading MASS is that this package masks the select() function from tidyverse. Attaching package: ‘MASS’ The following object is masked from ‘package:dplyr’: `select` When masking occurs, this means that both packages contain the same function. If you were to use the select() function, R will call the function from the MASS package, rather than from tidyverse (dplyr) package. This is because the MASS package is the one masking the function. If you intend to use the select() function as defined by the tidyverse package, it may not work as intended and/or you may be prompted by an error message such as: Error in select(...): unused argument (...) To avoid such issues, you must ‘call’ on the package from which you want R to use the masked function (e.g. dplyr::select()). This is why it is important to read through all warnings and messages provided in the console. Task 2 Find out more about the Boston dataset variables by accessing the R Documentation. ?Boston To explore the Boston dataset, simply type the name of the data object into the console or use View() What type of data structure is the Boston dataset? What are its dimensions? How many categorical and quantitative variables are there? The Boston dataset is a data frame with 506 rows (observations) and 14 columns (variables). There is one categorical variable (chas), and 13 quantitative variables. Task 3 Find the class of all 14 variables. Hint: use sapply. sapply(Boston, class) ## crim zn indus chas nox rm age dis ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;integer&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; ## rad tax ptratio black lstat medv ## &quot;integer&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; Task 4 Using a tidyverse approach, calculate the mean, median, and standard deviation of all variables of class numeric. Boston %&gt;% dplyr::select(dplyr::where(is.numeric)) %&gt;% pivot_longer(everything()) %&gt;% group_by(name) %&gt;% summarise(Mean = mean(value, na.rm = TRUE), SD = sd(value, na.rm = TRUE), Median = median(value, na.rm = TRUE)) ## # A tibble: 14 × 4 ## name Mean SD Median ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 age 68.6 28.1 77.5 ## 2 black 357. 91.3 391. ## 3 chas 0.0692 0.254 0 ## 4 crim 3.61 8.60 0.257 ## 5 dis 3.80 2.11 3.21 ## 6 indus 11.1 6.86 9.69 ## 7 lstat 12.7 7.14 11.4 ## 8 medv 22.5 9.20 21.2 ## 9 nox 0.555 0.116 0.538 ## 10 ptratio 18.5 2.16 19.0 ## 11 rad 9.55 8.71 5 ## 12 rm 6.28 0.703 6.21 ## 13 tax 408. 169. 330 ## 14 zn 11.4 23.3 0 What is the mean pupil-teacher ratio? What is the median and mean per capita crime rate? Which value do you think is more suitable to describe per capita crime rate? The mean pupil-teacher ratio is about 19. The median crime rate is 0.257 whilst the mean is larger at 3.61. Given the difference between the median and the mean, a skewed distribution is expected, therefore, the median may be a more a suitable summary statistic to describe crime rate (a histogram would be needed). Task 5 Using a base R approach, create a 2x2 multipanel plot of crim versus age, dis, rad, tax and ptratio. par(mfrow = c(2,2)) plot(Boston$age, Boston$crim) plot(Boston$dis, Boston$crim) plot(Boston$rad, Boston$crim) plot(Boston$tax, Boston$crim) What can you say about the relationships between age, dis, rad, tax, and crim? As the age of the home increases (age), crime also increases. There is also higher crime around employment centers (dis). With very high index of accessibility to radial highways (rad), and tax rates (tax) there also appears to be high crime rates. Task 6 Using a base R approach, create and display histograms of crim, tax and ptratio in a 1x2 multipanel plot. Set the breaks argument to 25 . par(mfrow=c(1,2)) hist(Boston$crim, breaks=25) hist(Boston$tax, breaks=25) What do these histograms indicate? Most areas have low crime rates, but there is a rather long tail showing high crime rates (although the frequency seems to be very low). Given the degree of skew, the mean would not be a good measure of central tendency. With respect to tax rates, there appears to be a large divide between low taxation and high taxation, with the highest peak at around 670. Remember to revert back to single panel display. "],["overview-1.html", "Overview", " Overview Section 2: Linear Regression and Prediction This section is comprised of two demonstrations and three practicals. Practical 3 is based on a demonstration developed by Dr. Tatjana Kecojevic, Lecturer in Social Statistics. Practicals 1 and 2 will make use of exercises adapted from the core textbook for this course: James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd ed. New York: Springer. https://www.statlearning.com/ Learning Outcomes: explain the relevance of the intercept; appreciate the impact of noise on coefficient estimates; produce and interpret diagnostic plots with base R; include non-linear transformations and interactions; compare model fit; compute and interpret confidence intervals. In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. Function Description Package lm() fit linear models base R predict() generic function for predictions from results of different models (e.g. predict.lm()) base R confint() compute confidence intervals base R plot() generic function for plotting base R legend() add legend (to plot()) arguments such as col, lty, and cex control colour, line type, and font size respectively base R abline() adding one or more straight lines to plot base R cor() computes correlation between variables base R rnorm() generates normal distribution base R poly() returns or evaluates polynomials base R par() set graphical parameters base R mfrow() par() parameter base R subset() return subset of a data object (vector, matrix, or dataframe) according to condition(s) base R anova() compute analysis of variance for base R rnorm() density, distribution function, quantile function and random generation for the normal distribution base R sqrt() compute square root base R "],["demonstration-1.html", "Demonstration 1 Simple Linear Models Without Intercept Simple Linear Models with Intercept", " Demonstration 1 Simple Linear Models Without Intercept Let’s investigate the t-statistic for the null hypothesis \\(H_{0}:\\beta = 0\\) in simple linear regression without an intercept. The equation for a model without the intercept would therefore be \\(Y = \\beta X\\). By excluding the intercept, the model is constrained to pass through the origin \\((0,0)\\), allowing the relationship between the response and predictor to be interpreted as proportional. In other words, the removal of the intercept forces the regression line to start at \\((0,0)\\), so when \\(x = 0\\), then \\(y = 0\\). Let’s first generate some data for a predictor \\(x\\) and a response \\(y\\). We select a seed value to ensure that we generate the same data every time. To generate values, we use the rnorm function to produce 100 data values drawn from a normal distribution (hence rnorm()). set.seed(1) x &lt;- rnorm(100) y &lt;- 2 * x + rnorm(100) Now that we generated our predictor and our response variable, let’s run a simple linear regression without an intercept using \\(y\\) as the response and \\(x\\) as a predictor. One way to do so is by adding \\(0\\) into the formula. fit &lt;- lm(y ~ x + 0) And now, let’s have a look at the results. coef(summary(fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## x 1.993876 0.1064767 18.72593 2.642197e-34 We can see a significant positive relationship between y and x. The coefficient estimate for \\(x\\) is \\(1.993876\\), and since the relationship between \\(x\\) and \\(y\\) is proportional, we interpret the estimate as the \\(y\\) values being predicted to be (a little below) twice the \\(x\\) values. But what happens if we swap \\(x\\) and \\(y\\) and run a model using \\(y\\) as the predictor and \\(x\\) as the response? fit2 &lt;- lm(x ~ y + 0) coef(summary(fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## y 0.3911145 0.02088625 18.72593 2.642197e-34 We again observe a significant positive relationship between \\(x\\) and \\(y\\), except that the \\(x\\) values are predicted to be (a little below) half the \\(y\\) values (since the coefficient estimate is \\(0.3911145\\)). Note also the t-statistic values for the two models. They are identical and of course so is the p-value (therefore, there is a significant relationships between \\(x\\) and \\(y\\)). Therefore, the results of the models of \\(y\\) onto \\(x\\) and \\(x\\) onto \\(y\\) indicate that the coefficients would be the inverse of each other (2 and 1/2) whilst the t-statistic values (and p-values) remain the same. Why are the t-statistic values identical? For each coefficient, the t statistic is calculated by dividing the coefficient estimate by its standard error. For example, for the fit2 model, we have a coefficient estimate of \\(0.3911145\\) and a standard error of \\(0.02088625\\) and so dividing \\(0.3911145\\) by \\(0.02088625\\) gives us \\(18.72593\\). You’ll also remember that the correlation coefficient between two variables is symmetric and so the correlation between \\(X\\) and \\(Y\\) is the same as for \\(Y\\) and \\(X\\). This is the reason why it is incorrect to state that “\\(X\\) causes a change in \\(Y\\)”. In a linear model, we are testing whether there is a linear association between \\(x\\) and \\(y\\) but not if X causes Y or Y causes X. Therefore, irrespective of whether we are regressing \\(y\\) onto \\(x\\) or \\(y\\) onto \\(y\\), the t-statistic is testing the same null hypothesis \\(H_{0} : \\beta = 0\\) (i.e. fundamentally, it is testing whether there is a linear correlation between \\(x\\) and \\(y\\)). So what exactly is the role of the intercept? As you already know, the intercept represents the value of \\(y\\) when \\(x = 0\\) which can be thought of as the initial value effect that exists independently of \\(x\\). This not only applies to the simple linear regression model but also to the multiple linear regression model (i.e. the intercept is the value of \\(y\\) when all predictors are zero). In other words, the intercept adjusts the starting point of regression line and allows for the line to shift up or down on the y-axis thus reflecting a “baseline” level of \\(y\\) that is not dependent on \\(x\\). With an intercept, the slope coefficient still tells us how much \\(Y\\) changes with a one-unit change in \\(x\\), but this change is relative to the value of \\(y\\) when \\(x = 0\\) and this is important when \\(x\\) can take a value of \\(0\\) that is meaningful to the model. Without the intercept, the line is forced to pass through the origin \\((0,0)\\), which may not be suitable unless the data naturally begin at zero (or there are some other theoretical or practical reasons which warrant the line passing through the origin). With an intercept, the regression line is no longer forced to pass through zero (and will only do so if the data naturally begin at zero). The intercept therefore allows for the regression line to better fit the data, particularly when the data do not actually begin at zero. In this way, the model can capture the average outcome when the predictor(s) is/are zero. Simple Linear Models with Intercept Do you think that the t-statistic will be the same for both regression of Y onto X and X onto Y if we were to include the intercept? We use the same data as before and run a regression with \\(y\\) as response and \\(x\\) as predictor and include the intercept. fit3 &lt;- lm(y ~ x) We then extract the model coefficients from the summary results of the model. coef(summary(fit3)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.03769261 0.09698729 -0.3886346 6.983896e-01 ## x 1.99893961 0.10772703 18.5555993 7.723851e-34 How does coefficient for fit3 compare to fit? How about the t-statistic value? coef(summary(fit)) ## Estimate Std. Error t value Pr(&gt;|t|) ## x 1.993876 0.1064767 18.72593 2.642197e-34 As you can see, the coefficient for the model with the intercept (fit3) is very similar to the coefficient for the model without the intercept (fit). The t-statistic is also very close (\\(18.72593\\) for the model without intercept and \\(18.5555993\\) for the model with the intercept). Now we run a regression with \\(x\\) as response and \\(y\\) as predictor. fit4 &lt;- lm(x ~ y) We then extract the model coefficients from the summary results of the model. coef(summary(fit4)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.03880394 0.04266144 0.9095787 3.652764e-01 ## y 0.38942451 0.02098690 18.5555993 7.723851e-34 How does the coefficient for fit4 compare to fit2? How about the t-statistic value? Are the t-statistic values different between the fit3 and fit4 models? coef(summary(fit2)) ## Estimate Std. Error t value Pr(&gt;|t|) ## y 0.3911145 0.02088625 18.72593 2.642197e-34 The slope coefficient for the model with the intercept (\\(0.38942451\\)) is very similar to the coefficient for the model without the intercept (\\(0.3911145\\)) and so is the t-statistic. Also, as expected, the t-statistic value for the two models for which we included the intercept are identical. Therefore, irrespective of whether we include the intercept or not, the t-statistic value for the regression of \\(y\\) onto \\(x\\) will be identical to the t-statistic value for the regression of \\(x\\) onto \\(y\\). "],["demonstration-2.html", "Demonstration 2 Population Parameters and Estimated Coefficients What happens if we reduce noise? What happens if we increase noise? How does noise affect confidence intervals for the coefficients?", " Demonstration 2 Population Parameters and Estimated Coefficients Let’s explore the differences between population parameters and estimated coefficients. We will generate a “true” population dataset. We create a variable \\(X\\) with 100 observations drawn from a normal distribution. To be more specific about the characteristic of our variable \\(X\\), we will not only specify the total number of observations (100), but also the mean (0), and standard deviation (1). This will be our predictor. set.seed(1) x &lt;- rnorm(100, 0, 1) We now create another vector called eps containing 100 observations drawn from a normal distribution with a mean of zero and a variance of 0.25. This will be the our error term \\(\\epsilon\\). eps &lt;- rnorm(100, 0, sqrt(0.25)) Using \\(X\\) and \\(\\epsilon\\), we now generate a vector \\(Y\\) according to the following formula: \\(Y = -1 + 0.5X + \\epsilon\\). Essentially, we specify our intercept, the slope coefficient, the predictor variable and the error to obtain our response variable. y &lt;- -1 + 0.5 * x + eps The values \\(-1\\) and \\(0.5\\) represent the “true” population values for the intercept \\(\\beta_{0}\\) and slope \\(\\beta_{1}\\) respectively. Now we can create a scatterplot to observe the association between \\(X\\) and \\(Y\\). plot(x, y) The plot indicates a linear relationship between \\(X\\) and \\(Y\\). The relationship is clearly not perfectly linear due to noise. If we were to instead estimate the intercept and slope, to what degree do you think these estimated coefficients will differ from the true population values? Ok, so we have the variables we generated, so our predictor X and our response X and we run a regression model. fit &lt;- lm(y ~ x) summary(fit) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.93842 -0.30688 -0.06975 0.26970 1.17309 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.01885 0.04849 -21.010 &lt; 2e-16 *** ## x 0.49947 0.05386 9.273 4.58e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4814 on 98 degrees of freedom ## Multiple R-squared: 0.4674, Adjusted R-squared: 0.4619 ## F-statistic: 85.99 on 1 and 98 DF, p-value: 4.583e-15 The results of the model show an estimated slope coefficient (\\(\\hat{\\beta_{1}}\\)) for \\(x\\) of \\(0.49947\\). This is very close to the population value (\\(\\beta_{1}\\)) which is \\(0.5\\). We see a similar estimated value for the intercept (\\(\\hat{\\beta_{0}}\\)) which is \\(-1.01885\\), again very close to the true value for the intercept (\\(\\beta_{0}\\)) which is \\(-1\\). Therefore, if we were to plot the population regression line and the estimated regression line, we would see that the two are difficult to distinguish (given the similarity of the estimated and true values for the coefficients). plot(x, y) abline(fit) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2), cex = 0.72, ) What if we were to fit a polynomial regression model? Would there be any evidence that adding a quadratic term improves the model fit? To add a polynomial term of degree two, we can use the poly base R function directly in the code for the model. Since the F-test is not statistically significant, there is no evidence that adding a quadratic term improves the model fit. fit2 &lt;- lm(y ~ poly(x, 2)) anova(fit, fit2) ## Analysis of Variance Table ## ## Model 1: y ~ x ## Model 2: y ~ poly(x, 2) ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 98 22.709 ## 2 97 22.257 1 0.45163 1.9682 0.1638 What happens if we reduce noise? For our first model (fit), we specified a variance of \\(0.5\\) (standard deviation 0.25) for \\(\\epsilon\\) and we noted an \\(R^2\\) value of \\(0.4674\\). set.seed(1) x &lt;- rnorm(100, 0, 1) eps &lt;- rnorm(100, 0, sqrt(0.25)) y &lt;- -1 + 0.5 * x + eps fit &lt;- lm(y ~ x) summary(fit) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.93842 -0.30688 -0.06975 0.26970 1.17309 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.01885 0.04849 -21.010 &lt; 2e-16 *** ## x 0.49947 0.05386 9.273 4.58e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4814 on 98 degrees of freedom ## Multiple R-squared: 0.4674, Adjusted R-squared: 0.4619 ## F-statistic: 85.99 on 1 and 98 DF, p-value: 4.583e-15 Now, let’s observe what happens to the \\(R^2\\) value if we reduce noise from 0.25 to 0.05. We can do so directly when we generate data for our variable \\(y\\) without needing to create a new eps object. The results show that the \\(R^{2}\\) value for fit3 is much higher than the \\(R^{2}\\) value for fit. x &lt;- rnorm(100, 0, 1) y &lt;- -1 + 0.5 * x + rnorm(100, 0, sqrt(0.05)) fit3 &lt;- lm(y ~ x) summary(fit3) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.61308 -0.12553 -0.00391 0.15199 0.41332 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.98917 0.02216 -44.64 &lt;2e-16 *** ## x 0.52375 0.02152 24.33 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2215 on 98 degrees of freedom ## Multiple R-squared: 0.858, Adjusted R-squared: 0.8565 ## F-statistic: 592.1 on 1 and 98 DF, p-value: &lt; 2.2e-16 By plotting the data we can clearly see that the data points are less dispersed than before and therefore, the association between x and y appears more linear. We can also observe that the estimated regression line deviates slightly from the population regression line (particularly at lowest and highest values). plot(x, y) abline(fit3) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2), cex = 0.72 ) What happens if we increase noise? Using the same approach as before, we now increase the standard deviation to 1. Now, the \\(R^{2}\\) value for fit4 is much lower than that of either fit3 or fit. x &lt;- rnorm(100, 0, 1) y &lt;- -1 + 0.5 * x + rnorm(100, 0, 1) fit4 &lt;- lm(y ~ x) summary(fit4) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.51014 -0.60549 0.02065 0.70483 2.08980 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.04745 0.09676 -10.825 &lt; 2e-16 *** ## x 0.42505 0.08310 5.115 1.56e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9671 on 98 degrees of freedom ## Multiple R-squared: 0.2107, Adjusted R-squared: 0.2027 ## F-statistic: 26.16 on 1 and 98 DF, p-value: 1.56e-06 If we plot the data, we can observe that the data points are more dispersed and therefore, the estimated regression line deviates to a greater extent from the population regression line. plot(x, y) abline(fit4) abline(-1, 0.5, col = &quot;red&quot;, lty = 2) legend(&quot;topleft&quot;, c(&quot;model fit&quot;, &quot;population regression&quot;), col = c(&quot;black&quot;, &quot;red&quot;), lty = c(1, 2), cex = 0.72, ) How does noise affect confidence intervals for the coefficients? The fit3 model is the model with the lowest amount of noise (standard deviation of 0.05), whilst fit4 is the model with the largest amount of noise (standard deviation of 1). The confidence interval for the coefficient for the model with the highest noise is the widest. The larger the amount of noise, the wider the interval and therefore, the less precise the coefficient estimates will be. Conversely, the narrowest interval is that of the model with the lowest noise which yielded the most precise estimates. confint(fit) ## 2.5 % 97.5 % ## (Intercept) -1.1150804 -0.9226122 ## x 0.3925794 0.6063602 confint(fit3) ## 2.5 % 97.5 % ## (Intercept) -1.033141 -0.9451916 ## x 0.481037 0.5664653 confint(fit4) ## 2.5 % 97.5 % ## (Intercept) -1.2394772 -0.8554276 ## x 0.2601391 0.5899632 "],["practical-1-2.html", "Practical 1", " Practical 1 For the tasks below, you will require the Auto dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Auto dataset loads automatically: library(ISLR2) The Auto dataset contains information such as engine horsepower, gas mileage, model year, and origin of car, for 392 vehicles. It is a dataframe with 392 observations and 9 variables. The variables are: mpg: miles per gallon cylinders: Number of cylinders between 4 and 8 displacement: Engine displacement (cu. inches) horsepower: Engine horsepower weight: Vehicle weight (lbs.) acceleration: Time to accelerate from 0 to 60 mph (sec.) year: Model year origin: Origin of car (1. American, 2. European, 3. Japanese) name: Vehicle name Task 1 Use the lm() function to perform simple linear regression with mpg as the response and horsepower as the predictor. Store the output in an object called fit. Task 2 Have a look at the results of the model. Is there a relationship between the predictor and the response? Task 3 What is the associated 95% confidence intervals for predicted miles per gallon associated with an engine horsepower of 98? Hint: use the predict() function. For confidence intervals, set the interval argument to confidence. Task 4 How about the prediction interval for the same value? Are the two intervals different? Why? Task 5 Using base R, plot the response and the predictor as well as the least squares regression line. Add suitable labels to the X and Y axes. Task 6 Use base R to produce diagnostic plots of the least squares regression fit. Display these in a 2X2 grid. Task 7 Subset the Auto dataset such that it excludes the name and origin variables and store this subsetted dataset in a new object called quant_vars. Task 8 Compute a correlation matrix of all variables. Did you use the Auto dataset or the quant_vars object? Why does it matter which data object you use? Task 9 Using the quant_vars object, perform multiple linear regression with miles per gallon as the response and all other variables as the predictors. Store the results in an object called fit2. Task 10 Have a look at the results of the multiple regression model. Is there a relationship between the predictors and the response? Which predictors appear to have a statistically significant relationship to the response? What does the coefficient for the year variable suggest? Task 11 Produce diagnostic plots of the multiple linear regression fit in a 2x2 grid. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage? Task 12 Fit separate linear regression models with interaction effect terms for: weight and horsepower, acceleration and horsepower, and cylinders and weight. Are any of the interaction terms statistically significant? Task 13 Using the Auto data object, apply transformations for the horsepower variable and plot the relationship between horsepower and mpg in a 2x2 grid. First plot: use the original variable; Second plot: apply log transform; Third plot: raise it to the power of two. Which of these transformations is most suitable? Task 14 Now run a multiple regression model with all variables as before but this time, apply a log transformation of the horsepower variable. Task 15 Have a look at the results. How do the results of model fit3 differ from those of model fit2? Task 16 Produce diagnostic plots for the fit3 object and display them in a 2x2 grid. How do the diagnostic plots differ? "],["practical-2-2.html", "Practical 2 Task 1 Task 2 Task 3 Task 4 Task 5", " Practical 2 For the tasks below, you will require the Carseats dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Carseats dataset loads automatically: library(ISLR2) Carseats is a simulated dataset comprising of sales of child car seats at 400 different stores. It is a datafrate with 400 observations and 11 variables. The variables are: Sales: Unit sales (thousands of dollars) at each location CompPrice: Price charged by competitor at each location Income: Community income level (thousands of dollars) Advertising: Local advertising budget for company at each location (thousands of dollars) Population: Population size in region (thousands of dollars) Price: Price company charges for car seats at each site ShelveLoc: Quality of the shelving location for the car seats at each site Age: Average age of the local population Education: Education level at each location Urban: Whether the store is in an urban or rural location US: Whether the store is in the US or not Task 1 Fit a multiple regression model to predict unit sales at each location based on price company charges for car seats, whether the store is in an urban or rural location, and whether the store is in the US or not. Task 2 Have a look at the results and interpret the coefficients. Which coefficients are statistically significant? What do they indicate? Task 3 Based on the conclusions you have drawn in Task 2, now fit a smaller model that only uses the predictors for which there is evidence of association with sales. Task 4 Compare the two models (fit and fit2). Which model is the better fit? Task 5 Produce diagnostic plots for fit2 and display these in a 2x2 grid. Is there evidence of outliers or high leverage observations in the fit2 model? "],["practical-3-the-quality-of-red-bordeaux-vintages.html", "Practical 3: The Quality of Red Bordeaux Vintages", " Practical 3: The Quality of Red Bordeaux Vintages This practical was developed by Dr. Tatjana Kecojevic, Lecturer in Social Statistics. We would like to analyse the quality of a vintage that has been quantified as the price and make the interpretation of our statistical findings. We are going to use wine.csv available from Eduardo García Portugués’s book: Notes for Predictive Modelling; https://bookdown.org/egarpor/PM-UC3M/. The wine dataset is formed by the auction price of 27 red Bordeaux vintages, five vintage descriptors (WinterRain, AGST, HarvestRain, Age, Year), and the population of France in the year of the vintage, FrancePop. Year: year in which grapes were harvested to make wine Price: logarithm of the average market price for Bordeaux vintages according to 1990–1991 auctions. The price is relative to the price of the 1961 vintage, regarded as the best one ever recorded WinterRain: winter rainfall (in mm) AGST: Average Growing Season Temperature (in degrees Celsius) HarvestRain: harvest rainfall (in mm) Age: age of the wine measured as the number of years stored in a cask FrancePop: population of France at Year (in thousands) You will require the GGally package; please make sure to install it first. library(GGally) And now let’s import the data. wine &lt;- read.csv(&quot;https://raw.githubusercontent.com/egarpor/handy/master/datasets/wine.csv&quot;) Let’s first obtain some summary statistics. summary(wine) ## Year Price WinterRain AGST HarvestRain ## Min. :1952 Min. :6.205 Min. :376.0 Min. :14.98 Min. : 38.0 ## 1st Qu.:1960 1st Qu.:6.508 1st Qu.:543.5 1st Qu.:16.15 1st Qu.: 88.0 ## Median :1967 Median :6.984 Median :600.0 Median :16.42 Median :123.0 ## Mean :1967 Mean :7.042 Mean :608.4 Mean :16.48 Mean :144.8 ## 3rd Qu.:1974 3rd Qu.:7.441 3rd Qu.:705.5 3rd Qu.:17.01 3rd Qu.:185.5 ## Max. :1980 Max. :8.494 Max. :830.0 Max. :17.65 Max. :292.0 ## Age FrancePop ## Min. : 3.00 Min. :43184 ## 1st Qu.: 9.50 1st Qu.:46856 ## Median :16.00 Median :50650 ## Mean :16.19 Mean :50085 ## 3rd Qu.:22.50 3rd Qu.:53511 ## Max. :31.00 Max. :55110 And a matrix of plots. The ggpairs() function which is part of the GGally package produces an information rich visualisation that includes pairwise relationships of all the variables in the dataset. ggpairs(wine) What conclusions can you draw based on the above information? We are going to investigate possible interactions between the rainfall (WinterRain) and the growing season temperature (AGST). We will start with the most complicated model that includes the highest-order interaction. In R we will specify the three-way interaction, which will automatically add all combinations of two-way interactions. model1 &lt;- lm(Price ~ WinterRain + AGST + HarvestRain + Age + WinterRain * AGST * HarvestRain, data = wine) summary(model1) ## ## Call: ## lm(formula = Price ~ WinterRain + AGST + HarvestRain + Age + ## WinterRain * AGST * HarvestRain, data = wine) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.35058 -0.19462 -0.02645 0.17194 0.52079 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.582e+00 1.924e+01 0.446 0.6609 ## WinterRain -1.858e-02 2.896e-02 -0.642 0.5292 ## AGST -1.748e-01 1.137e+00 -0.154 0.8795 ## HarvestRain -4.713e-02 1.540e-01 -0.306 0.7631 ## Age 2.476e-02 8.288e-03 2.987 0.0079 ** ## WinterRain:AGST 1.272e-03 1.712e-03 0.743 0.4671 ## WinterRain:HarvestRain 7.836e-05 2.600e-04 0.301 0.7665 ## AGST:HarvestRain 3.059e-03 9.079e-03 0.337 0.7401 ## WinterRain:AGST:HarvestRain -5.446e-06 1.540e-05 -0.354 0.7278 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2833 on 18 degrees of freedom ## Multiple R-squared: 0.8621, Adjusted R-squared: 0.8007 ## F-statistic: 14.06 on 8 and 18 DF, p-value: 2.675e-06 What can you say about the explained variability of the model? Which coefficients are statistically significant? Simplify the model in stages and decide on the final model. "],["answers-1.html", "Answers Practical 1 Practical 2 Practical 3: The Quality of Red Bordeaux Vintages", " Answers Practical 1 For the tasks below, you will require the Auto dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Auto dataset loads automatically: library(ISLR2) Remember to install it first install.packages(\"ISLR2\") This data file (text format) contains 398 observations of 9 variables. The variables are: mpg: miles per gallon cylinders: Number of cylinders between 4 and 8 displacement: Engine displacement (cu. inches) horsepower: Engine horsepower weight: Vehicle weight (lbs.) acceleration: Time to accelerate from 0 to 60 mph (sec.) year: Model year origin: Origin of car (1. American, 2. European, 3. Japanese) name: Vehicle name Task 1 Use the lm() function to perform simple linear regression with mpg as the response and horsepower as the predictor. Store the output in an object called fit. fit &lt;- lm(mpg ~ horsepower, data = Auto) Task 2 Have a look at the results of the model. summary(fit) ## ## Call: ## lm(formula = mpg ~ horsepower, data = Auto) ## ## Residuals: ## Min 1Q Median 3Q Max ## -13.5710 -3.2592 -0.3435 2.7630 16.9240 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.935861 0.717499 55.66 &lt;2e-16 *** ## horsepower -0.157845 0.006446 -24.49 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.906 on 390 degrees of freedom ## Multiple R-squared: 0.6059, Adjusted R-squared: 0.6049 ## F-statistic: 599.7 on 1 and 390 DF, p-value: &lt; 2.2e-16 Is there a relationship between the predictor and the response? The slope coefficient (-0.157845) is statistically significant (&lt;2e-16 ***). We can conclude that there is evidence to suggest a negative relationship between miles per gallon and engine horsepower. For a one-unit increase in engine horsepower, miles per gallon are reduced by 0.16. Task 3 What is the associated 95% confidence intervals for predicted miles per gallon associated with an engine horsepower of 98? Hint: use the predict() function. For confidence intervals, set the interval argument to confidence. predict(fit, data.frame(horsepower = 98), interval = &quot;confidence&quot;) ## fit lwr upr ## 1 24.46708 23.97308 24.96108 Task 4 How about the prediction interval for the same value? predict(fit, data.frame(horsepower = 98), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 24.46708 14.8094 34.12476 Are the two intervals different? Why? The prediction interval (lower limit 14.8094 and upper limit 34.12476) is wider (and therefore less precise) than the confidence interval (lower limit 23.97308 and upper limit 24.96108). The confidence interval measures the uncertainty around the estimate of the conditional mean whilst the prediction interval takes into account not only uncertainty but also the variability of the conditional distribution. Task 5 Using base R, plot the response and the predictor as well as the least squares regression line. Add suitable labels to the X and Y axes. plot(Auto$horsepower, Auto$mpg, xlab = &quot;horsepower&quot;, ylab = &quot;mpg&quot;) abline(fit) Task 6 Use base R to produce diagnostic plots of the least squares regression fit. Display these in a 2X2 grid. par(mfrow = c(2, 2)) plot(fit, cex = 0.2) Task 7 Subset the Auto dataset such that it excludes the name and origin variables and store this subsetted dataset in a new object called quant_vars. quant_vars &lt;- subset(Auto, select = -c(name, origin)) Task 8 Compute a correlation matrix of all variables. cor(quant_vars) ## mpg cylinders displacement horsepower weight ## mpg 1.0000000 -0.7776175 -0.8051269 -0.7784268 -0.8322442 ## cylinders -0.7776175 1.0000000 0.9508233 0.8429834 0.8975273 ## displacement -0.8051269 0.9508233 1.0000000 0.8972570 0.9329944 ## horsepower -0.7784268 0.8429834 0.8972570 1.0000000 0.8645377 ## weight -0.8322442 0.8975273 0.9329944 0.8645377 1.0000000 ## acceleration 0.4233285 -0.5046834 -0.5438005 -0.6891955 -0.4168392 ## year 0.5805410 -0.3456474 -0.3698552 -0.4163615 -0.3091199 ## acceleration year ## mpg 0.4233285 0.5805410 ## cylinders -0.5046834 -0.3456474 ## displacement -0.5438005 -0.3698552 ## horsepower -0.6891955 -0.4163615 ## weight -0.4168392 -0.3091199 ## acceleration 1.0000000 0.2903161 ## year 0.2903161 1.0000000 Did you use the Auto dataset or the quant_vars object? Why does it matter which data object you use? To compute the correlation matrix using all variables of a data object, these variables must all be numeric. In the Auto data object, the name variable is coded as a factor. class(Auto$name) [1] \"factor\" Therefore, if you try to use the cor() function with Auto dataset without excluding the name variable, you will get an error. cor(Auto) Error in cor(Auto) : 'x' must be numeric. Also, whilst the origin variable is of class integer and will not pose a problem when you apply the cor() function, you’ll remember from the variable description list that this is a nominal variable with its categories numerically labelled. Compute the correlation matrix using quant_vars. Task 9 Using the quant_vars object, perform multiple linear regression with miles per gallon as the response and all other variables as the predictors. Store the results in an object called fit2. fit2 &lt;- lm(mpg ~ ., data = quant_vars) Task 10 Have a look at the results of the multiple regression model. summary(fit2) ## ## Call: ## lm(formula = mpg ~ ., data = quant_vars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.6927 -2.3864 -0.0801 2.0291 14.3607 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.454e+01 4.764e+00 -3.051 0.00244 ** ## cylinders -3.299e-01 3.321e-01 -0.993 0.32122 ## displacement 7.678e-03 7.358e-03 1.044 0.29733 ## horsepower -3.914e-04 1.384e-02 -0.028 0.97745 ## weight -6.795e-03 6.700e-04 -10.141 &lt; 2e-16 *** ## acceleration 8.527e-02 1.020e-01 0.836 0.40383 ## year 7.534e-01 5.262e-02 14.318 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.435 on 385 degrees of freedom ## Multiple R-squared: 0.8093, Adjusted R-squared: 0.8063 ## F-statistic: 272.2 on 6 and 385 DF, p-value: &lt; 2.2e-16 Is there a relationship between the predictors and the response? Which predictors appear to have a statistically significant relationship to the response? What does the coefficient for the year variable suggest? Two of the predictors are statistically significant: weight and year. The relationship between weight and mpg is negative which suggests that for a one pound increase in weight of vehicle, the number of miles per gallon the vehicle can travel decreases, whilst that of mpg and year is positive which suggests that the more recent the vehicle is, the higher the number of miles per gallon it can travel. Task 11 Produce diagnostic plots of the multiple linear regression fit in a 2x2 grid. par(mfrow = c(2, 2)) plot(fit2, cex = 0.2) Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage? One point has high leverage, the residuals also show a trend with fitted values. Task 12 Fit separate linear regression models with interaction effect terms for: weight and horsepower, acceleration and horsepower, and cylinders and weight. summary(lm(mpg ~ . + weight:horsepower, data = quant_vars)) summary(lm(mpg ~ . + acceleration:horsepower, data = quant_vars)) summary(lm(mpg ~ . + cylinders:weight, data = quant_vars)) Are any of the interaction terms statistically significant? For each model, the interaction term is statistically significant. Task 13 Using the Auto data object, apply transformations for the horsepower variable and plot the relationship between horsepower and mpg in a 2x2 grid. First plot: use the original variable; Second plot: apply log transform; Third plot: raise it to the power of two. par(mfrow = c(2, 2)) plot(Auto$horsepower, Auto$mpg, cex = 0.2) plot(log(Auto$horsepower), Auto$mpg, cex = 0.2) plot(Auto$horsepower ^ 2, Auto$mpg, cex = 0.2) Which of these transformations is most suitable? The relationship between horsepower and miles per gallon is clearly non-linear (plot 1). The log transform seems to address this best. Task 14 Now run a multiple regression model with all variables as before but this time, apply a log transformation of the horsepower variable. quant_vars$horsepower &lt;- log(quant_vars$horsepower) fit3 &lt;- lm(mpg ~ ., data = quant_vars) Task 15 Have a look at the results. summary(fit3) ## ## Call: ## lm(formula = mpg ~ ., data = quant_vars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.6778 -2.0080 -0.3142 1.9262 14.0979 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.1713000 8.9291383 3.267 0.00118 ** ## cylinders -0.3563199 0.3181815 -1.120 0.26347 ## displacement 0.0088277 0.0068866 1.282 0.20066 ## horsepower -8.7568129 1.5958761 -5.487 7.42e-08 *** ## weight -0.0044304 0.0007213 -6.142 2.03e-09 *** ## acceleration -0.3317439 0.1077476 -3.079 0.00223 ** ## year 0.6979715 0.0503916 13.851 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.308 on 385 degrees of freedom ## Multiple R-squared: 0.8231, Adjusted R-squared: 0.8203 ## F-statistic: 298.5 on 6 and 385 DF, p-value: &lt; 2.2e-16 How do the results of model fit3 differ from those of model fit2? The fit2 model results showed that only two predictors were statistically significant: weight and year. The fit3 model has two additional predictors that are statistically significant: acceleration as well as horsepower.Also, the coefficient values can now be interpreted more easily. Task 16 Produce diagnostic plots for the fit3 object and display them in a 2x2 grid. par(mfrow = c(2, 2)) plot(fit3, cex = 0.2) How do the diagnostic plots differ? A log transformation of horsepower appears to give a more linear relationship with mpg but this difference does not seem to be substantial. Practical 2 For the tasks below, you will require the Carseats dataset from the core textbook (James et. al 2021). This dataset is part of the ISRL2 package. By loading the package, the Carseats dataset loads automatically: library(ISLR2) This dataframe object contains a simulated dataset of sales of child car seats at 400 different stores. The 9 variables are: Sales: Unit sales (thousands of dollars) at each location CompPrice: Price charged by competitor at each location Income: Community income level (thousands of dollars) Advertising: Local advertising budget for company at each location (thousands of dollars) Population: Population size in region (thousands of dollars) Price: Price company charges for car seats at each site ShelveLoc: Quality of the shelving location for the car seats at each site Age: Average age of the local population Education: Education level at each location Urban: Whether the store is in an urban or rural location US: Whether the store is in the US or not Task 1 Fit a multiple regression model to predict unit sales at each location based on price company charges for car seats, whether the store is in an urban or rural location, and whether the store is in the US or not. fit &lt;- lm(Sales ~ Price + Urban + US, data = Carseats) Task 2 Have a look at the results and interpret the coefficients. summary(fit) ## ## Call: ## lm(formula = Sales ~ Price + Urban + US, data = Carseats) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.9206 -1.6220 -0.0564 1.5786 7.0581 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.043469 0.651012 20.036 &lt; 2e-16 *** ## Price -0.054459 0.005242 -10.389 &lt; 2e-16 *** ## UrbanYes -0.021916 0.271650 -0.081 0.936 ## USYes 1.200573 0.259042 4.635 4.86e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.472 on 396 degrees of freedom ## Multiple R-squared: 0.2393, Adjusted R-squared: 0.2335 ## F-statistic: 41.52 on 3 and 396 DF, p-value: &lt; 2.2e-16 Which coefficients are statistically significant? What do they indicate? The null hypothesis for the slope being zero is rejected for the Price and US variables. The coefficient for Price is statistically significant; since it is negative, as price increases by a thousand dollars (i.e. one unit increase), the sales of child decrease by about 0.05. The slope for the US variable is also statistically significant but it is positive. Also, this is a binary factor variable coded as Yes and No (No is the reference category). Therefore, sales of child car seats are higher by 1.2 for car seat products that are produced in the US than for car seat products not produced in the US. Task 3 Based on the conclusions you have drawn in Task 2, now fit a smaller model that only uses the predictors for which there is evidence of association with sales. fit2 &lt;- lm(Sales ~ Price + US, data = Carseats) Task 4 Compare the two models (fit and fit2). anova(fit, fit2) ## Analysis of Variance Table ## ## Model 1: Sales ~ Price + Urban + US ## Model 2: Sales ~ Price + US ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 396 2420.8 ## 2 397 2420.9 -1 -0.03979 0.0065 0.9357 Which model is the better fit? They have similar r-squared values, and the fit model (containing the extra variable Urban) is non-significantly better. Task 5 Produce diagnostic plots for fit2 and display these in a 2x2 grid. par(mfrow = c(2, 2)) plot(fit2, cex = 0.2) Is there evidence of outliers or high leverage observations in the fit2 model? Yes, there is evidence of outliers and high leverage observations for fit2. Practical 3: The Quality of Red Bordeaux Vintages This demonstration was developed by Dr. Tatjana Kecojevic, Lecturer in Social Statistics. We would like to analyse the quality of a vintage that has been quantified as the price and make the interpretation of our statistical findings. We are going to use wine.csv available from Eduardo García Portugués’s book: Notes for Predictive Modelling; https://bookdown.org/egarpor/PM-UC3M/. The wine dataset is formed by the auction price of 27 red Bordeaux vintages, five vintage descriptors (WinterRain, AGST, HarvestRain, Age, Year), and the population of France in the year of the vintage, FrancePop. Year: year in which grapes were harvested to make wine Price: logarithm of the average market price for Bordeaux vintages according to 1990–1991 auctions. The price is relative to the price of the 1961 vintage, regarded as the best one ever recorded WinterRain: winter rainfall (in mm) AGST: Average Growing Season Temperature (in degrees Celsius) HarvestRain: harvest rainfall (in mm) Age: age of the wine measured as the number of years stored in a cask FrancePop: population of France at Year (in thousands) You will require the GGally package; please make sure to install it first. library(GGally) And now let’s import the data. wine &lt;- read.csv(&quot;https://raw.githubusercontent.com/egarpor/handy/master/datasets/wine.csv&quot;) Let’s first obtain some summary statistics. summary(wine) ## Year Price WinterRain AGST HarvestRain ## Min. :1952 Min. :6.205 Min. :376.0 Min. :14.98 Min. : 38.0 ## 1st Qu.:1960 1st Qu.:6.508 1st Qu.:543.5 1st Qu.:16.15 1st Qu.: 88.0 ## Median :1967 Median :6.984 Median :600.0 Median :16.42 Median :123.0 ## Mean :1967 Mean :7.042 Mean :608.4 Mean :16.48 Mean :144.8 ## 3rd Qu.:1974 3rd Qu.:7.441 3rd Qu.:705.5 3rd Qu.:17.01 3rd Qu.:185.5 ## Max. :1980 Max. :8.494 Max. :830.0 Max. :17.65 Max. :292.0 ## Age FrancePop ## Min. : 3.00 Min. :43184 ## 1st Qu.: 9.50 1st Qu.:46856 ## Median :16.00 Median :50650 ## Mean :16.19 Mean :50085 ## 3rd Qu.:22.50 3rd Qu.:53511 ## Max. :31.00 Max. :55110 And a matrix of plots. ggpairs(wine) What conclusions can you draw based on the above information? We can notice a perfect relationship between the variables Year and Age. This is to be expected since this data was collected in 1983 and Age was calculated as: Age = 1983 - Year. Knowing this, we are going to remove Year from the analysis and use Age as it will be easier to interpret. There is a strong relationship between Year, ie. Age and FrancePop and since we want to impose our viewpoint that the total population does not influence the quality of the wine we will not consider this variable in the model. We are going to investigate possible interactions between the rainfall (WinterRain) and the growing season temperature (AGST). We will start with the most complicated model that includes the highest-order interaction. In R we will specify the three-way interaction, which will automatically add all combinations of two-way interactions. model1 &lt;- lm(Price ~ WinterRain + AGST + HarvestRain + Age + WinterRain * AGST * HarvestRain, data = wine) summary(model1) ## ## Call: ## lm(formula = Price ~ WinterRain + AGST + HarvestRain + Age + ## WinterRain * AGST * HarvestRain, data = wine) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.35058 -0.19462 -0.02645 0.17194 0.52079 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.582e+00 1.924e+01 0.446 0.6609 ## WinterRain -1.858e-02 2.896e-02 -0.642 0.5292 ## AGST -1.748e-01 1.137e+00 -0.154 0.8795 ## HarvestRain -4.713e-02 1.540e-01 -0.306 0.7631 ## Age 2.476e-02 8.288e-03 2.987 0.0079 ** ## WinterRain:AGST 1.272e-03 1.712e-03 0.743 0.4671 ## WinterRain:HarvestRain 7.836e-05 2.600e-04 0.301 0.7665 ## AGST:HarvestRain 3.059e-03 9.079e-03 0.337 0.7401 ## WinterRain:AGST:HarvestRain -5.446e-06 1.540e-05 -0.354 0.7278 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2833 on 18 degrees of freedom ## Multiple R-squared: 0.8621, Adjusted R-squared: 0.8007 ## F-statistic: 14.06 on 8 and 18 DF, p-value: 2.675e-06 What can you say about the explained variability of the model? Which coefficients are statistically significant? Simplify the model in stages and decide on the final model. The model explains well over 80% of variability and is clearly a strong model, but the key question is whether we can simplify it. We will start the process of this model simplification by removing the three-way interaction as it is clearly not significant. model2 &lt;- update(model1, ~. -WinterRain:AGST:HarvestRain, data = wine) summary(model2) ## ## Call: ## lm(formula = Price ~ WinterRain + AGST + HarvestRain + Age + ## WinterRain:AGST + WinterRain:HarvestRain + AGST:HarvestRain, ## data = wine) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.35245 -0.19452 0.01643 0.17289 0.51420 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.980e+00 1.066e+01 0.279 0.78293 ## WinterRain -9.699e-03 1.408e-02 -0.689 0.49930 ## AGST 1.542e-01 6.383e-01 0.242 0.81168 ## HarvestRain 6.496e-03 2.610e-02 0.249 0.80610 ## Age 2.441e-02 8.037e-03 3.037 0.00678 ** ## WinterRain:AGST 7.490e-04 8.420e-04 0.890 0.38484 ## WinterRain:HarvestRain -1.350e-05 7.338e-06 -1.840 0.08144 . ## AGST:HarvestRain -1.032e-04 1.520e-03 -0.068 0.94656 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2767 on 19 degrees of freedom ## Multiple R-squared: 0.8611, Adjusted R-squared: 0.8099 ## F-statistic: 16.83 on 7 and 19 DF, p-value: 6.523e-07 The \\(\\overline{R}^2\\) has slightly increased in value. Next, we remove the least significant two-way interaction term. model3 &lt;- update(model2, ~. -AGST:HarvestRain, data = wine) summary(model3) ## ## Call: ## lm(formula = Price ~ WinterRain + AGST + HarvestRain + Age + ## WinterRain:AGST + WinterRain:HarvestRain, data = wine) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.35424 -0.19343 0.01176 0.17161 0.51218 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.518e+00 6.946e+00 0.507 0.61802 ## WinterRain -1.017e-02 1.195e-02 -0.851 0.40488 ## AGST 1.218e-01 4.138e-01 0.294 0.77147 ## HarvestRain 4.752e-03 4.553e-03 1.044 0.30901 ## Age 2.451e-02 7.710e-03 3.179 0.00472 ** ## WinterRain:AGST 7.769e-04 7.166e-04 1.084 0.29119 ## WinterRain:HarvestRain -1.342e-05 7.059e-06 -1.902 0.07174 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2697 on 20 degrees of freedom ## Multiple R-squared: 0.8611, Adjusted R-squared: 0.8194 ## F-statistic: 20.66 on 6 and 20 DF, p-value: 1.35e-07 Again, it is reassuring to notice an increase in the \\(\\overline{R}^2\\), but we can still simplify the model further by removing another least significant two-way interaction term. model4 &lt;- update(model3, ~. -WinterRain:AGST, data = wine) summary(model4) ## ## Call: ## lm(formula = Price ~ WinterRain + AGST + HarvestRain + Age + ## WinterRain:HarvestRain, data = wine) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5032 -0.1934 0.0109 0.1771 0.4621 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.812e+00 1.598e+00 -2.386 0.026553 * ## WinterRain 2.747e-03 9.471e-04 2.900 0.008560 ** ## AGST 5.586e-01 9.495e-02 5.883 7.71e-06 *** ## HarvestRain 4.717e-03 4.572e-03 1.032 0.313877 ## Age 2.785e-02 7.094e-03 3.926 0.000774 *** ## WinterRain:HarvestRain -1.349e-05 7.088e-06 -1.903 0.070835 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2708 on 21 degrees of freedom ## Multiple R-squared: 0.8529, Adjusted R-squared: 0.8179 ## F-statistic: 24.35 on 5 and 21 DF, p-value: 4.438e-08 There is an insignificant decrease in \\(\\overline{R}^2\\). We notice HarvestRain is now the least significant term, but it is used for the WinterRain:HarvestRain interaction, which is significant at \\(\\alpha = 0.05\\) and therefore we should keep it. However, as the concept of parsimony prefers a model without interactions to a model containing interactions between variables, we will remove the remaining interaction term and see if it significantly affects the explanatory power of the model. model5 &lt;- update(model4, ~. -WinterRain:HarvestRain, data = wine) summary(model5) ## ## Call: ## lm(formula = Price ~ WinterRain + AGST + HarvestRain + Age, data = wine) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.46024 -0.23862 0.01347 0.18601 0.53443 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -3.6515703 1.6880876 -2.163 0.04167 * ## WinterRain 0.0011667 0.0004820 2.420 0.02421 * ## AGST 0.6163916 0.0951747 6.476 1.63e-06 *** ## HarvestRain -0.0038606 0.0008075 -4.781 8.97e-05 *** ## Age 0.0238480 0.0071667 3.328 0.00305 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2865 on 22 degrees of freedom ## Multiple R-squared: 0.8275, Adjusted R-squared: 0.7962 ## F-statistic: 26.39 on 4 and 22 DF, p-value: 4.057e-08 The \\(\\overline{R}^2\\) is reduced by around 2%, but it has all the significant terms and it is easier to interpret. For those reasons and in the spirit of parsimony that argues that a model should be as simple as possible, we can suggest that this should be regarded as the best final fitted model. We realise that for the large numbers of explanatory variables, and many interactions and non-linear terms, the process of model simplification can take a very long time. There are many algorithms for automatic variable selection that can help us to choose the variables to include in a regression model. Stepwise regression and Best Subsets regression are two of the more common variable selection methods. The stepwise procedure starts from the saturated model (or the maximal model, whichever is appropriate) through a series of simplifications to the minimal adequate model. This progression is made on the basis of deletion tests: F tests, AIC, t-tests or chi-squared tests that assess the significance of the increase in deviance that results when a given term is removed from the current model. The best subset regression (BREG), also known as “all possible regressions”, as the name of the procedure indicates, fits a separate least squares regression for each possible combination of the p predictors, i.e. explanatory variables. After fitting all of the models, BREG then displays the best fitted models with one explanatory variable, two explanatory variables, three explanatory variables, and so on. Usually, either adjusted R-squared or Mallows Cp is the criterion for picking the best fitting models for this process. The result is a display of the best fitted models of different sizes up to the full/maximal model and the final fitted model can be selected by comparing displayed models based on the criteria of parsimony. You will learn more about variable selection later in the course. “These methods are frequently abused by naive researchers who seek to interpret the order of entry of variables into the regression equation as an index of their ‘importance’. This practice is potentially misleading.” J. Fox and S. Weisberg’s book: An R Companion to Applied Regression, Third Edition, Sage (2019) ’Parsimony says that, other things being equal, we prefer: a model with n−1 parameters to a model with n parameters a model with k−1 explanatory variables to a model with k explanatory variables a linear model to a model which is curved a model without a hump to a model with a hump a model without interactions to a model containing interactions between factors’ Crawley, M.J. 2013, The R Book. 2nd Edition. John Wiley, New York "],["overview-2.html", "Overview", " Overview Section 3: Classification This section is comprised of two demonstrations adapted from the core textbook for this course: James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R. 2nd ed. New York: Springer. https://www.statlearning.com/ Learning Outcomes: appreciate the difference between classic modelling and supervised learning; compute and plot a correlation matrix; compute and interpret a confusion matrix and overall prediction accuracy; apply a variety of classification methods and appreciate their advantages and limitations; understand the differences between modelling count data using a linear versus a Poisson model; interpret the results of a Poisson regression model. In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console. Function Description Package str() display structure of object base R cor() compute correlation base R corrplot() plot correlation matrix arguments such as type and diag control the structure of the plot corrplot glm() fit a generalised linear model arguments such as family control the distribution base R predict() generic function for predictions from results of models arguments such as type specify the type of predictions required base R diag() extract/replace diagonal of a matrix base R sum() compute the sum of values base R lda() perform LDA MASS table() build contingency table base R qda() perform QDA MASS knn() perform KNN classification arguments such as k control number of neighbours class naiveBayes() apply the Naive Bayes classifier e1071 "],["demonstration-1-classification-problems.html", "Demonstration 1: Classification Problems Dataset and Variables Correlation Matrix and Plot “Classic” Logistic Regression Logistic Regression in Statistical Learning Linear Discriminant Analysis Quadratic Discriminant Analysis \\(K\\)-nearest neighbours Naive Bayes", " Demonstration 1: Classification Problems In this demonstration, you will learn how to address classification problems using logistic regression, discriminant analysis, KNN, and Naive Bayes. You will need the Weekly dataset, part of the ISRL2 package. By loading the package, the Weekly dataset loads automatically. In addition to the ISLR2 package, will also require the following: library(MASS) library(class) library(tidyverse) library(corrplot) library(ISLR2) library(e1071) Dataset and Variables The Weekly dataset contains weekly percentage returns for the S&amp;P 500 stock index between 1990 and 2010. It is a dataframe with 1098 observations and 9 variables. The variables are: Year: year observation was recorded Lag1: Percentage return for previous week Lag2: Percentage return for 2 weeks previous Lag3: Percentage return for 3 weeks previous Lag4: Percentage return for 4 weeks previous Lag5: Percentage return for 5 weeks previous Volume: Volume of shares traded (average number of daily shares traded in billions) Today: percentage return for current week Direction: whether the market had a positive (up) or negative (down) return on a given week. In this demonstration, the goal is to predict whether the market had a positive (up) or negative (down) return on a given week. Therefore, Direction will be our response variable. Before we consider the model, let’s first explore our dataset. By exploring the structure of the dataframe, we find out that all variables are numeric, with the exception of the Direction variable. str(Weekly) ## &#39;data.frame&#39;: 1089 obs. of 9 variables: ## $ Year : num 1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ... ## $ Lag1 : num 0.816 -0.27 -2.576 3.514 0.712 ... ## $ Lag2 : num 1.572 0.816 -0.27 -2.576 3.514 ... ## $ Lag3 : num -3.936 1.572 0.816 -0.27 -2.576 ... ## $ Lag4 : num -0.229 -3.936 1.572 0.816 -0.27 ... ## $ Lag5 : num -3.484 -0.229 -3.936 1.572 0.816 ... ## $ Volume : num 0.155 0.149 0.16 0.162 0.154 ... ## $ Today : num -0.27 -2.576 3.514 0.712 1.178 ... ## $ Direction: Factor w/ 2 levels &quot;Down&quot;,&quot;Up&quot;: 1 1 2 2 2 1 2 2 2 1 ... Correlation Matrix and Plot Let’s now produce a correlation plot between all pairs of numeric variables in the dataset. Using the base R cor() function, we exclude the 9th variable (which is a factor) and compute the correlation among all pairs of numeric variables. cor(Weekly[, -9]) ## Year Lag1 Lag2 Lag3 Lag4 ## Year 1.00000000 -0.032289274 -0.03339001 -0.03000649 -0.031127923 ## Lag1 -0.03228927 1.000000000 -0.07485305 0.05863568 -0.071273876 ## Lag2 -0.03339001 -0.074853051 1.00000000 -0.07572091 0.058381535 ## Lag3 -0.03000649 0.058635682 -0.07572091 1.00000000 -0.075395865 ## Lag4 -0.03112792 -0.071273876 0.05838153 -0.07539587 1.000000000 ## Lag5 -0.03051910 -0.008183096 -0.07249948 0.06065717 -0.075675027 ## Volume 0.84194162 -0.064951313 -0.08551314 -0.06928771 -0.061074617 ## Today -0.03245989 -0.075031842 0.05916672 -0.07124364 -0.007825873 ## Lag5 Volume Today ## Year -0.030519101 0.84194162 -0.032459894 ## Lag1 -0.008183096 -0.06495131 -0.075031842 ## Lag2 -0.072499482 -0.08551314 0.059166717 ## Lag3 0.060657175 -0.06928771 -0.071243639 ## Lag4 -0.075675027 -0.06107462 -0.007825873 ## Lag5 1.000000000 -0.05851741 0.011012698 ## Volume -0.058517414 1.00000000 -0.033077783 ## Today 0.011012698 -0.03307778 1.000000000 We store the computed correlation matrix in a new object which we will then use to create a correlation matrix plot with the corrplot() function from the corrplot package. cor_matrix &lt;- cor(Weekly[, -9]) corrplot(cor_matrix) By using the default arguments, we obtain the correlation matrix in full, with each circle representing the correlation between each variable. The size of the circle represents the magnitude of the correlation, whilst the shade corresponds to both strength and direction of the correlation. As the legend illustrates, a perfect negative correlation (-1) is represented by dark red and a perfect positive correlation (+1) in dark blue. As you already know, the correlation matrix is symmetric around its diagonal. The diagonal area represents the correlation of each variable with itself, and therefore, this corresponds to a perfect correlation (dark blue). To facilitate interpretation, particularly when we are dealing with a large number of variables, we can set the diag argument to FALSE to remove the correlation of each variable with itself from the plot. Because of its symmetric property, we can also display just half of the square since the parts on either side of the diagonal are mirror images. We can achieve this using the type argument. We can either choose to display the area above the diagonal or the area below the diagonal, as I did below. There are many other options if you want to further customise your correlation plot such using a different visualisation method of the direction and strength of the correlation using the method argument. Have a look at the documentaion of the corrplot function using ?. corrplot(cor_matrix, type = &quot;lower&quot;, diag = FALSE) Now, the correlation plot only displays the correlations between the 8 numeric variables. We observe a strong positive correlation between volume of daily shares traded and the year the observation was recorded (dark blue). The correlations between other variables are quite weak but notably, we see that Lag1 and Lag3, Lag 2 and Lag4, Lag3 and Lag5, and Today and Lag2 are positively correlated with one another (albeit weakly). Other variables also appear weakly negatively correlated, such as Lag1 and Lag2. Ok, so what was the purpose of computing the correlation matrix? You’ll remember that multicollinearity is an issue in model building which can lead to inflated variances of the estimated coefficients. As a result of the shared variance between two highly correlated predictors, our ability to adequately evaluate the effect of the predictors on the outcome will be affected (e.g. increased risk of overfitting). One way to deal with multicollinearity is to eliminate one of the highly correlated predictors. “Classic” Logistic Regression Now let’s build our logistic regression model the classic way. Given the high correlation between Year and Volume, we have to reflect on how we want to address this. Assuming we have knowledge of important factors that can predict market movement (Direction), we decide to drop the Year variable rather than Volume* since the latter measures average number of daily shares traded (in billions). You may remember that in R, these models are built using the base R glm function within which the family argument must be set to binomial. Note that in this dataset, the Direction variable is already a factor so there is no need to perform any recoding/transformations but remember to ALWAYS explore your data in detail before you build any model. fit &lt;- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial) summary(fit) ## ## Call: ## glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + ## Volume, family = binomial, data = Weekly) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6949 -1.2565 0.9913 1.0849 1.4579 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.26686 0.08593 3.106 0.0019 ** ## Lag1 -0.04127 0.02641 -1.563 0.1181 ## Lag2 0.05844 0.02686 2.175 0.0296 * ## Lag3 -0.01606 0.02666 -0.602 0.5469 ## Lag4 -0.02779 0.02646 -1.050 0.2937 ## Lag5 -0.01447 0.02638 -0.549 0.5833 ## Volume -0.02274 0.03690 -0.616 0.5377 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1496.2 on 1088 degrees of freedom ## Residual deviance: 1486.4 on 1082 degrees of freedom ## AIC: 1500.4 ## ## Number of Fisher Scoring iterations: 4 The results show that only Lag2 is significant at an alpha level of 0.05. Ok, so let’s see now how well our model predicts whether the market had a positive or negative return in a given week. Confusion Matrix Now, let’s assess the effectiveness of our model by comparing the actual values with those predicted by the model. We can do so using a confusion matrix which compares the model predictions with the true values. Using predict(fit, type = \"response\"), we generate predictions from our model (fit) on the scale of the response variable (type = \"response\"). In this case, our response variable is measured on a probability scale. We choose the standard threshold of 0.5 such that we label an observation as belonging to the Up category if its posterior probability is above 0.5 or as Down if the posterior probability is below 0.5. Hence, in this context, the &gt; 0.5 argument transforms the predicted probabilities into a binary form such that predictions greater than 0.5 are labelled TRUE (so representing upward market movement), whilst the rest are labelled FALSE (representing downward market movement). pred &lt;- predict(fit, type = &quot;response&quot;) &gt; 0.5 Now that we have the frequencies of the TRUE and FALSE instances, let’s build our two-way confusion matrix using the base R table() function. The ifelse function nested inside table() converts the logical TRUE and FALSE values in the pred object intro descriptive labels to facilitate interpretation; so, if pred is TRUE (&gt; 0.5), it becomes labelled as Up (pred), whilst it is is FALSE, it is labelled as \"Down (pred)\". To also include the actual values of market movement from the dataset, we also need to add Weekly$Directionas our argument. Finally to ‘force’ R to display the conf_matrix object we just created, we can place the entire code in single parentheses. (conf_matrix &lt;- table(ifelse(pred, &quot;Up (pred)&quot;, &quot;Down (pred)&quot;), Weekly$Direction)) ## ## Down Up ## Down (pred) 54 48 ## Up (pred) 430 557 Now let’s interpret the results. The results in diagonal represent correct predictions of market movement whilst those in the off-diagonal represent misclassified observations. We can see that our model incorrectly classified 430 instances of market movement as upward movement when in fact they represented downward movement and 48 instances as downward movement when in fact they represented upward movement. Overall, our logistic regression model correctly predicts upwards movements well but it performs poorly at predicting downward movements. We can also compute the overall fraction of correct predictions by dividing the number of correct predictions by total number of predictions. We therefore divide the sum of the diagonal values in our confusion matrix (numerator) by the sum of all elements of the matrix (denominator). We extract the diagonal values from the matrix using the base R diag() function. sum(diag(conf_matrix)) / sum(conf_matrix) ## [1] 0.5610652 The overall fraction of correct predictions is 0.561 (so our model makes correct predictions about 56.1% of the time). Right, so does that mean that this model will make correct predictions 56% of the time on a new, unseen dataset? You already know the answer :)! We used our entire dataset to fit our model. This means that we cannot say anything about how our model will perform on a different dataset and we no longer have any ‘unseen’ data left to test this out. Logistic Regression in Statistical Learning Now, let’s consider logistic regression as applied in statistical learning. We will again consider a basic fixed split. In this example, we will fit our model using data from 1990 up to 2008 and set the data from 2009 and 2010 aside; this will be our test dataset. This is easily achieved by creating a vector of logical values from the data according to our Year criterion. train &lt;- Weekly$Year &lt; 2009 In our previous model, we observed that Lag2 was the only statistically significant predictor. To exemplify how we can use logistic regression in statistical learning, we will build a simple simple with only one predictor. The approach to building the model is the same as the one with which you are already familiar. The exception, of course, is that we will only use part of the dataset to build our model (which in this case is referred to as training the model). To subset our dataset to only include data from years previous to 2009, we use the logical vector train we just created. fit_log_SL &lt;- glm(Direction ~ Lag2, data = Weekly[train, ], family = binomial) Now let’s generate predictions; the function and the overall structure of the code is the same as discussed earlier in the demonstration. The exception is that we used the trained model fit_log_SL to make predictions on the test dataset (Weekly[!train, ]). Using !, we tell R to not include the training data when generating predictions. pred &lt;- predict(fit_log_SL, Weekly[!train, ], type = &quot;response&quot;) &gt; 0.5 Now let’s compute the confusion matrix such that we can compare our predictions on the test data (pred) against the actual values in our dataset (Weekly[!train, ]$Direction)). (t &lt;- table(ifelse(pred, &quot;Up (pred)&quot;, &quot;Down (pred)&quot;), Weekly[!train, ]$Direction)) ## ## Down Up ## Down (pred) 9 5 ## Up (pred) 34 56 If we then compute the overall fraction of correct predictions for the test data we can see that this is higher than the value we obtained using the classical approach (\\(0.561\\)). sum(diag(t)) / sum(t) ## [1] 0.625 Linear Discriminant Analysis How well would linear discriminant analysis address our binary classification problem? In R, LDA can be performed using the lda() function from the MASS package. The basic structure of the function is similar to the other models you have built. fit_lda &lt;- lda(Direction ~ Lag2, data = Weekly[train, ]) The output is, of course, different. prior probabilities of groups: these tells us the way in which the two classes are distributed in our training data (i.e. 44.8 % of the observations correspond to downward market movement whilst 55.2% to upward market movement). group means: the average of our single predictor Lag2 within each class, and are used by LDA as an estimate of \\(μ_{k}\\). coefficient(s) of linear discriminants: tells us how our predictor(s) influence the score that is used to classify the observations into one of the two categories. Here, the coefficient is positive 0.44 and so this indicates that higher values for Lag2 will make the modelmore likely classify an observation as belonging to the Up class; also, the larger the absolute value of the coefficient, the stronger the influence on the model. fit_lda ## Call: ## lda(Direction ~ Lag2, data = Weekly[train, ]) ## ## Prior probabilities of groups: ## Down Up ## 0.4477157 0.5522843 ## ## Group means: ## Lag2 ## Down -0.03568254 ## Up 0.26036581 ## ## Coefficients of linear discriminants: ## LD1 ## Lag2 0.4414162 Now let’s consider what the predict() function does when applied in the context of LDA. result_lda &lt;- predict(fit_lda, Weekly[!train, ]) The output will contain three components: class, posterior, and x, each of which can be accessed using $. The class component is a factor that contains the predictions for market movement (up/down). result_lda$class ## [1] Up Up Down Down Up Up Up Down Down Down Down Up Up Up Up ## [16] Up Up Up Up Up Down Up Up Up Up Up Up Up Up Up ## [31] Up Up Up Up Up Up Up Up Up Up Up Up Up Up Down ## [46] Up Up Up Up Up Up Up Up Up Up Up Down Up Up Up ## [61] Up Up Up Up Up Up Up Up Up Up Up Down Up Down Up ## [76] Up Up Up Down Down Up Up Up Up Up Down Up Up Up Up ## [91] Up Up Up Up Up Up Up Up Up Up Up Up Up Up ## Levels: Down Up The posterior component is matrix that contains the posterior probability that the corresponding observation belongs to a given class. result_lda$posterior ## Down Up ## 986 0.4736555 0.5263445 ## 987 0.3558617 0.6441383 ## 988 0.5132860 0.4867140 ## 989 0.5142948 0.4857052 ## 990 0.4799727 0.5200273 ## 991 0.4597586 0.5402414 ## 992 0.3771117 0.6228883 ## 993 0.5184724 0.4815276 ## 994 0.5480397 0.4519603 ## 995 0.5146118 0.4853882 ## 996 0.5504246 0.4495754 ## 997 0.3055404 0.6944596 ## 998 0.4268160 0.5731840 ## 999 0.3637275 0.6362725 ## 1000 0.4034316 0.5965684 ## 1001 0.4256310 0.5743690 ## 1002 0.4277053 0.5722947 ## 1003 0.4548626 0.5451374 ## 1004 0.4308002 0.5691998 ## 1005 0.3674066 0.6325934 ## 1006 0.5210641 0.4789359 ## 1007 0.4426627 0.5573373 ## 1008 0.3983332 0.6016668 ## 1009 0.4170520 0.5829480 ## 1010 0.4400457 0.5599543 ## 1011 0.4872186 0.5127814 ## 1012 0.4529323 0.5470677 ## 1013 0.4844231 0.5155769 ## 1014 0.4769786 0.5230214 ## 1015 0.3531293 0.6468707 ## 1016 0.3912903 0.6087097 ## 1017 0.4373753 0.5626247 ## 1018 0.4163510 0.5836490 ## 1019 0.4583549 0.5416451 ## 1020 0.4182305 0.5817695 ## 1021 0.4454253 0.5545747 ## 1022 0.4667580 0.5332420 ## 1023 0.4126831 0.5873169 ## 1024 0.4146279 0.5853721 ## 1025 0.4814414 0.5185586 ## 1026 0.4756405 0.5243595 ## 1027 0.3860819 0.6139181 ## 1028 0.4278606 0.5721394 ## 1029 0.4599449 0.5400551 ## 1030 0.5071309 0.4928691 ## 1031 0.4042648 0.5957352 ## 1032 0.4173045 0.5826955 ## 1033 0.4520606 0.5479394 ## 1034 0.4491759 0.5508241 ## 1035 0.4304467 0.5695533 ## 1036 0.4487621 0.5512379 ## 1037 0.4544049 0.5455951 ## 1038 0.4184691 0.5815309 ## 1039 0.4637729 0.5362271 ## 1040 0.4114393 0.5885607 ## 1041 0.4605038 0.5394962 ## 1042 0.5053429 0.4946571 ## 1043 0.4728071 0.5271929 ## 1044 0.4595437 0.5404563 ## 1045 0.4368785 0.5631215 ## 1046 0.4051682 0.5948318 ## 1047 0.4553490 0.5446510 ## 1048 0.4056270 0.5943730 ## 1049 0.4352188 0.5647812 ## 1050 0.4370488 0.5629512 ## 1051 0.4410978 0.5589022 ## 1052 0.4352756 0.5647244 ## 1053 0.4296973 0.5703027 ## 1054 0.4520034 0.5479966 ## 1055 0.4194240 0.5805760 ## 1056 0.4853885 0.5146115 ## 1057 0.5411727 0.4588273 ## 1058 0.4177113 0.5822887 ## 1059 0.5100863 0.4899137 ## 1060 0.4470646 0.5529354 ## 1061 0.4816287 0.5183713 ## 1062 0.4138300 0.5861700 ## 1063 0.4157203 0.5842797 ## 1064 0.5017234 0.4982766 ## 1065 0.5216975 0.4783025 ## 1066 0.3738247 0.6261753 ## 1067 0.4666863 0.5333137 ## 1068 0.3993705 0.6006295 ## 1069 0.4506892 0.5493108 ## 1070 0.4235170 0.5764830 ## 1071 0.5036414 0.4963586 ## 1072 0.4593288 0.5406712 ## 1073 0.4587988 0.5412012 ## 1074 0.3965787 0.6034213 ## 1075 0.4428192 0.5571808 ## 1076 0.4287787 0.5712213 ## 1077 0.4202670 0.5797330 ## 1078 0.4523464 0.5476536 ## 1079 0.4258989 0.5741011 ## 1080 0.4358286 0.5641714 ## 1081 0.4409698 0.5590302 ## 1082 0.4491046 0.5508954 ## 1083 0.3986650 0.6013350 ## 1084 0.4804910 0.5195090 ## 1085 0.4487050 0.5512950 ## 1086 0.4616361 0.5383639 ## 1087 0.4074084 0.5925916 ## 1088 0.4311115 0.5688885 ## 1089 0.4452828 0.5547172 The x component contains the linear discriminants. result_lda$x ## LD1 ## 986 -0.80594669 ## 987 2.92755168 ## 988 -2.01984129 ## 989 -2.05074043 ## 990 -0.99972841 ## 991 -0.37865579 ## 992 2.22702414 ## 993 -2.17875113 ## 994 -3.08806854 ## 995 -2.06045158 ## 996 -3.16178505 ## 997 4.66982149 ## 998 0.64322275 ## 999 2.66623327 ## 1000 1.38038783 ## 1001 0.68030171 ## 1002 0.61541353 ## 1003 -0.22769145 ## 1004 0.51874338 ## 1005 2.54484381 ## 1006 -2.25820605 ## 1007 0.14971942 ## 1008 1.54282900 ## 1009 0.94956560 ## 1010 0.23094000 ## 1011 -1.22176077 ## 1012 -0.16810026 ## 1013 -1.13612602 ## 1014 -0.90791384 ## 1015 3.01892483 ## 1016 1.76839269 ## 1017 0.31392625 ## 1018 0.97163642 ## 1019 -0.33539700 ## 1020 0.91248664 ## 1021 0.06408467 ## 1022 -0.59406691 ## 1023 1.08728746 ## 1024 1.02593061 ## 1025 -1.04475287 ## 1026 -0.86686213 ## 1027 1.93613085 ## 1028 0.61055795 ## 1029 -0.38439421 ## 1030 -1.83135657 ## 1031 1.35390286 ## 1032 0.94162011 ## 1033 -0.14117387 ## 1034 -0.05200779 ## 1035 0.52977878 ## 1036 -0.03920672 ## 1037 -0.21356613 ## 1038 0.90498257 ## 1039 -0.50225234 ## 1040 1.12657351 ## 1041 -0.40160944 ## 1042 -1.77662096 ## 1043 -0.77990314 ## 1044 -0.37203455 ## 1045 0.32937582 ## 1046 1.32521081 ## 1047 -0.24269960 ## 1048 1.31064407 ## 1049 0.38102152 ## 1050 0.32407882 ## 1051 0.19827520 ## 1052 0.37925585 ## 1053 0.55317384 ## 1054 -0.13940820 ## 1055 0.87496626 ## 1056 -1.16570091 ## 1057 -2.87618875 ## 1058 0.92881904 ## 1059 -1.92184689 ## 1060 0.01332181 ## 1061 -1.05049128 ## 1062 1.05109133 ## 1063 0.99150015 ## 1064 -1.66582548 ## 1065 -2.27762836 ## 1066 2.33428828 ## 1067 -0.59185983 ## 1068 1.50972278 ## 1069 -0.09879791 ## 1070 0.74651414 ## 1071 -1.72453384 ## 1072 -0.36541331 ## 1073 -0.34908091 ## 1074 1.59888886 ## 1075 0.14486384 ## 1076 0.58186590 ## 1077 0.84848129 ## 1078 -0.15000219 ## 1079 0.67191480 ## 1080 0.36204062 ## 1081 0.20224795 ## 1082 -0.04980071 ## 1083 1.53223501 ## 1084 -1.01561940 ## 1085 -0.03744106 ## 1086 -0.43648132 ## 1087 1.25414279 ## 1088 0.50903222 ## 1089 0.06849883 To obtain our predictions, we can simply extract the class element. Alternatively, if we want to directly extract just the predictions from the class element, we can use the predict function as we did earlier in the demonstration. #either pred_lda &lt;- result_lda$class #or pred_lda &lt;- predict(fit_lda, Weekly[!train, ], type = &quot;response&quot;)$class Now let’s compute the confusion matrix for our LDA classifier such that we can compare our predictions on the test data against the actual values in our dataset. (t &lt;- table(pred_lda, Weekly[!train, ]$Direction)) ## ## pred_lda Down Up ## Down 9 5 ## Up 34 56 And now the fraction of correct predictions which, we can see is identical to that obtained for logistic regression. sum(diag(t)) / sum(t) ## [1] 0.625 Quadratic Discriminant Analysis Let’s now consider how quadratic discriminant analysis would address our binary classification problem. The code syntax is identical to that for linear discriminant analysis and the qda function is also part of the MASS package. fit_qda &lt;- qda(Direction ~ Lag2, data = Weekly[train, ]) In terms of prior probabilities and group means, the output is identical to that of linear discriminant analysis. However, the output does not include the coefficients of the linear discriminants for obvious reasons. fit_qda ## Call: ## qda(Direction ~ Lag2, data = Weekly[train, ]) ## ## Prior probabilities of groups: ## Down Up ## 0.4477157 0.5522843 ## ## Group means: ## Lag2 ## Down -0.03568254 ## Up 0.26036581 The prediction function works in the same way as for LDA, except that it will produce only two elements (class, and posterior); again, the x element will not be included since we are dealing with a quadratic function. pred_qda &lt;- predict(fit_qda, Weekly[!train, ], type = &quot;response&quot;)$class The confusion matrix is computed in the same way. (t &lt;- table(pred_qda, Weekly[!train, ]$Direction)) ## ## pred_qda Down Up ## Down 0 0 ## Up 43 61 The fraction of correct predictions is lower than that for logistic regression and for LDA. We therefore conclude that in this context, QDA does not perform well in comparison to the previous two approaches. sum(diag(t)) / sum(t) ## [1] 0.5865385 \\(K\\)-nearest neighbours Earlier in the course, we covered K-nearest neighbour classification. Let’s now explore how this approach is used in R and how it performs in the context of our market movement problem. To implement KNN in R, the most commonly used package is class. Note that it is possible to be confronted with the following error when loading the package: Error: (converted from warning) package ‘class’ was built under R version ... This will occur if you are using an older version of R than that under which the package was built. The best option is to update RStudio. If that is not possible (e.g. due to system requirements), then another option is to suppress it using suppressWarnings(library(class)) in your console. This should allow you to use the functions from the package. In R, we build our model using the knn() function from the class package. This function works differently to those we have covered so far for linear and logistic regression, and for LDA and QDA. This is because the knn() function both fits the model AND generates predictions. There are four arguments required: argument 1: predictors in our training data, argument 2: predictors in our test data, argument 3: outcome variable in our training data, argument 4: the value for \\(k\\); note the function specifies 1 nearest neighbour by default but I added it here for illustration purposes (this value needs to be added only when using a value other than 1). Now, you may wonder why I also included the drop = FALSE argument when I subsetting the dataset. fit_knn &lt;- knn(Weekly[train, &quot;Lag2&quot;, drop = FALSE], Weekly[!train, &quot;Lag2&quot;, drop = FALSE], Weekly$Direction[train], k = 1 ) Before we proceed to interpret the results, let’s see what output we would produce if we subsetted our dataset such that we extract our predictor from the training data without the drop = FALSE argument. This looks like a vector, right? Weekly[train, &quot;Lag2&quot;] ## [1] 1.572 0.816 -0.270 -2.576 3.514 0.712 1.178 -1.372 0.807 ## [10] 0.041 1.253 -2.678 -1.793 2.820 4.022 0.750 -0.017 2.420 ## [19] -1.225 1.171 -2.061 0.729 0.112 2.480 -1.552 -2.259 -2.428 ## [28] -2.708 -2.292 -4.978 3.547 0.260 -2.032 -1.739 -1.693 1.781 ## [37] -3.682 4.150 -2.487 2.343 0.606 1.077 -0.637 2.260 1.716 ## [46] -0.284 1.508 -0.913 -2.349 -1.798 5.393 1.156 2.077 4.751 ## [55] 2.702 -0.924 1.318 1.209 -0.363 -1.635 2.106 0.037 1.343 ## [64] 0.999 -1.348 0.470 -1.329 -0.892 1.370 3.269 -2.668 0.754 ## [73] -1.188 -1.745 0.787 1.649 1.044 -0.856 1.641 -0.015 -0.398 ## [82] 2.228 0.320 -1.601 -1.416 1.129 -0.521 -1.205 0.052 2.897 ## [91] -2.115 1.853 0.401 -2.614 -1.694 -0.245 1.034 1.417 0.668 ## [100] 5.018 3.169 -1.011 0.906 -0.807 -1.613 0.565 0.338 -0.255 ## [109] 0.309 -2.001 0.346 1.345 -1.896 -0.483 0.682 2.906 -1.687 ## [118] 0.858 0.853 -1.433 0.958 0.321 -0.450 -0.900 -1.486 -0.054 ## [127] 2.062 0.692 0.241 -0.967 3.064 -1.256 0.246 -1.205 -0.002 ## [136] 0.540 0.599 0.798 -2.029 -0.936 -1.903 2.253 0.576 1.106 ## [145] -0.263 1.161 0.999 0.823 0.442 0.387 1.741 -0.342 -0.923 ## [154] -1.529 1.888 -0.238 0.612 2.313 -0.969 -2.330 2.110 0.616 ## [163] 0.834 0.078 -0.533 -1.427 0.102 1.607 -2.653 0.723 0.482 ## [172] -0.622 1.429 0.976 -0.029 -0.622 -0.800 0.884 -0.393 0.509 ## [181] -0.527 0.303 0.230 0.123 0.325 1.337 0.960 0.174 0.082 ## [190] -0.626 -0.262 0.798 -0.210 1.996 -1.327 0.984 -1.766 1.266 ## [199] -0.599 0.099 0.395 -0.207 0.528 0.214 -0.199 0.740 1.066 ## [208] -0.040 0.838 -1.857 0.079 -0.530 -0.346 -0.285 0.366 0.990 ## [217] -2.225 -3.216 0.298 -0.206 0.325 0.733 -0.685 -0.822 2.427 ## [226] 0.530 0.612 -0.317 -0.048 -3.414 0.768 0.751 1.025 -0.231 ## [235] 1.137 -0.255 1.061 0.377 2.183 -0.593 -0.597 0.643 -2.445 ## [244] 0.661 -1.645 3.076 -0.897 1.910 -2.425 0.015 -0.190 -1.989 ## [253] 0.223 -1.399 2.649 0.224 -0.122 0.307 1.148 -0.255 1.207 ## [262] 1.756 0.587 0.106 1.274 -0.551 0.855 1.215 1.100 -0.052 ## [271] 1.140 0.555 -0.145 1.223 1.051 1.044 -1.210 0.859 1.692 ## [280] -0.858 2.252 1.830 -0.902 2.133 0.633 -1.120 1.682 -0.709 ## [289] -0.685 0.739 0.159 0.668 1.568 1.863 -0.278 0.461 -0.329 ## [298] 0.345 0.506 -1.321 1.875 0.364 1.240 -0.017 1.168 1.730 ## [307] -0.185 -0.712 0.650 0.127 -2.416 1.665 1.600 2.288 3.229 ## [316] -1.278 1.713 -2.232 -1.687 1.252 1.433 -0.787 1.605 -2.920 ## [325] 1.313 1.301 -1.810 1.630 2.579 1.435 -1.384 0.626 -1.108 ## [334] 0.149 0.568 -1.967 -1.711 -1.154 -0.443 4.181 -0.059 0.470 ## [343] 0.274 -2.255 0.566 3.791 0.954 -0.122 2.225 -0.114 1.450 ## [352] -1.393 0.407 3.844 0.930 1.506 1.107 -2.301 -1.482 2.776 ## [361] 1.058 -1.158 1.533 2.195 -0.728 2.030 0.432 2.396 -0.830 ## [370] -1.366 1.789 -1.466 -1.144 -1.303 -2.065 -2.672 3.889 -0.127 ## [379] 6.219 1.453 0.603 2.083 0.148 1.147 4.110 0.608 -1.268 ## [388] 3.338 -0.026 -0.151 2.566 0.889 -1.436 -3.506 2.523 -2.606 ## [397] 3.289 -0.553 2.879 -0.557 2.096 0.202 -2.360 -0.267 -2.869 ## [406] 1.409 0.091 3.742 -0.798 2.972 -3.090 -0.693 -1.090 4.120 ## [415] -4.856 3.646 -0.408 2.369 3.283 0.754 1.384 1.463 0.605 ## [424] 1.224 2.859 -0.338 2.488 -1.072 1.085 -1.320 1.182 -1.147 ## [433] 0.053 0.157 -1.770 2.112 -1.348 0.165 2.957 1.167 1.562 ## [442] 1.926 -3.872 -1.765 -2.786 -2.451 1.740 -5.004 -5.184 3.611 ## [451] 1.093 2.417 -4.034 -1.816 7.317 1.349 2.615 3.854 -1.340 ## [460] 3.361 2.473 -1.308 -0.874 1.849 3.219 0.241 3.731 -2.496 ## [469] -1.453 4.444 -3.145 -0.748 0.739 -0.072 2.999 1.499 0.363 ## [478] -1.269 0.851 4.223 -2.177 2.870 -1.597 0.735 -0.535 -0.561 ## [487] -2.139 1.990 -2.569 3.803 -2.050 5.771 0.867 1.105 -4.359 ## [496] -2.080 -2.140 2.106 0.673 0.872 0.665 -0.411 -1.201 -4.348 ## [505] 0.427 4.148 -6.632 4.348 4.708 0.536 1.885 1.858 -0.378 ## [514] 1.177 -1.134 0.282 2.626 0.748 -1.891 1.643 -1.624 -5.634 ## [523] 4.721 -2.615 -2.958 -0.946 5.686 -1.001 4.975 4.301 -1.891 ## [532] 1.186 -10.538 5.748 1.247 -1.363 -0.815 -0.986 -2.056 7.202 ## [541] -1.375 0.515 -1.569 0.910 1.671 2.102 -1.973 -4.074 3.031 ## [550] 0.609 1.351 0.987 0.951 -1.727 -1.920 -1.166 -0.843 -1.916 ## [559] -2.471 1.656 -1.242 3.415 -4.255 0.127 -1.897 -1.978 4.156 ## [568] -4.215 -0.473 1.097 -1.661 1.556 1.819 0.924 -0.404 -2.572 ## [577] -1.006 -4.277 -0.938 -0.062 -6.720 -0.930 1.799 -2.749 4.880 ## [586] 5.026 0.810 1.082 -1.653 3.716 -1.089 -1.348 0.340 -4.000 ## [595] 0.905 -0.079 -2.760 2.107 -0.397 -0.415 0.707 -1.992 -2.369 ## [604] 1.976 -4.334 -4.217 -11.050 7.780 2.924 1.892 -1.664 2.900 ## [613] -1.576 3.045 1.637 1.027 -0.947 1.655 -3.041 1.941 1.409 ## [622] 0.990 -2.295 -1.573 0.506 -0.978 -2.315 0.726 -1.299 3.848 ## [631] 2.874 0.159 -1.497 -0.114 -2.149 -1.044 1.275 -4.342 -0.269 ## [640] -1.718 4.891 -2.058 -1.539 -3.712 -1.972 -1.800 0.069 -0.080 ## [649] -6.839 -7.992 0.600 1.337 5.137 2.215 1.302 -2.635 -2.418 ## [658] -0.460 -4.992 -2.132 -3.238 4.339 5.874 1.499 0.369 -0.690 ## [667] 1.687 2.277 0.619 -2.572 -2.494 0.706 -2.273 3.791 2.089 ## [676] -2.780 -4.478 -0.662 -3.040 0.627 1.591 -0.828 -1.458 0.528 ## [685] 7.503 -3.605 1.778 -1.200 2.911 0.585 3.479 0.358 1.167 ## [694] -1.173 3.254 2.508 0.086 0.716 -1.955 0.971 1.262 -0.483 ## [703] 0.540 -1.855 -0.261 1.338 0.241 1.505 1.327 -0.270 1.735 ## [712] -3.807 3.310 0.797 0.121 -1.002 2.119 0.238 -0.272 -1.435 ## [721] 2.214 0.312 1.191 1.352 0.664 1.149 1.207 1.602 0.151 ## [730] -0.913 1.028 0.267 -0.148 0.073 1.041 -3.137 -0.963 -0.155 ## [739] 3.046 -0.218 -0.413 0.528 -2.920 -0.777 -0.273 -0.195 2.480 ## [748] 0.162 1.245 -0.128 -0.052 -0.798 -1.117 -1.026 -1.379 1.429 ## [757] -3.426 0.078 3.151 0.858 0.529 0.924 0.412 -1.634 1.927 ## [766] -0.827 -1.242 -1.124 3.145 3.183 1.544 -1.168 1.052 0.720 ## [775] -0.266 0.522 1.334 0.148 -2.123 -0.141 -1.406 0.299 2.704 ## [784] 0.189 -0.308 0.814 0.887 -1.803 -0.869 -1.532 0.128 0.706 ## [793] -3.266 0.831 0.411 1.253 -1.477 3.053 0.799 -0.230 0.175 ## [802] 1.573 -2.086 0.241 1.458 1.325 0.469 0.041 -0.629 0.324 ## [811] -0.868 -1.198 1.072 1.926 -0.288 -1.827 1.112 -2.678 -0.780 ## [820] -0.588 1.595 1.813 1.195 1.097 1.601 -0.250 -0.451 0.631 ## [829] 0.106 -1.606 2.977 0.168 -2.029 1.762 -1.534 0.234 1.598 ## [838] 0.170 -0.171 -0.451 2.016 -0.329 -0.620 0.049 -0.492 1.719 ## [847] -0.051 1.156 -2.604 -1.875 1.036 0.630 -2.788 -0.061 -0.563 ## [856] 2.065 -0.372 -2.314 0.331 3.085 0.063 -0.986 2.807 -0.554 ## [865] 1.229 -0.922 1.597 -0.370 1.603 1.029 1.188 0.218 0.639 ## [874] -0.947 1.217 1.470 -0.018 -0.303 0.940 1.224 -1.144 0.534 ## [883] -0.606 1.491 -0.016 -0.582 1.843 -0.713 1.216 -0.299 -4.412 ## [892] 1.130 -1.133 3.544 -1.062 1.612 0.630 2.168 0.655 0.773 ## [901] 0.015 1.122 -0.461 1.360 -1.866 1.674 -1.980 0.053 1.802 ## [910] 1.441 -1.185 -4.899 -1.775 1.436 -0.530 2.312 -0.364 -1.387 ## [919] 2.112 2.796 0.066 2.020 0.270 -3.917 2.309 -1.669 -3.706 ## [928] 0.347 -1.237 2.807 1.588 -2.440 1.125 -0.402 -4.522 -0.752 ## [937] -5.412 0.409 4.871 -4.596 1.405 0.231 -1.661 -2.800 -0.404 ## [946] 3.212 -1.075 4.195 -2.742 4.314 0.540 1.149 -1.812 2.670 ## [955] -3.467 1.777 -2.835 -0.048 -3.096 -3.001 -1.211 -1.854 1.710 ## [964] -0.232 0.203 2.857 0.145 -0.462 -0.725 -3.159 0.756 0.270 ## [973] -3.331 -9.399 -18.195 4.596 -6.781 10.491 -3.898 -6.198 -8.389 ## [982] 12.026 -2.251 0.418 0.926 If you wrap the code within the class function you can indeed confirm that the output is a vector. class(Weekly[train, &quot;Lag2&quot;]) ## [1] &quot;numeric&quot; The knn() requires that the training and test data be specified as either a matrix or a dataframe. If we set the drop argument to FALSE, then we are essentially telling R NOT to delete the dimensions of our object when we are subsetting it such that it keeps the row numbers, thereby producing a dataframe. class(Weekly[train, &quot;Lag2&quot;, drop = FALSE]) ## [1] &quot;data.frame&quot; You must pay close attention to the requirements and specifications for the functions with which you build your models. Often, you will be prompted by error messages in the console. For example, the knn() function expects either a matrix or a dataframe. If you do not set drop = FALSE you will not be able to proceed: Error in knn(Weekly[train, \"Lag2\"], Weekly[!train, \"Lag2\"], Weekly$Direction[train], : dims of 'test' and 'train' differ Other times, the error may be not so severe as to impede the function from working, but incorrectly structured data or improperly coded variables will lead to the function producing invalid results. Remember to carefully explore the arguments using the Help tab (e.g. ?knn). Now let’s return to our results and produce a confusion matrix. (t &lt;- table(fit_knn, Weekly[!train, ]$Direction)) ## ## fit_knn Down Up ## Down 21 29 ## Up 22 32 Our overall fraction of correct predictions is 0.5. Therefore, the KNN classifier (\\(k = 1\\)) performs the worst out of all other classifiers we have explored so far (but only slightly worse than QDA). sum(diag(t)) / sum(t) ## [1] 0.5096154 But before we move on to our next classifier, let’s consider other values for \\(k\\) for illustration purposes. To ensure consistent results, we also set the seed (to 1 in this case). We fit KNN for up to \\(k = 30\\) by using the base R sapply to apply the knn() function to every integer from 1 to 30 and to then calculate the overall fraction of correct prediction. set.seed(1) knn_k &lt;- sapply(1:30, function(k) { fit &lt;- knn(Weekly[train, &quot;Lag2&quot;, drop = FALSE], Weekly[!train, &quot;Lag2&quot;, drop = FALSE], Weekly$Direction[train], k = k ) mean(fit == Weekly[!train, ]$Direction) }) We can then create a plot to observe at what value for k the overall fraction of correct predictions is highest. This fraction stabilises at a value for \\(k\\) somewhere between \\(k = 10\\) and \\(k = 15\\). plot(1:30, knn_k, type = &quot;o&quot;, xlab = &quot;k&quot;, ylab = &quot;Fraction correct&quot;) We can find this out directly by asking R the index of the first time a maximum value among all other values appears. Our classifier appears to perform best when \\(k = 12\\). (k &lt;- which.max(knn_k)) ## [1] 12 Now let’s re-evaluate our KNN classifier on the test data using `\\(k = 12\\) and compute the confusion matrix. fit_knn &lt;- knn(Weekly[train, &quot;Lag2&quot;, drop = FALSE], Weekly[!train, &quot;Lag2&quot;, drop = FALSE], Weekly$Direction[train], k = 12 ) table(fit_knn , Weekly[!train, ]$Direction) ## ## fit_knn Down Up ## Down 18 18 ## Up 25 43 Now, the overall fraction of correct predictions is higher than it was for \\(k = 1\\) but this fraction still does not outperform logistic regression and LDA. mean(fit_knn == Weekly[!train, ]$Direction) ## [1] 0.5865385 Naive Bayes Finally, let’s evaluate the performance of Naive Bayes and conclude which approach performs best for our market movement classification problem. A useful package for Naive Bayes is e1071. Like the class package, do note that you may be prompted with a similar error when loading the package (this can also be addressed by either updating RStudio or suppressing the warning). fit_NBayes &lt;- naiveBayes(Direction ~ Lag2, data = Weekly, subset = train) Before generating the predictions, let’s explore the output of the fit. There are two important components: - A-priori probabilities: i.e. prior probabilities (distribution of the classes for the response variable) - Conditional probabilities: parameters of the model for the predictor by class. For a numeric variable (as is our predictor in this case), the parameters shown are the mean [,1] and standard deviation [,2] for the predictor values in each class; for a categorical variable, these would be conditional probabilities for the predictor in each class. The a priori probabilities can be extracted by specifying fit_NBayes$apriori and the conditional probabilities can be extracted using fit_NBayes$tables. fit_NBayes ## ## Naive Bayes Classifier for Discrete Predictors ## ## Call: ## naiveBayes.default(x = X, y = Y, laplace = laplace) ## ## A-priori probabilities: ## Y ## Down Up ## 0.4477157 0.5522843 ## ## Conditional probabilities: ## Lag2 ## Y [,1] [,2] ## Down -0.03568254 2.199504 ## Up 0.26036581 2.317485 Now let’s predict market movement. pred_NBayes &lt;- predict(fit_NBayes, Weekly[!train, ], type = &quot;class&quot;) And finally, generate our confusion matrix. (t &lt;- table(pred_NBayes, Weekly[!train, ]$Direction)) ## ## pred_NBayes Down Up ## Down 0 0 ## Up 43 61 Our overall fraction of correct predictions is \\(0.5865385\\). Naive Bayes performs slightly better than KNN with \\(k = 1\\) (\\(0.5\\)) and the same as QDA (\\(0.5865385\\)). sum(diag(t)) / sum(t) ## [1] 0.5865385 Based on the approaches we have implemented in this demonstration, logistic regression and LDA perform best. "],["demonstration-2-poisson-versus-linear-regression.html", "Demonstration 2: Poisson versus Linear Regression The Linear Model The Poisson Model Linear versus Poisson Regression", " Demonstration 2: Poisson versus Linear Regression In this demonstration, we will cover the basics of building and interpreting a Poisson regression model. You will need the Bikeshare dataset, part of the ISRL2 package. By loading the package, the Weekly dataset loads automatically. This dataset measures the number of bike rentals (bikers) per hour in Washington, DC. It contains 8645 observations on several variables. The variables we will use in this practical and their descriptions are listed below. For further information on additional variables, type ?Bikeshare in your R console after loading the ISLR2 package. mnth: Month of the year, coded as a factor. hr: Hour of the day, coded as a factor from 0 to 23. workingday: Is it a work day? Yes=1, No=0. temp: Normalised temperature in Celsius. The values are derived via (t-t_min)/(t_max-t_min), t_min=-8, t_max=+39; weathersit: Weather, coded as a factor. Loading the necessary packages: library(ISLR2) ## Warning: package &#39;ISLR2&#39; was built under R version 4.2.3 Once you load the ISLR2 package, the Bikeshare dataset will be ‘loaded’ too and can be accessed without needing to assign it to a separate object. head(Bikeshare) ## season mnth day hr holiday weekday workingday weathersit temp atemp hum ## 1 1 Jan 1 0 0 6 0 clear 0.24 0.2879 0.81 ## 2 1 Jan 1 1 0 6 0 clear 0.22 0.2727 0.80 ## 3 1 Jan 1 2 0 6 0 clear 0.22 0.2727 0.80 ## 4 1 Jan 1 3 0 6 0 clear 0.24 0.2879 0.75 ## 5 1 Jan 1 4 0 6 0 clear 0.24 0.2879 0.75 ## 6 1 Jan 1 5 0 6 0 cloudy/misty 0.24 0.2576 0.75 ## windspeed casual registered bikers ## 1 0.0000 3 13 16 ## 2 0.0000 8 32 40 ## 3 0.0000 5 27 32 ## 4 0.0000 3 10 13 ## 5 0.0000 0 1 1 ## 6 0.0896 0 1 1 As usual, we can access variables within the dataset by indexing them. Bikeshare$temp ## [1] 0.24 0.22 0.22 0.24 0.24 0.24 0.22 0.20 0.24 0.32 0.38 0.36 0.42 0.46 ## [15] 0.46 0.44 0.42 0.44 0.42 0.42 0.40 0.40 0.40 0.46 0.46 0.44 0.42 0.46 ## [29] 0.46 0.42 0.40 0.40 0.38 0.36 0.36 0.36 0.36 0.36 0.34 0.34 0.34 0.36 ## [43] 0.32 0.30 0.26 0.24 0.22 0.22 0.20 0.16 0.16 0.14 0.14 0.14 0.16 0.18 ## [57] 0.20 0.22 0.24 0.26 0.26 0.26 0.24 0.24 0.20 0.20 0.18 0.14 0.18 0.16 ## [71] 0.16 0.14 0.14 0.12 0.12 0.12 0.14 0.16 0.16 0.22 0.22 0.24 0.26 0.28 ## [85] 0.30 0.28 0.26 0.24 0.24 0.22 0.22 0.20 0.20 0.16 0.16 0.24 0.22 0.20 ## [99] 0.18 0.20 0.22 0.22 0.26 0.26 0.28 0.30 0.30 0.30 0.24 0.24 0.24 0.22 ## [113] 0.20 0.18 0.20 0.18 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.18 0.20 0.22 ## [127] 0.26 0.26 0.28 0.28 0.26 0.22 0.22 0.22 0.20 0.22 0.22 0.20 0.20 0.20 ## [141] 0.20 0.20 0.22 0.20 0.20 0.20 0.20 0.22 0.20 0.20 0.20 0.20 0.20 0.20 ## [155] 0.20 0.20 0.16 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.16 0.16 ## [169] 0.16 0.16 0.16 0.18 0.20 0.20 0.20 0.20 0.20 0.18 0.16 0.14 0.14 0.12 ## [183] 0.12 0.12 0.10 0.10 0.10 0.10 0.10 0.08 0.08 0.10 0.08 0.10 0.12 0.14 ## [197] 0.16 0.18 0.20 0.22 0.22 0.20 0.18 0.16 0.16 0.14 0.14 0.14 0.12 0.12 ## [211] 0.12 0.12 0.12 0.10 0.10 0.12 0.12 0.12 0.14 0.14 0.16 0.20 0.20 0.20 ## [225] 0.20 0.20 0.20 0.20 0.16 0.16 0.14 0.14 0.14 0.14 0.14 0.16 0.16 0.16 ## [239] 0.16 0.18 0.18 0.20 0.20 0.20 0.20 0.20 0.16 0.16 0.16 0.16 0.16 0.16 ## [253] 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.12 0.14 0.16 0.16 0.18 0.20 0.20 ## [267] 0.22 0.20 0.20 0.22 0.20 0.20 0.18 0.16 0.16 0.16 0.14 0.14 0.14 0.14 ## [281] 0.14 0.14 0.14 0.12 0.12 0.14 0.14 0.16 0.20 0.20 0.22 0.22 0.24 0.24 ## [295] 0.20 0.20 0.16 0.16 0.14 0.14 0.12 0.12 0.10 0.10 0.10 0.10 0.10 0.10 ## [309] 0.12 0.14 0.18 0.18 0.20 0.22 0.22 0.24 0.22 0.22 0.20 0.16 0.18 0.16 ## [323] 0.16 0.18 0.18 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.14 0.16 0.18 0.20 ## [337] 0.24 0.28 0.30 0.32 0.34 0.32 0.30 0.32 0.32 0.32 0.30 0.30 0.26 0.26 ## [351] 0.26 0.22 0.26 0.26 0.26 0.24 0.22 0.22 0.22 0.24 0.24 0.26 0.28 0.26 ## [365] 0.24 0.22 0.20 0.18 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.18 0.18 0.18 ## [379] 0.18 0.16 0.16 0.16 0.16 0.16 0.18 0.18 0.18 0.20 0.20 0.20 0.18 0.18 ## [393] 0.16 0.16 0.14 0.16 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 ## [407] 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.24 0.24 0.24 0.26 0.28 0.30 ## [421] 0.40 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.32 0.30 0.30 0.26 0.26 0.26 ## [435] 0.26 0.26 0.24 0.22 0.22 0.22 0.24 0.26 0.28 0.30 0.28 0.30 0.32 0.30 ## [449] 0.30 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.22 0.22 0.24 0.22 ## [463] 0.20 0.20 0.20 0.20 0.22 0.22 0.20 0.20 0.16 0.16 0.14 0.12 0.12 0.10 ## [477] 0.08 0.06 0.06 0.04 0.04 0.04 0.04 0.02 0.02 0.02 0.02 0.04 0.04 0.06 ## [491] 0.06 0.08 0.10 0.12 0.12 0.12 0.08 0.08 0.06 0.06 0.06 0.04 0.04 0.04 ## [505] 0.02 0.02 0.04 0.04 0.08 0.06 0.10 0.14 0.14 0.16 0.14 0.16 0.16 0.16 ## [519] 0.14 0.12 0.12 0.10 0.10 0.08 0.06 0.06 0.04 0.04 0.02 0.02 0.02 0.02 ## [533] 0.04 0.06 0.10 0.10 0.12 0.14 0.14 0.16 0.16 0.14 0.14 0.14 0.14 0.14 ## [547] 0.14 0.16 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.16 0.20 0.22 0.24 0.26 ## [561] 0.26 0.30 0.32 0.32 0.30 0.30 0.26 0.24 0.24 0.22 0.22 0.22 0.24 0.22 ## [575] 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.20 0.22 ## [589] 0.22 0.20 0.20 0.18 0.18 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.18 0.16 ## [603] 0.16 0.18 0.18 0.18 0.18 0.18 0.22 0.20 0.22 0.24 0.24 0.24 0.24 0.22 ## [617] 0.24 0.24 0.22 0.22 0.22 0.20 0.16 0.16 0.16 0.18 0.18 0.18 0.18 0.20 ## [631] 0.22 0.22 0.22 0.24 0.24 0.22 0.22 0.18 0.18 0.16 0.16 0.16 0.14 0.16 ## [645] 0.14 0.14 0.14 0.14 0.14 0.16 0.18 0.22 0.30 0.28 0.28 0.30 0.30 0.30 ## [659] 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.18 0.16 0.16 ## [673] 0.16 0.16 0.16 0.16 0.18 0.16 0.18 0.16 0.16 0.16 0.16 0.30 0.16 0.16 ## [687] 0.16 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.16 0.16 0.18 0.20 ## [701] 0.20 0.22 0.24 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.22 0.22 0.22 ## [715] 0.22 0.22 0.22 0.22 0.22 0.22 0.24 0.22 0.24 0.24 0.34 0.38 0.38 0.36 ## [729] 0.36 0.34 0.28 0.24 0.22 0.22 0.20 0.20 0.20 0.18 0.18 0.16 0.16 0.14 ## [743] 0.14 0.16 0.18 0.18 0.20 0.20 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.18 ## [757] 0.18 0.20 0.20 0.16 0.14 0.14 0.14 0.16 0.14 0.14 0.16 0.20 0.22 0.24 ## [771] 0.26 0.28 0.28 0.30 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 ## [785] 0.24 0.22 0.20 0.20 0.22 0.20 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22 ## [799] 0.24 0.28 0.28 0.30 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 ## [813] 0.24 0.24 0.28 0.30 0.32 0.34 0.34 0.34 0.34 0.34 0.34 0.30 0.28 0.28 ## [827] 0.26 0.26 0.24 0.24 0.22 0.20 0.20 0.20 0.20 0.18 0.18 0.16 0.22 0.24 ## [841] 0.30 0.32 0.36 0.36 0.38 0.36 0.32 0.34 0.32 0.32 0.32 0.28 0.30 0.28 ## [855] 0.28 0.26 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.22 0.22 0.24 0.24 0.22 ## [869] 0.22 0.22 0.22 0.20 0.16 0.16 0.14 0.12 0.12 0.10 0.10 0.08 0.06 0.06 ## [883] 0.06 0.06 0.10 0.12 0.14 0.14 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.14 ## [897] 0.14 0.14 0.16 0.16 0.14 0.14 0.14 0.14 0.12 0.12 0.10 0.10 0.12 0.12 ## [911] 0.14 0.16 0.18 0.20 0.20 0.20 0.18 0.16 0.14 0.14 0.14 0.12 0.12 0.10 ## [925] 0.10 0.10 0.08 0.10 0.08 0.10 0.12 0.14 0.22 0.22 0.24 0.30 0.32 0.30 ## [939] 0.30 0.28 0.26 0.22 0.20 0.20 0.18 0.16 0.14 0.14 0.12 0.12 0.12 0.12 ## [953] 0.12 0.14 0.16 0.22 0.30 0.30 0.30 0.34 0.34 0.34 0.32 0.28 0.28 0.26 ## [967] 0.26 0.24 0.22 0.20 0.20 0.20 0.20 0.20 0.20 0.22 0.22 0.24 0.30 0.32 ## [981] 0.36 0.38 0.40 0.40 0.42 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 ## [995] 0.36 0.34 0.32 0.32 0.34 0.34 0.38 0.40 0.44 0.52 0.56 0.58 0.60 0.56 ## [1009] 0.52 0.46 0.40 0.38 0.36 0.36 0.34 0.32 0.30 0.30 0.28 0.22 0.22 0.20 ## [1023] 0.20 0.20 0.22 0.24 0.26 0.28 0.32 0.34 0.34 0.34 0.32 0.30 0.28 0.26 ## [1037] 0.24 0.24 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.20 0.22 0.24 0.26 0.34 ## [1051] 0.38 0.42 0.46 0.46 0.46 0.46 0.40 0.34 0.38 0.36 0.34 0.38 0.34 0.34 ## [1065] 0.34 0.34 0.32 0.32 0.30 0.32 0.32 0.36 0.38 0.44 0.48 0.54 0.60 0.60 ## [1079] 0.56 0.58 0.54 0.48 0.48 0.52 0.50 0.46 0.44 0.44 0.44 0.46 0.46 0.46 ## [1093] 0.44 0.42 0.42 0.42 0.44 0.44 0.50 0.60 0.66 0.66 0.66 0.66 0.64 0.62 ## [1107] 0.60 0.58 0.54 0.52 0.48 0.46 0.44 0.42 0.40 0.40 0.40 0.38 0.38 0.40 ## [1121] 0.42 0.44 0.44 0.44 0.46 0.44 0.44 0.42 0.36 0.34 0.32 0.32 0.30 0.28 ## [1135] 0.26 0.24 0.24 0.22 0.22 0.20 0.18 0.20 0.22 0.26 0.30 0.30 0.34 0.36 ## [1149] 0.36 0.36 0.34 0.34 0.34 0.34 0.32 0.32 0.30 0.34 0.34 0.34 0.34 0.32 ## [1163] 0.34 0.42 0.42 0.32 0.32 0.32 0.32 0.32 0.30 0.32 0.30 0.28 0.28 0.24 ## [1177] 0.24 0.24 0.22 0.20 0.20 0.12 0.12 0.12 0.14 0.16 0.16 0.20 0.22 0.22 ## [1191] 0.24 0.22 0.22 0.22 0.20 0.20 0.20 0.16 0.16 0.14 0.14 0.12 0.12 0.12 ## [1205] 0.12 0.12 0.14 0.18 0.20 0.24 0.26 0.30 0.32 0.34 0.34 0.34 0.32 0.30 ## [1219] 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.24 0.24 0.26 ## [1233] 0.32 0.36 0.38 0.40 0.40 0.38 0.36 0.34 0.34 0.34 0.34 0.34 0.32 0.32 ## [1247] 0.32 0.32 0.32 0.32 0.34 0.34 0.36 0.34 0.42 0.52 0.54 0.54 0.56 0.46 ## [1261] 0.32 0.32 0.32 0.30 0.30 0.28 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22 ## [1275] 0.22 0.22 0.24 0.26 0.30 0.30 0.32 0.34 0.34 0.36 0.36 0.36 0.34 0.32 ## [1289] 0.30 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.24 0.24 0.24 0.26 0.28 0.30 ## [1303] 0.36 0.40 0.42 0.44 0.46 0.48 0.42 0.40 0.40 0.40 0.38 0.38 0.36 0.36 ## [1317] 0.34 0.32 0.34 0.34 0.36 0.34 0.42 0.52 0.56 0.56 0.46 0.42 0.42 0.42 ## [1331] 0.40 0.46 0.44 0.44 0.38 0.34 0.32 0.30 0.26 0.24 0.22 0.22 0.20 0.20 ## [1345] 0.20 0.20 0.22 0.24 0.28 0.30 0.32 0.32 0.34 0.34 0.34 0.32 0.30 0.30 ## [1359] 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.20 0.22 0.22 0.22 0.24 0.28 0.32 ## [1373] 0.34 0.40 0.50 0.52 0.54 0.54 0.50 0.46 0.40 0.36 0.34 0.30 0.26 0.24 ## [1387] 0.24 0.20 0.20 0.16 0.14 0.14 0.12 0.14 0.16 0.18 0.20 0.22 0.22 0.24 ## [1401] 0.24 0.26 0.26 0.24 0.20 0.20 0.18 0.20 0.18 0.20 0.18 0.18 0.18 0.18 ## [1415] 0.16 0.16 0.16 0.18 0.22 0.24 0.28 0.32 0.34 0.36 0.36 0.36 0.36 0.34 ## [1429] 0.32 0.30 0.30 0.30 0.30 0.28 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 ## [1443] 0.32 0.34 0.40 0.44 0.46 0.48 0.46 0.48 0.48 0.48 0.46 0.44 0.44 0.42 ## [1457] 0.44 0.42 0.42 0.40 0.42 0.42 0.42 0.42 0.40 0.42 0.42 0.42 0.46 0.46 ## [1471] 0.44 0.44 0.36 0.34 0.32 0.30 0.28 0.24 0.22 0.22 0.20 0.20 0.20 0.20 ## [1485] 0.20 0.20 0.20 0.20 0.22 0.24 0.26 0.30 0.32 0.32 0.34 0.34 0.34 0.32 ## [1499] 0.30 0.30 0.28 0.26 0.28 0.26 0.24 0.24 0.24 0.22 0.20 0.20 0.18 0.22 ## [1513] 0.26 0.30 0.36 0.36 0.38 0.38 0.36 0.38 0.36 0.34 0.34 0.32 0.30 0.30 ## [1527] 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.26 0.30 0.32 0.32 ## [1541] 0.32 0.34 0.36 0.34 0.34 0.34 0.34 0.32 0.32 0.32 0.34 0.34 0.34 0.34 ## [1555] 0.36 0.36 0.38 0.38 0.40 0.40 0.40 0.42 0.42 0.44 0.44 0.42 0.44 0.44 ## [1569] 0.44 0.36 0.36 0.34 0.34 0.34 0.34 0.34 0.32 0.30 0.26 0.26 0.28 0.30 ## [1583] 0.32 0.32 0.34 0.36 0.36 0.34 0.34 0.34 0.32 0.30 0.30 0.30 0.30 0.30 ## [1597] 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.24 0.26 0.28 0.32 0.34 0.34 0.36 ## [1611] 0.40 0.42 0.46 0.46 0.42 0.42 0.40 0.38 0.36 0.38 0.38 0.36 0.34 0.34 ## [1625] 0.36 0.34 0.36 0.40 0.40 0.42 0.44 0.46 0.46 0.46 0.48 0.46 0.44 0.40 ## [1639] 0.36 0.32 0.30 0.30 0.26 0.26 0.26 0.26 0.26 0.24 0.26 0.28 0.30 0.32 ## [1653] 0.34 0.36 0.38 0.38 0.38 0.38 0.40 0.38 0.36 0.36 0.34 0.34 0.32 0.32 ## [1667] 0.32 0.30 0.30 0.24 0.24 0.22 0.24 0.26 0.30 0.32 0.34 0.36 0.36 0.38 ## [1681] 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.30 ## [1695] 0.30 0.30 0.32 0.32 0.36 0.36 0.40 0.40 0.40 0.40 0.40 0.44 0.44 0.44 ## [1709] 0.42 0.42 0.40 0.40 0.38 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.36 0.40 ## [1723] 0.44 0.44 0.50 0.52 0.50 0.52 0.50 0.50 0.46 0.44 0.42 0.42 0.40 0.42 ## [1737] 0.42 0.40 0.40 0.36 0.38 0.40 0.40 0.42 0.46 0.52 0.54 0.56 0.64 0.66 ## [1751] 0.68 0.68 0.70 0.68 0.66 0.62 0.62 0.62 0.60 0.60 0.58 0.56 0.54 0.52 ## [1765] 0.52 0.44 0.40 0.42 0.42 0.44 0.46 0.46 0.50 0.50 0.50 0.50 0.48 0.46 ## [1779] 0.44 0.42 0.40 0.40 0.38 0.34 0.32 0.30 0.28 0.26 0.26 0.26 0.24 0.28 ## [1793] 0.30 0.32 0.34 0.36 0.38 0.40 0.40 0.42 0.40 0.38 0.36 0.36 0.34 0.34 ## [1807] 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.30 0.30 0.34 0.38 0.42 0.44 0.50 ## [1821] 0.54 0.56 0.54 0.54 0.52 0.58 0.56 0.46 0.46 0.46 0.46 0.42 0.44 0.44 ## [1835] 0.42 0.40 0.40 0.40 0.40 0.40 0.44 0.44 0.46 0.50 0.50 0.50 0.50 0.50 ## [1849] 0.48 0.44 0.44 0.42 0.40 0.40 0.36 0.34 0.34 0.34 0.32 0.34 0.32 0.32 ## [1863] 0.32 0.34 0.34 0.34 0.34 0.36 0.38 0.40 0.40 0.38 0.38 0.36 0.32 0.32 ## [1877] 0.32 0.32 0.30 0.30 0.28 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.30 0.30 ## [1891] 0.32 0.30 0.30 0.30 0.30 0.30 0.30 0.28 0.26 0.26 0.24 0.24 0.20 0.20 ## [1905] 0.20 0.18 0.18 0.18 0.20 0.22 0.24 0.26 0.30 0.32 0.32 0.36 0.34 0.34 ## [1919] 0.32 0.30 0.30 0.30 0.30 0.28 0.26 0.26 0.26 0.22 0.20 0.22 0.20 0.18 ## [1933] 0.20 0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.34 0.34 0.32 0.32 0.28 0.28 ## [1947] 0.26 0.28 0.26 0.26 0.24 0.22 0.20 0.18 0.16 0.16 0.20 0.22 0.22 0.24 ## [1961] 0.26 0.30 0.32 0.32 0.34 0.32 0.30 0.30 0.28 0.26 0.26 0.26 0.22 0.22 ## [1975] 0.22 0.22 0.18 0.18 0.20 0.20 0.22 0.24 0.26 0.26 0.30 0.32 0.32 0.34 ## [1989] 0.34 0.32 0.30 0.32 0.32 0.30 0.28 0.26 0.24 0.24 0.24 0.20 0.22 0.22 ## [2003] 0.22 0.24 0.28 0.30 0.34 0.34 0.34 0.36 0.38 0.38 0.40 0.36 0.34 0.36 ## [2017] 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30 0.32 0.32 0.32 0.34 ## [2031] 0.34 0.34 0.36 0.36 0.28 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24 ## [2045] 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.26 0.26 0.28 0.28 0.30 0.30 ## [2059] 0.30 0.30 0.30 0.30 0.30 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.24 0.24 ## [2073] 0.24 0.24 0.24 0.26 0.32 0.32 0.32 0.34 0.36 0.36 0.34 0.34 0.34 0.34 ## [2087] 0.34 0.32 0.32 0.30 0.30 0.30 0.26 0.24 0.24 0.24 0.24 0.26 0.26 0.30 ## [2101] 0.34 0.36 0.40 0.32 0.34 0.32 0.34 0.38 0.38 0.38 0.36 0.34 0.32 0.32 ## [2115] 0.32 0.30 0.30 0.26 0.30 0.28 0.28 0.28 0.32 0.34 0.36 0.40 0.42 0.44 ## [2129] 0.44 0.46 0.46 0.46 0.46 0.46 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.38 ## [2143] 0.38 0.38 0.40 0.42 0.44 0.46 0.50 0.54 0.60 0.64 0.68 0.74 0.76 0.76 ## [2157] 0.74 0.72 0.70 0.70 0.70 0.68 0.64 0.62 0.62 0.54 0.54 0.50 0.46 0.48 ## [2171] 0.48 0.38 0.36 0.34 0.32 0.34 0.36 0.36 0.40 0.38 0.42 0.38 0.36 0.34 ## [2185] 0.34 0.32 0.30 0.30 0.26 0.24 0.26 0.24 0.24 0.24 0.26 0.32 0.36 0.40 ## [2199] 0.42 0.44 0.46 0.50 0.52 0.54 0.52 0.52 0.50 0.46 0.46 0.46 0.46 0.46 ## [2213] 0.42 0.42 0.36 0.34 0.34 0.32 0.34 0.36 0.40 0.42 0.46 0.46 0.52 0.56 ## [2227] 0.60 0.60 0.52 0.48 0.46 0.44 0.44 0.40 0.38 0.36 0.34 0.34 0.34 0.34 ## [2241] 0.32 0.34 0.34 0.34 0.36 0.36 0.40 0.38 0.36 0.34 0.34 0.32 0.32 0.32 ## [2255] 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.30 0.30 0.30 0.30 0.30 0.32 ## [2269] 0.34 0.34 0.36 0.36 0.36 0.36 0.36 0.38 0.38 0.38 0.38 0.38 0.36 0.36 ## [2283] 0.38 0.38 0.38 0.38 0.38 0.36 0.36 0.36 0.36 0.38 0.38 0.40 0.40 0.42 ## [2297] 0.46 0.50 0.50 0.52 0.52 0.50 0.50 0.46 0.44 0.44 0.46 0.48 0.46 0.46 ## [2311] 0.46 0.46 0.46 0.50 0.52 0.56 0.56 0.60 0.60 0.64 0.72 0.74 0.74 0.74 ## [2325] 0.72 0.72 0.68 0.66 0.64 0.58 0.62 0.62 0.60 0.58 0.56 0.54 0.54 0.54 ## [2339] 0.48 0.46 0.50 0.52 0.56 0.54 0.50 0.48 0.48 0.44 0.44 0.42 0.42 0.42 ## [2353] 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.38 0.38 0.40 0.42 ## [2367] 0.42 0.44 0.42 0.44 0.46 0.46 0.44 0.44 0.44 0.42 0.42 0.40 0.38 0.38 ## [2381] 0.36 0.34 0.34 0.34 0.34 0.38 0.42 0.46 0.50 0.52 0.54 0.56 0.56 0.60 ## [2395] 0.60 0.60 0.56 0.54 0.50 0.46 0.48 0.46 0.44 0.44 0.40 0.40 0.38 0.36 ## [2409] 0.36 0.40 0.44 0.50 0.50 0.52 0.52 0.54 0.54 0.54 0.52 0.50 0.46 0.42 ## [2423] 0.40 0.40 0.38 0.36 0.36 0.36 0.36 0.36 0.36 0.38 0.40 0.40 0.40 0.40 ## [2437] 0.42 0.42 0.46 0.46 0.52 0.52 0.50 0.50 0.50 0.52 0.44 0.44 0.42 0.44 ## [2451] 0.44 0.42 0.40 0.40 0.36 0.36 0.36 0.36 0.38 0.40 0.42 0.46 0.46 0.50 ## [2465] 0.52 0.54 0.54 0.56 0.56 0.56 0.52 0.50 0.50 0.44 0.46 0.46 0.42 0.42 ## [2479] 0.40 0.40 0.40 0.46 0.46 0.50 0.52 0.54 0.56 0.56 0.58 0.60 0.60 0.58 ## [2493] 0.64 0.56 0.60 0.56 0.52 0.50 0.50 0.46 0.46 0.48 0.46 0.46 0.48 0.52 ## [2507] 0.50 0.52 0.50 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.54 0.50 0.50 0.50 ## [2521] 0.48 0.46 0.44 0.42 0.42 0.42 0.40 0.40 0.42 0.44 0.62 0.66 0.62 0.62 ## [2535] 0.70 0.70 0.74 0.76 0.76 0.74 0.74 0.70 0.68 0.66 0.62 0.60 0.56 0.52 ## [2549] 0.50 0.46 0.44 0.42 0.40 0.42 0.40 0.42 0.42 0.44 0.46 0.48 0.50 0.52 ## [2563] 0.52 0.50 0.50 0.46 0.44 0.42 0.42 0.40 0.36 0.36 0.36 0.36 0.34 0.34 ## [2577] 0.34 0.34 0.34 0.34 0.36 0.34 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.32 ## [2591] 0.32 0.30 0.30 0.32 0.32 0.32 0.32 0.32 0.34 0.34 0.34 0.34 0.36 0.38 ## [2605] 0.42 0.46 0.52 0.52 0.58 0.60 0.60 0.60 0.58 0.56 0.54 0.56 0.58 0.54 ## [2619] 0.52 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.52 0.56 0.60 0.66 0.68 0.70 ## [2633] 0.70 0.66 0.74 0.66 0.64 0.60 0.60 0.54 0.54 0.54 0.52 0.54 0.54 0.50 ## [2647] 0.52 0.46 0.50 0.52 0.56 0.60 0.64 0.64 0.66 0.70 0.72 0.74 0.70 0.70 ## [2661] 0.68 0.66 0.66 0.62 0.60 0.58 0.62 0.62 0.56 0.54 0.56 0.54 0.56 0.58 ## [2675] 0.58 0.64 0.66 0.68 0.70 0.74 0.72 0.70 0.70 0.68 0.68 0.64 0.64 0.62 ## [2689] 0.60 0.60 0.60 0.60 0.58 0.58 0.56 0.56 0.56 0.58 0.58 0.60 0.62 0.64 ## [2703] 0.66 0.64 0.68 0.70 0.70 0.66 0.66 0.62 0.64 0.62 0.62 0.62 0.64 0.62 ## [2717] 0.62 0.64 0.62 0.62 0.64 0.64 0.66 0.62 0.62 0.62 0.62 0.62 0.62 0.66 ## [2731] 0.62 0.64 0.64 0.62 0.58 0.56 0.54 0.54 0.52 0.50 0.46 0.46 0.46 0.46 ## [2745] 0.50 0.52 0.54 0.54 0.56 0.60 0.56 0.56 0.60 0.56 0.54 0.52 0.52 0.46 ## [2759] 0.48 0.46 0.42 0.44 0.44 0.44 0.44 0.42 0.42 0.42 0.40 0.40 0.40 0.44 ## [2773] 0.48 0.50 0.52 0.54 0.54 0.56 0.58 0.56 0.54 0.54 0.44 0.44 0.44 0.44 ## [2787] 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.42 0.44 0.46 0.48 0.46 0.48 0.50 ## [2801] 0.50 0.50 0.50 0.48 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.44 0.44 ## [2815] 0.44 0.44 0.44 0.46 0.48 0.50 0.54 0.58 0.62 0.62 0.64 0.66 0.66 0.66 ## [2829] 0.64 0.62 0.62 0.60 0.60 0.56 0.56 0.56 0.56 0.54 0.52 0.52 0.52 0.54 ## [2843] 0.56 0.60 0.64 0.66 0.68 0.70 0.70 0.70 0.72 0.70 0.70 0.68 0.66 0.64 ## [2857] 0.58 0.56 0.52 0.50 0.50 0.42 0.38 0.36 0.34 0.34 0.34 0.34 0.34 0.40 ## [2871] 0.44 0.46 0.50 0.48 0.50 0.40 0.42 0.42 0.40 0.40 0.38 0.36 0.36 0.34 ## [2885] 0.34 0.34 0.34 0.34 0.34 0.38 0.42 0.46 0.50 0.52 0.52 0.54 0.54 0.56 ## [2899] 0.58 0.56 0.56 0.54 0.50 0.50 0.48 0.46 0.44 0.40 0.38 0.36 0.36 0.34 ## [2913] 0.36 0.40 0.42 0.46 0.54 0.54 0.56 0.58 0.60 0.60 0.60 0.58 0.54 0.54 ## [2927] 0.52 0.48 0.46 0.44 0.42 0.42 0.42 0.42 0.40 0.46 0.42 0.48 0.52 0.54 ## [2941] 0.56 0.56 0.60 0.60 0.60 0.60 0.60 0.58 0.58 0.56 0.56 0.54 0.54 0.50 ## [2955] 0.50 0.52 0.48 0.46 0.42 0.44 0.44 0.46 0.52 0.56 0.58 0.58 0.60 0.60 ## [2969] 0.60 0.60 0.60 0.58 0.58 0.56 0.52 0.52 0.50 0.46 0.46 0.44 0.44 0.46 ## [2983] 0.42 0.42 0.44 0.48 0.52 0.54 0.56 0.60 0.60 0.62 0.62 0.62 0.64 0.62 ## [2997] 0.62 0.58 0.54 0.52 0.52 0.50 0.48 0.46 0.44 0.44 0.42 0.40 0.42 0.44 ## [3011] 0.50 0.52 0.56 0.56 0.60 0.62 0.62 0.64 0.66 0.64 0.64 0.60 0.54 0.54 ## [3025] 0.52 0.52 0.52 0.50 0.52 0.50 0.48 0.46 0.46 0.48 0.48 0.52 0.54 0.56 ## [3039] 0.60 0.62 0.62 0.64 0.66 0.64 0.62 0.56 0.54 0.54 0.50 0.46 0.46 0.46 ## [3053] 0.44 0.44 0.42 0.42 0.44 0.46 0.48 0.50 0.54 0.58 0.58 0.62 0.62 0.64 ## [3067] 0.64 0.64 0.62 0.60 0.60 0.56 0.54 0.54 0.52 0.52 0.50 0.50 0.50 0.50 ## [3081] 0.50 0.50 0.50 0.50 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 ## [3095] 0.52 0.52 0.52 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.48 0.50 0.52 ## [3109] 0.52 0.52 0.52 0.52 0.54 0.54 0.54 0.56 0.56 0.54 0.54 0.54 0.54 0.52 ## [3123] 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.54 0.58 0.58 0.60 0.62 0.62 ## [3137] 0.64 0.66 0.64 0.56 0.56 0.56 0.54 0.54 0.56 0.54 0.52 0.52 0.50 0.50 ## [3151] 0.50 0.50 0.52 0.52 0.56 0.60 0.62 0.64 0.66 0.68 0.68 0.72 0.60 0.58 ## [3165] 0.58 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.56 0.54 0.54 0.52 0.52 0.52 ## [3179] 0.52 0.54 0.54 0.56 0.56 0.56 0.62 0.62 0.62 0.62 0.60 0.60 0.58 0.54 ## [3193] 0.54 0.54 0.54 0.54 0.52 0.52 0.52 0.52 0.52 0.54 0.56 0.56 0.54 0.54 ## [3207] 0.56 0.56 0.58 0.60 0.60 0.60 0.60 0.56 0.56 0.52 0.52 0.52 0.52 0.50 ## [3221] 0.50 0.48 0.48 0.48 0.50 0.50 0.52 0.54 0.54 0.54 0.58 0.58 0.54 0.56 ## [3235] 0.58 0.56 0.60 0.58 0.54 0.54 0.50 0.48 0.46 0.46 0.44 0.44 0.44 0.44 ## [3249] 0.46 0.50 0.54 0.54 0.56 0.58 0.60 0.60 0.62 0.60 0.60 0.62 0.60 0.58 ## [3263] 0.58 0.56 0.54 0.52 0.52 0.52 0.52 0.48 0.46 0.46 0.50 0.54 0.56 0.60 ## [3277] 0.62 0.64 0.66 0.70 0.72 0.72 0.72 0.72 0.70 0.68 0.62 0.62 0.60 0.58 ## [3291] 0.54 0.52 0.52 0.50 0.50 0.50 0.52 0.54 0.60 0.62 0.64 0.70 0.72 0.66 ## [3305] 0.62 0.66 0.68 0.70 0.66 0.66 0.64 0.62 0.60 0.58 0.56 0.56 0.56 0.54 ## [3319] 0.54 0.54 0.54 0.56 0.60 0.60 0.66 0.68 0.68 0.74 0.72 0.72 0.72 0.72 ## [3333] 0.70 0.70 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.58 0.60 0.58 0.58 0.64 ## [3347] 0.62 0.64 0.70 0.74 0.76 0.78 0.78 0.74 0.66 0.70 0.70 0.66 0.66 0.64 ## [3361] 0.62 0.66 0.60 0.58 0.56 0.54 0.54 0.56 0.56 0.62 0.64 0.68 0.70 0.74 ## [3375] 0.74 0.74 0.74 0.76 0.74 0.74 0.72 0.70 0.70 0.66 0.66 0.64 0.64 0.64 ## [3389] 0.62 0.60 0.60 0.60 0.60 0.62 0.66 0.72 0.70 0.74 0.78 0.82 0.82 0.82 ## [3403] 0.80 0.80 0.80 0.78 0.72 0.72 0.70 0.70 0.68 0.70 0.68 0.66 0.64 0.64 ## [3417] 0.64 0.64 0.66 0.70 0.72 0.74 0.76 0.74 0.76 0.78 0.76 0.74 0.72 0.60 ## [3431] 0.62 0.60 0.60 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.58 0.60 0.62 0.64 ## [3445] 0.70 0.70 0.72 0.72 0.74 0.74 0.76 0.74 0.72 0.70 0.70 0.66 0.66 0.64 ## [3459] 0.64 0.62 0.62 0.62 0.60 0.60 0.62 0.62 0.62 0.62 0.66 0.66 0.70 0.72 ## [3473] 0.74 0.74 0.74 0.74 0.72 0.72 0.70 0.68 0.66 0.66 0.64 0.64 0.64 0.64 ## [3487] 0.62 0.62 0.64 0.64 0.66 0.72 0.80 0.82 0.86 0.86 0.88 0.88 0.88 0.86 ## [3501] 0.86 0.52 0.76 0.74 0.72 0.70 0.70 0.68 0.66 0.64 0.66 0.66 0.68 0.74 ## [3515] 0.78 0.80 0.82 0.86 0.86 0.90 0.90 0.90 0.90 0.84 0.84 0.78 0.78 0.76 ## [3529] 0.74 0.72 0.70 0.70 0.70 0.68 0.68 0.66 0.68 0.70 0.72 0.74 0.76 0.82 ## [3543] 0.86 0.90 0.90 0.90 0.86 0.86 0.82 0.74 0.74 0.74 0.74 0.74 0.74 0.72 ## [3557] 0.70 0.66 0.66 0.66 0.66 0.70 0.70 0.72 0.72 0.74 0.76 0.80 0.78 0.80 ## [3571] 0.80 0.76 0.74 0.72 0.70 0.66 0.64 0.62 0.62 0.60 0.56 0.54 0.52 0.52 ## [3585] 0.54 0.54 0.56 0.58 0.62 0.64 0.66 0.68 0.70 0.70 0.72 0.72 0.70 0.68 ## [3599] 0.64 0.64 0.62 0.58 0.56 0.54 0.54 0.52 0.52 0.50 0.54 0.56 0.60 0.62 ## [3613] 0.64 0.66 0.70 0.74 0.74 0.74 0.72 0.72 0.74 0.70 0.70 0.66 0.64 0.64 ## [3627] 0.64 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.64 0.64 0.66 ## [3641] 0.70 0.70 0.72 0.74 0.70 0.68 0.66 0.64 0.64 0.62 0.62 0.60 0.58 0.58 ## [3655] 0.56 0.56 0.58 0.62 0.64 0.70 0.72 0.74 0.76 0.78 0.78 0.80 0.76 0.78 ## [3669] 0.76 0.72 0.68 0.66 0.66 0.64 0.64 0.62 0.60 0.60 0.58 0.58 0.60 0.64 ## [3683] 0.68 0.72 0.76 0.76 0.80 0.80 0.80 0.82 0.80 0.80 0.78 0.76 0.74 0.72 ## [3697] 0.70 0.68 0.66 0.66 0.64 0.64 0.62 0.62 0.64 0.66 0.76 0.76 0.82 0.84 ## [3711] 0.88 0.90 0.92 0.92 0.92 0.92 0.90 0.82 0.80 0.80 0.76 0.76 0.74 0.74 ## [3725] 0.72 0.72 0.72 0.70 0.72 0.72 0.76 0.84 0.86 0.90 0.92 0.90 0.92 0.94 ## [3739] 0.92 0.90 0.88 0.84 0.80 0.76 0.74 0.74 0.70 0.70 0.68 0.68 0.66 0.66 ## [3753] 0.66 0.72 0.74 0.76 0.78 0.82 0.84 0.84 0.86 0.84 0.82 0.82 0.80 0.78 ## [3767] 0.76 0.76 0.72 0.72 0.70 0.70 0.70 0.68 0.68 0.68 0.70 0.72 0.74 0.74 ## [3781] 0.74 0.76 0.80 0.80 0.82 0.82 0.80 0.74 0.72 0.70 0.68 0.66 0.66 0.66 ## [3795] 0.66 0.64 0.64 0.64 0.62 0.62 0.62 0.64 0.70 0.72 0.76 0.78 0.82 0.78 ## [3809] 0.82 0.80 0.68 0.68 0.70 0.70 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.60 ## [3823] 0.56 0.54 0.54 0.56 0.58 0.62 0.64 0.66 0.66 0.70 0.70 0.70 0.70 0.70 ## [3837] 0.70 0.66 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.58 0.54 0.54 0.54 0.56 ## [3851] 0.58 0.62 0.62 0.64 0.64 0.64 0.64 0.64 0.68 0.64 0.62 0.64 0.60 0.60 ## [3865] 0.58 0.56 0.56 0.54 0.52 0.50 0.50 0.48 0.50 0.54 0.58 0.60 0.64 0.68 ## [3879] 0.70 0.74 0.74 0.76 0.76 0.74 0.72 0.70 0.66 0.64 0.62 0.62 0.60 0.58 ## [3893] 0.60 0.56 0.56 0.60 0.58 0.56 0.60 0.62 0.66 0.68 0.72 0.72 0.72 0.70 ## [3907] 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.56 0.56 0.56 0.56 0.54 0.54 ## [3921] 0.56 0.60 0.60 0.70 0.70 0.68 0.70 0.74 0.76 0.78 0.76 0.76 0.76 0.64 ## [3935] 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.62 0.64 0.66 0.70 ## [3949] 0.72 0.72 0.74 0.76 0.80 0.82 0.76 0.76 0.74 0.74 0.72 0.72 0.72 0.72 ## [3963] 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.66 0.68 0.68 0.70 0.72 0.74 0.74 ## [3977] 0.74 0.74 0.76 0.74 0.74 0.72 0.70 0.68 0.66 0.66 0.66 0.64 0.64 0.64 ## [3991] 0.62 0.62 0.60 0.56 0.56 0.60 0.60 0.60 0.62 0.64 0.66 0.66 0.70 0.70 ## [4005] 0.70 0.66 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.60 ## [4019] 0.62 0.62 0.64 0.66 0.70 0.74 0.76 0.78 0.80 0.78 0.76 0.74 0.74 0.72 ## [4033] 0.70 0.70 0.66 0.66 0.66 0.66 0.64 0.64 0.66 0.70 0.72 0.72 0.74 0.74 ## [4047] 0.80 0.82 0.82 0.82 0.82 0.80 0.80 0.80 0.74 0.74 0.72 0.72 0.72 0.72 ## [4061] 0.72 0.72 0.70 0.72 0.72 0.72 0.74 0.74 0.76 0.76 0.76 0.76 0.76 0.76 ## [4075] 0.74 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.64 ## [4089] 0.66 0.66 0.70 0.74 0.80 0.80 0.80 0.80 0.78 0.82 0.80 0.76 0.76 0.74 ## [4103] 0.72 0.70 0.70 0.68 0.68 0.66 0.66 0.64 0.64 0.62 0.64 0.64 0.70 0.72 ## [4117] 0.72 0.74 0.74 0.74 0.74 0.74 0.76 0.74 0.72 0.72 0.70 0.68 0.68 0.66 ## [4131] 0.64 0.64 0.62 0.60 0.60 0.60 0.60 0.62 0.64 0.66 0.72 0.72 0.74 0.74 ## [4145] 0.74 0.74 0.74 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.64 0.62 0.62 0.62 ## [4159] 0.60 0.62 0.62 0.64 0.66 0.72 0.72 0.70 0.72 0.72 0.72 0.74 0.74 0.74 ## [4173] 0.74 0.72 0.70 0.70 0.68 0.68 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.68 ## [4187] 0.70 0.74 0.80 0.82 0.84 0.84 0.86 0.86 0.86 0.82 0.80 0.74 0.74 0.74 ## [4201] 0.70 0.70 0.68 0.68 0.66 0.66 0.66 0.64 0.66 0.70 0.70 0.72 0.74 0.76 ## [4215] 0.76 0.80 0.80 0.82 0.82 0.82 0.80 0.76 0.74 0.72 0.70 0.68 0.66 0.64 ## [4229] 0.62 0.60 0.58 0.58 0.60 0.62 0.68 0.70 0.72 0.74 0.76 0.76 0.78 0.78 ## [4243] 0.80 0.78 0.76 0.74 0.74 0.72 0.70 0.66 0.66 0.66 0.62 0.64 0.62 0.60 ## [4257] 0.62 0.66 0.70 0.74 0.76 0.80 0.80 0.80 0.82 0.82 0.82 0.82 0.80 0.78 ## [4271] 0.72 0.70 0.70 0.68 0.68 0.66 0.64 0.64 0.62 0.58 0.62 0.64 0.68 0.74 ## [4285] 0.80 0.80 0.82 0.82 0.84 0.86 0.88 0.84 0.82 0.80 0.76 0.74 0.72 0.72 ## [4299] 0.70 0.70 0.70 0.68 0.68 0.62 0.62 0.64 0.68 0.70 0.74 0.76 0.80 0.80 ## [4313] 0.82 0.84 0.84 0.80 0.80 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.64 ## [4327] 0.66 0.64 0.64 0.66 0.70 0.72 0.76 0.76 0.78 0.78 0.80 0.82 0.82 0.80 ## [4341] 0.80 0.78 0.76 0.74 0.74 0.72 0.70 0.70 0.66 0.66 0.66 0.66 0.66 0.70 ## [4355] 0.72 0.74 0.76 0.78 0.80 0.82 0.80 0.82 0.82 0.82 0.80 0.80 0.76 0.78 ## [4369] 0.76 0.74 0.72 0.70 0.70 0.70 0.68 0.68 0.70 0.72 0.72 0.70 0.70 0.72 ## [4383] 0.74 0.74 0.76 0.76 0.76 0.78 0.76 0.74 0.72 0.70 0.70 0.68 0.66 0.66 ## [4397] 0.66 0.64 0.64 0.64 0.64 0.66 0.70 0.74 0.78 0.82 0.84 0.86 0.86 0.86 ## [4411] 0.86 0.86 0.84 0.82 0.76 0.74 0.74 0.72 0.74 0.72 0.70 0.68 0.70 0.68 ## [4425] 0.70 0.70 0.72 0.76 0.74 0.74 0.76 0.78 0.80 0.80 0.68 0.66 0.66 0.66 ## [4439] 0.66 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.64 0.66 0.70 0.74 ## [4453] 0.76 0.78 0.80 0.82 0.84 0.84 0.82 0.84 0.82 0.76 0.76 0.74 0.72 0.72 ## [4467] 0.70 0.70 0.68 0.66 0.66 0.66 0.64 0.70 0.72 0.74 0.76 0.78 0.82 0.80 ## [4481] 0.82 0.84 0.84 0.84 0.82 0.80 0.76 0.74 0.74 0.72 0.70 0.70 0.70 0.68 ## [4495] 0.66 0.66 0.68 0.70 0.74 0.78 0.80 0.82 0.84 0.86 0.86 0.86 0.86 0.86 ## [4509] 0.86 0.84 0.72 0.70 0.72 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.74 0.76 ## [4523] 0.76 0.78 0.82 0.82 0.86 0.86 0.90 0.90 0.86 0.88 0.86 0.84 0.82 0.82 ## [4537] 0.80 0.78 0.76 0.76 0.76 0.74 0.74 0.74 0.74 0.76 0.80 0.82 0.82 0.84 ## [4551] 0.84 0.82 0.82 0.64 0.66 0.70 0.72 0.70 0.70 0.70 0.68 0.66 0.66 0.66 ## [4565] 0.64 0.62 0.62 0.60 0.60 0.62 0.64 0.68 0.70 0.72 0.74 0.76 0.74 0.76 ## [4579] 0.76 0.74 0.72 0.72 0.70 0.66 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.60 ## [4593] 0.60 0.64 0.64 0.68 0.66 0.70 0.70 0.70 0.70 0.72 0.72 0.74 0.72 0.70 ## [4607] 0.70 0.66 0.66 0.64 0.62 0.60 0.60 0.60 0.60 0.58 0.60 0.62 0.66 0.70 ## [4621] 0.72 0.74 0.76 0.76 0.74 0.76 0.76 0.76 0.76 0.74 0.72 0.70 0.70 0.68 ## [4635] 0.66 0.64 0.64 0.64 0.62 0.64 0.62 0.64 0.68 0.72 0.74 0.76 0.76 0.80 ## [4649] 0.80 0.82 0.80 0.80 0.80 0.76 0.76 0.74 0.72 0.70 0.70 0.70 0.66 0.66 ## [4663] 0.64 0.64 0.64 0.68 0.70 0.74 0.76 0.80 0.80 0.82 0.82 0.84 0.84 0.84 ## [4677] 0.82 0.80 0.78 0.74 0.76 0.74 0.74 0.74 0.72 0.72 0.72 0.70 0.72 0.74 ## [4691] 0.76 0.80 0.82 0.82 0.84 0.86 0.86 0.88 0.90 0.80 0.80 0.76 0.74 0.74 ## [4705] 0.74 0.72 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.70 0.74 0.74 0.76 0.80 ## [4719] 0.82 0.84 0.86 0.86 0.84 0.84 0.84 0.82 0.80 0.80 0.78 0.76 0.76 0.74 ## [4733] 0.74 0.74 0.72 0.72 0.72 0.74 0.76 0.80 0.82 0.86 0.88 0.88 0.90 0.90 ## [4747] 0.92 0.92 0.90 0.86 0.84 0.82 0.82 0.80 0.82 0.80 0.78 0.78 0.76 0.74 ## [4761] 0.76 0.80 0.84 0.86 0.90 0.90 0.94 0.94 0.96 0.94 0.90 0.88 0.90 0.86 ## [4775] 0.84 0.82 0.84 0.80 0.82 0.82 0.82 0.78 0.76 0.76 0.80 0.80 0.84 0.84 ## [4789] 0.86 0.90 0.92 0.94 0.92 0.94 0.94 0.94 0.92 0.82 0.82 0.82 0.80 0.80 ## [4803] 0.80 0.78 0.80 0.80 0.78 0.78 0.80 0.80 0.82 0.82 0.86 0.84 0.90 0.86 ## [4817] 0.86 0.90 0.90 0.90 0.88 0.86 0.84 0.80 0.78 0.76 0.76 0.76 0.74 0.74 ## [4831] 0.72 0.72 0.72 0.74 0.76 0.80 0.82 0.84 0.84 0.74 0.72 0.70 0.70 0.72 ## [4845] 0.74 0.74 0.72 0.70 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.66 0.68 0.72 ## [4859] 0.74 0.76 0.80 0.82 0.84 0.84 0.86 0.88 0.86 0.86 0.84 0.82 0.80 0.78 ## [4873] 0.76 0.76 0.78 0.78 0.76 0.72 0.72 0.70 0.70 0.72 0.74 0.76 0.78 0.80 ## [4887] 0.82 0.84 0.84 0.86 0.86 0.84 0.82 0.80 0.76 0.74 0.72 0.74 0.70 0.72 ## [4901] 0.72 0.72 0.70 0.70 0.70 0.72 0.74 0.78 0.80 0.84 0.84 0.86 0.84 0.86 ## [4915] 0.86 0.84 0.84 0.82 0.80 0.78 0.76 0.76 0.74 0.74 0.74 0.74 0.72 0.72 ## [4929] 0.72 0.74 0.76 0.86 0.90 0.92 0.96 0.94 0.96 0.96 0.96 0.96 0.92 0.90 ## [4943] 0.86 0.82 0.80 0.78 0.76 0.76 0.76 0.74 0.72 0.72 0.72 0.76 0.78 0.82 ## [4957] 0.82 0.84 0.84 0.88 0.90 0.90 0.90 0.90 0.88 0.74 0.82 0.80 0.78 0.76 ## [4971] 0.76 0.74 0.74 0.74 0.72 0.72 0.74 0.74 0.76 0.80 0.84 0.86 0.90 0.90 ## [4985] 0.90 0.92 0.92 0.92 0.86 0.80 0.80 0.78 0.74 0.74 0.72 0.72 0.70 0.70 ## [4999] 0.66 0.66 0.66 0.74 0.80 0.82 0.86 0.88 0.90 0.90 0.92 0.90 0.90 0.76 ## [5013] 0.78 0.74 0.72 0.70 0.70 0.68 0.66 0.66 0.68 0.66 0.66 0.66 0.68 0.72 ## [5027] 0.74 0.78 0.82 0.84 0.86 0.86 0.90 0.90 0.90 0.90 0.86 0.86 0.82 0.80 ## [5041] 0.78 0.80 0.80 0.78 0.78 0.76 0.76 0.76 0.72 0.74 0.74 0.74 0.74 0.74 ## [5055] 0.76 0.76 0.76 0.70 0.68 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.68 0.66 ## [5069] 0.66 0.66 0.68 0.68 0.68 0.68 0.70 0.72 0.72 0.74 0.76 0.76 0.80 0.80 ## [5083] 0.76 0.76 0.70 0.70 0.70 0.70 0.68 0.66 0.66 0.64 0.66 0.64 0.64 0.64 ## [5097] 0.64 0.66 0.70 0.72 0.74 0.76 0.76 0.78 0.78 0.76 0.80 0.78 0.76 0.74 ## [5111] 0.72 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.66 0.64 0.64 0.68 0.68 0.74 ## [5125] 0.74 0.78 0.80 0.80 0.82 0.84 0.74 0.74 0.72 0.72 0.72 0.70 0.70 0.70 ## [5139] 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.72 0.76 0.80 0.82 0.90 0.90 ## [5153] 0.86 0.72 0.72 0.74 0.72 0.72 0.72 0.72 0.70 0.70 0.70 0.68 0.66 0.66 ## [5167] 0.66 0.70 0.70 0.72 0.74 0.76 0.80 0.82 0.82 0.84 0.82 0.84 0.86 0.86 ## [5181] 0.84 0.82 0.80 0.76 0.76 0.74 0.72 0.72 0.72 0.72 0.70 0.70 0.72 0.72 ## [5195] 0.74 0.78 0.80 0.80 0.82 0.84 0.86 0.86 0.86 0.80 0.80 0.80 0.80 0.78 ## [5209] 0.78 0.76 0.74 0.72 0.72 0.70 0.70 0.68 0.68 0.70 0.74 0.76 0.80 0.80 ## [5223] 0.82 0.82 0.84 0.86 0.86 0.84 0.82 0.80 0.76 0.76 0.74 0.74 0.70 0.70 ## [5237] 0.66 0.64 0.64 0.64 0.62 0.66 0.70 0.72 0.74 0.76 0.78 0.76 0.80 0.80 ## [5251] 0.80 0.80 0.78 0.74 0.72 0.70 0.70 0.66 0.64 0.64 0.62 0.62 0.62 0.60 ## [5265] 0.62 0.64 0.68 0.72 0.74 0.76 0.80 0.78 0.80 0.80 0.82 0.82 0.76 0.74 ## [5279] 0.72 0.70 0.68 0.68 0.68 0.68 0.68 0.66 0.64 0.64 0.64 0.66 0.70 0.70 ## [5293] 0.72 0.74 0.66 0.74 0.74 0.68 0.70 0.72 0.70 0.68 0.68 0.68 0.68 0.66 ## [5307] 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.66 0.66 0.70 0.70 0.70 0.72 ## [5321] 0.74 0.74 0.74 0.76 0.74 0.72 0.70 0.60 0.60 0.60 0.60 0.60 0.60 0.60 ## [5335] 0.60 0.60 0.60 0.60 0.64 0.64 0.68 0.72 0.74 0.78 0.74 0.74 0.74 0.74 ## [5349] 0.70 0.70 0.66 0.66 0.66 0.64 0.64 0.64 0.62 0.62 0.64 0.62 0.62 0.64 ## [5363] 0.70 0.68 0.70 0.74 0.76 0.76 0.76 0.80 0.80 0.76 0.76 0.74 0.74 0.72 ## [5377] 0.70 0.66 0.66 0.64 0.66 0.64 0.64 0.64 0.62 0.66 0.70 0.74 0.76 0.78 ## [5391] 0.80 0.80 0.82 0.80 0.82 0.80 0.76 0.76 0.74 0.72 0.70 0.70 0.68 0.66 ## [5405] 0.66 0.66 0.64 0.64 0.64 0.64 0.66 0.72 0.74 0.76 0.80 0.80 0.82 0.80 ## [5419] 0.80 0.80 0.76 0.66 0.66 0.68 0.70 0.70 0.68 0.64 0.64 0.64 0.64 0.64 ## [5433] 0.64 0.66 0.66 0.70 0.72 0.72 0.74 0.76 0.78 0.80 0.80 0.76 0.76 0.62 ## [5447] 0.62 0.62 0.60 0.60 0.60 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.66 0.70 ## [5461] 0.72 0.72 0.74 0.76 0.80 0.80 0.80 0.80 0.76 0.74 0.74 0.72 0.70 0.70 ## [5475] 0.70 0.68 0.68 0.66 0.68 0.66 0.66 0.68 0.70 0.72 0.74 0.80 0.82 0.80 ## [5489] 0.70 0.70 0.72 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.68 0.68 0.70 0.70 ## [5503] 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.72 0.74 0.74 0.74 0.74 0.74 0.74 ## [5517] 0.72 0.70 0.66 0.64 0.64 0.62 0.60 0.58 0.56 0.56 0.54 0.54 0.54 0.60 ## [5531] 0.62 0.66 0.70 0.70 0.70 0.72 0.72 0.72 0.72 0.72 0.72 0.66 0.64 0.62 ## [5545] 0.62 0.62 0.62 0.60 0.58 0.58 0.56 0.56 0.56 0.60 0.62 0.64 0.70 0.72 ## [5559] 0.74 0.76 0.76 0.76 0.76 0.76 0.74 0.74 0.72 0.70 0.70 0.68 0.68 0.68 ## [5573] 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.72 0.74 0.70 0.70 0.70 0.72 0.74 ## [5587] 0.72 0.72 0.72 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.60 0.62 0.62 ## [5601] 0.62 0.64 0.66 0.70 0.74 0.76 0.76 0.78 0.80 0.80 0.78 0.74 0.74 0.72 ## [5615] 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 ## [5629] 0.70 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.62 0.66 0.70 0.70 0.74 ## [5643] 0.76 0.78 0.78 0.80 0.76 0.74 0.72 0.72 0.66 0.64 0.62 0.62 0.60 0.60 ## [5657] 0.60 0.56 0.56 0.56 0.60 0.62 0.62 0.66 0.66 0.68 0.70 0.70 0.70 0.72 ## [5671] 0.70 0.66 0.66 0.64 0.64 0.62 0.60 0.56 0.56 0.54 0.54 0.52 0.52 0.54 ## [5685] 0.56 0.62 0.66 0.70 0.72 0.72 0.74 0.74 0.74 0.74 0.72 0.70 0.66 0.66 ## [5699] 0.64 0.62 0.62 0.60 0.60 0.56 0.56 0.56 0.54 0.54 0.60 0.62 0.64 0.70 ## [5713] 0.72 0.74 0.74 0.74 0.74 0.74 0.72 0.70 0.84 0.66 0.66 0.64 0.60 0.60 ## [5727] 0.60 0.58 0.58 0.56 0.56 0.60 0.60 0.62 0.64 0.68 0.72 0.72 0.72 0.72 ## [5741] 0.72 0.74 0.72 0.72 0.70 0.66 0.66 0.66 0.64 0.64 0.62 0.62 0.60 0.60 ## [5755] 0.60 0.60 0.60 0.62 0.62 0.64 0.66 0.66 0.68 0.70 0.70 0.70 0.68 0.68 ## [5769] 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.64 ## [5783] 0.66 0.66 0.66 0.70 0.70 0.72 0.72 0.72 0.72 0.72 0.70 0.70 0.68 0.68 ## [5797] 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.66 0.66 0.66 0.70 0.74 0.76 ## [5811] 0.78 0.78 0.78 0.80 0.76 0.76 0.74 0.74 0.72 0.72 0.72 0.70 0.68 0.68 ## [5825] 0.68 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.70 0.72 0.74 0.74 0.68 0.68 ## [5839] 0.66 0.66 0.66 0.66 0.66 0.60 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 ## [5853] 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 ## [5867] 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.56 0.60 0.60 0.62 0.60 0.60 0.60 ## [5881] 0.56 0.60 0.60 0.62 0.64 0.64 0.64 0.64 0.64 0.64 0.62 0.62 0.60 0.62 ## [5895] 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.64 0.64 0.66 0.68 0.70 0.66 0.64 ## [5909] 0.64 0.64 0.62 0.62 0.64 0.62 0.62 0.64 0.62 0.62 0.62 0.62 0.62 0.62 ## [5923] 0.62 0.62 0.62 0.62 0.62 0.64 0.70 0.72 0.74 0.74 0.70 0.70 0.66 0.64 ## [5937] 0.66 0.62 0.62 0.62 0.62 0.60 0.58 0.58 0.58 0.58 0.60 0.62 0.64 0.70 ## [5951] 0.72 0.72 0.74 0.74 0.74 0.74 0.74 0.72 0.70 0.66 0.64 0.64 0.62 0.62 ## [5965] 0.62 0.62 0.62 0.60 0.60 0.58 0.60 0.64 0.66 0.70 0.70 0.70 0.74 0.72 ## [5979] 0.74 0.72 0.72 0.68 0.64 0.62 0.64 0.62 0.58 0.56 0.56 0.56 0.54 0.56 ## [5993] 0.56 0.58 0.60 0.64 0.68 0.70 0.72 0.72 0.74 0.74 0.72 0.72 0.70 0.68 ## [6007] 0.66 0.64 0.62 0.62 0.60 0.58 0.60 0.58 0.56 0.56 0.56 0.58 0.60 0.64 ## [6021] 0.68 0.70 0.72 0.74 0.74 0.74 0.74 0.74 0.70 0.68 0.66 0.64 0.64 0.64 ## [6035] 0.62 0.62 0.60 0.60 0.60 0.58 0.58 0.60 0.62 0.64 0.70 0.72 0.74 0.76 ## [6049] 0.78 0.78 0.76 0.76 0.72 0.70 0.72 0.66 0.66 0.64 0.64 0.64 0.62 0.62 ## [6063] 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.68 0.66 0.64 0.64 0.60 0.54 0.48 ## [6077] 0.48 0.46 0.46 0.46 0.44 0.44 0.42 0.42 0.40 0.40 0.40 0.38 0.38 0.40 ## [6091] 0.42 0.46 0.50 0.50 0.52 0.54 0.54 0.54 0.52 0.52 0.52 0.50 0.50 0.50 ## [6105] 0.48 0.50 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.48 0.50 0.52 ## [6119] 0.52 0.52 0.52 0.52 0.54 0.52 0.52 0.52 0.52 0.50 0.50 0.46 0.46 0.46 ## [6133] 0.44 0.44 0.44 0.44 0.44 0.46 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.58 ## [6147] 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.52 0.52 0.52 0.50 0.50 0.50 ## [6161] 0.50 0.50 0.52 0.54 0.56 0.58 0.58 0.60 0.60 0.60 0.60 0.58 0.56 0.56 ## [6175] 0.56 0.56 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 ## [6189] 0.56 0.56 0.56 0.56 0.58 0.62 0.60 0.60 0.60 0.58 0.56 0.56 0.56 0.56 ## [6203] 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.56 0.60 0.62 0.64 0.66 ## [6217] 0.66 0.66 0.66 0.66 0.64 0.62 0.62 0.60 0.62 0.60 0.60 0.60 0.60 0.60 ## [6231] 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.66 0.66 0.68 0.68 0.66 ## [6245] 0.64 0.64 0.62 0.64 0.62 0.62 0.62 0.60 0.60 0.60 0.60 0.62 0.62 0.62 ## [6259] 0.62 0.62 0.62 0.62 0.62 0.60 0.60 0.62 0.62 0.60 0.60 0.62 0.60 0.60 ## [6273] 0.60 0.58 0.58 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.58 0.60 0.60 0.62 ## [6287] 0.62 0.64 0.66 0.66 0.64 0.66 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60 ## [6301] 0.60 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.66 0.68 0.66 0.66 ## [6315] 0.70 0.68 0.68 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 ## [6329] 0.62 0.62 0.62 0.64 0.64 0.68 0.68 0.70 0.70 0.72 0.70 0.70 0.66 0.66 ## [6343] 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.64 0.64 ## [6357] 0.64 0.66 0.66 0.70 0.68 0.68 0.66 0.66 0.64 0.64 0.62 0.60 0.60 0.60 ## [6371] 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.62 0.62 0.62 0.66 0.68 0.70 ## [6385] 0.72 0.70 0.70 0.70 0.66 0.66 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 ## [6399] 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.62 0.66 0.64 0.68 0.68 0.68 0.66 ## [6413] 0.64 0.62 0.60 0.58 0.56 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 ## [6427] 0.54 0.56 0.60 0.64 0.64 0.66 0.64 0.64 0.62 0.62 0.58 0.54 0.54 0.52 ## [6441] 0.52 0.52 0.50 0.48 0.46 0.46 0.44 0.44 0.42 0.42 0.40 0.40 0.40 0.38 ## [6455] 0.40 0.40 0.42 0.42 0.40 0.40 0.38 0.38 0.36 0.36 0.36 0.36 0.36 0.34 ## [6469] 0.34 0.34 0.34 0.32 0.32 0.34 0.34 0.36 0.36 0.38 0.40 0.40 0.36 0.36 ## [6483] 0.36 0.36 0.36 0.38 0.36 0.36 0.36 0.36 0.34 0.36 0.36 0.36 0.34 0.36 ## [6497] 0.36 0.36 0.36 0.40 0.40 0.40 0.40 0.42 0.40 0.40 0.40 0.40 0.40 0.40 ## [6511] 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.42 0.44 0.46 ## [6525] 0.48 0.54 0.56 0.56 0.58 0.58 0.58 0.56 0.54 0.52 0.52 0.50 0.50 0.48 ## [6539] 0.48 0.48 0.46 0.46 0.44 0.44 0.44 0.46 0.52 0.54 0.56 0.60 0.62 0.64 ## [6553] 0.64 0.64 0.64 0.64 0.60 0.58 0.52 0.50 0.52 0.50 0.50 0.48 0.46 0.44 ## [6567] 0.44 0.42 0.40 0.42 0.44 0.46 0.52 0.54 0.56 0.56 0.58 0.58 0.58 0.56 ## [6581] 0.54 0.52 0.50 0.46 0.46 0.44 0.44 0.44 0.42 0.40 0.40 0.42 0.42 0.42 ## [6595] 0.46 0.48 0.52 0.56 0.60 0.60 0.62 0.66 0.64 0.60 0.56 0.56 0.54 0.50 ## [6609] 0.50 0.50 0.48 0.46 0.46 0.44 0.42 0.42 0.42 0.42 0.46 0.50 0.52 0.58 ## [6623] 0.62 0.62 0.62 0.66 0.64 0.62 0.60 0.54 0.52 0.52 0.50 0.48 0.46 0.46 ## [6637] 0.46 0.44 0.44 0.44 0.44 0.44 0.46 0.50 0.56 0.62 0.64 0.66 0.68 0.68 ## [6651] 0.66 0.62 0.64 0.56 0.56 0.54 0.52 0.50 0.50 0.48 0.48 0.46 0.46 0.46 ## [6665] 0.44 0.46 0.52 0.54 0.56 0.62 0.70 0.72 0.74 0.72 0.70 0.66 0.64 0.58 ## [6679] 0.60 0.56 0.56 0.54 0.54 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.56 0.60 ## [6693] 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.58 0.58 0.58 0.56 0.56 ## [6707] 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 ## [6721] 0.54 0.54 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 ## [6735] 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.62 0.62 0.64 0.66 0.66 0.66 0.62 ## [6749] 0.62 0.62 0.62 0.62 0.62 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.56 0.56 ## [6763] 0.56 0.56 0.54 0.52 0.52 0.56 0.62 0.62 0.60 0.58 0.54 0.54 0.52 0.50 ## [6777] 0.46 0.46 0.46 0.46 0.46 0.44 0.42 0.42 0.40 0.40 0.46 0.52 0.54 0.56 ## [6791] 0.58 0.60 0.60 0.62 0.62 0.58 0.54 0.50 0.50 0.50 0.50 0.48 0.46 0.44 ## [6805] 0.42 0.42 0.42 0.42 0.38 0.40 0.44 0.50 0.54 0.56 0.58 0.60 0.60 0.62 ## [6819] 0.60 0.58 0.56 0.54 0.54 0.54 0.56 0.56 0.54 0.56 0.56 0.56 0.52 0.50 ## [6833] 0.50 0.50 0.50 0.50 0.52 0.56 0.56 0.56 0.58 0.56 0.58 0.56 0.56 0.54 ## [6847] 0.52 0.50 0.50 0.48 0.48 0.46 0.44 0.44 0.44 0.46 0.46 0.46 0.50 0.52 ## [6861] 0.56 0.60 0.62 0.64 0.62 0.62 0.62 0.60 0.56 0.56 0.54 0.54 0.52 0.52 ## [6875] 0.52 0.52 0.52 0.50 0.50 0.52 0.52 0.54 0.52 0.52 0.52 0.52 0.54 0.54 ## [6889] 0.54 0.56 0.56 0.56 0.60 0.60 0.58 0.58 0.58 0.56 0.56 0.56 0.50 0.50 ## [6903] 0.48 0.44 0.42 0.44 0.44 0.46 0.48 0.48 0.50 0.48 0.48 0.48 0.48 0.46 ## [6917] 0.46 0.46 0.44 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.34 0.36 0.36 0.40 ## [6931] 0.42 0.46 0.50 0.50 0.48 0.52 0.50 0.50 0.46 0.44 0.44 0.44 0.42 0.42 ## [6945] 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.36 0.40 0.42 0.44 0.44 0.46 ## [6959] 0.50 0.48 0.48 0.50 0.46 0.44 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.34 ## [6973] 0.34 0.34 0.32 0.32 0.34 0.36 0.40 0.42 0.46 0.50 0.52 0.52 0.52 0.52 ## [6987] 0.50 0.50 0.46 0.44 0.44 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.38 0.40 ## [7001] 0.38 0.42 0.44 0.46 0.50 0.52 0.54 0.54 0.56 0.54 0.52 0.54 0.48 0.48 ## [7015] 0.46 0.48 0.46 0.44 0.44 0.42 0.40 0.38 0.38 0.38 0.40 0.44 0.48 0.50 ## [7029] 0.52 0.54 0.56 0.56 0.56 0.56 0.56 0.52 0.48 0.46 0.44 0.46 0.44 0.44 ## [7043] 0.44 0.44 0.44 0.42 0.42 0.42 0.42 0.44 0.48 0.52 0.52 0.58 0.56 0.52 ## [7057] 0.52 0.52 0.52 0.52 0.50 0.50 0.52 0.48 0.48 0.46 0.46 0.46 0.46 0.48 ## [7071] 0.48 0.50 0.46 0.48 0.50 0.50 0.50 0.50 0.50 0.52 0.52 0.56 0.50 0.48 ## [7085] 0.44 0.42 0.40 0.36 0.34 0.34 0.32 0.32 0.30 0.30 0.30 0.28 0.28 0.30 ## [7099] 0.32 0.34 0.36 0.38 0.38 0.36 0.36 0.36 0.36 0.36 0.36 0.34 0.32 0.30 ## [7113] 0.30 0.28 0.30 0.30 0.30 0.30 0.26 0.26 0.26 0.28 0.28 0.26 0.26 0.24 ## [7127] 0.24 0.24 0.22 0.22 0.22 0.22 0.24 0.24 0.24 0.22 0.22 0.22 0.22 0.22 ## [7141] 0.24 0.22 0.24 0.24 0.24 0.26 0.30 0.32 0.36 0.38 0.40 0.42 0.42 0.42 ## [7155] 0.40 0.36 0.56 0.34 0.32 0.30 0.26 0.26 0.26 0.24 0.24 0.24 0.22 0.24 ## [7169] 0.24 0.28 0.32 0.36 0.40 0.42 0.44 0.44 0.44 0.42 0.42 0.40 0.40 0.40 ## [7183] 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.34 0.32 0.32 0.34 0.36 0.40 0.44 ## [7197] 0.46 0.50 0.48 0.50 0.50 0.48 0.44 0.42 0.42 0.40 0.36 0.36 0.34 0.32 ## [7211] 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.34 0.40 0.42 0.46 0.48 0.50 ## [7225] 0.48 0.48 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32 ## [7239] 0.32 0.34 0.32 0.34 0.36 0.40 0.44 0.50 0.52 0.52 0.52 0.52 0.48 0.46 ## [7253] 0.44 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.38 0.38 0.40 0.40 ## [7267] 0.42 0.42 0.44 0.48 0.44 0.44 0.46 0.46 0.42 0.42 0.40 0.36 0.34 0.34 ## [7281] 0.32 0.32 0.32 0.30 0.30 0.28 0.26 0.26 0.26 0.28 0.30 0.32 0.36 0.36 ## [7295] 0.40 0.42 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.30 0.28 0.28 0.26 0.26 ## [7309] 0.24 0.24 0.24 0.26 0.26 0.28 0.30 0.36 0.42 0.44 0.46 0.46 0.48 0.46 ## [7323] 0.44 0.42 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.30 0.28 ## [7337] 0.28 0.30 0.34 0.36 0.42 0.46 0.54 0.56 0.56 0.52 0.50 0.46 0.46 0.40 ## [7351] 0.38 0.36 0.36 0.34 0.32 0.32 0.32 0.30 0.30 0.30 0.30 0.32 0.36 0.42 ## [7365] 0.46 0.52 0.54 0.56 0.58 0.56 0.52 0.48 0.46 0.40 0.40 0.36 0.36 0.36 ## [7379] 0.34 0.32 0.32 0.32 0.30 0.30 0.30 0.32 0.34 0.40 0.46 0.50 0.50 0.52 ## [7393] 0.52 0.52 0.46 0.44 0.44 0.44 0.40 0.40 0.38 0.40 0.40 0.38 0.38 0.38 ## [7407] 0.36 0.36 0.38 0.40 0.42 0.44 0.46 0.42 0.36 0.36 0.36 0.36 0.36 0.36 ## [7421] 0.36 0.36 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.30 0.30 0.28 0.28 0.30 ## [7435] 0.32 0.32 0.34 0.34 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.32 ## [7449] 0.30 0.24 0.24 0.24 0.24 0.20 0.22 0.22 0.22 0.26 0.30 0.34 0.38 0.44 ## [7463] 0.48 0.50 0.52 0.52 0.50 0.42 0.42 0.42 0.42 0.42 0.40 0.40 0.36 0.36 ## [7477] 0.36 0.36 0.34 0.34 0.34 0.34 0.40 0.44 0.46 0.52 0.52 0.54 0.50 0.54 ## [7491] 0.52 0.52 0.50 0.50 0.48 0.48 0.46 0.46 0.46 0.46 0.44 0.44 0.44 0.44 ## [7505] 0.44 0.46 0.48 0.50 0.54 0.56 0.60 0.62 0.64 0.62 0.62 0.56 0.60 0.60 ## [7519] 0.60 0.58 0.56 0.56 0.56 0.56 0.54 0.56 0.54 0.56 0.54 0.54 0.56 0.56 ## [7533] 0.56 0.54 0.54 0.54 0.54 0.52 0.50 0.50 0.50 0.48 0.50 0.46 0.46 0.46 ## [7547] 0.46 0.46 0.46 0.44 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 ## [7561] 0.48 0.48 0.48 0.46 0.46 0.44 0.44 0.42 0.42 0.42 0.42 0.42 0.42 0.40 ## [7575] 0.34 0.36 0.34 0.34 0.34 0.34 0.32 0.34 0.34 0.34 0.34 0.32 0.32 0.32 ## [7589] 0.30 0.30 0.30 0.26 0.26 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.22 ## [7603] 0.26 0.26 0.30 0.32 0.34 0.34 0.34 0.34 0.32 0.30 0.28 0.28 0.28 0.26 ## [7617] 0.26 0.26 0.26 0.24 0.26 0.26 0.24 0.24 0.24 0.26 0.28 0.32 0.36 0.40 ## [7631] 0.42 0.42 0.42 0.42 0.40 0.38 0.34 0.36 0.36 0.38 0.38 0.38 0.40 0.40 ## [7645] 0.40 0.40 0.42 0.42 0.42 0.42 0.44 0.44 0.50 0.50 0.54 0.52 0.52 0.52 ## [7659] 0.52 0.54 0.50 0.52 0.48 0.46 0.46 0.46 0.46 0.44 0.44 0.44 0.44 0.42 ## [7673] 0.46 0.46 0.48 0.52 0.52 0.50 0.50 0.48 0.44 0.44 0.42 0.42 0.40 0.40 ## [7687] 0.40 0.40 0.40 0.38 0.40 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.40 0.40 ## [7701] 0.40 0.40 0.42 0.42 0.44 0.44 0.44 0.44 0.46 0.46 0.46 0.50 0.48 0.48 ## [7715] 0.48 0.50 0.50 0.52 0.46 0.44 0.46 0.48 0.52 0.52 0.50 0.48 0.44 0.42 ## [7729] 0.42 0.40 0.38 0.40 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.28 0.30 ## [7743] 0.30 0.30 0.26 0.30 0.34 0.36 0.42 0.46 0.48 0.50 0.50 0.50 0.48 0.42 ## [7757] 0.40 0.36 0.36 0.36 0.34 0.34 0.34 0.28 0.28 0.30 0.28 0.26 0.26 0.26 ## [7771] 0.32 0.36 0.40 0.46 0.50 0.52 0.52 0.50 0.50 0.46 0.42 0.40 0.36 0.34 ## [7785] 0.34 0.34 0.32 0.30 0.30 0.30 0.30 0.30 0.26 0.32 0.34 0.36 0.40 0.44 ## [7799] 0.48 0.48 0.50 0.46 0.46 0.42 0.40 0.42 0.38 0.38 0.36 0.36 0.36 0.34 ## [7813] 0.34 0.34 0.36 0.38 0.38 0.40 0.46 0.46 0.50 0.54 0.54 0.62 0.62 0.56 ## [7827] 0.54 0.50 0.48 0.50 0.48 0.48 0.48 0.46 0.46 0.44 0.44 0.40 0.42 0.42 ## [7841] 0.42 0.44 0.48 0.52 0.56 0.58 0.60 0.58 0.56 0.58 0.56 0.54 0.54 0.54 ## [7855] 0.52 0.52 0.52 0.52 0.50 0.50 0.52 0.50 0.52 0.52 0.54 0.56 0.56 0.50 ## [7869] 0.42 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.38 0.36 0.36 0.34 ## [7883] 0.32 0.32 0.30 0.28 0.26 0.26 0.28 0.30 0.34 0.38 0.36 0.38 0.38 0.36 ## [7897] 0.36 0.34 0.34 0.32 0.32 0.32 0.30 0.28 0.28 0.26 0.26 0.26 0.26 0.26 ## [7911] 0.24 0.24 0.26 0.30 0.32 0.34 0.36 0.40 0.40 0.40 0.40 0.36 0.36 0.34 ## [7925] 0.34 0.30 0.30 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.24 0.28 ## [7939] 0.30 0.34 0.36 0.40 0.42 0.42 0.42 0.42 0.40 0.34 0.38 0.34 0.32 0.32 ## [7953] 0.30 0.26 0.26 0.24 0.22 0.24 0.24 0.22 0.24 0.26 0.32 0.32 0.36 0.36 ## [7967] 0.36 0.38 0.38 0.36 0.34 0.30 0.30 0.30 0.32 0.30 0.30 0.30 0.26 0.28 ## [7981] 0.26 0.26 0.24 0.26 0.26 0.30 0.32 0.34 0.36 0.40 0.42 0.42 0.42 0.38 ## [7995] 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.32 0.32 0.30 0.30 0.30 0.32 ## [8009] 0.32 0.36 0.36 0.36 0.40 0.42 0.46 0.46 0.50 0.42 0.44 0.46 0.46 0.44 ## [8023] 0.44 0.46 0.50 0.46 0.46 0.46 0.46 0.46 0.44 0.46 0.46 0.46 0.46 0.46 ## [8037] 0.46 0.46 0.48 0.50 0.46 0.46 0.46 0.46 0.46 0.44 0.46 0.46 0.44 0.46 ## [8051] 0.46 0.46 0.46 0.46 0.48 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.42 ## [8065] 0.42 0.40 0.40 0.34 0.34 0.30 0.24 0.24 0.26 0.26 0.28 0.26 0.24 0.22 ## [8079] 0.22 0.22 0.22 0.24 0.26 0.28 0.28 0.30 0.32 0.32 0.32 0.30 0.28 0.26 ## [8093] 0.26 0.28 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.24 0.26 ## [8107] 0.30 0.32 0.36 0.38 0.38 0.38 0.36 0.34 0.34 0.34 0.30 0.30 0.28 0.28 ## [8121] 0.26 0.26 0.28 0.28 0.26 0.26 0.24 0.24 0.26 0.28 0.32 0.32 0.32 0.34 ## [8135] 0.34 0.34 0.32 0.28 0.26 0.26 0.24 0.22 0.22 0.20 0.20 0.16 0.18 0.16 ## [8149] 0.16 0.18 0.16 0.18 0.16 0.20 0.24 0.26 0.26 0.30 0.30 0.30 0.30 0.28 ## [8163] 0.24 0.22 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.18 0.18 0.16 0.16 0.14 ## [8177] 0.16 0.18 0.22 0.26 0.28 0.30 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.26 ## [8191] 0.26 0.24 0.22 0.20 0.20 0.18 0.20 0.18 0.20 0.16 0.20 0.26 0.30 0.34 ## [8205] 0.36 0.40 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.30 0.30 0.26 0.26 0.26 ## [8219] 0.26 0.28 0.26 0.26 0.26 0.28 0.26 0.30 0.32 0.34 0.36 0.36 0.38 0.38 ## [8233] 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.32 ## [8247] 0.32 0.32 0.34 0.36 0.40 0.40 0.46 0.50 0.52 0.52 0.46 0.52 0.52 0.52 ## [8261] 0.52 0.50 0.52 0.52 0.50 0.50 0.48 0.46 0.50 0.48 0.44 0.38 0.36 0.36 ## [8275] 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30 ## [8289] 0.30 0.30 0.28 0.26 0.24 0.26 0.26 0.26 0.26 0.26 0.26 0.28 0.28 0.28 ## [8303] 0.28 0.28 0.28 0.26 0.26 0.24 0.22 0.20 0.20 0.20 0.20 0.20 0.22 0.22 ## [8317] 0.22 0.20 0.20 0.20 0.20 0.22 0.24 0.24 0.26 0.30 0.30 0.32 0.28 0.28 ## [8331] 0.28 0.26 0.24 0.22 0.22 0.20 0.20 0.18 0.18 0.16 0.16 0.14 0.16 0.18 ## [8345] 0.20 0.22 0.24 0.26 0.30 0.34 0.36 0.38 0.40 0.38 0.36 0.36 0.40 0.36 ## [8359] 0.36 0.36 0.36 0.36 0.34 0.34 0.36 0.36 0.36 0.36 0.42 0.36 0.42 0.40 ## [8373] 0.44 0.44 0.44 0.44 0.44 0.40 0.38 0.38 0.36 0.36 0.36 0.38 0.34 0.36 ## [8387] 0.36 0.36 0.36 0.38 0.36 0.36 0.36 0.40 0.48 0.48 0.46 0.44 0.48 0.48 ## [8401] 0.44 0.44 0.44 0.50 0.50 0.50 0.50 0.50 0.50 0.44 0.48 0.44 0.38 0.38 ## [8415] 0.36 0.34 0.36 0.38 0.40 0.44 0.48 0.46 0.46 0.48 0.46 0.44 0.44 0.44 ## [8429] 0.42 0.42 0.36 0.40 0.40 0.40 0.38 0.38 0.38 0.36 0.38 0.38 0.40 0.40 ## [8443] 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.36 0.34 0.32 0.32 0.32 0.32 ## [8457] 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.30 0.32 0.34 0.34 ## [8471] 0.34 0.32 0.32 0.28 0.30 0.30 0.26 0.26 0.24 0.24 0.24 0.22 0.22 0.22 ## [8485] 0.20 0.20 0.22 0.20 0.24 0.26 0.30 0.30 0.32 0.34 0.34 0.36 0.34 0.32 ## [8499] 0.32 0.32 0.30 0.28 0.26 0.22 0.28 0.34 0.34 0.34 0.32 0.32 0.32 0.34 ## [8513] 0.34 0.34 0.36 0.38 0.38 0.38 0.36 0.34 0.32 0.30 0.30 0.26 0.26 0.26 ## [8527] 0.26 0.26 0.26 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.32 0.32 0.30 0.30 ## [8541] 0.42 0.42 0.44 0.40 0.38 0.34 0.32 0.32 0.32 0.30 0.32 0.32 0.32 0.32 ## [8555] 0.32 0.32 0.32 0.32 0.32 0.34 0.36 0.34 0.34 0.32 0.32 0.30 0.28 0.26 ## [8569] 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.18 0.20 ## [8583] 0.20 0.22 0.24 0.26 0.28 0.30 0.30 0.30 0.30 0.30 0.30 0.28 0.28 0.28 ## [8597] 0.30 0.28 0.26 0.24 0.24 0.24 0.22 0.24 0.24 0.24 0.26 0.30 0.32 0.32 ## [8611] 0.36 0.40 0.42 0.42 0.38 0.36 0.34 0.34 0.36 0.34 0.36 0.38 0.40 0.40 ## [8625] 0.40 0.38 0.36 0.40 0.38 0.34 0.38 0.40 0.42 0.52 0.50 0.46 0.46 0.44 ## [8639] 0.42 0.42 0.42 0.42 0.40 0.38 0.36 However, if we want to access variables within the dataset without needing to index them we can use the base R attach() function. attach(Bikeshare) So now, we can call on the variables from the dataset directly (if we do not have other datasets loaded in the R environment with variables named in the same way). temp ## [1] 0.24 0.22 0.22 0.24 0.24 0.24 0.22 0.20 0.24 0.32 0.38 0.36 0.42 0.46 ## [15] 0.46 0.44 0.42 0.44 0.42 0.42 0.40 0.40 0.40 0.46 0.46 0.44 0.42 0.46 ## [29] 0.46 0.42 0.40 0.40 0.38 0.36 0.36 0.36 0.36 0.36 0.34 0.34 0.34 0.36 ## [43] 0.32 0.30 0.26 0.24 0.22 0.22 0.20 0.16 0.16 0.14 0.14 0.14 0.16 0.18 ## [57] 0.20 0.22 0.24 0.26 0.26 0.26 0.24 0.24 0.20 0.20 0.18 0.14 0.18 0.16 ## [71] 0.16 0.14 0.14 0.12 0.12 0.12 0.14 0.16 0.16 0.22 0.22 0.24 0.26 0.28 ## [85] 0.30 0.28 0.26 0.24 0.24 0.22 0.22 0.20 0.20 0.16 0.16 0.24 0.22 0.20 ## [99] 0.18 0.20 0.22 0.22 0.26 0.26 0.28 0.30 0.30 0.30 0.24 0.24 0.24 0.22 ## [113] 0.20 0.18 0.20 0.18 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.18 0.20 0.22 ## [127] 0.26 0.26 0.28 0.28 0.26 0.22 0.22 0.22 0.20 0.22 0.22 0.20 0.20 0.20 ## [141] 0.20 0.20 0.22 0.20 0.20 0.20 0.20 0.22 0.20 0.20 0.20 0.20 0.20 0.20 ## [155] 0.20 0.20 0.16 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.16 0.16 ## [169] 0.16 0.16 0.16 0.18 0.20 0.20 0.20 0.20 0.20 0.18 0.16 0.14 0.14 0.12 ## [183] 0.12 0.12 0.10 0.10 0.10 0.10 0.10 0.08 0.08 0.10 0.08 0.10 0.12 0.14 ## [197] 0.16 0.18 0.20 0.22 0.22 0.20 0.18 0.16 0.16 0.14 0.14 0.14 0.12 0.12 ## [211] 0.12 0.12 0.12 0.10 0.10 0.12 0.12 0.12 0.14 0.14 0.16 0.20 0.20 0.20 ## [225] 0.20 0.20 0.20 0.20 0.16 0.16 0.14 0.14 0.14 0.14 0.14 0.16 0.16 0.16 ## [239] 0.16 0.18 0.18 0.20 0.20 0.20 0.20 0.20 0.16 0.16 0.16 0.16 0.16 0.16 ## [253] 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.12 0.14 0.16 0.16 0.18 0.20 0.20 ## [267] 0.22 0.20 0.20 0.22 0.20 0.20 0.18 0.16 0.16 0.16 0.14 0.14 0.14 0.14 ## [281] 0.14 0.14 0.14 0.12 0.12 0.14 0.14 0.16 0.20 0.20 0.22 0.22 0.24 0.24 ## [295] 0.20 0.20 0.16 0.16 0.14 0.14 0.12 0.12 0.10 0.10 0.10 0.10 0.10 0.10 ## [309] 0.12 0.14 0.18 0.18 0.20 0.22 0.22 0.24 0.22 0.22 0.20 0.16 0.18 0.16 ## [323] 0.16 0.18 0.18 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.14 0.16 0.18 0.20 ## [337] 0.24 0.28 0.30 0.32 0.34 0.32 0.30 0.32 0.32 0.32 0.30 0.30 0.26 0.26 ## [351] 0.26 0.22 0.26 0.26 0.26 0.24 0.22 0.22 0.22 0.24 0.24 0.26 0.28 0.26 ## [365] 0.24 0.22 0.20 0.18 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.18 0.18 0.18 ## [379] 0.18 0.16 0.16 0.16 0.16 0.16 0.18 0.18 0.18 0.20 0.20 0.20 0.18 0.18 ## [393] 0.16 0.16 0.14 0.16 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 ## [407] 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.24 0.24 0.24 0.26 0.28 0.30 ## [421] 0.40 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.32 0.30 0.30 0.26 0.26 0.26 ## [435] 0.26 0.26 0.24 0.22 0.22 0.22 0.24 0.26 0.28 0.30 0.28 0.30 0.32 0.30 ## [449] 0.30 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.22 0.22 0.24 0.22 ## [463] 0.20 0.20 0.20 0.20 0.22 0.22 0.20 0.20 0.16 0.16 0.14 0.12 0.12 0.10 ## [477] 0.08 0.06 0.06 0.04 0.04 0.04 0.04 0.02 0.02 0.02 0.02 0.04 0.04 0.06 ## [491] 0.06 0.08 0.10 0.12 0.12 0.12 0.08 0.08 0.06 0.06 0.06 0.04 0.04 0.04 ## [505] 0.02 0.02 0.04 0.04 0.08 0.06 0.10 0.14 0.14 0.16 0.14 0.16 0.16 0.16 ## [519] 0.14 0.12 0.12 0.10 0.10 0.08 0.06 0.06 0.04 0.04 0.02 0.02 0.02 0.02 ## [533] 0.04 0.06 0.10 0.10 0.12 0.14 0.14 0.16 0.16 0.14 0.14 0.14 0.14 0.14 ## [547] 0.14 0.16 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.16 0.20 0.22 0.24 0.26 ## [561] 0.26 0.30 0.32 0.32 0.30 0.30 0.26 0.24 0.24 0.22 0.22 0.22 0.24 0.22 ## [575] 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.20 0.22 ## [589] 0.22 0.20 0.20 0.18 0.18 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.18 0.16 ## [603] 0.16 0.18 0.18 0.18 0.18 0.18 0.22 0.20 0.22 0.24 0.24 0.24 0.24 0.22 ## [617] 0.24 0.24 0.22 0.22 0.22 0.20 0.16 0.16 0.16 0.18 0.18 0.18 0.18 0.20 ## [631] 0.22 0.22 0.22 0.24 0.24 0.22 0.22 0.18 0.18 0.16 0.16 0.16 0.14 0.16 ## [645] 0.14 0.14 0.14 0.14 0.14 0.16 0.18 0.22 0.30 0.28 0.28 0.30 0.30 0.30 ## [659] 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.18 0.16 0.16 ## [673] 0.16 0.16 0.16 0.16 0.18 0.16 0.18 0.16 0.16 0.16 0.16 0.30 0.16 0.16 ## [687] 0.16 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.16 0.16 0.18 0.20 ## [701] 0.20 0.22 0.24 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.22 0.22 0.22 ## [715] 0.22 0.22 0.22 0.22 0.22 0.22 0.24 0.22 0.24 0.24 0.34 0.38 0.38 0.36 ## [729] 0.36 0.34 0.28 0.24 0.22 0.22 0.20 0.20 0.20 0.18 0.18 0.16 0.16 0.14 ## [743] 0.14 0.16 0.18 0.18 0.20 0.20 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.18 ## [757] 0.18 0.20 0.20 0.16 0.14 0.14 0.14 0.16 0.14 0.14 0.16 0.20 0.22 0.24 ## [771] 0.26 0.28 0.28 0.30 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 ## [785] 0.24 0.22 0.20 0.20 0.22 0.20 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22 ## [799] 0.24 0.28 0.28 0.30 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 ## [813] 0.24 0.24 0.28 0.30 0.32 0.34 0.34 0.34 0.34 0.34 0.34 0.30 0.28 0.28 ## [827] 0.26 0.26 0.24 0.24 0.22 0.20 0.20 0.20 0.20 0.18 0.18 0.16 0.22 0.24 ## [841] 0.30 0.32 0.36 0.36 0.38 0.36 0.32 0.34 0.32 0.32 0.32 0.28 0.30 0.28 ## [855] 0.28 0.26 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.22 0.22 0.24 0.24 0.22 ## [869] 0.22 0.22 0.22 0.20 0.16 0.16 0.14 0.12 0.12 0.10 0.10 0.08 0.06 0.06 ## [883] 0.06 0.06 0.10 0.12 0.14 0.14 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.14 ## [897] 0.14 0.14 0.16 0.16 0.14 0.14 0.14 0.14 0.12 0.12 0.10 0.10 0.12 0.12 ## [911] 0.14 0.16 0.18 0.20 0.20 0.20 0.18 0.16 0.14 0.14 0.14 0.12 0.12 0.10 ## [925] 0.10 0.10 0.08 0.10 0.08 0.10 0.12 0.14 0.22 0.22 0.24 0.30 0.32 0.30 ## [939] 0.30 0.28 0.26 0.22 0.20 0.20 0.18 0.16 0.14 0.14 0.12 0.12 0.12 0.12 ## [953] 0.12 0.14 0.16 0.22 0.30 0.30 0.30 0.34 0.34 0.34 0.32 0.28 0.28 0.26 ## [967] 0.26 0.24 0.22 0.20 0.20 0.20 0.20 0.20 0.20 0.22 0.22 0.24 0.30 0.32 ## [981] 0.36 0.38 0.40 0.40 0.42 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 ## [995] 0.36 0.34 0.32 0.32 0.34 0.34 0.38 0.40 0.44 0.52 0.56 0.58 0.60 0.56 ## [1009] 0.52 0.46 0.40 0.38 0.36 0.36 0.34 0.32 0.30 0.30 0.28 0.22 0.22 0.20 ## [1023] 0.20 0.20 0.22 0.24 0.26 0.28 0.32 0.34 0.34 0.34 0.32 0.30 0.28 0.26 ## [1037] 0.24 0.24 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.20 0.22 0.24 0.26 0.34 ## [1051] 0.38 0.42 0.46 0.46 0.46 0.46 0.40 0.34 0.38 0.36 0.34 0.38 0.34 0.34 ## [1065] 0.34 0.34 0.32 0.32 0.30 0.32 0.32 0.36 0.38 0.44 0.48 0.54 0.60 0.60 ## [1079] 0.56 0.58 0.54 0.48 0.48 0.52 0.50 0.46 0.44 0.44 0.44 0.46 0.46 0.46 ## [1093] 0.44 0.42 0.42 0.42 0.44 0.44 0.50 0.60 0.66 0.66 0.66 0.66 0.64 0.62 ## [1107] 0.60 0.58 0.54 0.52 0.48 0.46 0.44 0.42 0.40 0.40 0.40 0.38 0.38 0.40 ## [1121] 0.42 0.44 0.44 0.44 0.46 0.44 0.44 0.42 0.36 0.34 0.32 0.32 0.30 0.28 ## [1135] 0.26 0.24 0.24 0.22 0.22 0.20 0.18 0.20 0.22 0.26 0.30 0.30 0.34 0.36 ## [1149] 0.36 0.36 0.34 0.34 0.34 0.34 0.32 0.32 0.30 0.34 0.34 0.34 0.34 0.32 ## [1163] 0.34 0.42 0.42 0.32 0.32 0.32 0.32 0.32 0.30 0.32 0.30 0.28 0.28 0.24 ## [1177] 0.24 0.24 0.22 0.20 0.20 0.12 0.12 0.12 0.14 0.16 0.16 0.20 0.22 0.22 ## [1191] 0.24 0.22 0.22 0.22 0.20 0.20 0.20 0.16 0.16 0.14 0.14 0.12 0.12 0.12 ## [1205] 0.12 0.12 0.14 0.18 0.20 0.24 0.26 0.30 0.32 0.34 0.34 0.34 0.32 0.30 ## [1219] 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.24 0.24 0.26 ## [1233] 0.32 0.36 0.38 0.40 0.40 0.38 0.36 0.34 0.34 0.34 0.34 0.34 0.32 0.32 ## [1247] 0.32 0.32 0.32 0.32 0.34 0.34 0.36 0.34 0.42 0.52 0.54 0.54 0.56 0.46 ## [1261] 0.32 0.32 0.32 0.30 0.30 0.28 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22 ## [1275] 0.22 0.22 0.24 0.26 0.30 0.30 0.32 0.34 0.34 0.36 0.36 0.36 0.34 0.32 ## [1289] 0.30 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.24 0.24 0.24 0.26 0.28 0.30 ## [1303] 0.36 0.40 0.42 0.44 0.46 0.48 0.42 0.40 0.40 0.40 0.38 0.38 0.36 0.36 ## [1317] 0.34 0.32 0.34 0.34 0.36 0.34 0.42 0.52 0.56 0.56 0.46 0.42 0.42 0.42 ## [1331] 0.40 0.46 0.44 0.44 0.38 0.34 0.32 0.30 0.26 0.24 0.22 0.22 0.20 0.20 ## [1345] 0.20 0.20 0.22 0.24 0.28 0.30 0.32 0.32 0.34 0.34 0.34 0.32 0.30 0.30 ## [1359] 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.20 0.22 0.22 0.22 0.24 0.28 0.32 ## [1373] 0.34 0.40 0.50 0.52 0.54 0.54 0.50 0.46 0.40 0.36 0.34 0.30 0.26 0.24 ## [1387] 0.24 0.20 0.20 0.16 0.14 0.14 0.12 0.14 0.16 0.18 0.20 0.22 0.22 0.24 ## [1401] 0.24 0.26 0.26 0.24 0.20 0.20 0.18 0.20 0.18 0.20 0.18 0.18 0.18 0.18 ## [1415] 0.16 0.16 0.16 0.18 0.22 0.24 0.28 0.32 0.34 0.36 0.36 0.36 0.36 0.34 ## [1429] 0.32 0.30 0.30 0.30 0.30 0.28 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30 ## [1443] 0.32 0.34 0.40 0.44 0.46 0.48 0.46 0.48 0.48 0.48 0.46 0.44 0.44 0.42 ## [1457] 0.44 0.42 0.42 0.40 0.42 0.42 0.42 0.42 0.40 0.42 0.42 0.42 0.46 0.46 ## [1471] 0.44 0.44 0.36 0.34 0.32 0.30 0.28 0.24 0.22 0.22 0.20 0.20 0.20 0.20 ## [1485] 0.20 0.20 0.20 0.20 0.22 0.24 0.26 0.30 0.32 0.32 0.34 0.34 0.34 0.32 ## [1499] 0.30 0.30 0.28 0.26 0.28 0.26 0.24 0.24 0.24 0.22 0.20 0.20 0.18 0.22 ## [1513] 0.26 0.30 0.36 0.36 0.38 0.38 0.36 0.38 0.36 0.34 0.34 0.32 0.30 0.30 ## [1527] 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.26 0.30 0.32 0.32 ## [1541] 0.32 0.34 0.36 0.34 0.34 0.34 0.34 0.32 0.32 0.32 0.34 0.34 0.34 0.34 ## [1555] 0.36 0.36 0.38 0.38 0.40 0.40 0.40 0.42 0.42 0.44 0.44 0.42 0.44 0.44 ## [1569] 0.44 0.36 0.36 0.34 0.34 0.34 0.34 0.34 0.32 0.30 0.26 0.26 0.28 0.30 ## [1583] 0.32 0.32 0.34 0.36 0.36 0.34 0.34 0.34 0.32 0.30 0.30 0.30 0.30 0.30 ## [1597] 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.24 0.26 0.28 0.32 0.34 0.34 0.36 ## [1611] 0.40 0.42 0.46 0.46 0.42 0.42 0.40 0.38 0.36 0.38 0.38 0.36 0.34 0.34 ## [1625] 0.36 0.34 0.36 0.40 0.40 0.42 0.44 0.46 0.46 0.46 0.48 0.46 0.44 0.40 ## [1639] 0.36 0.32 0.30 0.30 0.26 0.26 0.26 0.26 0.26 0.24 0.26 0.28 0.30 0.32 ## [1653] 0.34 0.36 0.38 0.38 0.38 0.38 0.40 0.38 0.36 0.36 0.34 0.34 0.32 0.32 ## [1667] 0.32 0.30 0.30 0.24 0.24 0.22 0.24 0.26 0.30 0.32 0.34 0.36 0.36 0.38 ## [1681] 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.30 ## [1695] 0.30 0.30 0.32 0.32 0.36 0.36 0.40 0.40 0.40 0.40 0.40 0.44 0.44 0.44 ## [1709] 0.42 0.42 0.40 0.40 0.38 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.36 0.40 ## [1723] 0.44 0.44 0.50 0.52 0.50 0.52 0.50 0.50 0.46 0.44 0.42 0.42 0.40 0.42 ## [1737] 0.42 0.40 0.40 0.36 0.38 0.40 0.40 0.42 0.46 0.52 0.54 0.56 0.64 0.66 ## [1751] 0.68 0.68 0.70 0.68 0.66 0.62 0.62 0.62 0.60 0.60 0.58 0.56 0.54 0.52 ## [1765] 0.52 0.44 0.40 0.42 0.42 0.44 0.46 0.46 0.50 0.50 0.50 0.50 0.48 0.46 ## [1779] 0.44 0.42 0.40 0.40 0.38 0.34 0.32 0.30 0.28 0.26 0.26 0.26 0.24 0.28 ## [1793] 0.30 0.32 0.34 0.36 0.38 0.40 0.40 0.42 0.40 0.38 0.36 0.36 0.34 0.34 ## [1807] 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.30 0.30 0.34 0.38 0.42 0.44 0.50 ## [1821] 0.54 0.56 0.54 0.54 0.52 0.58 0.56 0.46 0.46 0.46 0.46 0.42 0.44 0.44 ## [1835] 0.42 0.40 0.40 0.40 0.40 0.40 0.44 0.44 0.46 0.50 0.50 0.50 0.50 0.50 ## [1849] 0.48 0.44 0.44 0.42 0.40 0.40 0.36 0.34 0.34 0.34 0.32 0.34 0.32 0.32 ## [1863] 0.32 0.34 0.34 0.34 0.34 0.36 0.38 0.40 0.40 0.38 0.38 0.36 0.32 0.32 ## [1877] 0.32 0.32 0.30 0.30 0.28 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.30 0.30 ## [1891] 0.32 0.30 0.30 0.30 0.30 0.30 0.30 0.28 0.26 0.26 0.24 0.24 0.20 0.20 ## [1905] 0.20 0.18 0.18 0.18 0.20 0.22 0.24 0.26 0.30 0.32 0.32 0.36 0.34 0.34 ## [1919] 0.32 0.30 0.30 0.30 0.30 0.28 0.26 0.26 0.26 0.22 0.20 0.22 0.20 0.18 ## [1933] 0.20 0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.34 0.34 0.32 0.32 0.28 0.28 ## [1947] 0.26 0.28 0.26 0.26 0.24 0.22 0.20 0.18 0.16 0.16 0.20 0.22 0.22 0.24 ## [1961] 0.26 0.30 0.32 0.32 0.34 0.32 0.30 0.30 0.28 0.26 0.26 0.26 0.22 0.22 ## [1975] 0.22 0.22 0.18 0.18 0.20 0.20 0.22 0.24 0.26 0.26 0.30 0.32 0.32 0.34 ## [1989] 0.34 0.32 0.30 0.32 0.32 0.30 0.28 0.26 0.24 0.24 0.24 0.20 0.22 0.22 ## [2003] 0.22 0.24 0.28 0.30 0.34 0.34 0.34 0.36 0.38 0.38 0.40 0.36 0.34 0.36 ## [2017] 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30 0.32 0.32 0.32 0.34 ## [2031] 0.34 0.34 0.36 0.36 0.28 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24 ## [2045] 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.26 0.26 0.28 0.28 0.30 0.30 ## [2059] 0.30 0.30 0.30 0.30 0.30 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.24 0.24 ## [2073] 0.24 0.24 0.24 0.26 0.32 0.32 0.32 0.34 0.36 0.36 0.34 0.34 0.34 0.34 ## [2087] 0.34 0.32 0.32 0.30 0.30 0.30 0.26 0.24 0.24 0.24 0.24 0.26 0.26 0.30 ## [2101] 0.34 0.36 0.40 0.32 0.34 0.32 0.34 0.38 0.38 0.38 0.36 0.34 0.32 0.32 ## [2115] 0.32 0.30 0.30 0.26 0.30 0.28 0.28 0.28 0.32 0.34 0.36 0.40 0.42 0.44 ## [2129] 0.44 0.46 0.46 0.46 0.46 0.46 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.38 ## [2143] 0.38 0.38 0.40 0.42 0.44 0.46 0.50 0.54 0.60 0.64 0.68 0.74 0.76 0.76 ## [2157] 0.74 0.72 0.70 0.70 0.70 0.68 0.64 0.62 0.62 0.54 0.54 0.50 0.46 0.48 ## [2171] 0.48 0.38 0.36 0.34 0.32 0.34 0.36 0.36 0.40 0.38 0.42 0.38 0.36 0.34 ## [2185] 0.34 0.32 0.30 0.30 0.26 0.24 0.26 0.24 0.24 0.24 0.26 0.32 0.36 0.40 ## [2199] 0.42 0.44 0.46 0.50 0.52 0.54 0.52 0.52 0.50 0.46 0.46 0.46 0.46 0.46 ## [2213] 0.42 0.42 0.36 0.34 0.34 0.32 0.34 0.36 0.40 0.42 0.46 0.46 0.52 0.56 ## [2227] 0.60 0.60 0.52 0.48 0.46 0.44 0.44 0.40 0.38 0.36 0.34 0.34 0.34 0.34 ## [2241] 0.32 0.34 0.34 0.34 0.36 0.36 0.40 0.38 0.36 0.34 0.34 0.32 0.32 0.32 ## [2255] 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.30 0.30 0.30 0.30 0.30 0.32 ## [2269] 0.34 0.34 0.36 0.36 0.36 0.36 0.36 0.38 0.38 0.38 0.38 0.38 0.36 0.36 ## [2283] 0.38 0.38 0.38 0.38 0.38 0.36 0.36 0.36 0.36 0.38 0.38 0.40 0.40 0.42 ## [2297] 0.46 0.50 0.50 0.52 0.52 0.50 0.50 0.46 0.44 0.44 0.46 0.48 0.46 0.46 ## [2311] 0.46 0.46 0.46 0.50 0.52 0.56 0.56 0.60 0.60 0.64 0.72 0.74 0.74 0.74 ## [2325] 0.72 0.72 0.68 0.66 0.64 0.58 0.62 0.62 0.60 0.58 0.56 0.54 0.54 0.54 ## [2339] 0.48 0.46 0.50 0.52 0.56 0.54 0.50 0.48 0.48 0.44 0.44 0.42 0.42 0.42 ## [2353] 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.38 0.38 0.40 0.42 ## [2367] 0.42 0.44 0.42 0.44 0.46 0.46 0.44 0.44 0.44 0.42 0.42 0.40 0.38 0.38 ## [2381] 0.36 0.34 0.34 0.34 0.34 0.38 0.42 0.46 0.50 0.52 0.54 0.56 0.56 0.60 ## [2395] 0.60 0.60 0.56 0.54 0.50 0.46 0.48 0.46 0.44 0.44 0.40 0.40 0.38 0.36 ## [2409] 0.36 0.40 0.44 0.50 0.50 0.52 0.52 0.54 0.54 0.54 0.52 0.50 0.46 0.42 ## [2423] 0.40 0.40 0.38 0.36 0.36 0.36 0.36 0.36 0.36 0.38 0.40 0.40 0.40 0.40 ## [2437] 0.42 0.42 0.46 0.46 0.52 0.52 0.50 0.50 0.50 0.52 0.44 0.44 0.42 0.44 ## [2451] 0.44 0.42 0.40 0.40 0.36 0.36 0.36 0.36 0.38 0.40 0.42 0.46 0.46 0.50 ## [2465] 0.52 0.54 0.54 0.56 0.56 0.56 0.52 0.50 0.50 0.44 0.46 0.46 0.42 0.42 ## [2479] 0.40 0.40 0.40 0.46 0.46 0.50 0.52 0.54 0.56 0.56 0.58 0.60 0.60 0.58 ## [2493] 0.64 0.56 0.60 0.56 0.52 0.50 0.50 0.46 0.46 0.48 0.46 0.46 0.48 0.52 ## [2507] 0.50 0.52 0.50 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.54 0.50 0.50 0.50 ## [2521] 0.48 0.46 0.44 0.42 0.42 0.42 0.40 0.40 0.42 0.44 0.62 0.66 0.62 0.62 ## [2535] 0.70 0.70 0.74 0.76 0.76 0.74 0.74 0.70 0.68 0.66 0.62 0.60 0.56 0.52 ## [2549] 0.50 0.46 0.44 0.42 0.40 0.42 0.40 0.42 0.42 0.44 0.46 0.48 0.50 0.52 ## [2563] 0.52 0.50 0.50 0.46 0.44 0.42 0.42 0.40 0.36 0.36 0.36 0.36 0.34 0.34 ## [2577] 0.34 0.34 0.34 0.34 0.36 0.34 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.32 ## [2591] 0.32 0.30 0.30 0.32 0.32 0.32 0.32 0.32 0.34 0.34 0.34 0.34 0.36 0.38 ## [2605] 0.42 0.46 0.52 0.52 0.58 0.60 0.60 0.60 0.58 0.56 0.54 0.56 0.58 0.54 ## [2619] 0.52 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.52 0.56 0.60 0.66 0.68 0.70 ## [2633] 0.70 0.66 0.74 0.66 0.64 0.60 0.60 0.54 0.54 0.54 0.52 0.54 0.54 0.50 ## [2647] 0.52 0.46 0.50 0.52 0.56 0.60 0.64 0.64 0.66 0.70 0.72 0.74 0.70 0.70 ## [2661] 0.68 0.66 0.66 0.62 0.60 0.58 0.62 0.62 0.56 0.54 0.56 0.54 0.56 0.58 ## [2675] 0.58 0.64 0.66 0.68 0.70 0.74 0.72 0.70 0.70 0.68 0.68 0.64 0.64 0.62 ## [2689] 0.60 0.60 0.60 0.60 0.58 0.58 0.56 0.56 0.56 0.58 0.58 0.60 0.62 0.64 ## [2703] 0.66 0.64 0.68 0.70 0.70 0.66 0.66 0.62 0.64 0.62 0.62 0.62 0.64 0.62 ## [2717] 0.62 0.64 0.62 0.62 0.64 0.64 0.66 0.62 0.62 0.62 0.62 0.62 0.62 0.66 ## [2731] 0.62 0.64 0.64 0.62 0.58 0.56 0.54 0.54 0.52 0.50 0.46 0.46 0.46 0.46 ## [2745] 0.50 0.52 0.54 0.54 0.56 0.60 0.56 0.56 0.60 0.56 0.54 0.52 0.52 0.46 ## [2759] 0.48 0.46 0.42 0.44 0.44 0.44 0.44 0.42 0.42 0.42 0.40 0.40 0.40 0.44 ## [2773] 0.48 0.50 0.52 0.54 0.54 0.56 0.58 0.56 0.54 0.54 0.44 0.44 0.44 0.44 ## [2787] 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.42 0.44 0.46 0.48 0.46 0.48 0.50 ## [2801] 0.50 0.50 0.50 0.48 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.44 0.44 ## [2815] 0.44 0.44 0.44 0.46 0.48 0.50 0.54 0.58 0.62 0.62 0.64 0.66 0.66 0.66 ## [2829] 0.64 0.62 0.62 0.60 0.60 0.56 0.56 0.56 0.56 0.54 0.52 0.52 0.52 0.54 ## [2843] 0.56 0.60 0.64 0.66 0.68 0.70 0.70 0.70 0.72 0.70 0.70 0.68 0.66 0.64 ## [2857] 0.58 0.56 0.52 0.50 0.50 0.42 0.38 0.36 0.34 0.34 0.34 0.34 0.34 0.40 ## [2871] 0.44 0.46 0.50 0.48 0.50 0.40 0.42 0.42 0.40 0.40 0.38 0.36 0.36 0.34 ## [2885] 0.34 0.34 0.34 0.34 0.34 0.38 0.42 0.46 0.50 0.52 0.52 0.54 0.54 0.56 ## [2899] 0.58 0.56 0.56 0.54 0.50 0.50 0.48 0.46 0.44 0.40 0.38 0.36 0.36 0.34 ## [2913] 0.36 0.40 0.42 0.46 0.54 0.54 0.56 0.58 0.60 0.60 0.60 0.58 0.54 0.54 ## [2927] 0.52 0.48 0.46 0.44 0.42 0.42 0.42 0.42 0.40 0.46 0.42 0.48 0.52 0.54 ## [2941] 0.56 0.56 0.60 0.60 0.60 0.60 0.60 0.58 0.58 0.56 0.56 0.54 0.54 0.50 ## [2955] 0.50 0.52 0.48 0.46 0.42 0.44 0.44 0.46 0.52 0.56 0.58 0.58 0.60 0.60 ## [2969] 0.60 0.60 0.60 0.58 0.58 0.56 0.52 0.52 0.50 0.46 0.46 0.44 0.44 0.46 ## [2983] 0.42 0.42 0.44 0.48 0.52 0.54 0.56 0.60 0.60 0.62 0.62 0.62 0.64 0.62 ## [2997] 0.62 0.58 0.54 0.52 0.52 0.50 0.48 0.46 0.44 0.44 0.42 0.40 0.42 0.44 ## [3011] 0.50 0.52 0.56 0.56 0.60 0.62 0.62 0.64 0.66 0.64 0.64 0.60 0.54 0.54 ## [3025] 0.52 0.52 0.52 0.50 0.52 0.50 0.48 0.46 0.46 0.48 0.48 0.52 0.54 0.56 ## [3039] 0.60 0.62 0.62 0.64 0.66 0.64 0.62 0.56 0.54 0.54 0.50 0.46 0.46 0.46 ## [3053] 0.44 0.44 0.42 0.42 0.44 0.46 0.48 0.50 0.54 0.58 0.58 0.62 0.62 0.64 ## [3067] 0.64 0.64 0.62 0.60 0.60 0.56 0.54 0.54 0.52 0.52 0.50 0.50 0.50 0.50 ## [3081] 0.50 0.50 0.50 0.50 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 ## [3095] 0.52 0.52 0.52 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.48 0.50 0.52 ## [3109] 0.52 0.52 0.52 0.52 0.54 0.54 0.54 0.56 0.56 0.54 0.54 0.54 0.54 0.52 ## [3123] 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.54 0.58 0.58 0.60 0.62 0.62 ## [3137] 0.64 0.66 0.64 0.56 0.56 0.56 0.54 0.54 0.56 0.54 0.52 0.52 0.50 0.50 ## [3151] 0.50 0.50 0.52 0.52 0.56 0.60 0.62 0.64 0.66 0.68 0.68 0.72 0.60 0.58 ## [3165] 0.58 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.56 0.54 0.54 0.52 0.52 0.52 ## [3179] 0.52 0.54 0.54 0.56 0.56 0.56 0.62 0.62 0.62 0.62 0.60 0.60 0.58 0.54 ## [3193] 0.54 0.54 0.54 0.54 0.52 0.52 0.52 0.52 0.52 0.54 0.56 0.56 0.54 0.54 ## [3207] 0.56 0.56 0.58 0.60 0.60 0.60 0.60 0.56 0.56 0.52 0.52 0.52 0.52 0.50 ## [3221] 0.50 0.48 0.48 0.48 0.50 0.50 0.52 0.54 0.54 0.54 0.58 0.58 0.54 0.56 ## [3235] 0.58 0.56 0.60 0.58 0.54 0.54 0.50 0.48 0.46 0.46 0.44 0.44 0.44 0.44 ## [3249] 0.46 0.50 0.54 0.54 0.56 0.58 0.60 0.60 0.62 0.60 0.60 0.62 0.60 0.58 ## [3263] 0.58 0.56 0.54 0.52 0.52 0.52 0.52 0.48 0.46 0.46 0.50 0.54 0.56 0.60 ## [3277] 0.62 0.64 0.66 0.70 0.72 0.72 0.72 0.72 0.70 0.68 0.62 0.62 0.60 0.58 ## [3291] 0.54 0.52 0.52 0.50 0.50 0.50 0.52 0.54 0.60 0.62 0.64 0.70 0.72 0.66 ## [3305] 0.62 0.66 0.68 0.70 0.66 0.66 0.64 0.62 0.60 0.58 0.56 0.56 0.56 0.54 ## [3319] 0.54 0.54 0.54 0.56 0.60 0.60 0.66 0.68 0.68 0.74 0.72 0.72 0.72 0.72 ## [3333] 0.70 0.70 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.58 0.60 0.58 0.58 0.64 ## [3347] 0.62 0.64 0.70 0.74 0.76 0.78 0.78 0.74 0.66 0.70 0.70 0.66 0.66 0.64 ## [3361] 0.62 0.66 0.60 0.58 0.56 0.54 0.54 0.56 0.56 0.62 0.64 0.68 0.70 0.74 ## [3375] 0.74 0.74 0.74 0.76 0.74 0.74 0.72 0.70 0.70 0.66 0.66 0.64 0.64 0.64 ## [3389] 0.62 0.60 0.60 0.60 0.60 0.62 0.66 0.72 0.70 0.74 0.78 0.82 0.82 0.82 ## [3403] 0.80 0.80 0.80 0.78 0.72 0.72 0.70 0.70 0.68 0.70 0.68 0.66 0.64 0.64 ## [3417] 0.64 0.64 0.66 0.70 0.72 0.74 0.76 0.74 0.76 0.78 0.76 0.74 0.72 0.60 ## [3431] 0.62 0.60 0.60 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.58 0.60 0.62 0.64 ## [3445] 0.70 0.70 0.72 0.72 0.74 0.74 0.76 0.74 0.72 0.70 0.70 0.66 0.66 0.64 ## [3459] 0.64 0.62 0.62 0.62 0.60 0.60 0.62 0.62 0.62 0.62 0.66 0.66 0.70 0.72 ## [3473] 0.74 0.74 0.74 0.74 0.72 0.72 0.70 0.68 0.66 0.66 0.64 0.64 0.64 0.64 ## [3487] 0.62 0.62 0.64 0.64 0.66 0.72 0.80 0.82 0.86 0.86 0.88 0.88 0.88 0.86 ## [3501] 0.86 0.52 0.76 0.74 0.72 0.70 0.70 0.68 0.66 0.64 0.66 0.66 0.68 0.74 ## [3515] 0.78 0.80 0.82 0.86 0.86 0.90 0.90 0.90 0.90 0.84 0.84 0.78 0.78 0.76 ## [3529] 0.74 0.72 0.70 0.70 0.70 0.68 0.68 0.66 0.68 0.70 0.72 0.74 0.76 0.82 ## [3543] 0.86 0.90 0.90 0.90 0.86 0.86 0.82 0.74 0.74 0.74 0.74 0.74 0.74 0.72 ## [3557] 0.70 0.66 0.66 0.66 0.66 0.70 0.70 0.72 0.72 0.74 0.76 0.80 0.78 0.80 ## [3571] 0.80 0.76 0.74 0.72 0.70 0.66 0.64 0.62 0.62 0.60 0.56 0.54 0.52 0.52 ## [3585] 0.54 0.54 0.56 0.58 0.62 0.64 0.66 0.68 0.70 0.70 0.72 0.72 0.70 0.68 ## [3599] 0.64 0.64 0.62 0.58 0.56 0.54 0.54 0.52 0.52 0.50 0.54 0.56 0.60 0.62 ## [3613] 0.64 0.66 0.70 0.74 0.74 0.74 0.72 0.72 0.74 0.70 0.70 0.66 0.64 0.64 ## [3627] 0.64 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.64 0.64 0.66 ## [3641] 0.70 0.70 0.72 0.74 0.70 0.68 0.66 0.64 0.64 0.62 0.62 0.60 0.58 0.58 ## [3655] 0.56 0.56 0.58 0.62 0.64 0.70 0.72 0.74 0.76 0.78 0.78 0.80 0.76 0.78 ## [3669] 0.76 0.72 0.68 0.66 0.66 0.64 0.64 0.62 0.60 0.60 0.58 0.58 0.60 0.64 ## [3683] 0.68 0.72 0.76 0.76 0.80 0.80 0.80 0.82 0.80 0.80 0.78 0.76 0.74 0.72 ## [3697] 0.70 0.68 0.66 0.66 0.64 0.64 0.62 0.62 0.64 0.66 0.76 0.76 0.82 0.84 ## [3711] 0.88 0.90 0.92 0.92 0.92 0.92 0.90 0.82 0.80 0.80 0.76 0.76 0.74 0.74 ## [3725] 0.72 0.72 0.72 0.70 0.72 0.72 0.76 0.84 0.86 0.90 0.92 0.90 0.92 0.94 ## [3739] 0.92 0.90 0.88 0.84 0.80 0.76 0.74 0.74 0.70 0.70 0.68 0.68 0.66 0.66 ## [3753] 0.66 0.72 0.74 0.76 0.78 0.82 0.84 0.84 0.86 0.84 0.82 0.82 0.80 0.78 ## [3767] 0.76 0.76 0.72 0.72 0.70 0.70 0.70 0.68 0.68 0.68 0.70 0.72 0.74 0.74 ## [3781] 0.74 0.76 0.80 0.80 0.82 0.82 0.80 0.74 0.72 0.70 0.68 0.66 0.66 0.66 ## [3795] 0.66 0.64 0.64 0.64 0.62 0.62 0.62 0.64 0.70 0.72 0.76 0.78 0.82 0.78 ## [3809] 0.82 0.80 0.68 0.68 0.70 0.70 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.60 ## [3823] 0.56 0.54 0.54 0.56 0.58 0.62 0.64 0.66 0.66 0.70 0.70 0.70 0.70 0.70 ## [3837] 0.70 0.66 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.58 0.54 0.54 0.54 0.56 ## [3851] 0.58 0.62 0.62 0.64 0.64 0.64 0.64 0.64 0.68 0.64 0.62 0.64 0.60 0.60 ## [3865] 0.58 0.56 0.56 0.54 0.52 0.50 0.50 0.48 0.50 0.54 0.58 0.60 0.64 0.68 ## [3879] 0.70 0.74 0.74 0.76 0.76 0.74 0.72 0.70 0.66 0.64 0.62 0.62 0.60 0.58 ## [3893] 0.60 0.56 0.56 0.60 0.58 0.56 0.60 0.62 0.66 0.68 0.72 0.72 0.72 0.70 ## [3907] 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.56 0.56 0.56 0.56 0.54 0.54 ## [3921] 0.56 0.60 0.60 0.70 0.70 0.68 0.70 0.74 0.76 0.78 0.76 0.76 0.76 0.64 ## [3935] 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.62 0.64 0.66 0.70 ## [3949] 0.72 0.72 0.74 0.76 0.80 0.82 0.76 0.76 0.74 0.74 0.72 0.72 0.72 0.72 ## [3963] 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.66 0.68 0.68 0.70 0.72 0.74 0.74 ## [3977] 0.74 0.74 0.76 0.74 0.74 0.72 0.70 0.68 0.66 0.66 0.66 0.64 0.64 0.64 ## [3991] 0.62 0.62 0.60 0.56 0.56 0.60 0.60 0.60 0.62 0.64 0.66 0.66 0.70 0.70 ## [4005] 0.70 0.66 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.60 ## [4019] 0.62 0.62 0.64 0.66 0.70 0.74 0.76 0.78 0.80 0.78 0.76 0.74 0.74 0.72 ## [4033] 0.70 0.70 0.66 0.66 0.66 0.66 0.64 0.64 0.66 0.70 0.72 0.72 0.74 0.74 ## [4047] 0.80 0.82 0.82 0.82 0.82 0.80 0.80 0.80 0.74 0.74 0.72 0.72 0.72 0.72 ## [4061] 0.72 0.72 0.70 0.72 0.72 0.72 0.74 0.74 0.76 0.76 0.76 0.76 0.76 0.76 ## [4075] 0.74 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.64 ## [4089] 0.66 0.66 0.70 0.74 0.80 0.80 0.80 0.80 0.78 0.82 0.80 0.76 0.76 0.74 ## [4103] 0.72 0.70 0.70 0.68 0.68 0.66 0.66 0.64 0.64 0.62 0.64 0.64 0.70 0.72 ## [4117] 0.72 0.74 0.74 0.74 0.74 0.74 0.76 0.74 0.72 0.72 0.70 0.68 0.68 0.66 ## [4131] 0.64 0.64 0.62 0.60 0.60 0.60 0.60 0.62 0.64 0.66 0.72 0.72 0.74 0.74 ## [4145] 0.74 0.74 0.74 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.64 0.62 0.62 0.62 ## [4159] 0.60 0.62 0.62 0.64 0.66 0.72 0.72 0.70 0.72 0.72 0.72 0.74 0.74 0.74 ## [4173] 0.74 0.72 0.70 0.70 0.68 0.68 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.68 ## [4187] 0.70 0.74 0.80 0.82 0.84 0.84 0.86 0.86 0.86 0.82 0.80 0.74 0.74 0.74 ## [4201] 0.70 0.70 0.68 0.68 0.66 0.66 0.66 0.64 0.66 0.70 0.70 0.72 0.74 0.76 ## [4215] 0.76 0.80 0.80 0.82 0.82 0.82 0.80 0.76 0.74 0.72 0.70 0.68 0.66 0.64 ## [4229] 0.62 0.60 0.58 0.58 0.60 0.62 0.68 0.70 0.72 0.74 0.76 0.76 0.78 0.78 ## [4243] 0.80 0.78 0.76 0.74 0.74 0.72 0.70 0.66 0.66 0.66 0.62 0.64 0.62 0.60 ## [4257] 0.62 0.66 0.70 0.74 0.76 0.80 0.80 0.80 0.82 0.82 0.82 0.82 0.80 0.78 ## [4271] 0.72 0.70 0.70 0.68 0.68 0.66 0.64 0.64 0.62 0.58 0.62 0.64 0.68 0.74 ## [4285] 0.80 0.80 0.82 0.82 0.84 0.86 0.88 0.84 0.82 0.80 0.76 0.74 0.72 0.72 ## [4299] 0.70 0.70 0.70 0.68 0.68 0.62 0.62 0.64 0.68 0.70 0.74 0.76 0.80 0.80 ## [4313] 0.82 0.84 0.84 0.80 0.80 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.64 ## [4327] 0.66 0.64 0.64 0.66 0.70 0.72 0.76 0.76 0.78 0.78 0.80 0.82 0.82 0.80 ## [4341] 0.80 0.78 0.76 0.74 0.74 0.72 0.70 0.70 0.66 0.66 0.66 0.66 0.66 0.70 ## [4355] 0.72 0.74 0.76 0.78 0.80 0.82 0.80 0.82 0.82 0.82 0.80 0.80 0.76 0.78 ## [4369] 0.76 0.74 0.72 0.70 0.70 0.70 0.68 0.68 0.70 0.72 0.72 0.70 0.70 0.72 ## [4383] 0.74 0.74 0.76 0.76 0.76 0.78 0.76 0.74 0.72 0.70 0.70 0.68 0.66 0.66 ## [4397] 0.66 0.64 0.64 0.64 0.64 0.66 0.70 0.74 0.78 0.82 0.84 0.86 0.86 0.86 ## [4411] 0.86 0.86 0.84 0.82 0.76 0.74 0.74 0.72 0.74 0.72 0.70 0.68 0.70 0.68 ## [4425] 0.70 0.70 0.72 0.76 0.74 0.74 0.76 0.78 0.80 0.80 0.68 0.66 0.66 0.66 ## [4439] 0.66 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.64 0.66 0.70 0.74 ## [4453] 0.76 0.78 0.80 0.82 0.84 0.84 0.82 0.84 0.82 0.76 0.76 0.74 0.72 0.72 ## [4467] 0.70 0.70 0.68 0.66 0.66 0.66 0.64 0.70 0.72 0.74 0.76 0.78 0.82 0.80 ## [4481] 0.82 0.84 0.84 0.84 0.82 0.80 0.76 0.74 0.74 0.72 0.70 0.70 0.70 0.68 ## [4495] 0.66 0.66 0.68 0.70 0.74 0.78 0.80 0.82 0.84 0.86 0.86 0.86 0.86 0.86 ## [4509] 0.86 0.84 0.72 0.70 0.72 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.74 0.76 ## [4523] 0.76 0.78 0.82 0.82 0.86 0.86 0.90 0.90 0.86 0.88 0.86 0.84 0.82 0.82 ## [4537] 0.80 0.78 0.76 0.76 0.76 0.74 0.74 0.74 0.74 0.76 0.80 0.82 0.82 0.84 ## [4551] 0.84 0.82 0.82 0.64 0.66 0.70 0.72 0.70 0.70 0.70 0.68 0.66 0.66 0.66 ## [4565] 0.64 0.62 0.62 0.60 0.60 0.62 0.64 0.68 0.70 0.72 0.74 0.76 0.74 0.76 ## [4579] 0.76 0.74 0.72 0.72 0.70 0.66 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.60 ## [4593] 0.60 0.64 0.64 0.68 0.66 0.70 0.70 0.70 0.70 0.72 0.72 0.74 0.72 0.70 ## [4607] 0.70 0.66 0.66 0.64 0.62 0.60 0.60 0.60 0.60 0.58 0.60 0.62 0.66 0.70 ## [4621] 0.72 0.74 0.76 0.76 0.74 0.76 0.76 0.76 0.76 0.74 0.72 0.70 0.70 0.68 ## [4635] 0.66 0.64 0.64 0.64 0.62 0.64 0.62 0.64 0.68 0.72 0.74 0.76 0.76 0.80 ## [4649] 0.80 0.82 0.80 0.80 0.80 0.76 0.76 0.74 0.72 0.70 0.70 0.70 0.66 0.66 ## [4663] 0.64 0.64 0.64 0.68 0.70 0.74 0.76 0.80 0.80 0.82 0.82 0.84 0.84 0.84 ## [4677] 0.82 0.80 0.78 0.74 0.76 0.74 0.74 0.74 0.72 0.72 0.72 0.70 0.72 0.74 ## [4691] 0.76 0.80 0.82 0.82 0.84 0.86 0.86 0.88 0.90 0.80 0.80 0.76 0.74 0.74 ## [4705] 0.74 0.72 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.70 0.74 0.74 0.76 0.80 ## [4719] 0.82 0.84 0.86 0.86 0.84 0.84 0.84 0.82 0.80 0.80 0.78 0.76 0.76 0.74 ## [4733] 0.74 0.74 0.72 0.72 0.72 0.74 0.76 0.80 0.82 0.86 0.88 0.88 0.90 0.90 ## [4747] 0.92 0.92 0.90 0.86 0.84 0.82 0.82 0.80 0.82 0.80 0.78 0.78 0.76 0.74 ## [4761] 0.76 0.80 0.84 0.86 0.90 0.90 0.94 0.94 0.96 0.94 0.90 0.88 0.90 0.86 ## [4775] 0.84 0.82 0.84 0.80 0.82 0.82 0.82 0.78 0.76 0.76 0.80 0.80 0.84 0.84 ## [4789] 0.86 0.90 0.92 0.94 0.92 0.94 0.94 0.94 0.92 0.82 0.82 0.82 0.80 0.80 ## [4803] 0.80 0.78 0.80 0.80 0.78 0.78 0.80 0.80 0.82 0.82 0.86 0.84 0.90 0.86 ## [4817] 0.86 0.90 0.90 0.90 0.88 0.86 0.84 0.80 0.78 0.76 0.76 0.76 0.74 0.74 ## [4831] 0.72 0.72 0.72 0.74 0.76 0.80 0.82 0.84 0.84 0.74 0.72 0.70 0.70 0.72 ## [4845] 0.74 0.74 0.72 0.70 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.66 0.68 0.72 ## [4859] 0.74 0.76 0.80 0.82 0.84 0.84 0.86 0.88 0.86 0.86 0.84 0.82 0.80 0.78 ## [4873] 0.76 0.76 0.78 0.78 0.76 0.72 0.72 0.70 0.70 0.72 0.74 0.76 0.78 0.80 ## [4887] 0.82 0.84 0.84 0.86 0.86 0.84 0.82 0.80 0.76 0.74 0.72 0.74 0.70 0.72 ## [4901] 0.72 0.72 0.70 0.70 0.70 0.72 0.74 0.78 0.80 0.84 0.84 0.86 0.84 0.86 ## [4915] 0.86 0.84 0.84 0.82 0.80 0.78 0.76 0.76 0.74 0.74 0.74 0.74 0.72 0.72 ## [4929] 0.72 0.74 0.76 0.86 0.90 0.92 0.96 0.94 0.96 0.96 0.96 0.96 0.92 0.90 ## [4943] 0.86 0.82 0.80 0.78 0.76 0.76 0.76 0.74 0.72 0.72 0.72 0.76 0.78 0.82 ## [4957] 0.82 0.84 0.84 0.88 0.90 0.90 0.90 0.90 0.88 0.74 0.82 0.80 0.78 0.76 ## [4971] 0.76 0.74 0.74 0.74 0.72 0.72 0.74 0.74 0.76 0.80 0.84 0.86 0.90 0.90 ## [4985] 0.90 0.92 0.92 0.92 0.86 0.80 0.80 0.78 0.74 0.74 0.72 0.72 0.70 0.70 ## [4999] 0.66 0.66 0.66 0.74 0.80 0.82 0.86 0.88 0.90 0.90 0.92 0.90 0.90 0.76 ## [5013] 0.78 0.74 0.72 0.70 0.70 0.68 0.66 0.66 0.68 0.66 0.66 0.66 0.68 0.72 ## [5027] 0.74 0.78 0.82 0.84 0.86 0.86 0.90 0.90 0.90 0.90 0.86 0.86 0.82 0.80 ## [5041] 0.78 0.80 0.80 0.78 0.78 0.76 0.76 0.76 0.72 0.74 0.74 0.74 0.74 0.74 ## [5055] 0.76 0.76 0.76 0.70 0.68 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.68 0.66 ## [5069] 0.66 0.66 0.68 0.68 0.68 0.68 0.70 0.72 0.72 0.74 0.76 0.76 0.80 0.80 ## [5083] 0.76 0.76 0.70 0.70 0.70 0.70 0.68 0.66 0.66 0.64 0.66 0.64 0.64 0.64 ## [5097] 0.64 0.66 0.70 0.72 0.74 0.76 0.76 0.78 0.78 0.76 0.80 0.78 0.76 0.74 ## [5111] 0.72 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.66 0.64 0.64 0.68 0.68 0.74 ## [5125] 0.74 0.78 0.80 0.80 0.82 0.84 0.74 0.74 0.72 0.72 0.72 0.70 0.70 0.70 ## [5139] 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.72 0.76 0.80 0.82 0.90 0.90 ## [5153] 0.86 0.72 0.72 0.74 0.72 0.72 0.72 0.72 0.70 0.70 0.70 0.68 0.66 0.66 ## [5167] 0.66 0.70 0.70 0.72 0.74 0.76 0.80 0.82 0.82 0.84 0.82 0.84 0.86 0.86 ## [5181] 0.84 0.82 0.80 0.76 0.76 0.74 0.72 0.72 0.72 0.72 0.70 0.70 0.72 0.72 ## [5195] 0.74 0.78 0.80 0.80 0.82 0.84 0.86 0.86 0.86 0.80 0.80 0.80 0.80 0.78 ## [5209] 0.78 0.76 0.74 0.72 0.72 0.70 0.70 0.68 0.68 0.70 0.74 0.76 0.80 0.80 ## [5223] 0.82 0.82 0.84 0.86 0.86 0.84 0.82 0.80 0.76 0.76 0.74 0.74 0.70 0.70 ## [5237] 0.66 0.64 0.64 0.64 0.62 0.66 0.70 0.72 0.74 0.76 0.78 0.76 0.80 0.80 ## [5251] 0.80 0.80 0.78 0.74 0.72 0.70 0.70 0.66 0.64 0.64 0.62 0.62 0.62 0.60 ## [5265] 0.62 0.64 0.68 0.72 0.74 0.76 0.80 0.78 0.80 0.80 0.82 0.82 0.76 0.74 ## [5279] 0.72 0.70 0.68 0.68 0.68 0.68 0.68 0.66 0.64 0.64 0.64 0.66 0.70 0.70 ## [5293] 0.72 0.74 0.66 0.74 0.74 0.68 0.70 0.72 0.70 0.68 0.68 0.68 0.68 0.66 ## [5307] 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.66 0.66 0.70 0.70 0.70 0.72 ## [5321] 0.74 0.74 0.74 0.76 0.74 0.72 0.70 0.60 0.60 0.60 0.60 0.60 0.60 0.60 ## [5335] 0.60 0.60 0.60 0.60 0.64 0.64 0.68 0.72 0.74 0.78 0.74 0.74 0.74 0.74 ## [5349] 0.70 0.70 0.66 0.66 0.66 0.64 0.64 0.64 0.62 0.62 0.64 0.62 0.62 0.64 ## [5363] 0.70 0.68 0.70 0.74 0.76 0.76 0.76 0.80 0.80 0.76 0.76 0.74 0.74 0.72 ## [5377] 0.70 0.66 0.66 0.64 0.66 0.64 0.64 0.64 0.62 0.66 0.70 0.74 0.76 0.78 ## [5391] 0.80 0.80 0.82 0.80 0.82 0.80 0.76 0.76 0.74 0.72 0.70 0.70 0.68 0.66 ## [5405] 0.66 0.66 0.64 0.64 0.64 0.64 0.66 0.72 0.74 0.76 0.80 0.80 0.82 0.80 ## [5419] 0.80 0.80 0.76 0.66 0.66 0.68 0.70 0.70 0.68 0.64 0.64 0.64 0.64 0.64 ## [5433] 0.64 0.66 0.66 0.70 0.72 0.72 0.74 0.76 0.78 0.80 0.80 0.76 0.76 0.62 ## [5447] 0.62 0.62 0.60 0.60 0.60 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.66 0.70 ## [5461] 0.72 0.72 0.74 0.76 0.80 0.80 0.80 0.80 0.76 0.74 0.74 0.72 0.70 0.70 ## [5475] 0.70 0.68 0.68 0.66 0.68 0.66 0.66 0.68 0.70 0.72 0.74 0.80 0.82 0.80 ## [5489] 0.70 0.70 0.72 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.68 0.68 0.70 0.70 ## [5503] 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.72 0.74 0.74 0.74 0.74 0.74 0.74 ## [5517] 0.72 0.70 0.66 0.64 0.64 0.62 0.60 0.58 0.56 0.56 0.54 0.54 0.54 0.60 ## [5531] 0.62 0.66 0.70 0.70 0.70 0.72 0.72 0.72 0.72 0.72 0.72 0.66 0.64 0.62 ## [5545] 0.62 0.62 0.62 0.60 0.58 0.58 0.56 0.56 0.56 0.60 0.62 0.64 0.70 0.72 ## [5559] 0.74 0.76 0.76 0.76 0.76 0.76 0.74 0.74 0.72 0.70 0.70 0.68 0.68 0.68 ## [5573] 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.72 0.74 0.70 0.70 0.70 0.72 0.74 ## [5587] 0.72 0.72 0.72 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.60 0.62 0.62 ## [5601] 0.62 0.64 0.66 0.70 0.74 0.76 0.76 0.78 0.80 0.80 0.78 0.74 0.74 0.72 ## [5615] 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 ## [5629] 0.70 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.62 0.66 0.70 0.70 0.74 ## [5643] 0.76 0.78 0.78 0.80 0.76 0.74 0.72 0.72 0.66 0.64 0.62 0.62 0.60 0.60 ## [5657] 0.60 0.56 0.56 0.56 0.60 0.62 0.62 0.66 0.66 0.68 0.70 0.70 0.70 0.72 ## [5671] 0.70 0.66 0.66 0.64 0.64 0.62 0.60 0.56 0.56 0.54 0.54 0.52 0.52 0.54 ## [5685] 0.56 0.62 0.66 0.70 0.72 0.72 0.74 0.74 0.74 0.74 0.72 0.70 0.66 0.66 ## [5699] 0.64 0.62 0.62 0.60 0.60 0.56 0.56 0.56 0.54 0.54 0.60 0.62 0.64 0.70 ## [5713] 0.72 0.74 0.74 0.74 0.74 0.74 0.72 0.70 0.84 0.66 0.66 0.64 0.60 0.60 ## [5727] 0.60 0.58 0.58 0.56 0.56 0.60 0.60 0.62 0.64 0.68 0.72 0.72 0.72 0.72 ## [5741] 0.72 0.74 0.72 0.72 0.70 0.66 0.66 0.66 0.64 0.64 0.62 0.62 0.60 0.60 ## [5755] 0.60 0.60 0.60 0.62 0.62 0.64 0.66 0.66 0.68 0.70 0.70 0.70 0.68 0.68 ## [5769] 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.64 ## [5783] 0.66 0.66 0.66 0.70 0.70 0.72 0.72 0.72 0.72 0.72 0.70 0.70 0.68 0.68 ## [5797] 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.66 0.66 0.66 0.70 0.74 0.76 ## [5811] 0.78 0.78 0.78 0.80 0.76 0.76 0.74 0.74 0.72 0.72 0.72 0.70 0.68 0.68 ## [5825] 0.68 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.70 0.72 0.74 0.74 0.68 0.68 ## [5839] 0.66 0.66 0.66 0.66 0.66 0.60 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 ## [5853] 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 ## [5867] 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.56 0.60 0.60 0.62 0.60 0.60 0.60 ## [5881] 0.56 0.60 0.60 0.62 0.64 0.64 0.64 0.64 0.64 0.64 0.62 0.62 0.60 0.62 ## [5895] 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.64 0.64 0.66 0.68 0.70 0.66 0.64 ## [5909] 0.64 0.64 0.62 0.62 0.64 0.62 0.62 0.64 0.62 0.62 0.62 0.62 0.62 0.62 ## [5923] 0.62 0.62 0.62 0.62 0.62 0.64 0.70 0.72 0.74 0.74 0.70 0.70 0.66 0.64 ## [5937] 0.66 0.62 0.62 0.62 0.62 0.60 0.58 0.58 0.58 0.58 0.60 0.62 0.64 0.70 ## [5951] 0.72 0.72 0.74 0.74 0.74 0.74 0.74 0.72 0.70 0.66 0.64 0.64 0.62 0.62 ## [5965] 0.62 0.62 0.62 0.60 0.60 0.58 0.60 0.64 0.66 0.70 0.70 0.70 0.74 0.72 ## [5979] 0.74 0.72 0.72 0.68 0.64 0.62 0.64 0.62 0.58 0.56 0.56 0.56 0.54 0.56 ## [5993] 0.56 0.58 0.60 0.64 0.68 0.70 0.72 0.72 0.74 0.74 0.72 0.72 0.70 0.68 ## [6007] 0.66 0.64 0.62 0.62 0.60 0.58 0.60 0.58 0.56 0.56 0.56 0.58 0.60 0.64 ## [6021] 0.68 0.70 0.72 0.74 0.74 0.74 0.74 0.74 0.70 0.68 0.66 0.64 0.64 0.64 ## [6035] 0.62 0.62 0.60 0.60 0.60 0.58 0.58 0.60 0.62 0.64 0.70 0.72 0.74 0.76 ## [6049] 0.78 0.78 0.76 0.76 0.72 0.70 0.72 0.66 0.66 0.64 0.64 0.64 0.62 0.62 ## [6063] 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.68 0.66 0.64 0.64 0.60 0.54 0.48 ## [6077] 0.48 0.46 0.46 0.46 0.44 0.44 0.42 0.42 0.40 0.40 0.40 0.38 0.38 0.40 ## [6091] 0.42 0.46 0.50 0.50 0.52 0.54 0.54 0.54 0.52 0.52 0.52 0.50 0.50 0.50 ## [6105] 0.48 0.50 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.48 0.50 0.52 ## [6119] 0.52 0.52 0.52 0.52 0.54 0.52 0.52 0.52 0.52 0.50 0.50 0.46 0.46 0.46 ## [6133] 0.44 0.44 0.44 0.44 0.44 0.46 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.58 ## [6147] 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.52 0.52 0.52 0.50 0.50 0.50 ## [6161] 0.50 0.50 0.52 0.54 0.56 0.58 0.58 0.60 0.60 0.60 0.60 0.58 0.56 0.56 ## [6175] 0.56 0.56 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 ## [6189] 0.56 0.56 0.56 0.56 0.58 0.62 0.60 0.60 0.60 0.58 0.56 0.56 0.56 0.56 ## [6203] 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.56 0.60 0.62 0.64 0.66 ## [6217] 0.66 0.66 0.66 0.66 0.64 0.62 0.62 0.60 0.62 0.60 0.60 0.60 0.60 0.60 ## [6231] 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.66 0.66 0.68 0.68 0.66 ## [6245] 0.64 0.64 0.62 0.64 0.62 0.62 0.62 0.60 0.60 0.60 0.60 0.62 0.62 0.62 ## [6259] 0.62 0.62 0.62 0.62 0.62 0.60 0.60 0.62 0.62 0.60 0.60 0.62 0.60 0.60 ## [6273] 0.60 0.58 0.58 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.58 0.60 0.60 0.62 ## [6287] 0.62 0.64 0.66 0.66 0.64 0.66 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60 ## [6301] 0.60 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.66 0.68 0.66 0.66 ## [6315] 0.70 0.68 0.68 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 ## [6329] 0.62 0.62 0.62 0.64 0.64 0.68 0.68 0.70 0.70 0.72 0.70 0.70 0.66 0.66 ## [6343] 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.64 0.64 ## [6357] 0.64 0.66 0.66 0.70 0.68 0.68 0.66 0.66 0.64 0.64 0.62 0.60 0.60 0.60 ## [6371] 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.62 0.62 0.62 0.66 0.68 0.70 ## [6385] 0.72 0.70 0.70 0.70 0.66 0.66 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 ## [6399] 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.62 0.66 0.64 0.68 0.68 0.68 0.66 ## [6413] 0.64 0.62 0.60 0.58 0.56 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 ## [6427] 0.54 0.56 0.60 0.64 0.64 0.66 0.64 0.64 0.62 0.62 0.58 0.54 0.54 0.52 ## [6441] 0.52 0.52 0.50 0.48 0.46 0.46 0.44 0.44 0.42 0.42 0.40 0.40 0.40 0.38 ## [6455] 0.40 0.40 0.42 0.42 0.40 0.40 0.38 0.38 0.36 0.36 0.36 0.36 0.36 0.34 ## [6469] 0.34 0.34 0.34 0.32 0.32 0.34 0.34 0.36 0.36 0.38 0.40 0.40 0.36 0.36 ## [6483] 0.36 0.36 0.36 0.38 0.36 0.36 0.36 0.36 0.34 0.36 0.36 0.36 0.34 0.36 ## [6497] 0.36 0.36 0.36 0.40 0.40 0.40 0.40 0.42 0.40 0.40 0.40 0.40 0.40 0.40 ## [6511] 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.42 0.44 0.46 ## [6525] 0.48 0.54 0.56 0.56 0.58 0.58 0.58 0.56 0.54 0.52 0.52 0.50 0.50 0.48 ## [6539] 0.48 0.48 0.46 0.46 0.44 0.44 0.44 0.46 0.52 0.54 0.56 0.60 0.62 0.64 ## [6553] 0.64 0.64 0.64 0.64 0.60 0.58 0.52 0.50 0.52 0.50 0.50 0.48 0.46 0.44 ## [6567] 0.44 0.42 0.40 0.42 0.44 0.46 0.52 0.54 0.56 0.56 0.58 0.58 0.58 0.56 ## [6581] 0.54 0.52 0.50 0.46 0.46 0.44 0.44 0.44 0.42 0.40 0.40 0.42 0.42 0.42 ## [6595] 0.46 0.48 0.52 0.56 0.60 0.60 0.62 0.66 0.64 0.60 0.56 0.56 0.54 0.50 ## [6609] 0.50 0.50 0.48 0.46 0.46 0.44 0.42 0.42 0.42 0.42 0.46 0.50 0.52 0.58 ## [6623] 0.62 0.62 0.62 0.66 0.64 0.62 0.60 0.54 0.52 0.52 0.50 0.48 0.46 0.46 ## [6637] 0.46 0.44 0.44 0.44 0.44 0.44 0.46 0.50 0.56 0.62 0.64 0.66 0.68 0.68 ## [6651] 0.66 0.62 0.64 0.56 0.56 0.54 0.52 0.50 0.50 0.48 0.48 0.46 0.46 0.46 ## [6665] 0.44 0.46 0.52 0.54 0.56 0.62 0.70 0.72 0.74 0.72 0.70 0.66 0.64 0.58 ## [6679] 0.60 0.56 0.56 0.54 0.54 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.56 0.60 ## [6693] 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.58 0.58 0.58 0.56 0.56 ## [6707] 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 ## [6721] 0.54 0.54 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 ## [6735] 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.62 0.62 0.64 0.66 0.66 0.66 0.62 ## [6749] 0.62 0.62 0.62 0.62 0.62 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.56 0.56 ## [6763] 0.56 0.56 0.54 0.52 0.52 0.56 0.62 0.62 0.60 0.58 0.54 0.54 0.52 0.50 ## [6777] 0.46 0.46 0.46 0.46 0.46 0.44 0.42 0.42 0.40 0.40 0.46 0.52 0.54 0.56 ## [6791] 0.58 0.60 0.60 0.62 0.62 0.58 0.54 0.50 0.50 0.50 0.50 0.48 0.46 0.44 ## [6805] 0.42 0.42 0.42 0.42 0.38 0.40 0.44 0.50 0.54 0.56 0.58 0.60 0.60 0.62 ## [6819] 0.60 0.58 0.56 0.54 0.54 0.54 0.56 0.56 0.54 0.56 0.56 0.56 0.52 0.50 ## [6833] 0.50 0.50 0.50 0.50 0.52 0.56 0.56 0.56 0.58 0.56 0.58 0.56 0.56 0.54 ## [6847] 0.52 0.50 0.50 0.48 0.48 0.46 0.44 0.44 0.44 0.46 0.46 0.46 0.50 0.52 ## [6861] 0.56 0.60 0.62 0.64 0.62 0.62 0.62 0.60 0.56 0.56 0.54 0.54 0.52 0.52 ## [6875] 0.52 0.52 0.52 0.50 0.50 0.52 0.52 0.54 0.52 0.52 0.52 0.52 0.54 0.54 ## [6889] 0.54 0.56 0.56 0.56 0.60 0.60 0.58 0.58 0.58 0.56 0.56 0.56 0.50 0.50 ## [6903] 0.48 0.44 0.42 0.44 0.44 0.46 0.48 0.48 0.50 0.48 0.48 0.48 0.48 0.46 ## [6917] 0.46 0.46 0.44 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.34 0.36 0.36 0.40 ## [6931] 0.42 0.46 0.50 0.50 0.48 0.52 0.50 0.50 0.46 0.44 0.44 0.44 0.42 0.42 ## [6945] 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.36 0.40 0.42 0.44 0.44 0.46 ## [6959] 0.50 0.48 0.48 0.50 0.46 0.44 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.34 ## [6973] 0.34 0.34 0.32 0.32 0.34 0.36 0.40 0.42 0.46 0.50 0.52 0.52 0.52 0.52 ## [6987] 0.50 0.50 0.46 0.44 0.44 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.38 0.40 ## [7001] 0.38 0.42 0.44 0.46 0.50 0.52 0.54 0.54 0.56 0.54 0.52 0.54 0.48 0.48 ## [7015] 0.46 0.48 0.46 0.44 0.44 0.42 0.40 0.38 0.38 0.38 0.40 0.44 0.48 0.50 ## [7029] 0.52 0.54 0.56 0.56 0.56 0.56 0.56 0.52 0.48 0.46 0.44 0.46 0.44 0.44 ## [7043] 0.44 0.44 0.44 0.42 0.42 0.42 0.42 0.44 0.48 0.52 0.52 0.58 0.56 0.52 ## [7057] 0.52 0.52 0.52 0.52 0.50 0.50 0.52 0.48 0.48 0.46 0.46 0.46 0.46 0.48 ## [7071] 0.48 0.50 0.46 0.48 0.50 0.50 0.50 0.50 0.50 0.52 0.52 0.56 0.50 0.48 ## [7085] 0.44 0.42 0.40 0.36 0.34 0.34 0.32 0.32 0.30 0.30 0.30 0.28 0.28 0.30 ## [7099] 0.32 0.34 0.36 0.38 0.38 0.36 0.36 0.36 0.36 0.36 0.36 0.34 0.32 0.30 ## [7113] 0.30 0.28 0.30 0.30 0.30 0.30 0.26 0.26 0.26 0.28 0.28 0.26 0.26 0.24 ## [7127] 0.24 0.24 0.22 0.22 0.22 0.22 0.24 0.24 0.24 0.22 0.22 0.22 0.22 0.22 ## [7141] 0.24 0.22 0.24 0.24 0.24 0.26 0.30 0.32 0.36 0.38 0.40 0.42 0.42 0.42 ## [7155] 0.40 0.36 0.56 0.34 0.32 0.30 0.26 0.26 0.26 0.24 0.24 0.24 0.22 0.24 ## [7169] 0.24 0.28 0.32 0.36 0.40 0.42 0.44 0.44 0.44 0.42 0.42 0.40 0.40 0.40 ## [7183] 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.34 0.32 0.32 0.34 0.36 0.40 0.44 ## [7197] 0.46 0.50 0.48 0.50 0.50 0.48 0.44 0.42 0.42 0.40 0.36 0.36 0.34 0.32 ## [7211] 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.34 0.40 0.42 0.46 0.48 0.50 ## [7225] 0.48 0.48 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32 ## [7239] 0.32 0.34 0.32 0.34 0.36 0.40 0.44 0.50 0.52 0.52 0.52 0.52 0.48 0.46 ## [7253] 0.44 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.38 0.38 0.40 0.40 ## [7267] 0.42 0.42 0.44 0.48 0.44 0.44 0.46 0.46 0.42 0.42 0.40 0.36 0.34 0.34 ## [7281] 0.32 0.32 0.32 0.30 0.30 0.28 0.26 0.26 0.26 0.28 0.30 0.32 0.36 0.36 ## [7295] 0.40 0.42 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.30 0.28 0.28 0.26 0.26 ## [7309] 0.24 0.24 0.24 0.26 0.26 0.28 0.30 0.36 0.42 0.44 0.46 0.46 0.48 0.46 ## [7323] 0.44 0.42 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.30 0.28 ## [7337] 0.28 0.30 0.34 0.36 0.42 0.46 0.54 0.56 0.56 0.52 0.50 0.46 0.46 0.40 ## [7351] 0.38 0.36 0.36 0.34 0.32 0.32 0.32 0.30 0.30 0.30 0.30 0.32 0.36 0.42 ## [7365] 0.46 0.52 0.54 0.56 0.58 0.56 0.52 0.48 0.46 0.40 0.40 0.36 0.36 0.36 ## [7379] 0.34 0.32 0.32 0.32 0.30 0.30 0.30 0.32 0.34 0.40 0.46 0.50 0.50 0.52 ## [7393] 0.52 0.52 0.46 0.44 0.44 0.44 0.40 0.40 0.38 0.40 0.40 0.38 0.38 0.38 ## [7407] 0.36 0.36 0.38 0.40 0.42 0.44 0.46 0.42 0.36 0.36 0.36 0.36 0.36 0.36 ## [7421] 0.36 0.36 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.30 0.30 0.28 0.28 0.30 ## [7435] 0.32 0.32 0.34 0.34 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.32 ## [7449] 0.30 0.24 0.24 0.24 0.24 0.20 0.22 0.22 0.22 0.26 0.30 0.34 0.38 0.44 ## [7463] 0.48 0.50 0.52 0.52 0.50 0.42 0.42 0.42 0.42 0.42 0.40 0.40 0.36 0.36 ## [7477] 0.36 0.36 0.34 0.34 0.34 0.34 0.40 0.44 0.46 0.52 0.52 0.54 0.50 0.54 ## [7491] 0.52 0.52 0.50 0.50 0.48 0.48 0.46 0.46 0.46 0.46 0.44 0.44 0.44 0.44 ## [7505] 0.44 0.46 0.48 0.50 0.54 0.56 0.60 0.62 0.64 0.62 0.62 0.56 0.60 0.60 ## [7519] 0.60 0.58 0.56 0.56 0.56 0.56 0.54 0.56 0.54 0.56 0.54 0.54 0.56 0.56 ## [7533] 0.56 0.54 0.54 0.54 0.54 0.52 0.50 0.50 0.50 0.48 0.50 0.46 0.46 0.46 ## [7547] 0.46 0.46 0.46 0.44 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 ## [7561] 0.48 0.48 0.48 0.46 0.46 0.44 0.44 0.42 0.42 0.42 0.42 0.42 0.42 0.40 ## [7575] 0.34 0.36 0.34 0.34 0.34 0.34 0.32 0.34 0.34 0.34 0.34 0.32 0.32 0.32 ## [7589] 0.30 0.30 0.30 0.26 0.26 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.22 ## [7603] 0.26 0.26 0.30 0.32 0.34 0.34 0.34 0.34 0.32 0.30 0.28 0.28 0.28 0.26 ## [7617] 0.26 0.26 0.26 0.24 0.26 0.26 0.24 0.24 0.24 0.26 0.28 0.32 0.36 0.40 ## [7631] 0.42 0.42 0.42 0.42 0.40 0.38 0.34 0.36 0.36 0.38 0.38 0.38 0.40 0.40 ## [7645] 0.40 0.40 0.42 0.42 0.42 0.42 0.44 0.44 0.50 0.50 0.54 0.52 0.52 0.52 ## [7659] 0.52 0.54 0.50 0.52 0.48 0.46 0.46 0.46 0.46 0.44 0.44 0.44 0.44 0.42 ## [7673] 0.46 0.46 0.48 0.52 0.52 0.50 0.50 0.48 0.44 0.44 0.42 0.42 0.40 0.40 ## [7687] 0.40 0.40 0.40 0.38 0.40 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.40 0.40 ## [7701] 0.40 0.40 0.42 0.42 0.44 0.44 0.44 0.44 0.46 0.46 0.46 0.50 0.48 0.48 ## [7715] 0.48 0.50 0.50 0.52 0.46 0.44 0.46 0.48 0.52 0.52 0.50 0.48 0.44 0.42 ## [7729] 0.42 0.40 0.38 0.40 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.28 0.30 ## [7743] 0.30 0.30 0.26 0.30 0.34 0.36 0.42 0.46 0.48 0.50 0.50 0.50 0.48 0.42 ## [7757] 0.40 0.36 0.36 0.36 0.34 0.34 0.34 0.28 0.28 0.30 0.28 0.26 0.26 0.26 ## [7771] 0.32 0.36 0.40 0.46 0.50 0.52 0.52 0.50 0.50 0.46 0.42 0.40 0.36 0.34 ## [7785] 0.34 0.34 0.32 0.30 0.30 0.30 0.30 0.30 0.26 0.32 0.34 0.36 0.40 0.44 ## [7799] 0.48 0.48 0.50 0.46 0.46 0.42 0.40 0.42 0.38 0.38 0.36 0.36 0.36 0.34 ## [7813] 0.34 0.34 0.36 0.38 0.38 0.40 0.46 0.46 0.50 0.54 0.54 0.62 0.62 0.56 ## [7827] 0.54 0.50 0.48 0.50 0.48 0.48 0.48 0.46 0.46 0.44 0.44 0.40 0.42 0.42 ## [7841] 0.42 0.44 0.48 0.52 0.56 0.58 0.60 0.58 0.56 0.58 0.56 0.54 0.54 0.54 ## [7855] 0.52 0.52 0.52 0.52 0.50 0.50 0.52 0.50 0.52 0.52 0.54 0.56 0.56 0.50 ## [7869] 0.42 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.38 0.36 0.36 0.34 ## [7883] 0.32 0.32 0.30 0.28 0.26 0.26 0.28 0.30 0.34 0.38 0.36 0.38 0.38 0.36 ## [7897] 0.36 0.34 0.34 0.32 0.32 0.32 0.30 0.28 0.28 0.26 0.26 0.26 0.26 0.26 ## [7911] 0.24 0.24 0.26 0.30 0.32 0.34 0.36 0.40 0.40 0.40 0.40 0.36 0.36 0.34 ## [7925] 0.34 0.30 0.30 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.24 0.28 ## [7939] 0.30 0.34 0.36 0.40 0.42 0.42 0.42 0.42 0.40 0.34 0.38 0.34 0.32 0.32 ## [7953] 0.30 0.26 0.26 0.24 0.22 0.24 0.24 0.22 0.24 0.26 0.32 0.32 0.36 0.36 ## [7967] 0.36 0.38 0.38 0.36 0.34 0.30 0.30 0.30 0.32 0.30 0.30 0.30 0.26 0.28 ## [7981] 0.26 0.26 0.24 0.26 0.26 0.30 0.32 0.34 0.36 0.40 0.42 0.42 0.42 0.38 ## [7995] 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.32 0.32 0.30 0.30 0.30 0.32 ## [8009] 0.32 0.36 0.36 0.36 0.40 0.42 0.46 0.46 0.50 0.42 0.44 0.46 0.46 0.44 ## [8023] 0.44 0.46 0.50 0.46 0.46 0.46 0.46 0.46 0.44 0.46 0.46 0.46 0.46 0.46 ## [8037] 0.46 0.46 0.48 0.50 0.46 0.46 0.46 0.46 0.46 0.44 0.46 0.46 0.44 0.46 ## [8051] 0.46 0.46 0.46 0.46 0.48 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.42 ## [8065] 0.42 0.40 0.40 0.34 0.34 0.30 0.24 0.24 0.26 0.26 0.28 0.26 0.24 0.22 ## [8079] 0.22 0.22 0.22 0.24 0.26 0.28 0.28 0.30 0.32 0.32 0.32 0.30 0.28 0.26 ## [8093] 0.26 0.28 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.24 0.26 ## [8107] 0.30 0.32 0.36 0.38 0.38 0.38 0.36 0.34 0.34 0.34 0.30 0.30 0.28 0.28 ## [8121] 0.26 0.26 0.28 0.28 0.26 0.26 0.24 0.24 0.26 0.28 0.32 0.32 0.32 0.34 ## [8135] 0.34 0.34 0.32 0.28 0.26 0.26 0.24 0.22 0.22 0.20 0.20 0.16 0.18 0.16 ## [8149] 0.16 0.18 0.16 0.18 0.16 0.20 0.24 0.26 0.26 0.30 0.30 0.30 0.30 0.28 ## [8163] 0.24 0.22 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.18 0.18 0.16 0.16 0.14 ## [8177] 0.16 0.18 0.22 0.26 0.28 0.30 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.26 ## [8191] 0.26 0.24 0.22 0.20 0.20 0.18 0.20 0.18 0.20 0.16 0.20 0.26 0.30 0.34 ## [8205] 0.36 0.40 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.30 0.30 0.26 0.26 0.26 ## [8219] 0.26 0.28 0.26 0.26 0.26 0.28 0.26 0.30 0.32 0.34 0.36 0.36 0.38 0.38 ## [8233] 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.32 ## [8247] 0.32 0.32 0.34 0.36 0.40 0.40 0.46 0.50 0.52 0.52 0.46 0.52 0.52 0.52 ## [8261] 0.52 0.50 0.52 0.52 0.50 0.50 0.48 0.46 0.50 0.48 0.44 0.38 0.36 0.36 ## [8275] 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30 ## [8289] 0.30 0.30 0.28 0.26 0.24 0.26 0.26 0.26 0.26 0.26 0.26 0.28 0.28 0.28 ## [8303] 0.28 0.28 0.28 0.26 0.26 0.24 0.22 0.20 0.20 0.20 0.20 0.20 0.22 0.22 ## [8317] 0.22 0.20 0.20 0.20 0.20 0.22 0.24 0.24 0.26 0.30 0.30 0.32 0.28 0.28 ## [8331] 0.28 0.26 0.24 0.22 0.22 0.20 0.20 0.18 0.18 0.16 0.16 0.14 0.16 0.18 ## [8345] 0.20 0.22 0.24 0.26 0.30 0.34 0.36 0.38 0.40 0.38 0.36 0.36 0.40 0.36 ## [8359] 0.36 0.36 0.36 0.36 0.34 0.34 0.36 0.36 0.36 0.36 0.42 0.36 0.42 0.40 ## [8373] 0.44 0.44 0.44 0.44 0.44 0.40 0.38 0.38 0.36 0.36 0.36 0.38 0.34 0.36 ## [8387] 0.36 0.36 0.36 0.38 0.36 0.36 0.36 0.40 0.48 0.48 0.46 0.44 0.48 0.48 ## [8401] 0.44 0.44 0.44 0.50 0.50 0.50 0.50 0.50 0.50 0.44 0.48 0.44 0.38 0.38 ## [8415] 0.36 0.34 0.36 0.38 0.40 0.44 0.48 0.46 0.46 0.48 0.46 0.44 0.44 0.44 ## [8429] 0.42 0.42 0.36 0.40 0.40 0.40 0.38 0.38 0.38 0.36 0.38 0.38 0.40 0.40 ## [8443] 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.36 0.34 0.32 0.32 0.32 0.32 ## [8457] 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.30 0.32 0.34 0.34 ## [8471] 0.34 0.32 0.32 0.28 0.30 0.30 0.26 0.26 0.24 0.24 0.24 0.22 0.22 0.22 ## [8485] 0.20 0.20 0.22 0.20 0.24 0.26 0.30 0.30 0.32 0.34 0.34 0.36 0.34 0.32 ## [8499] 0.32 0.32 0.30 0.28 0.26 0.22 0.28 0.34 0.34 0.34 0.32 0.32 0.32 0.34 ## [8513] 0.34 0.34 0.36 0.38 0.38 0.38 0.36 0.34 0.32 0.30 0.30 0.26 0.26 0.26 ## [8527] 0.26 0.26 0.26 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.32 0.32 0.30 0.30 ## [8541] 0.42 0.42 0.44 0.40 0.38 0.34 0.32 0.32 0.32 0.30 0.32 0.32 0.32 0.32 ## [8555] 0.32 0.32 0.32 0.32 0.32 0.34 0.36 0.34 0.34 0.32 0.32 0.30 0.28 0.26 ## [8569] 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.18 0.20 ## [8583] 0.20 0.22 0.24 0.26 0.28 0.30 0.30 0.30 0.30 0.30 0.30 0.28 0.28 0.28 ## [8597] 0.30 0.28 0.26 0.24 0.24 0.24 0.22 0.24 0.24 0.24 0.26 0.30 0.32 0.32 ## [8611] 0.36 0.40 0.42 0.42 0.38 0.36 0.34 0.34 0.36 0.34 0.36 0.38 0.40 0.40 ## [8625] 0.40 0.38 0.36 0.40 0.38 0.34 0.38 0.40 0.42 0.52 0.50 0.46 0.46 0.44 ## [8639] 0.42 0.42 0.42 0.42 0.40 0.38 0.36 The Linear Model Approaches to variable coding The purpose of this task is to predict the number of bikers (bikers) using month (mnth), hour (hr), whether it’s a working day (workingday), temperature (temp), and weather situation (weathersit). We begin by fitting a least squares linear regression model to the data. mod.lm &lt;- lm(bikers ~ mnth + hr + workingday + temp + weathersit) summary(mod.lm) ## ## Call: ## lm(formula = bikers ~ mnth + hr + workingday + temp + weathersit) ## ## Residuals: ## Min 1Q Median 3Q Max ## -299.00 -45.70 -6.23 41.08 425.29 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -68.632 5.307 -12.932 &lt; 2e-16 *** ## mnthFeb 6.845 4.287 1.597 0.110398 ## mnthMarch 16.551 4.301 3.848 0.000120 *** ## mnthApril 41.425 4.972 8.331 &lt; 2e-16 *** ## mnthMay 72.557 5.641 12.862 &lt; 2e-16 *** ## mnthJune 67.819 6.544 10.364 &lt; 2e-16 *** ## mnthJuly 45.324 7.081 6.401 1.63e-10 *** ## mnthAug 53.243 6.640 8.019 1.21e-15 *** ## mnthSept 66.678 5.925 11.254 &lt; 2e-16 *** ## mnthOct 75.834 4.950 15.319 &lt; 2e-16 *** ## mnthNov 60.310 4.610 13.083 &lt; 2e-16 *** ## mnthDec 46.458 4.271 10.878 &lt; 2e-16 *** ## hr1 -14.579 5.699 -2.558 0.010536 * ## hr2 -21.579 5.733 -3.764 0.000168 *** ## hr3 -31.141 5.778 -5.389 7.26e-08 *** ## hr4 -36.908 5.802 -6.361 2.11e-10 *** ## hr5 -24.135 5.737 -4.207 2.61e-05 *** ## hr6 20.600 5.704 3.612 0.000306 *** ## hr7 120.093 5.693 21.095 &lt; 2e-16 *** ## hr8 223.662 5.690 39.310 &lt; 2e-16 *** ## hr9 120.582 5.693 21.182 &lt; 2e-16 *** ## hr10 83.801 5.705 14.689 &lt; 2e-16 *** ## hr11 105.423 5.722 18.424 &lt; 2e-16 *** ## hr12 137.284 5.740 23.916 &lt; 2e-16 *** ## hr13 136.036 5.760 23.617 &lt; 2e-16 *** ## hr14 126.636 5.776 21.923 &lt; 2e-16 *** ## hr15 132.087 5.780 22.852 &lt; 2e-16 *** ## hr16 178.521 5.772 30.927 &lt; 2e-16 *** ## hr17 296.267 5.749 51.537 &lt; 2e-16 *** ## hr18 269.441 5.736 46.976 &lt; 2e-16 *** ## hr19 186.256 5.714 32.596 &lt; 2e-16 *** ## hr20 125.549 5.704 22.012 &lt; 2e-16 *** ## hr21 87.554 5.693 15.378 &lt; 2e-16 *** ## hr22 59.123 5.689 10.392 &lt; 2e-16 *** ## hr23 26.838 5.688 4.719 2.41e-06 *** ## workingday 1.270 1.784 0.711 0.476810 ## temp 157.209 10.261 15.321 &lt; 2e-16 *** ## weathersitcloudy/misty -12.890 1.964 -6.562 5.60e-11 *** ## weathersitlight rain/snow -66.494 2.965 -22.425 &lt; 2e-16 *** ## weathersitheavy rain/snow -109.745 76.667 -1.431 0.152341 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 76.5 on 8605 degrees of freedom ## Multiple R-squared: 0.6745, Adjusted R-squared: 0.6731 ## F-statistic: 457.3 on 39 and 8605 DF, p-value: &lt; 2.2e-16 In mod.lm, the first level of hr (0) and mnth (January) are treated as baseline values, meaning no coefficient estimates are provided for them. Implicitly, their coefficient estimates are zero, and all other levels are measured relative to these baselines. For instance, the February coefficient of \\(6.845\\) indicates that, holding all other variables constant, there are on average about 7 more riders in February than in January. Similarly, there are about 16.5 more riders in March compared to January. However, what if we were to recode the hr and mnth variables? contrasts(Bikeshare$hr) = contr.sum(24) contrasts(Bikeshare$mnth) = contr.sum(12) mod.lm2 &lt;- lm(bikers ~ mnth + hr + workingday + temp + weathersit, data = Bikeshare) summary(mod.lm2) ## ## Call: ## lm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, ## data = Bikeshare) ## ## Residuals: ## Min 1Q Median 3Q Max ## -299.00 -45.70 -6.23 41.08 425.29 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 73.5974 5.1322 14.340 &lt; 2e-16 *** ## mnth1 -46.0871 4.0855 -11.281 &lt; 2e-16 *** ## mnth2 -39.2419 3.5391 -11.088 &lt; 2e-16 *** ## mnth3 -29.5357 3.1552 -9.361 &lt; 2e-16 *** ## mnth4 -4.6622 2.7406 -1.701 0.08895 . ## mnth5 26.4700 2.8508 9.285 &lt; 2e-16 *** ## mnth6 21.7317 3.4651 6.272 3.75e-10 *** ## mnth7 -0.7626 3.9084 -0.195 0.84530 ## mnth8 7.1560 3.5347 2.024 0.04295 * ## mnth9 20.5912 3.0456 6.761 1.46e-11 *** ## mnth10 29.7472 2.6995 11.019 &lt; 2e-16 *** ## mnth11 14.2229 2.8604 4.972 6.74e-07 *** ## hr1 -96.1420 3.9554 -24.307 &lt; 2e-16 *** ## hr2 -110.7213 3.9662 -27.916 &lt; 2e-16 *** ## hr3 -117.7212 4.0165 -29.310 &lt; 2e-16 *** ## hr4 -127.2828 4.0808 -31.191 &lt; 2e-16 *** ## hr5 -133.0495 4.1168 -32.319 &lt; 2e-16 *** ## hr6 -120.2775 4.0370 -29.794 &lt; 2e-16 *** ## hr7 -75.5424 3.9916 -18.925 &lt; 2e-16 *** ## hr8 23.9511 3.9686 6.035 1.65e-09 *** ## hr9 127.5199 3.9500 32.284 &lt; 2e-16 *** ## hr10 24.4399 3.9360 6.209 5.57e-10 *** ## hr11 -12.3407 3.9361 -3.135 0.00172 ** ## hr12 9.2814 3.9447 2.353 0.01865 * ## hr13 41.1417 3.9571 10.397 &lt; 2e-16 *** ## hr14 39.8939 3.9750 10.036 &lt; 2e-16 *** ## hr15 30.4940 3.9910 7.641 2.39e-14 *** ## hr16 35.9445 3.9949 8.998 &lt; 2e-16 *** ## hr17 82.3786 3.9883 20.655 &lt; 2e-16 *** ## hr18 200.1249 3.9638 50.488 &lt; 2e-16 *** ## hr19 173.2989 3.9561 43.806 &lt; 2e-16 *** ## hr20 90.1138 3.9400 22.872 &lt; 2e-16 *** ## hr21 29.4071 3.9362 7.471 8.74e-14 *** ## hr22 -8.5883 3.9332 -2.184 0.02902 * ## hr23 -37.0194 3.9344 -9.409 &lt; 2e-16 *** ## workingday 1.2696 1.7845 0.711 0.47681 ## temp 157.2094 10.2612 15.321 &lt; 2e-16 *** ## weathersitcloudy/misty -12.8903 1.9643 -6.562 5.60e-11 *** ## weathersitlight rain/snow -66.4944 2.9652 -22.425 &lt; 2e-16 *** ## weathersitheavy rain/snow -109.7446 76.6674 -1.431 0.15234 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 76.5 on 8605 degrees of freedom ## Multiple R-squared: 0.6745, Adjusted R-squared: 0.6731 ## F-statistic: 457.3 on 39 and 8605 DF, p-value: &lt; 2.2e-16 What is the difference between the two codings? In mod.lm2, a coefficient estimate is reported for all but the last level of hr and mnth. Importantly, in mod.lm2, the coefficient estimate for the last level of mnth is not zero; instead, it equals the negative of the sum of the coefficient estimates for all the other levels. Similarly, in mod.lm2, the coefficient estimate for the last level of hr is the negative of the sum of the coefficient estimates for all the other levels. This means the coefficients of hr and mnth in mod.lm2 will always sum to zero and can be interpreted as the difference from the mean level. For example, the coefficient for January of \\(-46.087\\) indicates that, holding all other variables constant, there are typically 46 fewer riders in January compared to the yearly average. The predictions from either of the two models is identical, as confirmed below. However, the choice of coding is crucial for correct interpretation of the model results. sum((predict(mod.lm) - predict(mod.lm2))^2) ## [1] 1.426274e-18 The sum of squared differences is zero. We can also see this using the all.equal() function: all.equal(predict(mod.lm), predict(mod.lm2)) ## [1] TRUE Plotting Coefficient Estimates Let’s plot the coefficient estimates for the variable mnth. First we obtain the coefficients for January through November from the mod.lm2 object. The coefficient for December must be explicitly computed as the negative sum of all the other months. coef.months &lt;- c(coef(mod.lm2)[2:12], -sum(coef(mod.lm2)[2:12])) To make the plot, we manually label the \\(x\\)-axis with the names of the months. plot(coef.months, xlab = &quot;Month&quot;, ylab = &quot;Coefficient&quot;, xaxt = &quot;n&quot;, col = &quot;blue&quot;, pch = 19, type = &quot;o&quot;) axis(side = 1, at = 1:12, labels = c(&quot;J&quot;, &quot;F&quot;, &quot;M&quot;, &quot;A&quot;, &quot;M&quot;, &quot;J&quot;, &quot;J&quot;, &quot;A&quot;, &quot;S&quot;, &quot;O&quot;, &quot;N&quot;, &quot;D&quot;)) Now let’s do the same for the variable mhr. coef.hours &lt;- c(coef(mod.lm2)[13:35], -sum(coef(mod.lm2)[13:35])) plot(coef.hours, xlab = &quot;Hour&quot;, ylab = &quot;Coefficient&quot;, col = &quot;blue&quot;, pch = 19, type = &quot;o&quot;) What do these two plots show? The Poisson Model Now let’s fit a Poisson model that is more appropriate given that we are dealing with count data. The approach to fitting the model is very similar to that of logistic regression except that we use the argument family = poisson. As with the linear model, the purpose is to predict the number of bikers (bikers) using month (mnth), hour (hr), whether it’s a working day (workingday), temperature (temp), and weather situation (weathersit). mod.pois &lt;- glm(bikers ~ mnth + hr + workingday + temp + weathersit, data = Bikeshare, family = poisson) summary(mod.pois) ## ## Call: ## glm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, ## family = poisson, data = Bikeshare) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -20.7574 -3.3441 -0.6549 2.6999 21.9628 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 4.118245 0.006021 683.964 &lt; 2e-16 *** ## mnth1 -0.670170 0.005907 -113.445 &lt; 2e-16 *** ## mnth2 -0.444124 0.004860 -91.379 &lt; 2e-16 *** ## mnth3 -0.293733 0.004144 -70.886 &lt; 2e-16 *** ## mnth4 0.021523 0.003125 6.888 5.66e-12 *** ## mnth5 0.240471 0.002916 82.462 &lt; 2e-16 *** ## mnth6 0.223235 0.003554 62.818 &lt; 2e-16 *** ## mnth7 0.103617 0.004125 25.121 &lt; 2e-16 *** ## mnth8 0.151171 0.003662 41.281 &lt; 2e-16 *** ## mnth9 0.233493 0.003102 75.281 &lt; 2e-16 *** ## mnth10 0.267573 0.002785 96.091 &lt; 2e-16 *** ## mnth11 0.150264 0.003180 47.248 &lt; 2e-16 *** ## hr1 -0.754386 0.007879 -95.744 &lt; 2e-16 *** ## hr2 -1.225979 0.009953 -123.173 &lt; 2e-16 *** ## hr3 -1.563147 0.011869 -131.702 &lt; 2e-16 *** ## hr4 -2.198304 0.016424 -133.846 &lt; 2e-16 *** ## hr5 -2.830484 0.022538 -125.586 &lt; 2e-16 *** ## hr6 -1.814657 0.013464 -134.775 &lt; 2e-16 *** ## hr7 -0.429888 0.006896 -62.341 &lt; 2e-16 *** ## hr8 0.575181 0.004406 130.544 &lt; 2e-16 *** ## hr9 1.076927 0.003563 302.220 &lt; 2e-16 *** ## hr10 0.581769 0.004286 135.727 &lt; 2e-16 *** ## hr11 0.336852 0.004720 71.372 &lt; 2e-16 *** ## hr12 0.494121 0.004392 112.494 &lt; 2e-16 *** ## hr13 0.679642 0.004069 167.040 &lt; 2e-16 *** ## hr14 0.673565 0.004089 164.722 &lt; 2e-16 *** ## hr15 0.624910 0.004178 149.570 &lt; 2e-16 *** ## hr16 0.653763 0.004132 158.205 &lt; 2e-16 *** ## hr17 0.874301 0.003784 231.040 &lt; 2e-16 *** ## hr18 1.294635 0.003254 397.848 &lt; 2e-16 *** ## hr19 1.212281 0.003321 365.084 &lt; 2e-16 *** ## hr20 0.914022 0.003700 247.065 &lt; 2e-16 *** ## hr21 0.616201 0.004191 147.045 &lt; 2e-16 *** ## hr22 0.364181 0.004659 78.173 &lt; 2e-16 *** ## hr23 0.117493 0.005225 22.488 &lt; 2e-16 *** ## workingday 0.014665 0.001955 7.502 6.27e-14 *** ## temp 0.785292 0.011475 68.434 &lt; 2e-16 *** ## weathersitcloudy/misty -0.075231 0.002179 -34.528 &lt; 2e-16 *** ## weathersitlight rain/snow -0.575800 0.004058 -141.905 &lt; 2e-16 *** ## weathersitheavy rain/snow -0.926287 0.166782 -5.554 2.79e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 1052921 on 8644 degrees of freedom ## Residual deviance: 228041 on 8605 degrees of freedom ## AIC: 281159 ## ## Number of Fisher Scoring iterations: 5 Below is a table which summarises what the coefficients tell us: intercept baseline log count of bikers when all predictors are at their reference levels mnth effects of different months (1-11) relative to the reference month (mnth 12 - December) For example, the estimate for January (mnth1) is \\(-0.670170\\). This tells us that the log count of bikers in January is lower by approximately 0.67 units compared to the reference month (December). To facilitate interpretation, we exponentiate the coefficient: exp(-0.670170) ≈ 0.5116. Therefore, the expected number of bikers in January is about \\(51\\%\\) that of December. This indicates a decrease of approximately \\(0.511 6-1 = 0.4884(100) = 48.84\\%\\) compared to December, holding all other variables constant. hr effects of different hours (1-23) relative to the reference hour (hr24) For example, the estimate for 8:00 (hr8) is \\(0.575181\\). This tells us that the log count of bikers at 8 in the morning is higher by approximately 0.58 units compared to the reference hour (midnight). We exponentiate the coefficient: exp(0.575181) ≈ 1.777452. Therefore, the expected number of bikers at 8 in the morning is about \\(1 .777-1 = 0.77(100) = 77.7\\%\\) higher than the number of bikers at midnight, holding all other variables constant. workingday since the coefficient is positive, it suggests that there are more bikers on working days We exponentiate the coefficient: exp(0.014665) ≈ 1.014773. Therefore, the expected number of bikers on working day is about \\(1.01 4773-1 = 0.015(100) = 1.5\\%\\) higher than the number of bikers during a non-working day, holding all other variables constant. temp since the coefficient is positive, it suggests that higher temperatures are associated with more bikers We exponentiate the coefficient: exp(0.785292) ≈ 2.193047. Therefore, for each one-unit increase in the normalised temperature, the expected number of bikers is about \\(2.193047- 1 = 1.193047(100) = 119.3\\%\\) higher, holding all other variables constant weathersit indicates the effect of different weather conditions on the number of bikers relative to the reference (clear weather). Since the coefficients are negative, this suggests that adverse weather conditions (e.g., heavy rain/snow) significantly reduce the number of bikers For cloudy/misty weather, the expected number of bikers is exp(-0.075231) ≈ 0.9275 times the number of bikers in clear weather, indicating a decrease of about 7.25%. For light rain/snow, the expected number of bikers is exp(-0.575800) ≈ 0.5621 times the number of bikers in clear weather, indicating a decrease of about 43.79%. For heavy rain/snow, the expected number of bikers is exp(-0.926287) ≈ 0.3957 times the number of bikers in clear weather, indicating a decrease of about 60.43%. Let’s plot the coefficients associated with the mnth variable. coef.mnth &lt;- c(coef(mod.pois)[2:12], -sum(coef(mod.pois)[2:12])) plot(coef.mnth, xlab = &quot;Month&quot;, ylab = &quot;Coefficient&quot;, xaxt = &quot;n&quot;, col = &quot;blue&quot;, pch = 19, type = &quot;o&quot;) axis(side = 1, at = 1:12, labels = c(&quot;J&quot;, &quot;F&quot;, &quot;M&quot;, &quot;A&quot;, &quot;M&quot;, &quot;J&quot;, &quot;J&quot;, &quot;A&quot;, &quot;S&quot;, &quot;O&quot;, &quot;N&quot;, &quot;D&quot;)) And the coefficients associated with the hr variable. coef.hours &lt;- c(coef(mod.pois)[13:35], -sum(coef(mod.pois)[13:35])) plot(coef.hours, xlab = &quot;Hour&quot;, ylab = &quot;Coefficient&quot;, col = &quot;blue&quot;, pch = 19, type = &quot;o&quot;) What do these plots suggest? Linear versus Poisson Regression Let’s plot the fitted values from the Poisson model and compare them to those of the linear model. For the Poisson predictions, we must use the argument type = \"response\" to specify that we want R to output \\(\\exp(\\hat\\beta_0 + \\hat\\beta_1 X_1 + \\ldots +\\hat\\beta_p X_p)\\) rather than \\(\\hat\\beta_0 + \\hat\\beta_1 X_1 + \\ldots + \\hat\\beta_p X_p\\), which it will output by default. plot(predict(mod.lm2), predict(mod.pois, type = &quot;response&quot;)) abline(0, 1, col = 2, lwd = 3) As you can see, the predictions from the Poisson regression model are correlated with those from the linear model; however, the former are non-negative. As a result the Poisson regression predictions tend to be larger than those from the linear model for either very low or very high levels of ridership. "],["practical-predicting-a-companys-bankruptcy.html", "Practical: Predicting a company’s bankruptcy Data and Variables Importing the data Loading required packages Correlation Matrix and Plot Logistic Regression Explaining the Logit Tasks: Linear Discriminant Analysis Tasks: Quadratic Discriminant Analysis Tasks: \\(K\\)-nearest neighbours Tasks: Naive Bayes", " Practical: Predicting a company’s bankruptcy This practical is adapted from a demonstration created by Dr. Tatjana Kecojevic, Lecturer in Social Statistics. For the tasks below, you will require the four_ratios dataset. Click here to download the file: four_ratios.csv . Remember to place your data file in a separate subfolder within your R project working directory. Data and Variables In order to build a model to predict a company’s bankruptcy, an analyst has collected a sample of data as follows: Four financial ratios (predictors): x1: Retained Earnings / Total Assets x2: Earnings Before Interest and Taxes / Total Assets x3: Sales / Total Assets x4: Cash Flow / Total Debt And a binary response variable y: 0 - if the company went bankrupt 1 - if the company stayed solvent Importing the data four_ratios &lt;- read.csv(&quot;data/four_ratios.csv&quot;, header = TRUE) Loading required packages library(MASS) library(class) library(tidyverse) library(corrplot) library(ISLR2) library(e1071) Now let’s explore the dataset. We can see that the predictors are all of class double. The response is of class integer, despite this being a binary categorical variable. There are a total of 111 observations, so quite a small dataset. glimpse(four_ratios) ## Rows: 111 ## Columns: 5 ## $ x1 &lt;dbl&gt; 0.3778, 0.2783, 0.1192, 0.6070, 0.5334, 0.3868, 0.3312, 0.0130, 0.6… ## $ x2 &lt;dbl&gt; 0.1316, 0.1166, 0.2035, 0.2040, 0.1650, 0.0681, 0.2157, 0.2366, 0.2… ## $ x3 &lt;dbl&gt; 1.0911, 1.3441, 0.8130, 14.4090, 8.0734, 0.5755, 3.0679, 2.4709, 5.… ## $ x4 &lt;dbl&gt; 1.2784, 0.2216, 1.6702, 0.9844, 1.3474, 1.0579, 2.0899, 1.2230, 1.7… ## $ y &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… We therefore transform the response to a factor. four_ratios$y &lt;- as_factor(four_ratios$y) Correlation Matrix and Plot Let’s now produce a correlation plot between all pairs of variables in the dataset. cor_matrix &lt;- cor(four_ratios[,-5]) corrplot(cor_matrix, type = &quot;lower&quot;, diag = FALSE) We observe a strong positive correlation between \\(X_1\\) and \\(X_2\\). The correlations between other variables are quite weak. Before we consider the model, we split the data into training and test sets as we will later on assess model accuracy in correct prediction using the test data set. We consider a basic fixed split of 80:20. Since there are a total of 111 observations, we split at row 89 set.seed(123) split_idx = sample(nrow(four_ratios), 89) train = four_ratios[split_idx, ] test = four_ratios[-split_idx, ] Logistic Regression Now let’s first consider logistic regression and build the model. Given the high correlation between \\(x_1\\) and \\(x_2\\) (this is to be expected, given what they measure), we have to reflect on how we want to address this. Assuming we have knowledge of important factors that can predict bankruptcy, we can decide to drop one of the two financial ratios. In this case, we drop \\(x_2\\). fit &lt;- glm(y ~ x1 + x3 + x4, data = train, family = binomial) summary(fit) ## ## Call: ## glm(formula = y ~ x1 + x3 + x4, family = binomial, data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1151 -0.4682 0.0000 0.3720 2.2542 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.6970 0.8896 -3.032 0.00243 ** ## x1 12.6569 3.0942 4.090 4.3e-05 *** ## x3 1.6400 0.5764 2.845 0.00444 ** ## x4 -0.5247 0.3297 -1.591 0.11151 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 123.369 on 88 degrees of freedom ## Residual deviance: 54.552 on 85 degrees of freedom ## AIC: 62.552 ## ## Number of Fisher Scoring iterations: 7 The key components of R’s summary() function for generalised linear models for binomial family with the logit link are: Call: just like in the case of fitting the lm() model this is R reminding us what model we ran, what options we specified, etc the Deviance Residuals are a measure of model fit. This part of the output shows the distribution of the deviance residuals for individual cases used in the model. Below we discuss how to use summaries of the deviance statistic to assess model fit the Coefficients, their standard errors, the z-statistic (sometimes called a Wald z-statistic), and the associated p-values. The logistic regression coefficients give the change in the log odds of the outcome for a one unit increase in the predictor variable At the end there are fit indices, including the null and deviance residuals and the AIC, which are used to assess overall model fit. We realise that the output above has a resemblance to the standard regression output. Although they differ in the type of information, they serve similar functions. Let us make an interpretation of it. The fitted logarithm of the odds ratio, i.e. logit of the probability p of the firm remaining solvent is modelled as: \\[\\hat{g}(X_1,X_2,...X_q)=−9.8846+0.3238X_1+0.1779X_2+4.9443X_3\\] Remember that here instead of predicting Y we obtain the model to predict \\(\\log \\left( \\frac{p}{1-p} \\right)\\). Using the transformation of the logit we get the predicted probabilities of the firm being solvent. The estimated parameters are expected changes in the logit for unit change in their corresponding variables when the other, remaining variables are held fixed. That is, the logistic regression coefficients give the change in the log odds of the response variable for a unit increase in the explanatory variable. This is very hard to make sense of. We have predicted log odds and in order to interpret them into a more sensible fashion we need to “anti-log” them as the changes in odds ratio for a unit change in variable \\(X_i\\) , while the other variables are held fixed. round(exp(coef(fit)), 4) ## (Intercept) x1 x3 x4 ## 0.0674 313912.8230 5.1550 0.5917 Now, we can interpret the coefficients in terms of odds. For example, the odds of a firm being solvent (vs being bankrupt) increases by 313913 for a unit change in ratio \\(X_1\\), all else in the model being being fixed. Explaining the Logit Let’s return to the summary of the fit: summary(fit) ## ## Call: ## glm(formula = y ~ x1 + x3 + x4, family = binomial, data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1151 -0.4682 0.0000 0.3720 2.2542 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.6970 0.8896 -3.032 0.00243 ** ## x1 12.6569 3.0942 4.090 4.3e-05 *** ## x3 1.6400 0.5764 2.845 0.00444 ** ## x4 -0.5247 0.3297 -1.591 0.11151 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 123.369 on 88 degrees of freedom ## Residual deviance: 54.552 on 85 degrees of freedom ## AIC: 62.552 ## ## Number of Fisher Scoring iterations: 7 The column headed as z value is the ratio of the coefficients (Estimate) to their standard errors (Std. Error) known as Wald statistics for testing the hypothesis that the corresponding parameter are zeros. In standard regression this would be the t-test. Next to Wald test statistics we have their corresponding p-values (Pr(&gt;|z|)) which are used to judge the significance of the coefficients. Values smaller than 0.5 would lead to the conclusion that the coefficient is significantly different from 0 at 5% significance level. From the output obtained we see that two p-values are smaller than 0.5. We now need to make a proper assessment to check if the variables collectively contribute in explaining the logit. That is, we need to examine whether the coefficients \\(\\beta_1\\), \\(\\beta_2\\), \\(\\beta_3\\) and \\(\\beta_4\\) are all zeros. We do this using the G statistic, which stands for goodness of fit G = likelihood without the predictors − likelihood with the predictors. G is distributed as a chi-square statistic with 1 degree of freedom, so a chi-square test is the test of the fit of the model (note, that G is similar to an \\(R^2\\) type test). \\(H_0:\\beta_i=0\\) \\(H_1:\\) at least one is different from \\(0\\) where \\(i = 1, 2...\\). We decide on \\(\\alpha = 0.05\\). If its associated p-value is greater than 0.05, we conclude that the variables do not collectively influence the logits, if however p-value is less that 0.05 we conclude that they do collectively influence the logits. Let’s calculate the G statistic. G_calc &lt;- fit$null.deviance - fit$deviance G_calc ## [1] 68.81702 Then the degrees of freedom of the predictors. Gdf &lt;- fit$df.null - fit$df.residual Gdf ## [1] 3 We find the critical value for the G statistic. qchisq(.95, df = Gdf) ## [1] 7.814728 And finally, the p-value associated with the G statistic. 1 - pchisq(G_calc, Gdf) ## [1] 7.660539e-15 Now, we have to decide whether our model is a statistically valid one. Since \\(G_{calc} = 68.82 &gt; G_{crit} = 7.81 ⇒ H1,\\) (and the p-value is much smaller than 0.05), we can conclude that this is a statistically valid model and that the variables collectively have explanatory power. However, do we need ALL three variables? In linear regression we assess the significance of individual regression coefficients, i.e. the contribution of the individual variables using the t-test. Here, we use the z scores to conduct the equivalent Wald statistic (test). The ratio of the logistic regression has a normal distribution as opposed to the t-distribution we have seen in linear regression. Nonetheless, the set of hypotheses we wish to test is the same: \\(H_0:\\beta_i = 0\\) (coefficient ii is not significant, thus Xi is not important) \\(H_1:\\beta_i \\ne 0\\) (coefficient ii is significant, thus Xi is important) Decision Rule: If the Wald’s z statistic lies between -2 and +2, then the financial ratio, \\(X_i\\), is not needed and can be dropped from the analysis. Otherwise, we will keep the financial ratio.However, there is some scope here for subjective judgement depending upon how near to +/-2 the Wald’s z value is. The p-value decision rule: if \\(p &gt; 0.05\\) ⇒ the variable can be taken out, otherwise if p-value &lt; 0.05 keep the variable in the model. Rather than fitting individual models and doing a manual comparison we can make use of the anova function for comparing the nested model using the chi-square test. anova(fit, test=&quot;Chisq&quot;) ## Warning: glm.fit: fitted probabilities numerically 0 or 1 occurred ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: y ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 88 123.369 ## x1 1 54.441 87 68.928 1.602e-13 *** ## x3 1 12.044 86 56.884 0.0005197 *** ## x4 1 2.332 85 54.552 0.1267323 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Based on the output, we can remove \\(X_4\\) ratio variable from the model and we update our model (we also know that \\(X_4\\) is not statistically significant from the summary of the model results earlier). fit2 &lt;- update(fit, ~. -x4, data = train) summary(fit2) ## ## Call: ## glm(formula = y ~ x1 + x3, family = binomial, data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2116 -0.4985 0.0000 0.4300 2.3366 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.4779 0.8117 -4.285 1.83e-05 *** ## x1 11.6639 2.7793 4.197 2.71e-05 *** ## x3 1.6202 0.5754 2.816 0.00487 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 123.369 on 88 degrees of freedom ## Residual deviance: 56.884 on 86 degrees of freedom ## AIC: 62.884 ## ## Number of Fisher Scoring iterations: 7 To compare the fit of the new model we will use the Akaike Information Criterion (AIC), which is an index of fit that takes account of parsimony of the model by penalising for the number of parameters. It is defined as: AIC = −2× maximum log-likelihood + 2× number of parameters Therefore, smaller values are indicative of a better fit to the data. In the context of logit fit, the AIC is simply the residual deviance plus twice the number of regression coefficients. The AIC can be found at the end of the output. summary(fit2) ## ## Call: ## glm(formula = y ~ x1 + x3, family = binomial, data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2116 -0.4985 0.0000 0.4300 2.3366 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.4779 0.8117 -4.285 1.83e-05 *** ## x1 11.6639 2.7793 4.197 2.71e-05 *** ## x3 1.6202 0.5754 2.816 0.00487 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 123.369 on 88 degrees of freedom ## Residual deviance: 56.884 on 86 degrees of freedom ## AIC: 62.884 ## ## Number of Fisher Scoring iterations: 7 The AIC of our initial model is 62.552 and of the new model 62.884 (very little difference). Checking the new model, we can see that it consists of the variables that all significantly contribute in explaining the logits. So, in the spirit of parsimony we can choose the second model to be a better fit. You will learn more about the AIC and other similar metrics later in the course. To obtain the overall accuracy rate we need to find the predicted probabilities of the observations kept aside in the test subset. pred &lt;- predict(fit2, test, type = &quot;response&quot;) &gt; 0.5 Now let’s compute the confusion matrix such that we can compare our predictions on the test data against the actual values in our dataset. We can see that our model predicts one instance incorrectly (it predicts bankrupt when in fact it should predict solvent) (t &lt;- table(ifelse(pred, &quot;Solvent (pred)&quot;, &quot;Bankrupt (pred)&quot;), test$y)) ## ## 0 1 ## Bankrupt (pred) 11 1 ## Solvent (pred) 0 10 If we then compute the overall fraction of correct predictions, we can see that this is extremely high (0.955) which means that our model is performing extremely well. In practice: Although the overall accuracy rate might be easy to compute and to interpret, it makes no distinction about the type of errors being made. Although we did remove one the variables due to high correlation, other variables do show some degree of correlation and so we must be cautious about the strength of the relationship and therefore, the overall predictive performance. sum(diag(t)) / sum(t) ## [1] 0.9545455 Do note that if you performing any rounding of the probabilities prior to classification, values near the threshold of 0.5 may be classified differently than if you were not to round the probabilities. Tasks: Linear Discriminant Analysis Task 1 Build a LDA classifier for the final logistic model with two predictors and explain what the output means. Task 2 Compute the predictions and explain what the results mean. Task 3 Compute the confusion matrix for the LDA classifier and calculate the fraction of correct predictions. Explain the results. Tasks: Quadratic Discriminant Analysis Task 1 Build a QDA clasifier and describe the output. Task 2 Compute the predictions, the confusion matrix, and the fraction of correct predictions and explain what the results mean. Tasks: \\(K\\)-nearest neighbours Task 1 Perform KNN regression on the same model as in the previous tasks. Task 2 Produce a confusion matrix and compute the fraction of correct predictions. Comment on the results. Task 3 Fit KNN for up to \\(k = 30\\) and then calculate the overall fraction of correct prediction. Task 4 Create a plot to observe at what value for k the overall fraction of correct predictions is highest. Tasks: Naive Bayes Task 1 Build a Naive Bayes classifier for the model used previously and describe the output. Task 2 Compute the predictions, build a confusion matrix, calculate the fraction of correct predictions and interpret the results. "],["answers-2.html", "Answers Practical: Predicting a company’s bankruptcy", " Answers Practical: Predicting a company’s bankruptcy This practical is adapted from a demonstration created by Dr. Tatjana Kecojevic, Lecturer in Social Statistics. For the tasks below, you will require the four_ratios dataset. Click here to download the file: four_ratios.csv . Remember to place your data file in a separate subfolder within your R project working directory. Data and Variables In order to build a model to predict a company’s bankruptcy, an analyst has collected a sample of data as follows: Four financial ratios (predictors): x1: Retained Earnings / Total Assets x2: Earnings Before Interest and Taxes / Total Assets x3: Sales / Total Assets x4: Cash Flow / Total Debt And a binary response variable y: 0 - if the company went bankrupt 1 - if the company stayed solvent Importing the data four_ratios &lt;- read.csv(&quot;data/four_ratios.csv&quot;, header = TRUE) Loading required packages library(MASS) library(class) library(tidyverse) library(corrplot) library(ISLR2) library(e1071) We can see that the predictors are all of class double. The response is of class integer, despite this being a binary categorical variable. There are a total of 111 observations, so quite a small dataset. glimpse(four_ratios) ## Rows: 111 ## Columns: 5 ## $ x1 &lt;dbl&gt; 0.3778, 0.2783, 0.1192, 0.6070, 0.5334, 0.3868, 0.3312, 0.0130, 0.6… ## $ x2 &lt;dbl&gt; 0.1316, 0.1166, 0.2035, 0.2040, 0.1650, 0.0681, 0.2157, 0.2366, 0.2… ## $ x3 &lt;dbl&gt; 1.0911, 1.3441, 0.8130, 14.4090, 8.0734, 0.5755, 3.0679, 2.4709, 5.… ## $ x4 &lt;dbl&gt; 1.2784, 0.2216, 1.6702, 0.9844, 1.3474, 1.0579, 2.0899, 1.2230, 1.7… ## $ y &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… We therefore transform the response to a factor. four_ratios$y &lt;- as_factor(four_ratios$y) Correlation Matrix and Plot Let’s now produce a correlation plot between all pairs of variables in the dataset. cor_matrix &lt;- cor(four_ratios[,-5]) corrplot(cor_matrix, type = &quot;lower&quot;, diag = FALSE) We observe a strong positive correlation between \\(X_1\\) and \\(X_2\\). The correlations between other variables are quite weak. Before we consider the model, we split the data into training and test sets as we will later on assess model accuracy in correct prediction using the test data set. We consider a basic fixed split of 80:20. Since there are a total of 111 observations, we split at row 89 set.seed(123) split_idx = sample(nrow(four_ratios), 89) train = four_ratios[split_idx, ] test = four_ratios[-split_idx, ] Logistic Regression Now let’s first consider logistic regression and build the model. Given the high correlation between \\(x_1\\) and \\(x_2\\) (this is to be expected, given what they measure), we have to reflect on how we want to address this. Assuming we have knowledge of important factors that can predict bankruptcy, we can decide to drop one of the two financial ratios. In this case, we drop \\(x_2\\). fit &lt;- glm(y ~ x1 + x3 + x4, data = train, family = binomial) summary(fit) ## ## Call: ## glm(formula = y ~ x1 + x3 + x4, family = binomial, data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1151 -0.4682 0.0000 0.3720 2.2542 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.6970 0.8896 -3.032 0.00243 ** ## x1 12.6569 3.0942 4.090 4.3e-05 *** ## x3 1.6400 0.5764 2.845 0.00444 ** ## x4 -0.5247 0.3297 -1.591 0.11151 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 123.369 on 88 degrees of freedom ## Residual deviance: 54.552 on 85 degrees of freedom ## AIC: 62.552 ## ## Number of Fisher Scoring iterations: 7 We now need to make a proper assessment to check if the variables collectively contribute in explaining the logit. Therefore, we calculate the G statistic. G_calc &lt;- fit$null.deviance - fit$deviance G_calc ## [1] 68.81702 Then the degrees of freedom of the predictors. Gdf &lt;- fit$df.null - fit$df.residual Gdf ## [1] 3 We find the critical value for the G statistic. qchisq(.95, df = Gdf) ## [1] 7.814728 And finally, the p-value associated with the G statistic. 1 - pchisq(G_calc, Gdf) ## [1] 7.660539e-15 Now, we have to decide whether our model is a statistically valid one. Since \\(G_{calc} = 68.82 &gt; G_{crit} = 7.81 ⇒ H1,\\) (and the p-value is much smaller than 0.05), we can conclude that this is a statistically valid model and that the variables collectively have explanatory power. However, do we need ALL three variables? Rather than fitting individual models and doing a manual comparison we can make use of the anova function for comparing the nested model using the chi-square test. anova(fit, test=&quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: y ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 88 123.369 ## x1 1 54.441 87 68.928 1.602e-13 *** ## x3 1 12.044 86 56.884 0.0005197 *** ## x4 1 2.332 85 54.552 0.1267323 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Based on the output, we can remove \\(X_4\\) ratio variable from the model and we update our model (we also know that \\(X_4\\) is not statistically significant from the summary of the model results earlier). fit2 &lt;- update(fit, ~. -x4, data = train) summary(fit2) ## ## Call: ## glm(formula = y ~ x1 + x3, family = binomial, data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2116 -0.4985 0.0000 0.4300 2.3366 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.4779 0.8117 -4.285 1.83e-05 *** ## x1 11.6639 2.7793 4.197 2.71e-05 *** ## x3 1.6202 0.5754 2.816 0.00487 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 123.369 on 88 degrees of freedom ## Residual deviance: 56.884 on 86 degrees of freedom ## AIC: 62.884 ## ## Number of Fisher Scoring iterations: 7 To compare the fit of the new model we will use the Akaike Information Criterion (AIC), which is an index of fit that takes account of parsimony of the model by penalising for the number of parameters. summary(fit2) ## ## Call: ## glm(formula = y ~ x1 + x3, family = binomial, data = train) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2116 -0.4985 0.0000 0.4300 2.3366 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.4779 0.8117 -4.285 1.83e-05 *** ## x1 11.6639 2.7793 4.197 2.71e-05 *** ## x3 1.6202 0.5754 2.816 0.00487 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 123.369 on 88 degrees of freedom ## Residual deviance: 56.884 on 86 degrees of freedom ## AIC: 62.884 ## ## Number of Fisher Scoring iterations: 7 The AIC of our initial model is 62.552 and of the new model 62.884 (very little difference). Checking the new model, we can see that it consists of the variables that all significantly contribute in explaining the logits. So, in the spirit of parsimony we can choose the second model to be a better fit. You will learn more about the AIC and other similar metrics later in the course. To obtain the overall accuracy rate we need to find the predicted probabilities of the observations kept aside in the test subset. pred &lt;- predict(fit2, test, type = &quot;response&quot;) &gt; 0.5 Now let’s compute the confusion matrix such that we can compare our predictions on the test data against the actual values in our dataset. We can see that our model predicts one instance incorrectly (it predicts bankrupt when in fact it should predict solvent) (t &lt;- table(ifelse(pred, &quot;Solvent (pred)&quot;, &quot;Bankrupt (pred)&quot;), test$y)) ## ## 0 1 ## Bankrupt (pred) 11 1 ## Solvent (pred) 0 10 If we then compute the overall fraction of correct predictions, we can see that this is extremely high (0.955) which means that our model is performing extremely well. In practice: Although the overall accuracy rate might be easy to compute and to interpret, it makes no distinction about the type of errors being made. Although we did remove one the variables due to high correlation, other variables do show some degree of correlation and so we must be cautious about the strength of the relationship and therefore, the overall predictive performance. sum(diag(t)) / sum(t) ## [1] 0.9545455 Do note that if you performing any rounding of the probabilities prior to classification, values near the threshold of 0.5 may be classified differently than if you were not to round the probabilities. Linear Discriminant Analysis Task 1 Build a LDA classifier for the final logistic model with two predictors and explain what the output means. fit_lda &lt;- lda(y ~ x1 + x3, data = train) fit_lda ## Call: ## lda(y ~ x1 + x3, data = train) ## ## Prior probabilities of groups: ## 0 1 ## 0.494382 0.505618 ## ## Group means: ## x1 x3 ## 0 0.009738636 0.4628477 ## 1 0.341413333 2.6057978 ## ## Coefficients of linear discriminants: ## LD1 ## x1 4.61295546 ## x3 0.06590811 prior probabilities of groups: these tells us the way in which the two classes are distributed in our training data (i.e. 49.4 % of the observations correspond to bankruptcy whilst 50.6 % to solvency). group means: the average of the two predictors within each class which are used by LDA as an estimate of \\(μ_{k}\\). coefficient(s) of linear discriminants: tells us how our predictor(s) influence the score that is used to classify the observations into one of the two categories. Here, the coefficients both predictors are positive and so this indicates that higher values for \\(x_1\\) and \\(x_2\\) will make the model more likely classify an observation as belonging to the solvent class; also, the larger the absolute value of the coefficient, the stronger the influence on the model. fit_lda ## Call: ## lda(y ~ x1 + x3, data = train) ## ## Prior probabilities of groups: ## 0 1 ## 0.494382 0.505618 ## ## Group means: ## x1 x3 ## 0 0.009738636 0.4628477 ## 1 0.341413333 2.6057978 ## ## Coefficients of linear discriminants: ## LD1 ## x1 4.61295546 ## x3 0.06590811 Task 2 Compute the predictions and explain what the results mean. result_lda &lt;- predict(fit_lda, test) The class component is a factor that contains the predictions for bankruptcy status (solvent/bankrupt). result_lda$class ## [1] 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 ## Levels: 0 1 The posterior component is matrix that contains the posterior probability that the corresponding observation belongs to a given class. result_lda$posterior ## 0 1 ## 1 0.17760365 8.223963e-01 ## 2 0.31142722 6.885728e-01 ## 10 0.07228194 9.277181e-01 ## 11 0.33538335 6.646166e-01 ## 19 0.04495309 9.550469e-01 ## 20 0.17728895 8.227110e-01 ## 24 0.40407718 5.959228e-01 ## 28 0.22738932 7.726107e-01 ## 29 0.14095907 8.590409e-01 ## 37 0.53834405 4.616560e-01 ## 40 0.70105814 2.989419e-01 ## 44 0.85424906 1.457509e-01 ## 45 0.73762416 2.623758e-01 ## 49 0.95002382 4.997618e-02 ## 56 0.99999946 5.413229e-07 ## 58 0.83468414 1.653159e-01 ## 62 0.87475890 1.252411e-01 ## 64 0.94351212 5.648788e-02 ## 66 0.25093559 7.490644e-01 ## 85 0.61062622 3.893738e-01 ## 105 0.99988023 1.197708e-04 ## 109 0.96722224 3.277776e-02 The x component contains the linear discriminants. result_lda$x ## LD1 ## 1 0.8942494 ## 2 0.4519351 ## 10 1.5042674 ## 11 0.3864033 ## 19 1.8058326 ## 20 0.8955396 ## 24 0.2096297 ## 28 0.7090237 ## 29 1.0586059 ## 37 -0.1147903 ## 40 -0.5328419 ## 44 -1.0809275 ## 45 -0.6413331 ## 49 -1.7849665 ## 56 -8.6567024 ## 58 -0.9916955 ## 62 -1.1858701 ## 64 -1.7075644 ## 66 0.6315464 ## 85 -0.2920645 ## 105 -5.4259016 ## 109 -2.0480872 To obtain our predictions, we can simply extract the class element. pred_lda &lt;- result_lda$class Task 3 Compute the confusion matrix for the LDA classifier and calculate the fraction of correct predictions. Explain the results. (t &lt;- table(pred_lda, test$y)) ## ## pred_lda 0 1 ## 0 11 1 ## 1 0 10 And now the fraction of correct predictions which, we can see is identical to that obtained for logistic regression. sum(diag(t)) / sum(t) ## [1] 0.9545455 LDA performs identically to logistic regression. Quadratic Discriminant Analysis Task 1 Build a QDA clasifier and describe the output. fit_qda &lt;- qda(y ~ x1 + x3, data = train) fit_qda ## Call: ## qda(y ~ x1 + x3, data = train) ## ## Prior probabilities of groups: ## 0 1 ## 0.494382 0.505618 ## ## Group means: ## x1 x3 ## 0 0.009738636 0.4628477 ## 1 0.341413333 2.6057978 In terms of prior probabilities and group means, the output is identical to that of linear discriminant analysis. However, the output does not include the coefficients of the linear discriminants for obvious reasons. Task 2 Compute the predictions, the confusion matrix, and the fraction of correct predictions and explain what the results mean. pred_qda &lt;- predict(fit_qda, test, type = &quot;response&quot;)$class (t &lt;- table(pred_qda, test$y)) ## ## pred_qda 0 1 ## 0 11 6 ## 1 0 5 sum(diag(t)) / sum(t) ## [1] 0.7272727 The fraction of correct predictions is lower (0.73) than that for logistic regression and for LDA (0.95). We therefore conclude that in this context, QDA does not perform well in comparison to the previous two approaches. \\(K\\)-nearest neighbours Task 1 Perform KNN regression on the same model as in the previous tasks. fit_knn &lt;- knn(train[, c(&quot;x1&quot;, &quot;x3&quot;), drop = FALSE], test[, c(&quot;x1&quot;, &quot;x3&quot;), drop = FALSE], train$y, k = 1 ) Task 2 Produce a confusion matrix and compute the fraction of correct predictions. Comment on the results. (t &lt;- table(fit_knn, test$y)) ## ## fit_knn 0 1 ## 0 10 1 ## 1 1 10 sum(diag(t)) / sum(t) ## [1] 0.9090909 Our overall fraction of correct predictions is 0.91. Therefore, the KNN classifier (\\(k = 1\\)) performs better than QDA (0.73), but worse than logistic regression and and LDA (0.95). Task 3 Fit KNN for up to \\(k = 30\\) and then calculate the overall fraction of correct prediction. set.seed(1) knn_k &lt;- sapply(1:30, function(k) { fit_knn &lt;- knn(train[, c(&quot;x1&quot;, &quot;x3&quot;), drop = FALSE], test[, c(&quot;x1&quot;, &quot;x3&quot;), drop = FALSE], train$y, k = k) mean(fit_knn == test$y) }) Task 4 Create a plot to observe at what value for k the overall fraction of correct predictions is highest. plot(1:30, knn_k, type = &quot;o&quot;, xlab = &quot;k&quot;, ylab = &quot;Fraction correct&quot;) As you can see, the highest fraction of correct predictions is when \\(k = 1\\) (also confirmed by using which.max). (k &lt;- which.max(knn_k)) ## [1] 1 The behaviour you observe results from several reasons, one of which is the size of the dataset. Naive Bayes Task 1 Build a Naive Bayes classifier for the model used previously and describe the output. fit_NBayes &lt;- naiveBayes(y~ x1 + x3, data = train) fit_NBayes ## ## Naive Bayes Classifier for Discrete Predictors ## ## Call: ## naiveBayes.default(x = X, y = Y, laplace = laplace) ## ## A-priori probabilities: ## Y ## 0 1 ## 0.494382 0.505618 ## ## Conditional probabilities: ## x1 ## Y [,1] [,2] ## 0 0.009738636 0.2458544 ## 1 0.341413333 0.1529747 ## ## x3 ## Y [,1] [,2] ## 0 0.4628477 0.5992219 ## 1 2.6057978 5.0402790 A-priori probabilities: i.e. prior probabilities (distribution of the classes for the response variable) Conditional probabilities: parameters of the model for each predictor by class. Since the predictors are numeric variables, the parameters shown are the mean [,1] and standard deviation [,2] for the predictor values in each class. Task 2 Compute the predictions, build a confusion matrix, calculate the fraction of correct predictions and interpret the results. pred_NBayes &lt;- predict(fit_NBayes, test, type = &quot;class&quot;) And finally, generate our confusion matrix. (t &lt;- table(pred_NBayes, test$y)) ## ## pred_NBayes 0 1 ## 0 11 8 ## 1 0 3 Our overall fraction of correct predictions is \\(0.64\\). sum(diag(t)) / sum(t) ## [1] 0.6363636 Naive Bayes performs the worst out of all classifiers we have explored. Based on the approaches we have implemented in this demonstration, logistic regression and LDA perform best. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
