---
editor_options:
  markdown:
    wrap: 72
---

# Answers {-}

Loading the necessary packages: 


```r
library(ISLR2)
library(boot)
```

## Part I: The Validation Set Approach {-}

::: file
For the tasks below, you will require the **Default** dataset. This dataset is part of the `ISRL2` package from the core textbook (James et. al 2021). By loading the package, the **Default** dataset will load automatically. 
:::

The **Default** dataset contains 10,000 observations on four variables:   

- default: A factor with levels No and Yes indicating whether the customer defaulted on their debt  

- student: A factor with levels No and Yes indicating whether the customer is a student  

- balance:The average balance that the customer has remaining on their credit card after making their monthly payment  

- income: Income of customer   

In this exercise, we will use the validation set approach to estimate the test error of a logistic regression model that predicts the probability of credit card **default** using **income** and **balance**. 

### Task 1 {-}

Using the validation set approach, estimate the test error of this model following the guidance below: 

1. Split the sample set into a training set and a validation set (80/20)  
2. Fit a multiple logistic regression model using only the training observations.   
3. Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.  
4. Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.

*Don't forget that it is important to set a random seed before splitting the data and running the analysis, to obtain consistent results.*

**1. Split the sample set into a training set and a validation set (80/20)** {-}


```r
set.seed(42)
split_idx <- sample(nrow(Default), 8000)
default_train <- Default[split_idx, ]
default_test <- Default[-split_idx, ]
```


**2. Fit a multiple logistic regression model using only the training observations.** {-}


```r
fit <- glm(default ~ income + balance, data = default_train, 
           family = "binomial")
```


**3. Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.** {-}


```r
pred <- ifelse(predict(fit, default_test, type = "response") > 0.5, "Yes", "No")

table(pred, default_test$default)
```

```
##      
## pred    No  Yes
##   No  1932   44
##   Yes    7   17
```


**4. Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.** {-}


```r
mean(pred != default_test$default)
```

```
## [1] 0.0255
```


### Task 2 {-}

Write a function that repeats the process above three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained. *Hint: use the base R function `replicate()`*.


```r
set.seed(42)
replicate(3, {
  split_idx <- sample(nrow(Default), 8000)
  default_train <- Default[split_idx, ]
  default_test <- Default[-split_idx, ]
  
  fit <- glm(default ~ income + balance, data = default_train, family = "binomial")
  
  pred <- ifelse(predict(fit, default_test, type = "response") > 0.5, "Yes", "No")
  
  mean(pred != default_test$default)
})
```

```
## [1] 0.0255 0.0310 0.0270
```

The results obtained vary (as expected) and depend on the samples allocated to training and the test dataset.


### Task 3 {-}

Write a similar function as in Task 2, but this time include the variable **student** in the model. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate.


```r
replicate(3, {
  split_idx <- sample(nrow(Default), 8000)
  default_train <- Default[split_idx, ]
  default_test <- Default[-split_idx, ]
  
  fit <- glm(default ~ income + balance + student, data = default_train, family = "binomial")
  
  pred <- ifelse(predict(fit, newdata = default_test, type = "response") > 0.5, "Yes", "No")
  
  mean(pred != default_test$default)
})
```

```
## [1] 0.0265 0.0305 0.0275
```

Including the **student** variable does not seem to make a substantial improvement to the test error.


## Part II: Leave-one-out Cross-validation {-}

::: file
For the tasks below, you will require the **Weekly** dataset. This dataset is part of the `ISRL2` package from the core textbook (James et. al 2021). By loading the package, the **Default** dataset will load automatically. 
:::

The **Weekly** dataset contains 1,089 observations on nine variables. For the tasks in this section, you will require the variables below. For further information on the **Weekly** dataset, type `?Weekly` in your console after you load the `ISLR2` package.   

- Direction: A factor with levels Down and Up indicating whether the market had a positive or negative return on a given week  

- Lag1: Percentage return for previous week  

- Lag2: Percentage return for 2 weeks previous   

We will use the validation set approach to estimate the test error of a logistic regression model that predicts market movement (**Direction**) using percentage returns (**Lag1** and **Lag2**)

### Task 1 {-}

Fit a logistic regression model that predicts market movement using **Lag1** and **Lag2**


```r
fit <- glm(Direction ~ Lag1 + Lag2, data = Weekly[-1, ], family = "binomial")
```

### Task 2 {-}

Use the model to predict the classification of the first observation. Was this observation correctly classified?
 

```r
predict(fit, newdata = Weekly[1, , drop = FALSE], type = "response") > 0.5
```

```
##    1 
## TRUE
```
 
No, the observation was incorrectly classified.

### Task 3 {-}

Write a for loop that performs each of the steps below: 

1. Fit a logistic regression model using all but the $i^{th}$ observation to predict **Direction** using **Lag1** and **Lag2**. 
2. Compute the posterior probability of upward market movement for the $i^{th}$ observation.
3. Use the posterior probability for the $i^{th}$ observation in order to predict whether or not there is upward market movement.
4. Determine whether or not an error was made in predicting the direction for the $i^{th}$ observation. If an error was made, then indicate this as a 1, and otherwise indicate it as a 0.



```r
error <- numeric(nrow(Weekly))
for (i in 1:nrow(Weekly)) {
  fit <- glm(Direction ~ Lag1 + Lag2, data = Weekly[-i, ], family = "binomial")
  p <- predict(fit, newdata = Weekly[i, , drop = FALSE], type = "response") > 0.5
  error[i] <- ifelse(p, "Down", "Up") == Weekly$Direction[i]
}
```

### Task 4 {-}

Obtain the LOOCV estimate for the test error. Comment on the results.
  

```r
mean(error)
```

```
## [1] 0.4499541
```
  
The LOOCV test error rate is 45% which implies that our predictions are marginally more often correct than not.

## Part III: The Bootstrap {-}

In this exercise, we return to our logistic regression model that predicts the probability of credit card **default** using **income** and **balance** and implement the bootstrap approach to compute estimates for the standard errors of the coefficients. 


```r
fit <- glm(default ~ income + balance, data = Default, family = "binomial")

summary(fit)
```

```
## 
## Call:
## glm(formula = default ~ income + balance, family = "binomial", 
##     data = Default)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4725  -0.1444  -0.0574  -0.0211   3.7245  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(>|z|)    
## (Intercept) -1.154e+01  4.348e-01 -26.545  < 2e-16 ***
## income       2.081e-05  4.985e-06   4.174 2.99e-05 ***
## balance      5.647e-03  2.274e-04  24.836  < 2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2920.6  on 9999  degrees of freedom
## Residual deviance: 1579.0  on 9997  degrees of freedom
## AIC: 1585
## 
## Number of Fisher Scoring iterations: 8
```

### Task 1 {-}

Write a function called `boot.fn()`, that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.


```r
boot.fn <- function(x, i) {
  fit <- glm(default ~ income + balance, data = x[i, ], family = "binomial")
  coef(fit)[-1]
}
```

### Task 2 {-}

Use the `boot()` function together with the new `boot.fn()` function to estimate the standard errors of the logistic regression coefficients for income and balance. Comment on the estimated standard errors obtained using the `glm()` function and using your own bootstrap function.


```r
set.seed(42)
boot(Default, boot.fn, R = 1000)
```

```
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Default, statistic = boot.fn, R = 1000)
## 
## 
## Bootstrap Statistics :
##         original       bias     std. error
## t1* 2.080898e-05 2.737444e-08 5.073444e-06
## t2* 5.647103e-03 1.176249e-05 2.299133e-04
```

The standard errors obtained by bootstrapping are similar to those estimated by `glm()`.


## Bonus {-}

::: file
For the tasks below, you will require the **Boston** dataset. This dataset is part of the `ISRL2` package from the core textbook (James et. al 2021). By loading the package, the **Boston** dataset will load automatically. 
:::

### Task 1 {-}

Provide an estimate for the population mean of the **medv** variable which is the median value of owner-occupied homes. 


```r
(mu <- mean(Boston$medv))
```

```
## [1] 22.53281
```

### Task 2 {-}

Provide an estimate of the standard error of  $\hat{\mu}$ and interpret the result. **Hint: You can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.**
 

```r
sd(Boston$medv) / sqrt(length(Boston$medv))
```

```
## [1] 0.4088611
```
 
### Task 3 {-}
Now estimate the standard error of  $\hat{\mu}$ using the bootstrap. How does this compare to your answer from Task 2?
  

```r
set.seed(42)
(bs <- boot(Boston$medv, function(v, i) mean(v[i]), 10000))
```

```
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Boston$medv, statistic = function(v, i) mean(v[i]), 
##     R = 10000)
## 
## 
## Bootstrap Statistics :
##     original      bias    std. error
## t1* 22.53281 0.002175751   0.4029139
```

The standard error using the bootstrap (0.403) is very close to that obtained from the formula above (0.409).

### Task 4 {-}

Based on your bootstrap estimate, provide a 95% confidence interval for the mean of medv. Compare it to the results obtained using `t.test(Boston$medv)`. Hint: You can approximate a 95% confidence interval using the formula  $[\hat{\mu}âˆ’2SE(\hat{\mu}),\hat{\mu} + 2SE(\hat{\mu})]$. 


```r
se <- sd(bs$t)
c(mu - 2 * se, mu + 2 * se)
```

```
## [1] 21.72698 23.33863
```

### Task 5 {-}

Based on this data set, provide an estimate,  $\hat{\mu}_{med}$ for the median value of **medv** in the population.
 

```r
median(Boston$medv)
```

```
## [1] 21.2
```
 
### Task 6 {-}

Estimate the standard error of $\hat{\mu}_{med}$ using the bootstrap. Comment on your findings.
 

```r
set.seed(42)
boot(Boston$medv, function(v, i) median(v[i]), 10000)
```

```
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Boston$medv, statistic = function(v, i) median(v[i]), 
##     R = 10000)
## 
## 
## Bootstrap Statistics :
##     original   bias    std. error
## t1*     21.2 -0.01331   0.3744634
```
 
The estimated standard error of the median is 0.374. This is lower than the standard error of the mean.

### Task 7 {-}

Based on this data set, provide an estimate for the tenth percentile of medv in Boston census tracts. Call this quantity  $\hat{\mu}_{0.1}$. *Hint: you can use the `quantile()` function.*
 

```r
quantile(Boston$medv, 0.1)
```

```
##   10% 
## 12.75
```

### Task 8 {-}
Use the bootstrap to estimate the standard error of  $\hat{\mu}_{0.1}$ and comment on your findings.
 

```r
set.seed(42)
boot(Boston$medv, function(v, i) quantile(v[i], 0.1), 10000)
```

```
## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Boston$medv, statistic = function(v, i) quantile(v[i], 
##     0.1), R = 10000)
## 
## 
## Bootstrap Statistics :
##     original   bias    std. error
## t1*    12.75 0.013405    0.497298
```

We get a standard error of about 0.5. This is higher than the standard error of the median. Nevertheless the standard error is still quite small, thus we can be fairly confidence about the value of the 10th percentile.

