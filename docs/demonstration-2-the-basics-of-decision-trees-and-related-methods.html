<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Demonstration 2: The Basics of Decision Trees and Related Methods | SOST70033 Data Science Modelling</title>
  <meta name="description" content="Notebook hosting practical materials for SOST70033." />
  <meta name="generator" content="bookdown 0.38 and GitBook 2.6.7" />

  <meta property="og:title" content="Demonstration 2: The Basics of Decision Trees and Related Methods | SOST70033 Data Science Modelling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Notebook hosting practical materials for SOST70033." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Demonstration 2: The Basics of Decision Trees and Related Methods | SOST70033 Data Science Modelling" />
  
  <meta name="twitter:description" content="Notebook hosting practical materials for SOST70033." />
  

<meta name="author" content="Dr.Â Ioana Macoveciuc" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="demonstration-1-propublicas-analysis-of-the-compas-tool.html"/>
<link rel="next" href="answers-4.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="toc-logo"><a href="./"><img src="images/logos/uom_logo.png"></a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a></li>
<li class="part"><span><b>Section 1</b></span></li>
<li class="chapter" data-level="" data-path="overview.html"><a href="overview.html"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="" data-path="demonstration-a-more-in-depth-consideration-of-model-accuracy.html"><a href="demonstration-a-more-in-depth-consideration-of-model-accuracy.html"><i class="fa fa-check"></i>Demonstration: A more in-depth consideration of model accuracy</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-a-more-in-depth-consideration-of-model-accuracy.html"><a href="demonstration-a-more-in-depth-consideration-of-model-accuracy.html#the-simulation"><i class="fa fa-check"></i>The Simulation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html"><i class="fa fa-check"></i>Practical 1</a>
<ul>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-1"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-2"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-3"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-4"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-5"><i class="fa fa-check"></i>Task 5</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-6"><i class="fa fa-check"></i>Task 6</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-7"><i class="fa fa-check"></i>Task 7</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-8"><i class="fa fa-check"></i>Task 8</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-9"><i class="fa fa-check"></i>Task 9</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-10"><i class="fa fa-check"></i>Task 10</a></li>
<li class="chapter" data-level="" data-path="practical-1.html"><a href="practical-1.html#task-11"><i class="fa fa-check"></i>Task 11</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-2.html"><a href="practical-2.html"><i class="fa fa-check"></i>Practical 2</a>
<ul>
<li class="chapter" data-level="" data-path="practical-2.html"><a href="practical-2.html#task-1-1"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical-2.html"><a href="practical-2.html#task-2-1"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="practical-2.html"><a href="practical-2.html#task-3-1"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="practical-2.html"><a href="practical-2.html#task-4-1"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="practical-2.html"><a href="practical-2.html#task-5-1"><i class="fa fa-check"></i>Task 5</a></li>
<li class="chapter" data-level="" data-path="practical-2.html"><a href="practical-2.html#task-6-1"><i class="fa fa-check"></i>Task 6</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html"><i class="fa fa-check"></i>Answers</a>
<ul>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#practical-1-1"><i class="fa fa-check"></i>Practical 1</a>
<ul>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-1-2"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-2-2"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-3-2"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-4-2"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-5-2"><i class="fa fa-check"></i>Task 5</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-6-2"><i class="fa fa-check"></i>Task 6</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-7-1"><i class="fa fa-check"></i>Task 7</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-8-1"><i class="fa fa-check"></i>Task 8</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-9-1"><i class="fa fa-check"></i>Task 9</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-10-1"><i class="fa fa-check"></i>Task 10</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-11-1"><i class="fa fa-check"></i>Task 11</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#practical-2-1"><i class="fa fa-check"></i>Practical 2</a>
<ul>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-1-3"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-2-3"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-3-3"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-4-3"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-5-3"><i class="fa fa-check"></i>Task 5</a></li>
<li class="chapter" data-level="" data-path="answers.html"><a href="answers.html#task-6-3"><i class="fa fa-check"></i>Task 6</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Section 2</b></span></li>
<li class="chapter" data-level="" data-path="overview-1.html"><a href="overview-1.html"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="" data-path="demonstration-1.html"><a href="demonstration-1.html"><i class="fa fa-check"></i>Demonstration 1</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-1.html"><a href="demonstration-1.html#simple-linear-models-without-intercept"><i class="fa fa-check"></i>Simple Linear Models Without Intercept</a></li>
<li class="chapter" data-level="" data-path="demonstration-1.html"><a href="demonstration-1.html#simple-linear-models-with-intercept"><i class="fa fa-check"></i>Simple Linear Models with Intercept</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="demonstration-2.html"><a href="demonstration-2.html"><i class="fa fa-check"></i>Demonstration 2</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-2.html"><a href="demonstration-2.html#population-parameters-and-estimated-coefficients"><i class="fa fa-check"></i>Population Parameters and Estimated Coefficients</a></li>
<li class="chapter" data-level="" data-path="demonstration-2.html"><a href="demonstration-2.html#what-happens-if-we-reduce-noise"><i class="fa fa-check"></i>What happens if we <em>reduce</em> noise?</a></li>
<li class="chapter" data-level="" data-path="demonstration-2.html"><a href="demonstration-2.html#what-happens-if-we-increase-noise"><i class="fa fa-check"></i>What happens if we <em>increase</em> noise?</a></li>
<li class="chapter" data-level="" data-path="demonstration-2.html"><a href="demonstration-2.html#how-does-noise-affect-confidence-intervals-for-the-coefficients"><i class="fa fa-check"></i>How does noise affect confidence intervals for the coefficients?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html"><i class="fa fa-check"></i>Practical 1</a>
<ul>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-1-4"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-2-4"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-3-4"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-4-4"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-5-4"><i class="fa fa-check"></i>Task 5</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-6-4"><i class="fa fa-check"></i>Task 6</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-7-2"><i class="fa fa-check"></i>Task 7</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-8-2"><i class="fa fa-check"></i>Task 8</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-9-2"><i class="fa fa-check"></i>Task 9</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-10-2"><i class="fa fa-check"></i>Task 10</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-11-2"><i class="fa fa-check"></i>Task 11</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-12"><i class="fa fa-check"></i>Task 12</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-13"><i class="fa fa-check"></i>Task 13</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-14"><i class="fa fa-check"></i>Task 14</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-15"><i class="fa fa-check"></i>Task 15</a></li>
<li class="chapter" data-level="" data-path="practical-1-2.html"><a href="practical-1-2.html#task-16"><i class="fa fa-check"></i>Task 16</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-2-2.html"><a href="practical-2-2.html"><i class="fa fa-check"></i>Practical 2</a>
<ul>
<li class="chapter" data-level="" data-path="practical-2-2.html"><a href="practical-2-2.html#task-1-5"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical-2-2.html"><a href="practical-2-2.html#task-2-5"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="practical-2-2.html"><a href="practical-2-2.html#task-3-5"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="practical-2-2.html"><a href="practical-2-2.html#task-4-5"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="practical-2-2.html"><a href="practical-2-2.html#task-5-5"><i class="fa fa-check"></i>Task 5</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-3-the-quality-of-red-bordeaux-vintages.html"><a href="practical-3-the-quality-of-red-bordeaux-vintages.html"><i class="fa fa-check"></i>Practical 3: The Quality of Red Bordeaux Vintages</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html"><i class="fa fa-check"></i>Answers</a>
<ul>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#practical-1-3"><i class="fa fa-check"></i>Practical 1</a>
<ul>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-1-6"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-2-6"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-3-6"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-4-6"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-5-6"><i class="fa fa-check"></i>Task 5</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-6-5"><i class="fa fa-check"></i>Task 6</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-7-3"><i class="fa fa-check"></i>Task 7</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-8-3"><i class="fa fa-check"></i>Task 8</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-9-3"><i class="fa fa-check"></i>Task 9</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-10-3"><i class="fa fa-check"></i>Task 10</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-11-3"><i class="fa fa-check"></i>Task 11</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-12-1"><i class="fa fa-check"></i>Task 12</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-13-1"><i class="fa fa-check"></i>Task 13</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-14-1"><i class="fa fa-check"></i>Task 14</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-15-1"><i class="fa fa-check"></i>Task 15</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-16-1"><i class="fa fa-check"></i>Task 16</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#practical-2-3"><i class="fa fa-check"></i>Practical 2</a>
<ul>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-1-7"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-2-7"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-3-7"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-4-7"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#task-5-7"><i class="fa fa-check"></i>Task 5</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="answers-1.html"><a href="answers-1.html#practical-3-the-quality-of-red-bordeaux-vintages-1"><i class="fa fa-check"></i>Practical 3: The Quality of Red Bordeaux Vintages</a></li>
</ul></li>
<li class="part"><span><b>Section 3</b></span></li>
<li class="chapter" data-level="" data-path="overview-2.html"><a href="overview-2.html"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html"><i class="fa fa-check"></i>Demonstration 1: Classification Problems</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#dataset-and-variables"><i class="fa fa-check"></i>Dataset and Variables</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#correlation-matrix-and-plot"><i class="fa fa-check"></i>Correlation Matrix and Plot</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#classic-logistic-regression"><i class="fa fa-check"></i>âClassicâ Logistic Regression</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#confusion-matrix"><i class="fa fa-check"></i>Confusion Matrix</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#logistic-regression-in-statistical-learning"><i class="fa fa-check"></i>Logistic Regression in Statistical Learning</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#linear-discriminant-analysis"><i class="fa fa-check"></i>Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#quadratic-discriminant-analysis"><i class="fa fa-check"></i>Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#k-nearest-neighbours"><i class="fa fa-check"></i><span class="math inline">\(K\)</span>-nearest neighbours</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-classification-problems.html"><a href="demonstration-1-classification-problems.html#naive-bayes"><i class="fa fa-check"></i>Naive Bayes</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="demonstration-2-poisson-versus-linear-regression.html"><a href="demonstration-2-poisson-versus-linear-regression.html"><i class="fa fa-check"></i>Demonstration 2: Poisson versus Linear Regression</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-2-poisson-versus-linear-regression.html"><a href="demonstration-2-poisson-versus-linear-regression.html#the-linear-model"><i class="fa fa-check"></i>The Linear Model</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-2-poisson-versus-linear-regression.html"><a href="demonstration-2-poisson-versus-linear-regression.html#approaches-to-variable-coding"><i class="fa fa-check"></i>Approaches to variable coding</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-poisson-versus-linear-regression.html"><a href="demonstration-2-poisson-versus-linear-regression.html#plotting-coefficient-estimates"><i class="fa fa-check"></i>Plotting Coefficient Estimates</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="demonstration-2-poisson-versus-linear-regression.html"><a href="demonstration-2-poisson-versus-linear-regression.html#the-poisson-model"><i class="fa fa-check"></i>The Poisson Model</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-poisson-versus-linear-regression.html"><a href="demonstration-2-poisson-versus-linear-regression.html#linear-versus-poisson-regression"><i class="fa fa-check"></i>Linear versus Poisson Regression</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html"><i class="fa fa-check"></i>Practical: Predicting a companyâs bankruptcy</a>
<ul>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#data-and-variables"><i class="fa fa-check"></i>Data and Variables</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#importing-the-data"><i class="fa fa-check"></i>Importing the data</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#loading-required-packages"><i class="fa fa-check"></i>Loading required packages</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#correlation-matrix-and-plot-1"><i class="fa fa-check"></i>Correlation Matrix and Plot</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#logistic-regression"><i class="fa fa-check"></i>Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#explaining-the-logit"><i class="fa fa-check"></i>Explaining the Logit</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#tasks-linear-discriminant-analysis"><i class="fa fa-check"></i>Tasks: Linear Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-1-8"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-2-8"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-3-8"><i class="fa fa-check"></i>Task 3</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#tasks-quadratic-discriminant-analysis"><i class="fa fa-check"></i>Tasks: Quadratic Discriminant Analysis</a>
<ul>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-1-9"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-2-9"><i class="fa fa-check"></i>Task 2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#tasks-k-nearest-neighbours"><i class="fa fa-check"></i>Tasks: <span class="math inline">\(K\)</span>-nearest neighbours</a>
<ul>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-1-10"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-2-10"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-3-9"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-4-8"><i class="fa fa-check"></i>Task 4</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#tasks-naive-bayes"><i class="fa fa-check"></i>Tasks: Naive Bayes</a>
<ul>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-1-11"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical-predicting-a-companys-bankruptcy.html"><a href="practical-predicting-a-companys-bankruptcy.html#task-2-11"><i class="fa fa-check"></i>Task 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html"><i class="fa fa-check"></i>Answers</a>
<ul>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#practical-predicting-a-companys-bankruptcy-1"><i class="fa fa-check"></i>Practical: Predicting a companyâs bankruptcy</a>
<ul>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#data-and-variables-1"><i class="fa fa-check"></i>Data and Variables</a></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#importing-the-data-1"><i class="fa fa-check"></i>Importing the data</a></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#loading-required-packages-1"><i class="fa fa-check"></i>Loading required packages</a></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#correlation-matrix-and-plot-2"><i class="fa fa-check"></i>Correlation Matrix and Plot</a></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#logistic-regression-1"><i class="fa fa-check"></i>Logistic Regression</a></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#linear-discriminant-analysis-1"><i class="fa fa-check"></i>Linear Discriminant Analysis</a></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#quadratic-discriminant-analysis-1"><i class="fa fa-check"></i>Quadratic Discriminant Analysis</a></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#k-nearest-neighbours-1"><i class="fa fa-check"></i><span class="math inline">\(K\)</span>-nearest neighbours</a></li>
<li class="chapter" data-level="" data-path="answers-2.html"><a href="answers-2.html#naive-bayes-1"><i class="fa fa-check"></i>Naive Bayes</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Section 4</b></span></li>
<li class="chapter" data-level="" data-path="overview-3.html"><a href="overview-3.html"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-cross-validation.html"><a href="demonstration-1-cross-validation.html"><i class="fa fa-check"></i>Demonstration 1: Cross-validation</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-1-cross-validation.html"><a href="demonstration-1-cross-validation.html#data-and-variables-2"><i class="fa fa-check"></i>Data and Variables</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-cross-validation.html"><a href="demonstration-1-cross-validation.html#the-validation-set-approach"><i class="fa fa-check"></i>The Validation Set Approach</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-cross-validation.html"><a href="demonstration-1-cross-validation.html#leave-one-out-cross-validation"><i class="fa fa-check"></i>Leave-One-Out Cross-Validation</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-cross-validation.html"><a href="demonstration-1-cross-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><span class="math inline">\(k\)</span>-Fold Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="demonstration-2-bootstrapping.html"><a href="demonstration-2-bootstrapping.html"><i class="fa fa-check"></i>Demonstration 2: Bootstrapping</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-2-bootstrapping.html"><a href="demonstration-2-bootstrapping.html#data-and-variables-3"><i class="fa fa-check"></i>Data and Variables</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-bootstrapping.html"><a href="demonstration-2-bootstrapping.html#estimating-the-accuracy-of-a-statistic-of-interest"><i class="fa fa-check"></i>Estimating the Accuracy of a Statistic of Interest</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-bootstrapping.html"><a href="demonstration-2-bootstrapping.html#estimating-the-accuracy-of-a-linear-regression-model"><i class="fa fa-check"></i>Estimating the Accuracy of a Linear Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html"><i class="fa fa-check"></i>Practical</a>
<ul>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#part-i-the-validation-set-approach"><i class="fa fa-check"></i>Part I: The Validation Set Approach</a>
<ul>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-1-16"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-2-16"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-3-12"><i class="fa fa-check"></i>Task 3</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#part-ii-leave-one-out-cross-validation"><i class="fa fa-check"></i>Part II: Leave-one-out Cross-validation</a>
<ul>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-1-17"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-2-17"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-3-13"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-4-10"><i class="fa fa-check"></i>Task 4</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#part-iii-the-bootstrap"><i class="fa fa-check"></i>Part III: The Bootstrap</a>
<ul>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-1-18"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-2-18"><i class="fa fa-check"></i>Task 2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#bonus"><i class="fa fa-check"></i>Bonus</a>
<ul>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-1-19"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-2-19"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-3-14"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-4-11"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-5-8"><i class="fa fa-check"></i>Task 5</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-6-6"><i class="fa fa-check"></i>Task 6</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-7-4"><i class="fa fa-check"></i>Task 7</a></li>
<li class="chapter" data-level="" data-path="practical.html"><a href="practical.html#task-8-4"><i class="fa fa-check"></i>Task 8</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html"><i class="fa fa-check"></i>Answers</a>
<ul>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#part-i-the-validation-set-approach-1"><i class="fa fa-check"></i>Part I: The Validation Set Approach</a>
<ul>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-1-20"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-2-20"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-3-15"><i class="fa fa-check"></i>Task 3</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#part-ii-leave-one-out-cross-validation-1"><i class="fa fa-check"></i>Part II: Leave-one-out Cross-validation</a>
<ul>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-1-21"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-2-21"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-3-16"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-4-12"><i class="fa fa-check"></i>Task 4</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#part-iii-the-bootstrap-1"><i class="fa fa-check"></i>Part III: The Bootstrap</a>
<ul>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-1-22"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-2-22"><i class="fa fa-check"></i>Task 2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#bonus-1"><i class="fa fa-check"></i>Bonus</a>
<ul>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-1-23"><i class="fa fa-check"></i>Task 1</a></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-2-23"><i class="fa fa-check"></i>Task 2</a></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-3-17"><i class="fa fa-check"></i>Task 3</a></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-4-13"><i class="fa fa-check"></i>Task 4</a></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-5-9"><i class="fa fa-check"></i>Task 5</a></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-6-7"><i class="fa fa-check"></i>Task 6</a></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-7-5"><i class="fa fa-check"></i>Task 7</a></li>
<li class="chapter" data-level="" data-path="answers-3.html"><a href="answers-3.html#task-8-5"><i class="fa fa-check"></i>Task 8</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Section 5</b></span></li>
<li class="chapter" data-level="" data-path="overview-4.html"><a href="overview-4.html"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="" data-path="practical-1-academic-salary.html"><a href="practical-1-academic-salary.html"><i class="fa fa-check"></i>Practical 1: Academic Salary</a>
<ul>
<li class="chapter" data-level="" data-path="practical-1-academic-salary.html"><a href="practical-1-academic-salary.html#part-i"><i class="fa fa-check"></i>Part I</a>
<ul>
<li class="chapter" data-level="" data-path="practical-1-academic-salary.html"><a href="practical-1-academic-salary.html#exploring-the-data"><i class="fa fa-check"></i>Exploring the data</a></li>
<li class="chapter" data-level="" data-path="practical-1-academic-salary.html"><a href="practical-1-academic-salary.html#salary-and-years-since-phd"><i class="fa fa-check"></i>Salary and Years since PhD</a></li>
<li class="chapter" data-level="" data-path="practical-1-academic-salary.html"><a href="practical-1-academic-salary.html#salary-and-years-of-service"><i class="fa fa-check"></i>Salary and Years of Service</a></li>
<li class="chapter" data-level="" data-path="practical-1-academic-salary.html"><a href="practical-1-academic-salary.html#the-model"><i class="fa fa-check"></i>The Model</a></li>
<li class="chapter" data-level="" data-path="practical-1-academic-salary.html"><a href="practical-1-academic-salary.html#test-a-does-the-fitted-model-make-sense"><i class="fa fa-check"></i>Test a): Does the fitted model make sense?</a></li>
<li class="chapter" data-level="" data-path="practical-1-academic-salary.html"><a href="practical-1-academic-salary.html#test-b-overall-is-the-model-a-good-fit"><i class="fa fa-check"></i>Test b): Overall, is the model a good fit?</a></li>
<li class="chapter" data-level="" data-path="practical-1-academic-salary.html"><a href="practical-1-academic-salary.html#test-c-individually-are-the-explanatory-variables-important"><i class="fa fa-check"></i>Test c): Individually, are the explanatory variables important?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-1-academic-salary.html"><a href="practical-1-academic-salary.html#part-ii"><i class="fa fa-check"></i>Part II</a>
<ul>
<li class="chapter" data-level="" data-path="practical-1-academic-salary.html"><a href="practical-1-academic-salary.html#interpreting-coefficients-of-attribute-variables"><i class="fa fa-check"></i>Interpreting coefficients of attribute variables</a></li>
<li class="chapter" data-level="" data-path="practical-1-academic-salary.html"><a href="practical-1-academic-salary.html#fitting-a-multivariate-regression-model"><i class="fa fa-check"></i>Fitting a Multivariate Regression Model</a></li>
<li class="chapter" data-level="" data-path="practical-1-academic-salary.html"><a href="practical-1-academic-salary.html#fitting-the-model"><i class="fa fa-check"></i>Fitting the Model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="practical-2-foreign-direct-investment-fdi-study.html"><a href="practical-2-foreign-direct-investment-fdi-study.html"><i class="fa fa-check"></i>Practical 2: Foreign Direct Investment (FDI) Study</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><i class="fa fa-check"></i>Demonstration 1: ProPublicaâs Analysis of the COMPAS Tool</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#notes-on-the-data"><i class="fa fa-check"></i>Notes on the data</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#setup"><i class="fa fa-check"></i>Setup</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#load-packages"><i class="fa fa-check"></i>Load packages</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#load-data"><i class="fa fa-check"></i>Load data</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#inspect-data"><i class="fa fa-check"></i>Inspect data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#preprocess-data"><i class="fa fa-check"></i>Preprocess data</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#inspect-data-again"><i class="fa fa-check"></i>Inspect data again</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#exploratory-analysis"><i class="fa fa-check"></i>Exploratory analysis</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#exercise"><i class="fa fa-check"></i>ð Exercise</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#risk-labels"><i class="fa fa-check"></i>Risk labels</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#bias-in-compas"><i class="fa fa-check"></i>Bias in COMPAS</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#preprocess-data-for-logistic-regression"><i class="fa fa-check"></i>Preprocess data for logistic regression</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#estimate-the-logistic-regression-model"><i class="fa fa-check"></i>Estimate the logistic regression model</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#interpret-estimates"><i class="fa fa-check"></i>Interpret estimates</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#exercise-1"><i class="fa fa-check"></i>ð Exercise</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#predictive-accuracy"><i class="fa fa-check"></i>Predictive Accuracy</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#exercise-2"><i class="fa fa-check"></i>ð Exercise</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-propublicas-analysis-of-the-compas-tool.html"><a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html#exercise-3"><i class="fa fa-check"></i>ð Exercise</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="demonstration-2-the-basics-of-decision-trees-and-related-methods.html"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html"><i class="fa fa-check"></i>Demonstration 2: The Basics of Decision Trees and Related Methods</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-2-the-basics-of-decision-trees-and-related-methods.html"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#decision-trees"><i class="fa fa-check"></i>Decision Trees</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-2-the-basics-of-decision-trees-and-related-methods.html"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#classification-trees"><i class="fa fa-check"></i>Classification Trees</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-the-basics-of-decision-trees-and-related-methods.html"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#regression-trees"><i class="fa fa-check"></i>Regression Trees</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="demonstration-2-the-basics-of-decision-trees-and-related-methods.html"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#ensemble-methods"><i class="fa fa-check"></i>Ensemble Methods</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-2-the-basics-of-decision-trees-and-related-methods.html"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#bagging"><i class="fa fa-check"></i>Bagging</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-the-basics-of-decision-trees-and-related-methods.html"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#random-forests"><i class="fa fa-check"></i>Random Forests</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-the-basics-of-decision-trees-and-related-methods.html"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#boosting"><i class="fa fa-check"></i>Boosting</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="answers-4.html"><a href="answers-4.html"><i class="fa fa-check"></i>Answers</a>
<ul>
<li class="chapter" data-level="" data-path="answers-4.html"><a href="answers-4.html#practical-1-4"><i class="fa fa-check"></i>Practical 1</a>
<ul>
<li class="chapter" data-level="" data-path="answers-4.html"><a href="answers-4.html#part-i-1"><i class="fa fa-check"></i>Part I</a></li>
<li class="chapter" data-level="" data-path="answers-4.html"><a href="answers-4.html#part-ii-1"><i class="fa fa-check"></i>Part II</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="answers-4.html"><a href="answers-4.html#practical-2-4"><i class="fa fa-check"></i>Practical 2</a></li>
</ul></li>
<li class="part"><span><b>Section 7</b></span></li>
<li class="chapter" data-level="" data-path="overview-5.html"><a href="overview-5.html"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-k-means-clustering-in-r.html"><a href="demonstration-1-k-means-clustering-in-r.html"><i class="fa fa-check"></i>Demonstration 1: K-means Clustering in R</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-1-k-means-clustering-in-r.html"><a href="demonstration-1-k-means-clustering-in-r.html#loading-the-necessary-packages"><i class="fa fa-check"></i>Loading the necessary packages</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-k-means-clustering-in-r.html"><a href="demonstration-1-k-means-clustering-in-r.html#profile-the-palmer-penguins-dataset"><i class="fa fa-check"></i>Profile the Palmer Penguins dataset</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-k-means-clustering-in-r.html"><a href="demonstration-1-k-means-clustering-in-r.html#plot"><i class="fa fa-check"></i>Plot</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-k-means-clustering-in-r.html"><a href="demonstration-1-k-means-clustering-in-r.html#preprocess-the-data-for-clustering"><i class="fa fa-check"></i>Preprocess the data for clustering</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-k-means-clustering-in-r.html"><a href="demonstration-1-k-means-clustering-in-r.html#perform-k-means-clustering"><i class="fa fa-check"></i>Perform k-means clustering</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-k-means-clustering-in-r.html"><a href="demonstration-1-k-means-clustering-in-r.html#visualise-the-clusters"><i class="fa fa-check"></i>Visualise the clusters</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-k-means-clustering-in-r.html"><a href="demonstration-1-k-means-clustering-in-r.html#tasks"><i class="fa fa-check"></i>ð TASKS</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-1-k-means-clustering-in-r.html"><a href="demonstration-1-k-means-clustering-in-r.html#task-1-how-well-did-k-means-clustering-perform"><i class="fa fa-check"></i>TASK 1: How well did k-means clustering perform?</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-k-means-clustering-in-r.html"><a href="demonstration-1-k-means-clustering-in-r.html#task-2-how-many-clusters-should-we-use"><i class="fa fa-check"></i>TASK 2: How many clusters should we use?</a></li>
<li class="chapter" data-level="" data-path="demonstration-1-k-means-clustering-in-r.html"><a href="demonstration-1-k-means-clustering-in-r.html#task-3-clustering-cars"><i class="fa fa-check"></i>TASK 3: Clustering cars</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="demonstration-2-principal-component-analysis-in-r.html"><a href="demonstration-2-principal-component-analysis-in-r.html"><i class="fa fa-check"></i>Demonstration 2: Principal Component Analysis in R</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration-2-principal-component-analysis-in-r.html"><a href="demonstration-2-principal-component-analysis-in-r.html#load-packages-1"><i class="fa fa-check"></i>Load packages</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-principal-component-analysis-in-r.html"><a href="demonstration-2-principal-component-analysis-in-r.html#preprocess-the-world-happiness-report-data"><i class="fa fa-check"></i>Preprocess the world happiness report data</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-principal-component-analysis-in-r.html"><a href="demonstration-2-principal-component-analysis-in-r.html#profile-the-happiness-data"><i class="fa fa-check"></i>Profile the happiness data</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-principal-component-analysis-in-r.html"><a href="demonstration-2-principal-component-analysis-in-r.html#plot-happiness-score-against-feature"><i class="fa fa-check"></i>Plot happiness score against feature</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-principal-component-analysis-in-r.html"><a href="demonstration-2-principal-component-analysis-in-r.html#run-pca-using-five-features"><i class="fa fa-check"></i>Run PCA using five features</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-principal-component-analysis-in-r.html"><a href="demonstration-2-principal-component-analysis-in-r.html#correlations-of-happiness-score-with-pc1-and-pc2"><i class="fa fa-check"></i>Correlations of happiness score with PC1 and PC2</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-principal-component-analysis-in-r.html"><a href="demonstration-2-principal-component-analysis-in-r.html#plot-first-two-principal-components"><i class="fa fa-check"></i>Plot first two principal components</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-principal-component-analysis-in-r.html"><a href="demonstration-2-principal-component-analysis-in-r.html#regress-happiness-on-the-five-indicators"><i class="fa fa-check"></i>Regress happiness on the five indicators</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-principal-component-analysis-in-r.html"><a href="demonstration-2-principal-component-analysis-in-r.html#visualise-happiness-against-principal-components"><i class="fa fa-check"></i>Visualise happiness against principal components</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-principal-component-analysis-in-r.html"><a href="demonstration-2-principal-component-analysis-in-r.html#regress-happiness-on-first-principal-component"><i class="fa fa-check"></i>Regress happiness on first principal component</a></li>
<li class="chapter" data-level="" data-path="demonstration-2-principal-component-analysis-in-r.html"><a href="demonstration-2-principal-component-analysis-in-r.html#task"><i class="fa fa-check"></i>ð TASK</a></li>
</ul></li>
<li class="part"><span><b>Section 8</b></span></li>
<li class="chapter" data-level="" data-path="overview-6.html"><a href="overview-6.html"><i class="fa fa-check"></i>Overview</a></li>
<li class="chapter" data-level="" data-path="demonstration.html"><a href="demonstration.html"><i class="fa fa-check"></i>Demonstration</a>
<ul>
<li class="chapter" data-level="" data-path="demonstration.html"><a href="demonstration.html#data-set"><i class="fa fa-check"></i>Data Set</a></li>
<li class="chapter" data-level="" data-path="demonstration.html"><a href="demonstration.html#project-planning-setting-the-stage-for-success"><i class="fa fa-check"></i>Project Planning: Setting the Stage for Success</a></li>
<li class="chapter" data-level="" data-path="demonstration.html"><a href="demonstration.html#data-preprocessing"><i class="fa fa-check"></i>Data Preprocessing</a></li>
<li class="chapter" data-level="" data-path="demonstration.html"><a href="demonstration.html#exploratory-data-analysis-eda"><i class="fa fa-check"></i>Exploratory Data Analysis (EDA)</a></li>
<li class="chapter" data-level="" data-path="demonstration.html"><a href="demonstration.html#model-selection-and-model-training"><i class="fa fa-check"></i>Model Selection and Model Training</a></li>
<li class="chapter" data-level="" data-path="demonstration.html"><a href="demonstration.html#model-evaluation"><i class="fa fa-check"></i>Model Evaluation</a></li>
<li class="chapter" data-level="" data-path="demonstration.html"><a href="demonstration.html#model-interpretation"><i class="fa fa-check"></i>Model Interpretation</a></li>
<li class="chapter" data-level="" data-path="demonstration.html"><a href="demonstration.html#report-conclusion"><i class="fa fa-check"></i>Report Conclusion</a></li>
<li class="chapter" data-level="" data-path="demonstration.html"><a href="demonstration.html#key-insights"><i class="fa fa-check"></i>Key Insights</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="your-turn.html"><a href="your-turn.html"><i class="fa fa-check"></i>ð Your Turn!</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">SOST70033 Data Science Modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<hr>
<center> 
  <div class="header">
   <img src="images/banners/DSM_banner.png" alt="Trulli">
  </div>
</center>
<div id="demonstration-2-the-basics-of-decision-trees-and-related-methods" class="section level1 unnumbered hasAnchor">
<h1>Demonstration 2: The Basics of Decision Trees and Related Methods<a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#demonstration-2-the-basics-of-decision-trees-and-related-methods" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this demonstration, you will learn about <em>decision trees</em>: <em>regression</em> trees are used when the outcome is quantitative and <em>classification</em> trees used when the outcome is categorical.</p>
<p>The basics are quite simple (even simpler than linear regression!): we split the predictor space to a number of regions and the prediction for every outcome in a region is the <em>mean</em> (for regression) or <em>mode</em> (for classification) of the observations in that region. Given that the structure of decision trees resemble human decision-making (to a certain extent) and that the output can be easily illustrated, this makes them quite easy to interpret, even by non-experts. Also, there is no need for dummy variables since trees easy handle categorical predictors.</p>
<p>We will explore each of these in greater detail in the walkthrough below. Before beginning, you will need to install and load the <code>gbm</code>, <code>tree</code>, and <code>randomForest</code> packages. The <code>tree</code> library is used to construct classification and regression trees.</p>
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb517-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb517-1" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb517-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb517-2" tabindex="-1"></a><span class="fu">library</span>(tree)</span>
<span id="cb517-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb517-3" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb517-4"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb517-4" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb517-5"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb517-5" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
<div id="decision-trees" class="section level2 unnumbered hasAnchor">
<h2>Decision Trees<a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="classification-trees" class="section level3 unnumbered hasAnchor">
<h3>Classification Trees<a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#classification-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="file">
<p>For the tasks below, you will require the <strong>Carseats</strong> dataset. This dataset is part of the <code>ISRL2</code> package from the core textbook (James et. al 2021). By loading the package, the <strong>Carseats</strong> dataset will load automatically.</p>
</div>
<p>The <strong>Carseats</strong> is a simulated dataset on sales of child car seats at different stores. There are 400 observations on 11 variables.</p>
<ul>
<li><p>Sales: Unit sales (in thousands) at each location</p></li>
<li><p>CompPrice: Price charged by competitor at each location</p></li>
<li><p>Income: Community income level (in thousands of dollars)</p></li>
<li><p>Advertising: Local advertising budget for company at each location (in thousands of dollars)<br />
</p></li>
<li><p>Population: Population size in region (in thousands)</p></li>
<li><p>Price: Price company charges for car seats at each site</p></li>
<li><p>ShelveLoc: A factor with levels Bad, Good and Medium indicating the quality of the shelving location for the car seats at each site</p></li>
<li><p>Age: Average age of the local population</p></li>
<li><p>Education: Education level at each location</p></li>
<li><p>Urban: A factor with levels No and Yes to indicate whether the store is in an urban or rural location</p></li>
<li><p>US: A factor with levels No and Yes to indicate whether the store is in the US or not</p></li>
</ul>
<p>We will now explore how classification trees can be used to predict whether sales of car seats are high or low. Here <strong>Sales</strong> is continuous and so we create a new binary variable such that all unit sales over 8,000 dollars are classed as âYesâ (i.e.Â so high sales) and everything else and âNoâ (i.e.Â so low sales). We store this as a vector with our response values for the test set for evaluating our model later and we also add this as a variable in our dataset.</p>
<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb518-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb518-1" tabindex="-1"></a><span class="fu">attach</span>(Carseats)</span>
<span id="cb518-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb518-2" tabindex="-1"></a>High <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">ifelse</span>(Sales <span class="sc">&lt;=</span> <span class="dv">8</span>, <span class="st">&quot;No&quot;</span>, <span class="st">&quot;Yes&quot;</span>))</span>
<span id="cb518-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb518-3" tabindex="-1"></a>Carseats <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(Carseats, High)</span></code></pre></div>
<p>Letâs now split the data into training and test sets. We randomly select half of the observations from the dataset for our training set and we allocate the rest to the test set (<code>Carseats.test</code>).</p>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb519-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb519-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb519-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb519-2" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(Carseats), <span class="dv">200</span>)</span></code></pre></div>
<p>Now letâs fit the tree using the <code>tree()</code> function from the package <code>tree</code> that you have just installed. As shown below, the basic syntax is quite simple. Since we want to use all variables in the data object as predictors, there is no need to list them all in the formula. Instead, we can simply use a dot. However, we must drop the original <strong>Sales</strong> variable for obvious reasons. To fit the tree to the training data only, we must subset the Carseats data using the <strong>train</strong> object which contains a vector of the randomly selected indices that tells R which values to subset.</p>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb520-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb520-1" tabindex="-1"></a>tree.carseats <span class="ot">&lt;-</span> <span class="fu">tree</span>(High <span class="sc">~</span> . <span class="sc">-</span> Sales, Carseats, <span class="at">subset =</span> train)</span></code></pre></div>
<p>To better understand trees, letâs explore the structure of our tree visually first. We simply specify the name of the model and complement the <code>plot()</code> function with the function <code>text()</code> within which we set the <code>pretty</code> argument to <code>0</code> in order to include the category names for categorical predictors.</p>
<div class="sourceCode" id="cb521"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb521-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb521-1" tabindex="-1"></a><span class="fu">plot</span>(tree.carseats)</span>
<span id="cb521-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb521-2" tabindex="-1"></a><span class="fu">text</span>(tree.carseats, <span class="at">pretty =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Our first split occurs at <strong>Price</strong>, which indicates that this is the feature (i.e.Â variable) that is most important for classifying sales as high or not. From this point forward, further splits are made and the features at which these splits are made are referred to as <em>internal nodes</em>. The tree splits first on <strong>Price</strong>, thus dividing the dataset into two subsets based on whether Price is less than <span class="math inline">\(96.5\)</span>. This is our split criterion. The left side represents the subset within which price is less than <span class="math inline">\(96.5\)</span> whilst the right side represents the subset within which price is equal to or greater than <span class="math inline">\(96.5\)</span>.</p>
<p>Letâs first consider the left side of the tree. The subsequent internal node is <strong>Population</strong> and the split criterion is such that population is less than <span class="math inline">\(414\)</span> which means that within the subset where price is less than <span class="math inline">\(96.5\)</span> (so the left side), the next most significant predictor is <strong>Population</strong>. Within population, it is <strong>ShelveLoc</strong>, then <strong>Age</strong> and so on. Hence, the tree continues to split based on these features and values, further refining the subsets.</p>
<p>The right side represents the subset within which price is greater than or equal to <span class="math inline">\(96.5\)</span>. Our subsequent node after <strong>Price</strong> is <strong>ShelveLoc</strong> which is a factor variable. This means that within the subset where <strong>Price</strong> is greater than or equal to <span class="math inline">\(96.5\)</span>, the location of the shelves is the next most significant predictor. From there onward, as on the left side, the tree continues to split further and further at subsequent internal nodes according to specific split criteria. Therefore, since the feature at which the first split occurs represents the feature with the most importance regarding classification, the internal nodes that are closest to this node are generally more important than those closer to the bottom of the tree.</p>
<p>The vertical lines that connect a feature with an outcome are called <em>branches</em>. Each branch represents a decision based on a feature value, which then progresses to the next internal node or finally, to a <em>leaf node</em>. A leaf node (also called a terminal node) are the nodes at the bottom of the tree that provide the final classification. Each leaf node represents a final decision regarding the classification (e.g., âYesâ or âNoâ for high sales). The path from the root to a leaf node gives the sequence of decisions made to classify an observation. Essentially, the splits on either side simply refine the classification until a decision is reached.</p>
<p>So we start off at the first split where <span class="math inline">\(Price &lt; 96.5\)</span>. If <span class="math inline">\(Price &lt; 96.5\)</span> is true, we move on to the next node on the left hand-side which is <span class="math inline">\(Population &lt; 414\)</span>. We follow subsequent nodes and branches based on the feature values of the observation and we reach a terminal (leaf) node with the final classification (e.g., âYesâ for high sales). So a terminal node is one from which no further splits occur. In summary, our tree make use of features to split the data into subsets to classify whether sales are high or not.</p>
<p>Now letâs explore the output of the tree in greater detail. If we just type the name of the tree object, R prints output detailed information about the tree structure that we illustrated . The first line of the output: <code>node), split, n, deviance, yval, (yprob) * denotes terminal node</code> describes what each component of the output means.</p>
<div class="sourceCode" id="cb522"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb522-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb522-1" tabindex="-1"></a>tree.carseats</span></code></pre></div>
<pre><code>## node), split, n, deviance, yval, (yprob)
##       * denotes terminal node
## 
##   1) root 200 270.000 No ( 0.59500 0.40500 )  
##     2) Price &lt; 96.5 40  47.050 Yes ( 0.27500 0.72500 )  
##       4) Population &lt; 414 31  40.320 Yes ( 0.35484 0.64516 )  
##         8) ShelveLoc: Bad,Medium 25  34.300 Yes ( 0.44000 0.56000 )  
##          16) Age &lt; 64.5 17  20.600 Yes ( 0.29412 0.70588 )  
##            32) Education &lt; 13.5 7   0.000 Yes ( 0.00000 1.00000 ) *
##            33) Education &gt; 13.5 10  13.860 Yes ( 0.50000 0.50000 )  
##              66) Education &lt; 16.5 5   5.004 No ( 0.80000 0.20000 ) *
##              67) Education &gt; 16.5 5   5.004 Yes ( 0.20000 0.80000 ) *
##          17) Age &gt; 64.5 8   8.997 No ( 0.75000 0.25000 ) *
##         9) ShelveLoc: Good 6   0.000 Yes ( 0.00000 1.00000 ) *
##       5) Population &gt; 414 9   0.000 Yes ( 0.00000 1.00000 ) *
##     3) Price &gt; 96.5 160 201.800 No ( 0.67500 0.32500 )  
##       6) ShelveLoc: Bad,Medium 135 154.500 No ( 0.74074 0.25926 )  
##        12) Price &lt; 124.5 82 107.700 No ( 0.63415 0.36585 )  
##          24) Age &lt; 49.5 34  45.230 Yes ( 0.38235 0.61765 )  
##            48) CompPrice &lt; 130.5 21  28.680 No ( 0.57143 0.42857 )  
##              96) Population &lt; 134.5 6   0.000 No ( 1.00000 0.00000 ) *
##              97) Population &gt; 134.5 15  20.190 Yes ( 0.40000 0.60000 )  
##               194) Population &lt; 343 7   5.742 Yes ( 0.14286 0.85714 ) *
##               195) Population &gt; 343 8  10.590 No ( 0.62500 0.37500 ) *
##            49) CompPrice &gt; 130.5 13   7.051 Yes ( 0.07692 0.92308 ) *
##          25) Age &gt; 49.5 48  46.330 No ( 0.81250 0.18750 )  
##            50) CompPrice &lt; 124.5 28  14.410 No ( 0.92857 0.07143 )  
##             100) Price &lt; 101.5 8   8.997 No ( 0.75000 0.25000 ) *
##             101) Price &gt; 101.5 20   0.000 No ( 1.00000 0.00000 ) *
##            51) CompPrice &gt; 124.5 20  25.900 No ( 0.65000 0.35000 )  
##             102) Price &lt; 119 14  19.410 No ( 0.50000 0.50000 )  
##               204) Advertising &lt; 10.5 9  11.460 No ( 0.66667 0.33333 ) *
##               205) Advertising &gt; 10.5 5   5.004 Yes ( 0.20000 0.80000 ) *
##             103) Price &gt; 119 6   0.000 No ( 1.00000 0.00000 ) *
##        13) Price &gt; 124.5 53  33.120 No ( 0.90566 0.09434 )  
##          26) Population &lt; 393.5 34   0.000 No ( 1.00000 0.00000 ) *
##          27) Population &gt; 393.5 19  21.900 No ( 0.73684 0.26316 )  
##            54) CompPrice &lt; 143.5 13   7.051 No ( 0.92308 0.07692 ) *
##            55) CompPrice &gt; 143.5 6   7.638 Yes ( 0.33333 0.66667 ) *
##       7) ShelveLoc: Good 25  31.340 Yes ( 0.32000 0.68000 )  
##        14) Income &lt; 43 7   8.376 No ( 0.71429 0.28571 ) *
##        15) Income &gt; 43 18  16.220 Yes ( 0.16667 0.83333 )  
##          30) US: No 6   8.318 Yes ( 0.50000 0.50000 ) *
##          31) US: Yes 12   0.000 Yes ( 0.00000 1.00000 ) *</code></pre>
<p>So letâs take <code>2) Price &lt; 96.5 40  47.050 Yes</code>( 0.27500 0.72500 )` as an example.</p>
<ul>
<li><code>2)</code> denotes the unique identifier of the node (so this is node 2),<br />
</li>
<li><code>Price &lt; 96.5</code> is the condition of the split</li>
<li><code>40</code> is the number of observations that reach the node *in this case 40)</li>
<li><code>47.050</code> is the deviance (i.e.Â impurity of the node after the split)</li>
<li><code>Yes</code> is the predicted class for this node</li>
<li><code>( 0.27500 0.72500 )</code> are the class probabilities and so the proportion of observations belonging to each class at the node (in other words the fraction of observations in that branch that take on values of <code>Yes</code> and <code>No</code>)<br />
</li>
<li>the <code>*</code> denotes a terminal node</li>
</ul>
<p>Now you may wonder about the root node (<code>1)</code>). This node (node 1) represents the entire dataset with 200 observations in total, the most common class at the root node (in this case <code>No</code>) and the proportion of classes (i.e.Â Yes and No) in the dataset. The root node corresponds to the decision to split at <code>Price &lt; 96.5</code> in the graphic, but essentially, it represents the entire dataset <em>before</em> any splits are made; as you can see, there is no information about <code>split</code> for the root node itself since it doesnât split. In other words, the root node starts the solitting process, and each subsequent split aims to further refine the classification by reducing <em>impurity</em> (i.e.Â variance), with the deviance value helping in measuring the effectiveness of each split. Therefore <code>Price &lt; 96.5</code> is the condition for the split the root node.</p>
<p>To better understand how each feature and split contribute to the final prediction, we must consider not only explore the structure as a whole, but also consider how the data are split into branches based on different features, how probabilities change at each node, and finally, what final predictions are provided by the terminal nodes given the path from the root node.</p>
<p>Starting at the root node, the splits are conducted according to the following conditions:
Price &lt; 96.5, Population &lt; 414, ShelveLoc: Bad, Medium, Age &lt; 64.5, and Education &lt; 13.5.</p>
<p>Therefore:</p>
<ul>
<li>Price &lt; 96.5: Among stores with prices lower than 96.5 (40 stores in total), 72.5% are likely have high sales (hence High = âYesâ).</li>
<li>Population &lt; 414: Of these, 64.5% in lower population areas tend to have high sales.<br />
</li>
<li>ShelveLoc: Bad, Medium: Among these, 56.0% stores with poor shelving location still tend to have high sales<br />
</li>
<li>Age &lt; 64.5: Of these, in regions where the average age of the population is less than 64.5, 70.6% of stores are still likely have high sales<br />
</li>
<li>Education &lt; 13.5: Finally, in areas where education is less than 13.5, stores are still likely to have high sales with 100% probability.</li>
</ul>
<p>As with other models weâve explored so far, <code>summary()</code> also comes in handy. The output contains additional information.</p>
<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb524-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb524-1" tabindex="-1"></a><span class="fu">summary</span>(tree.carseats)</span></code></pre></div>
<pre><code>## 
## Classification tree:
## tree(formula = High ~ . - Sales, data = Carseats, subset = train)
## Variables actually used in tree construction:
## [1] &quot;Price&quot;       &quot;Population&quot;  &quot;ShelveLoc&quot;   &quot;Age&quot;         &quot;Education&quot;  
## [6] &quot;CompPrice&quot;   &quot;Advertising&quot; &quot;Income&quot;      &quot;US&quot;         
## Number of terminal nodes:  21 
## Residual mean deviance:  0.5543 = 99.22 / 179 
## Misclassification error rate: 0.115 = 23 / 200</code></pre>
<ul>
<li>variables actually used in tree construction: here we note 9 variables (<strong>Urban</strong> is missing)<br />
</li>
<li>number of terminal nodes: also referred to as âleavesâ, the tree has 21 terminal nodes; these represent the final decision or classification outcome.<br />
</li>
<li>residual mean deviance: For classification trees, the deviance is given by <span class="math inline">\(-2 \sum_m \sum_k n_{mk} \log \hat{p}_{mk}\)</span>, where <span class="math inline">\(n_{mk}\)</span> is the number of observations in the <span class="math inline">\(m\)</span>th terminal node that belong to the <span class="math inline">\(k\)</span>th class. The smaller the deviance, the better the fit to the training data. The <em>residual mean deviance</em> is the deviance divided by <span class="math inline">\(n-|{T}_0|\)</span>.<br />
</li>
<li>misclassification error rate: this is the training error rate, which in this case is <span class="math inline">\(11.5\%\)</span>. This seems quite good.</li>
</ul>
<p>Now that you have a good grasp of how trees are built and interpreted, letâs evaluate the performance of the algorithm on the test data.The <code>predict()</code> function can be used for this purpose. In the case of a classification tree, the argument <code>type = "class"</code> instructs <code>R</code> to return the actual class prediction. This approach leads to correct predictions for around <span class="math inline">\(77 \%\)</span> of the locations in the test data set.</p>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb526-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb526-1" tabindex="-1"></a>High.test <span class="ot">&lt;-</span> High[<span class="sc">-</span>train]</span>
<span id="cb526-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb526-2" tabindex="-1"></a>Carseats.test <span class="ot">&lt;-</span> Carseats[<span class="sc">-</span>train, ]</span>
<span id="cb526-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb526-3" tabindex="-1"></a>tree.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree.carseats, Carseats.test, <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb526-4"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb526-4" tabindex="-1"></a><span class="fu">table</span>(tree.pred, High.test)</span></code></pre></div>
<pre><code>##          High.test
## tree.pred  No Yes
##       No  104  33
##       Yes  13  50</code></pre>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb528-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb528-1" tabindex="-1"></a>(<span class="dv">104</span> <span class="sc">+</span> <span class="dv">50</span>) <span class="sc">/</span> <span class="dv">200</span></span></code></pre></div>
<pre><code>## [1] 0.77</code></pre>
<p>(If you re-run the <code>predict()</code> function then you might get slightly different results, due to âtiesâ: for instance, this can happen when the training observations corresponding to a terminal node are evenly split between <code>Yes</code> and <code>No</code> response values.)</p>
<p>Next, we consider whether <em>pruning</em> the tree might lead to improved results. Pruning is used to reduce size and complexity of the tree and the main goal is to remove parts of the tree that do not provide additional power in predicting target variables. Pruning is therefore very important in preventing overfitting, improving interpretability and enhancing the performance of the tree. One approach is <em>cost-complexity pruning</em> which involves pruning the tree such that it balances the trade-off between the accuracy and size of the tree (by minimising the cost-complexity criterion).</p>
<p>Ok so letâs see how this works in practice. The function <code>cv.tree()</code> performs <span class="math inline">\(k\)</span>-fold cross-validation in order to determine the optimal level of tree complexity (i.e.Â to find the deviance or number of misclassifications as a function of the cost-complexity parameter <span class="math inline">\(k\)</span>). This approach involves growing the tree to its full depth and then removing nodes that provide little predictive power. We use the argument <code>FUN = prune.misclass</code> to tell R that we want the classification error rate to guide the cross-validation and pruning process, rather than the default for the <code>cv.tree()</code> function, which is deviance. The <code>cv.tree()</code> function reports the number of terminal nodes of each tree considered (<code>size</code>) as well as the corresponding error rate and the value of the cost-complexity parameter used (<code>k</code>).</p>
<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb530-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb530-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb530-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb530-2" tabindex="-1"></a>cv.carseats <span class="ot">&lt;-</span> <span class="fu">cv.tree</span>(tree.carseats, <span class="at">FUN =</span> prune.misclass)</span></code></pre></div>
<p>The output contains several pieces of important information such as size, deviance and the values for <span class="math inline">\(k\)</span>. Each of these is paired with the other, and so the lowest deviance which is 74 (i.e.Â 74 cross-validation errors) corresponds to a tree of size 9 (i.e.Â a tree with 9 terminal nodes) and a value of 1.4 for <span class="math inline">\(k\)</span>. The lower the value for <span class="math inline">\(k\)</span>, the less pruning required.</p>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb531-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb531-1" tabindex="-1"></a><span class="fu">names</span>(cv.carseats)</span></code></pre></div>
<pre><code>## [1] &quot;size&quot;   &quot;dev&quot;    &quot;k&quot;      &quot;method&quot;</code></pre>
<div class="sourceCode" id="cb533"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb533-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb533-1" tabindex="-1"></a>cv.carseats</span></code></pre></div>
<pre><code>## $size
## [1] 21 19 14  9  8  5  3  2  1
## 
## $dev
## [1] 75 75 75 74 82 83 83 85 82
## 
## $k
## [1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0
## 
## $method
## [1] &quot;misclass&quot;
## 
## attr(,&quot;class&quot;)
## [1] &quot;prune&quot;         &quot;tree.sequence&quot;</code></pre>
<p>We can also plot the error rate as a function of both <code>size</code> and <code>k</code>.</p>
<div class="sourceCode" id="cb535"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb535-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb535-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb535-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb535-2" tabindex="-1"></a><span class="fu">plot</span>(cv.carseats<span class="sc">$</span>size, cv.carseats<span class="sc">$</span>dev, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span>
<span id="cb535-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb535-3" tabindex="-1"></a><span class="fu">plot</span>(cv.carseats<span class="sc">$</span>k, cv.carseats<span class="sc">$</span>dev, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>We now apply the <code>prune.misclass()</code> function in order to prune the tree to obtain the nine-node tree as indicated by the cross-validation results. We specify the size using the <code>best</code> argument.</p>
<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb536-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb536-1" tabindex="-1"></a>prune.carseats <span class="ot">&lt;-</span> <span class="fu">prune.misclass</span>(tree.carseats, <span class="at">best =</span> <span class="dv">9</span>)</span>
<span id="cb536-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb536-2" tabindex="-1"></a><span class="fu">plot</span>(prune.carseats)</span>
<span id="cb536-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb536-3" tabindex="-1"></a><span class="fu">text</span>(prune.carseats, <span class="at">pretty =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>How well does this pruned tree perform on the test data set? Once again, we apply the <code>predict()</code> function.</p>
<div class="sourceCode" id="cb537"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb537-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb537-1" tabindex="-1"></a>tree.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(prune.carseats, Carseats.test,</span>
<span id="cb537-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb537-2" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb537-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb537-3" tabindex="-1"></a><span class="fu">table</span>(tree.pred, High.test)</span></code></pre></div>
<pre><code>##          High.test
## tree.pred No Yes
##       No  97  25
##       Yes 20  58</code></pre>
<div class="sourceCode" id="cb539"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb539-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb539-1" tabindex="-1"></a>(<span class="dv">97</span> <span class="sc">+</span> <span class="dv">58</span>) <span class="sc">/</span> <span class="dv">200</span></span></code></pre></div>
<pre><code>## [1] 0.775</code></pre>
<p>Now <span class="math inline">\(77.5 \%\)</span> of the test observations are correctly classified, so not only has the pruning process produced a more interpretable tree, but it has also slightly improved the classification accuracy.</p>
<p>To illustrate what happens if we increase the size of the tree, letâs set the <code>best</code> argument to 14. As you can see, a tree of size 14 produces a more complex tree that is somewhat harder to interpret and the fraction of correct predictions is also slightly lower.</p>
<div class="sourceCode" id="cb541"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb541-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb541-1" tabindex="-1"></a>prune.carseats <span class="ot">&lt;-</span> <span class="fu">prune.misclass</span>(tree.carseats, <span class="at">best =</span> <span class="dv">14</span>)</span>
<span id="cb541-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb541-2" tabindex="-1"></a><span class="fu">plot</span>(prune.carseats)</span>
<span id="cb541-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb541-3" tabindex="-1"></a><span class="fu">text</span>(prune.carseats, <span class="at">pretty =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb542-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb542-1" tabindex="-1"></a>tree.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(prune.carseats, Carseats.test,</span>
<span id="cb542-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb542-2" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb542-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb542-3" tabindex="-1"></a><span class="fu">table</span>(tree.pred, High.test)</span></code></pre></div>
<pre><code>##          High.test
## tree.pred  No Yes
##       No  102  31
##       Yes  15  52</code></pre>
<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb544-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb544-1" tabindex="-1"></a>(<span class="dv">102</span> <span class="sc">+</span> <span class="dv">52</span>) <span class="sc">/</span> <span class="dv">200</span></span></code></pre></div>
<pre><code>## [1] 0.77</code></pre>
</div>
<div id="regression-trees" class="section level3 unnumbered hasAnchor">
<h3>Regression Trees<a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#regression-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="file">
<p>For the tasks below, you will require the <strong>Boston</strong> dataset. This dataset is part of the <code>ISRL2</code> package from the core textbook (James et. al 2021).</p>
</div>
<p>The <strong>Boston</strong> is a datast that contains housing values in 506 suburbs of Boston. There are 13 variables:</p>
<ul>
<li><p>crim: per capita crime rate by town.</p></li>
<li><p>zn: proportion of residential land zoned for lots over 25,000 sq.ft.</p></li>
<li><p>indus: proportion of non-retail business acres per town.</p></li>
<li><p>chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).</p></li>
<li><p>nox: nitrogen oxides concentration (parts per 10 million).</p></li>
<li><p>rm: average number of rooms per dwelling.</p></li>
<li><p>age: proportion of owner-occupied units built prior to 1940.</p></li>
<li><p>dis: weighted mean of distances to five Boston employment centers.</p></li>
<li><p>rad: index of accessibility to radial highways.</p></li>
<li><p>tax: full-value property-tax rate per $10,000.</p></li>
<li><p>ptratio: pupil-teacher ratio by town.</p></li>
<li><p>lstat: lower status of the population (percent).</p></li>
<li><p>medv: median value of owner-occupied homes in $1000s.</p></li>
</ul>
<p>We will now explore how a regression decision tree can be used to predict the median value of owner-occupied homes (<strong>medv</strong>) using all variables in the dataset.</p>
<p>Letâs split the data into a training and test set.</p>
<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb546-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb546-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb546-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb546-2" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(Boston), <span class="fu">nrow</span>(Boston) <span class="sc">/</span> <span class="dv">2</span>)</span></code></pre></div>
<p>We fit the model in the same way as we did for the classification tree.</p>
<div class="sourceCode" id="cb547"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb547-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb547-1" tabindex="-1"></a>tree.boston <span class="ot">&lt;-</span> <span class="fu">tree</span>(medv <span class="sc">~</span> ., Boston, <span class="at">subset =</span> train)</span>
<span id="cb547-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb547-2" tabindex="-1"></a><span class="fu">summary</span>(tree.boston)</span></code></pre></div>
<pre><code>## 
## Regression tree:
## tree(formula = medv ~ ., data = Boston, subset = train)
## Variables actually used in tree construction:
## [1] &quot;rm&quot;    &quot;lstat&quot; &quot;crim&quot;  &quot;age&quot;  
## Number of terminal nodes:  7 
## Residual mean deviance:  10.38 = 2555 / 246 
## Distribution of residuals:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -10.1800  -1.7770  -0.1775   0.0000   1.9230  16.5800</code></pre>
<p>The structure of the results are similar to those of the classification tree. But note that for our model here, only four variables were used to construct the tree, namely <strong>rm</strong>, <strong>lstat</strong>, <strong>crim</strong>, <strong>age</strong>. We saw this already in the case of the classification tree where one of the variables was missing.</p>
<p>Why is this the case? The algorithm selects variables according to their role in improving predictive accuracy at each split. The reason for this is that we did not add any further arguments to this function and so we allowed R to use the default settings for tree growth, which include thresholds for minimum deviance, minimum node size, and maximum tree depth. By not specifying the control parameter, the function uses default settings for tree growth, which include thresholds for minimum deviance, minimum node size, and maximum tree depth (31), preventing the tree from growing excessively large. However, it is important to note that we <em>could have fit</em> a much bigger tree, by passing <code>control = tree.control(nobs = length(train), mindev = 0)</code> into the <code>tree()</code> function.</p>
<p>Since in our case only four variables were selected, this suggests that only these four provide a sufficiently significant improvement in reducing <em>variance</em> at different splits. There are several reasons for this, besides the predictive power of the variables and the associated impurity, such as sample size, correlation among variables, and complexity. We also note that there are 7 terminal nodes, and that the residual mean deviance is 10.38. In the context of a regression tree, the deviance is simply the sum of squared errors for the tree.</p>
<div class="sourceCode" id="cb549"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb549-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb549-1" tabindex="-1"></a>tree.boston</span></code></pre></div>
<pre><code>## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 253 19450.0 21.79  
##    2) rm &lt; 6.9595 222  6794.0 19.35  
##      4) lstat &lt; 14.405 135  1816.0 22.51  
##        8) rm &lt; 6.543 111   763.1 21.38 *
##        9) rm &gt; 6.543 24   256.5 27.73 *
##      5) lstat &gt; 14.405 87  1554.0 14.46  
##       10) crim &lt; 11.4863 61   613.8 16.23  
##         20) age &lt; 93.95 30   245.7 18.09 *
##         21) age &gt; 93.95 31   164.1 14.43 *
##       11) crim &gt; 11.4863 26   302.7 10.32 *
##    3) rm &gt; 6.9595 31  1929.0 39.21  
##      6) rm &lt; 7.553 16   505.5 33.42 *
##      7) rm &gt; 7.553 15   317.0 45.38 *</code></pre>
<p>In terms of the details of each node, the results are structured similarly to a classification tree. So for example, for the root node we have 253 observations, with a total deviance of 19450.0 and a predicted value (mean response) of 21.79. Our most important feature appears to be the average number of rooms per dwelling (<strong>rm</strong>) and our split criterion is <span class="math inline">\(rm &lt; 6.9595\)</span>.</p>
<p>Letâs visualise the tree.</p>
<div class="sourceCode" id="cb551"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb551-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb551-1" tabindex="-1"></a><span class="fu">plot</span>(tree.boston)</span>
<span id="cb551-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb551-2" tabindex="-1"></a><span class="fu">text</span>(tree.boston, <span class="at">pretty =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>On the left hand side we have multiple splits corresponding to less than 7 average number of rooms per dwelling whilst of the right side we have a single split corresponding to 7 or more rooms per dwelling. We also note one more split in the same variable before the terminal nodes which tell us the median house prices of owner-occupied homes. For homes in census tracts in which <code>rm &gt;= 7.553</code>, the regression tree predicts a median house price of <span class="math inline">\(45.38\)</span>.</p>
<p>Now we apply <span class="math inline">\(k\)</span> fold cross-validation and plot the results to see whether pruning the tree will improve performance. The <code>cv.tree()</code> runs <span class="math inline">\(k\)</span>-fold cross-validation to find the deviance or number of misclassifications as a function of the cost-complexity parameter <span class="math inline">\(k\)</span>.</p>
<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb552-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb552-1" tabindex="-1"></a>cv.boston <span class="ot">&lt;-</span> <span class="fu">cv.tree</span>(tree.boston)</span>
<span id="cb552-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb552-2" tabindex="-1"></a></span>
<span id="cb552-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb552-3" tabindex="-1"></a><span class="fu">plot</span>(cv.boston<span class="sc">$</span>size, cv.boston<span class="sc">$</span>dev, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>In this case, the most complex tree under consideration is selected by cross-validation and so we use the unpruned tree to make predictions on the test set.</p>
<div class="sourceCode" id="cb553"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb553-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb553-1" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree.boston, <span class="at">newdata =</span> Boston[<span class="sc">-</span>train, ])</span>
<span id="cb553-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb553-2" tabindex="-1"></a>boston.test <span class="ot">&lt;-</span> Boston[<span class="sc">-</span>train, <span class="st">&quot;medv&quot;</span>]</span>
<span id="cb553-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb553-3" tabindex="-1"></a><span class="fu">plot</span>(yhat, boston.test)</span>
<span id="cb553-4"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb553-4" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb554-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb554-1" tabindex="-1"></a><span class="fu">mean</span>((yhat <span class="sc">-</span> boston.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 35.28688</code></pre>
<p>The test set MSE associated with the regression tree is <span class="math inline">\(35.29\)</span>. The square root of the MSE is therefore around <span class="math inline">\(5.941\)</span>, indicating that this model leads to test predictions that are (on average) within approximately <span class="math inline">\(5.941\)</span> of the true median home value for the census tract.</p>
</div>
</div>
<div id="ensemble-methods" class="section level2 unnumbered hasAnchor">
<h2>Ensemble Methods<a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#ensemble-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Despite the simplicity of decision trees, this approach is not necessarily competitive to other supervised approaches we explored so far in the course, particularly due to high variance, but remember that this depends on context. For example, when dealing with complex and highly non-linear relationships, it is possible for decision trees to outperform more classic supervised learning approaches; we can assess performance using either cross-validation or the validation set approach. Do remember that selecting a statistical learning method by only relying on the test error may not always be sufficient and we may prefer a decision tree simply because it is easier to interpret and illustrate.</p>
<p>Nevertheless, since trees do suffer from high variance and are not very robust with respect to changes in the data, how can we improve prediction accuracy? Well, we can implement approaches that, at their core, rely on regression or classification trees as their main building blocks. These methods are referred to as <em>ensemble methods</em>. Below, we explore <em>bagging</em>, <em>random forests</em>, and <em>boosting</em>.</p>
<div id="bagging" class="section level3 unnumbered hasAnchor">
<h3>Bagging<a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#bagging" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Also called <em>bootstrap aggregation</em>, bagging is essentially bootstrapping as you know it from earlier in the course but this time used in a completely different context, that is, to reduce the variance of a statistical learning method. It is particularly useful in the case of decision trees precisely because these suffer from high variance. Here we consider bagging for in the context of regression trees but it can also be extended to classification trees as well. Also, bagging is not restricted to decision trees but can be used for many regression methods.<br />
In this exercise, we continue with the <code>Boston</code> data and use the bagging approach to predict median value of owner-occupied homes using all variables in the dataset.</p>
<p>Using the <code>randomForest</code> package and the function with the same name, we fit a bagging tree. In addition to the formula, we also must specify the <code>mtry</code> and <code>importance</code> arguments. The argument <code>mtry = 12</code> tells R that all <span class="math inline">\(12\)</span> predictors should be considered for each split of the tree, whilst setting the <code>importance</code> argument to <code>TRUE</code> tells R to also assess the importance of the predictors.</p>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb556-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb556-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb556-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb556-2" tabindex="-1"></a>bag.boston <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston,</span>
<span id="cb556-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb556-3" tabindex="-1"></a>                           <span class="at">subset =</span> train, <span class="at">mtry =</span> <span class="dv">12</span>, <span class="at">importance =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>Now letâs have a look at the output. <em>Note that the exact results obtained may differ, depending on our R version and version of your <code>randomForest</code> package</em>.</p>
<div class="sourceCode" id="cb557"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb557-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb557-1" tabindex="-1"></a>bag.boston</span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = medv ~ ., data = Boston, mtry = 12, importance = TRUE,      subset = train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 12
## 
##           Mean of squared residuals: 11.40162
##                     % Var explained: 85.17</code></pre>
<p>Here we have a few pieces of important information. We have 500 trees in total. This means that the bagging procedure built 500 decision trees on different bootstrapped samples of the training data. In other words, for a regression problem, the bagging procedures constructs B regression trees using B bootstrapped training sets, and averages the resulting predictions. There are 12 variables tried at each split; since it uses all predictors (as specified by <code>mtry</code>), the trees are grown deep and are NOT pruned. As we saw earlier with the decision trees, each individual tree will have high variance (and low bias) but since the bagging procedure averages all B trees, then this <em>reduces</em> the variance.</p>
<p>The mean of squared residuals tells us the average squared difference between predicted and actual values. The percent variance explained is <span class="math inline">\(85.17\%\)</span>, which indicates a very good fit.</p>
<p>Right, so how about interpretation? Well, given that with bagging we grow many trees, it is no longer possible to represent the results as a single tree, and so the resulting model can be difficult to interpret. Nevertheless, we can learn more about the importance of each predictor using the <code>varImpPlot()</code> function from the <code>randomForest</code> package that produces dotcharts.</p>
<div class="sourceCode" id="cb559"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb559-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb559-1" tabindex="-1"></a><span class="fu">varImpPlot</span>(bag.boston)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Or we can use the <code>importance()</code> function from the same package.</p>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb560-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb560-1" tabindex="-1"></a><span class="fu">importance</span>(bag.boston)</span></code></pre></div>
<pre><code>##           %IncMSE IncNodePurity
## crim    24.948900     873.41761
## zn       4.824575      55.59650
## indus    3.185194     111.59825
## chas    -1.407962      12.46427
## nox     17.549697     260.21778
## rm      52.852626   12231.53572
## age     18.325696     346.97045
## dis      6.046766     272.36149
## rad      3.677714      66.96846
## tax     10.224698     148.96271
## ptratio  9.254946     145.16605
## lstat   53.389661    4789.60479</code></pre>
<p>The first measure is the percent increase in mean squared error (%IncMSE) and is based upon the <em>out-of-bag error estimation</em> which provides an internal measure of model performance without the need for a separate validation set. This is because the out-of-bag observations are observations that were not used to fit the model and were instead used to validate the model (usually only two thirds of the observations are used to fit the model).</p>
<p>The second measure is the increase in node purity (IncNodePurity) which reflects the total decrease in node impurity that results from splits over that variable, averaged over all trees. In the case of regression trees, the node impurity is measured by the training RSS, and for classification trees by the deviance. By far, <code>rm</code> and <code>lstat</code> seem to be the most important predictors of the model.</p>
<p>But how well does this bagged model perform on the test set?</p>
<div class="sourceCode" id="cb562"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb562-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb562-1" tabindex="-1"></a>yhat.bag <span class="ot">&lt;-</span> <span class="fu">predict</span>(bag.boston, <span class="at">newdata =</span> Boston[<span class="sc">-</span>train, ])</span>
<span id="cb562-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb562-2" tabindex="-1"></a><span class="fu">plot</span>(yhat.bag, boston.test)</span>
<span id="cb562-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb562-3" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<div class="sourceCode" id="cb563"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb563-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb563-1" tabindex="-1"></a><span class="fu">mean</span>((yhat.bag <span class="sc">-</span> boston.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 23.41916</code></pre>
<p>The test set MSE associated with the bagged regression tree is lower than the test MSE for an optimally-pruned single tree, but not by muchâ¦</p>
<p>We saw earlier that the bagging was performed using 500 trees. We can customise the number of trees by telling R how many trees to âgrowâ using the <code>ntree()</code> argument.</p>
<div class="sourceCode" id="cb565"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb565-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb565-1" tabindex="-1"></a>bag.boston <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston,</span>
<span id="cb565-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb565-2" tabindex="-1"></a>    <span class="at">subset =</span> train, <span class="at">mtry =</span> <span class="dv">12</span>, <span class="at">ntree =</span> <span class="dv">25</span>)</span>
<span id="cb565-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb565-3" tabindex="-1"></a>yhat.bag <span class="ot">&lt;-</span> <span class="fu">predict</span>(bag.boston, <span class="at">newdata =</span> Boston[<span class="sc">-</span>train, ])</span>
<span id="cb565-4"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb565-4" tabindex="-1"></a><span class="fu">mean</span>((yhat.bag <span class="sc">-</span> boston.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 25.75055</code></pre>
<p>With 25 trees, we see a slight increase in the test MSE and if we have a look at the results, we also see a reduction in the variance explained.</p>
<div class="sourceCode" id="cb567"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb567-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb567-1" tabindex="-1"></a>bag.boston</span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = medv ~ ., data = Boston, mtry = 12, ntree = 25,      subset = train) 
##                Type of random forest: regression
##                      Number of trees: 25
## No. of variables tried at each split: 12
## 
##           Mean of squared residuals: 13.51568
##                     % Var explained: 82.42</code></pre>
</div>
<div id="random-forests" class="section level3 unnumbered hasAnchor">
<h3>Random Forests<a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#random-forests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Bagging is a special case of a <em>random forest</em> which can be seen as an improvement over bagged trees because it decorrelates the trees. In other words, the random forest algorithm does not consider a majority of the available predictors at each split but only a subset. In this way, it address the limitation of bagged trees looking very similar to one another due to strong influences of particular predictors. As we saw earlier, bagging did improve the performance of the model but this improvement was not very dramatic relative to the individual optimally pruned tree we built at the beginning of the demonstration. Hence, random forests can make trees more reliable precisely because the average of the resulting trees have less variance.</p>
<p>Growing a random forest is accomplished in exactly the same way as bagging, except that we use a smaller value of the <code>mtry</code> argument. By default, <code>randomForest()</code> uses <span class="math inline">\(p/3\)</span> variables when building a random forest of regression trees, and <span class="math inline">\(\sqrt{p}\)</span> variables when building a random forest of classification trees. Here we will use <code>mtry = 6</code>.</p>
<div class="sourceCode" id="cb569"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb569-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb569-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb569-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb569-2" tabindex="-1"></a>rf.boston <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston,</span>
<span id="cb569-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb569-3" tabindex="-1"></a>    <span class="at">subset =</span> train, <span class="at">mtry =</span> <span class="dv">6</span>, <span class="at">importance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb569-4"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb569-4" tabindex="-1"></a>yhat.rf <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf.boston, <span class="at">newdata =</span> Boston[<span class="sc">-</span>train, ])</span>
<span id="cb569-5"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb569-5" tabindex="-1"></a><span class="fu">mean</span>((yhat.rf <span class="sc">-</span> boston.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 20.06644</code></pre>
<p>The test set MSE is lower than the test MSE for bagging and much lower than that of the decision tree.</p>
<p>Using the <code>importance()</code> function, we can view the importance of each variable.</p>
<div class="sourceCode" id="cb571"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb571-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb571-1" tabindex="-1"></a><span class="fu">importance</span>(rf.boston)</span></code></pre></div>
<pre><code>##           %IncMSE IncNodePurity
## crim    19.435587    1070.42307
## zn       3.091630      82.19257
## indus    6.140529     590.09536
## chas     1.370310      36.70356
## nox     13.263466     859.97091
## rm      35.094741    8270.33906
## age     15.144821     634.31220
## dis      9.163776     684.87953
## rad      4.793720      83.18719
## tax      4.410714     292.20949
## ptratio  8.612780     902.20190
## lstat   28.725343    5813.04833</code></pre>
<p>Or display the resuts as dotcharts.</p>
<div class="sourceCode" id="cb573"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb573-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb573-1" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf.boston)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/chunk24-1.png" width="672" /></p>
<p>The results indicate that across all of the trees considered in the random forest, the wealth of the community (<code>lstat</code>) and the house size (<code>rm</code>) are by far the two most important variables.</p>
</div>
<div id="boosting" class="section level3 unnumbered hasAnchor">
<h3>Boosting<a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#boosting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Finally, letâs consider boosting that works somewhat similarly to bagging except that it grows trees sequentially. In other words, each tree is built to correct the errors of the previous trees by focusing on the residuals (errors) of the model up to that point. Therefore, many small, shallow trees incrementally improve the performance of the model which often requires a large number of trees. There are three tuning parameters to boosting: the number of trees B (seleced using cross-validation), the shrinkage parameter <span class="math inline">\(\lambda\)</span> (lambda) which controls the rate at which boosting learns, and the number of splits which controls complexity.</p>
<p>In this example, we focus on using boosting for a regression tree but this approach can also be used with other regression or classification methods.</p>
<p>Here, we continue with the Boston dataset and fit a boosting regression tree on the same model as earlier. To implement boosting, we can use the <code>gbm()</code> function from the <code>gbm</code> package.
We run <code>gbm()</code> with the option <code>distribution = "gaussian"</code> since this is a regression problem. The argument <code>n.trees = 5000</code> indicates that we want <span class="math inline">\(5000\)</span> trees, and the option <code>interaction.depth = 4</code> limits the depth of each tree. Hence, we are going for many, many trees of small size to incrementally improve the model.</p>
<div class="sourceCode" id="cb574"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb574-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb574-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb574-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb574-2" tabindex="-1"></a>boost.boston <span class="ot">&lt;-</span> <span class="fu">gbm</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston[train, ],</span>
<span id="cb574-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb574-3" tabindex="-1"></a>                    <span class="at">distribution =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="at">n.trees =</span> <span class="dv">5000</span>, </span>
<span id="cb574-4"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb574-4" tabindex="-1"></a>                    <span class="at">interaction.depth =</span> <span class="dv">4</span>)</span></code></pre></div>
<p>For boosting, the <code>summary()</code> function works in a slightly different way. It produces a relative influence plot and also outputs the relative influence statistics.</p>
<div class="sourceCode" id="cb575"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb575-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb575-1" tabindex="-1"></a><span class="fu">summary</span>(boost.boston)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<pre><code>##             var     rel.inf
## rm           rm 44.48249588
## lstat     lstat 32.70281223
## crim       crim  4.85109954
## dis         dis  4.48693083
## nox         nox  3.75222394
## age         age  3.19769210
## ptratio ptratio  2.81354826
## tax         tax  1.54417603
## indus     indus  1.03384666
## rad         rad  0.87625748
## zn           zn  0.16220479
## chas       chas  0.09671228</code></pre>
<p>We again see that <code>lstat</code> and <code>rm</code> are by far the most important variables.</p>
<p>We can also produce <em>partial dependence plots</em> for these two variables. The plots facilitate our interpretation of complex models results from ensemble methods (or other complex non-linear methods) by allowing us to visualise the effect of a single predictor on the response.</p>
<div class="sourceCode" id="cb577"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb577-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb577-1" tabindex="-1"></a><span class="fu">plot</span>(boost.boston, <span class="at">i =</span> <span class="st">&quot;rm&quot;</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<div class="sourceCode" id="cb578"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb578-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb578-1" tabindex="-1"></a><span class="fu">plot</span>(boost.boston, <span class="at">i =</span> <span class="st">&quot;lstat&quot;</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-31-2.png" width="672" /></p>
<p>These plots illustrate the marginal effect of the selected variables on the response after <em>integrating</em> out the other variables. In this case, as we might expect, median house prices are increasing with <code>rm</code> and decreasing with <code>lstat</code>.</p>
<p>We now use the boosted model to predict <code>medv</code> on the test set:</p>
<div class="sourceCode" id="cb579"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb579-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb579-1" tabindex="-1"></a>yhat.boost <span class="ot">&lt;-</span> <span class="fu">predict</span>(boost.boston,</span>
<span id="cb579-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb579-2" tabindex="-1"></a>    <span class="at">newdata =</span> Boston[<span class="sc">-</span>train, ], <span class="at">n.trees =</span> <span class="dv">5000</span>)</span>
<span id="cb579-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb579-3" tabindex="-1"></a><span class="fu">mean</span>((yhat.boost <span class="sc">-</span> boston.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 18.39057</code></pre>
<p>The test MSE obtained is the lowest thus far and therefore superior to the test MSE of random forests and bagging. If we want to, we can perform boosting with a different value of the shrinkage parameter <span class="math inline">\(\lambda\)</span>. The default value is <span class="math inline">\(0.001\)</span>, but this is easily modified.
Here we take <span class="math inline">\(\lambda=0.2\)</span>.</p>
<div class="sourceCode" id="cb581"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb581-1"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb581-1" tabindex="-1"></a>boost.boston <span class="ot">&lt;-</span> <span class="fu">gbm</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston[train, ],</span>
<span id="cb581-2"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb581-2" tabindex="-1"></a>    <span class="at">distribution =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="at">n.trees =</span> <span class="dv">5000</span>,</span>
<span id="cb581-3"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb581-3" tabindex="-1"></a>    <span class="at">interaction.depth =</span> <span class="dv">4</span>, <span class="at">shrinkage =</span> <span class="fl">0.2</span>, <span class="at">verbose =</span> F)</span>
<span id="cb581-4"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb581-4" tabindex="-1"></a>yhat.boost <span class="ot">&lt;-</span> <span class="fu">predict</span>(boost.boston,</span>
<span id="cb581-5"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb581-5" tabindex="-1"></a>    <span class="at">newdata =</span> Boston[<span class="sc">-</span>train, ], <span class="at">n.trees =</span> <span class="dv">5000</span>)</span>
<span id="cb581-6"><a href="demonstration-2-the-basics-of-decision-trees-and-related-methods.html#cb581-6" tabindex="-1"></a><span class="fu">mean</span>((yhat.boost <span class="sc">-</span> boston.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 16.54778</code></pre>
<p>In this case, using <span class="math inline">\(\lambda=0.2\)</span> leads to an even lower test MSE than <span class="math inline">\(\lambda=0.001\)</span>. Although typical values are 0.01 or 0.001, the choice for lambda will depend on the problem at hand.</p>
<p>In this demonstration, you have learned the basics of decision trees, bagging, random forests, and boosting. For a more in-depth exploration of these topics, please see the reading assigned for this section.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="demonstration-1-propublicas-analysis-of-the-compas-tool.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="answers-4.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": null,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
