---
editor_options:
  markdown:
    wrap: 72
---

# Demonstration 1 {-}

So far, we have established the importance of training and test data and you now should have a robust understanding of the differences between training and test error rates in relation to model performance. However, several challenges remain and these include:

-  the size of the training set and whether it is sufficient to adequately train our model;  
-  the size of the test set and whether it is sufficient to adequately evaluate our model;  
-  the process of testing our model on just a single test set;    
-  the nature of the training set.    

We are dealing with a gap here: the gap between the need to reliably measure model performance and the process of splitting the data we have available.

In this demonstration, you will learn more about how cross-validation provides us with ways to address this gap.  

## Data and Variables {-}

In this demonstration, we will make use the **Auto** dataset from the core textbook (James et. al 2021). This dataset is part of the `ISRL2` package. By loading the package, the **Auto** dataset loads automatically.

You should already be familiar with the **Auto** dataset but below is a reminder of the variables it contains:

-   mpg: miles per gallon
-   cylinders: Number of cylinders between 4 and 8
-   displacement: Engine displacement (cu. inches)
-   horsepower: Engine horsepower
-   weight: Vehicle weight (lbs.)
-   acceleration: Time to accelerate from 0 to 60 mph (sec.)
-   year: Model year
-   origin: Origin of car (1. American, 2. European, 3. Japanese)
-   name: Vehicle name

In addition to `ISRL2`, you will also require the `boot` package which first needs to be installed. This package is required for both LOOCV and $k$-fold CV.

```{r warning = FALSE, message = FALSE}
library(ISLR2)
library(boot) #don't forget to install it first
```

## The Validation Set Approach {-}

Let's first consider the validation set approach.  

The **Auto** dataset has a total of 392 observations and the goal is to randomly split these observations using a 50/50 ratio. We can perform this randomised split using the base R `sample()` function. The first argument within `sample()` is 392. This is the total number of observations available to us from the **Auto** dataset. The second argument is 196. This represents the number of observations we want to select from the total we have available which is 392. Therefore, the `sample()` function will return a vector of 196 unique integers which represents a subset of 196 indices from a total of 392.   

*Note: by default, the `sample()` function conducts the sampling without replacement.*

```{r}
training_data <- sample(392, 196)

training_data
```

Now we need to tell R to fit a linear regression model using only the observations corresponding to the training set. We can do so by using the `subset` argument which tells R to only select the 196 observations that are indexed at the specific positions as defined by the  **training_data** object. In this way, the model will be fitted using only the observations from **Auto** dataset that are defined by this vector of indices. 

```{r}
fit <- lm(mpg ~ horsepower, data = Auto, subset = training_data)
```

Let's now calculate the Mean Squared Error for the test dataset. We obtain the predictions using the`predict()` function with which you are already familiar.

```{r}
mean((Auto$mpg - predict(fit, Auto))[-training_data]^2)
```

Let's try applying some transformations to our predictor **horsepower** using the **poly()** function and then calculate the test MSE values to observe how the MSE values change.   

We first fit a second-degree polynomial regression model:  

```{r}
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = training_data)
```

Then calculate the test MSE:   

```{r}
mean((Auto$mpg - predict(lm.fit2, Auto))[-training_data]^2)
```

Now we fit a third-degree polynomial regression model  

```{r}
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = training_data)
```

Then calculate the test MSE:    

```{r}
mean((Auto$mpg - predict(lm.fit3, Auto))[-training_data]^2)
```

What would happen if we choose a different training dataset instead? We can try this out by setting the seed to 2, for example.   

```{r}
set.seed(2)  
training_data2 <- sample(392, 196)
```

We then run a new set of models: a linear regression model, a second degree polynomial model, and a third degree polynomial model and then calculate the test MSE values for each of the three models.    

**linear regression model and corresponding test MSE:**  

```{r}
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = training_data2)
mean((Auto$mpg - predict(lm.fit, Auto))[-training_data2]^2)
```

**quadratic regression model and corresponding test MSE:** 

```{r}
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = training_data2)
mean((Auto$mpg - predict(lm.fit2, Auto))[-training_data2]^2)
```

**cubic regression model and corresponding test MSE:** 

```{r}
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = training_data2)
mean((Auto$mpg - predict(lm.fit3, Auto))[-training_data2]^2)
```

The results obtained using the second training dataset are consistent with the findings from the first training dataset whereby a model that predicts **mpg** using a quadratic function of **horsepower** performs better than a model that involves only a linear function of **horsepower**. Also, there is little evidence in favor of a model that uses a cubic function of **horsepower**.  

Overall, the main take-away point is that if we were to choose different training sets, we will obtain somewhat different errors on the validation set. 

This is an important point of reflection: if we implement the fixed split approach directly such that we fit our model once on our single training set and then immediately evaluate the predictive accuracy of that model once on our single test set, 

Nevertheless, the validation set approach still does not solve this challenge. 

## Leave-One-Out Cross-Validation {-}

Now, let's consider LOOCV. In R, one way to compute the LOOCV estimate is by using functions from the `boot` package. This package nevertheless requires that the models are built using the `glm()` function, including linear models. The `glm()` function will produce identical results to `lm()` when fitting a linear model. You do not need to specify the `family` argument as you did for logistic regression since it is already set by default to `gaussian`. 

Fitting a linear model with `glm()` is the same as with `lm()`. 

```{r}
glm.fit <- glm(mpg ~ horsepower, data = Auto)
```

We then perform LOOCV using the `cv.glm()` function from the `boot` package.

```{r}
cv.err <- cv.glm(Auto, glm.fit)
```

The output includes a list of several components. The component of relevance for LOOCV is `delta`. 

```{r}
cv.err$delta
```

The `delta` component therefore provides us with the results of the cross-validation. The first value is the raw cross-validation estimate of prediction error whilst the second value is the adjusted cross-validation estimate that compensates for the bias introduced by not using leave-one-out cross-validation. In our case, the two values are identical to two decimal places and so our cross-validation estimate for the test error is approximately $24.23$. 

We can repeat this procedure for increasingly complex polynomial fits. Instead of writing separate code for each model fit, we can automate the process using the  `for()` function that initiates a *for loop* which iteratively fits polynomial regressions for polynomials of order $i=1$ to $i=10$, computes the associated cross-validation error, and stores it in the $i$th element of the vector `cv.error`.

```{r}
cv.error <- rep(0, 10)
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
```

The output shows us the estimated test MSE for the linear model and polynomials up to the 10th degree. 

```{r}
cv.error
```

We can see a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials.

## $k$-Fold Cross-Validation {-}

Finally, let's consider $k$-fold cross-Validation. The same function (`cv.glm()`) can be used by setting the value of K. By default, the value of K is equal to the number of observations in data which therefore gives us LOOCV.

In this example, we will use $k=10$, a common choice for $k$. We will again perform the same approach of increasingly complex polynomial fits as we did for LOOCV. The code is identical to the one we used to LOOCV except that of course, we specified `K = 10`.

```{r}
set.seed(17)
cv.error.10 <- rep(0, 10)
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
```

The output shows us a similar pattern to the estimated test MSE for the linear model and polynomials up to the 10th degree: we see a sharp drop between the linear and quadratic fits but once again no improvement with higher order polynomials. We can therefore conclude that a quadratic fit is suitable.

```{r}
cv.error.10
```

We saw earlier that when we LOOCV, the two values were essentially the same. In the case of $k$-fold CV, these values differ slightly but they are still very similar to each other. 

```{r}
cv.glm(Auto, glm.fit, K = 10)$delta
```
