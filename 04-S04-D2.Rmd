---
editor_options:
  markdown:
    wrap: 72
---

# Demonstration 2 {-}

How can we implement bootstrapping in R? Follow the steps outlined in this demonstration to build basic coding skills required to perform bootstrapping. 

## Data and Variables {-}

In this demonstration, we will make use the **Portfolio** and **Auto** package from the core textbook (James et. al 2021). These datasets are part of the `ISRL2` package. 

**Portfolio** is a dataframe contains 100 observations specifically designed to illustrate the concept of bootstrapping and the way in which it is applied in R. The dataframe contains only two variables: *X*: returns for Asset X, and *Y*: returns for Asset Y.

In addition to `ISRL2`, you will also require the `boot` package. 

```{r warning = FALSE, message = FALSE}
library(ISLR2)
library(boot) #you should have already installed this for Demo 1
```

## Estimating the Accuracy of a Statistic of Interest {-}

One of the advantages of bootstrapping as a resampling method is that it can be applied in almost all situations and it is quite simple to perform it with R. 

The first step is to create a function that computes our statistic of
interest. This function should take as input the $(X,Y)$ data as well as a vector indicating which observations should be used to estimate $\alpha$. The function then outputs the estimate for $\alpha$ based on the selected observations. We will call our function `alpha.fn()`, 

```{r}
alpha.fn <- function(data, index) {
  X <- data$X[index]
  Y <- data$Y[index]
  (var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X, Y))
}
```

For example, the following command tells `R` to estimate $\alpha$ using
all $100$ observations from our dataset.

```{r}
alpha.fn(Portfolio, 1:100)
```

We now use the `sample()` function to randomly select $100$ observations from the range $1$ to $100$, with replacement. This is equivalent to constructing one new bootstrap dataset and recomputing $\hat{\alpha}$
based on the new data set. We can perform this command many, many times, recording all of the corresponding estimates for $\alpha$, and computing the resulting standard deviation. 

```{r}
set.seed(7)
alpha.fn(Portfolio, sample(100, 100, replace = T))
```

However, the `boot()` function automates this approach. For example, we can tell R to repeat this command 1000 times and we obtain $R=1,000$ bootstrap estimates for $\alpha$. The final output shows that using the original data,$\hat{\alpha}=0.5758$, and that the bootstrap estimate for ${\rm SE}(\hat{\alpha})$ is $0.0897$.

```{r}
boot(Portfolio, alpha.fn, R = 1000)
```


## Estimating the Accuracy of a Linear Regression Model {-}

The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. In this example, we will assess the variability of the estimates for $\beta_0$ and $\beta_1$, the intercept and slope terms for a simple linear regression model that uses `horsepower` to predict `mpg` in the `Auto` data set. 

We will compare the estimates obtained using the bootstrap to those obtained using the formulas for ${\rm SE}(\hat{\beta}_0)$ and ${\rm SE}(\hat{\beta}_1)$.

We first create a simple function, `boot.fn()`, which takes in the
`Auto` data set as well as a set of indices for the observations, and
returns the intercept and slope estimates for the linear regression model. We then apply this function to the full set of $392$ observations in order to compute the estimates of $\beta_0$ and $\beta_1$ on the entire data set. *Note that we do not need the `{` and `}` at the beginning and end of the function because it is only one line long.*

```{r}
boot.fn <- function(data, index)
  coef(lm(mpg ~ horsepower, data = data, subset = index))
boot.fn(Auto, 1:392)
```

The `boot.fn()` function can also be used in order to create bootstrap estimates for the intercept and slope terms by randomly sampling from among the observations with replacement (as we did in the previous example with the `**Portfolio** dataset). We can see slight differences for the coefficient estimates each time we repeat this procedure.

```{r}
set.seed(1)
boot.fn(Auto, sample(392, 392, replace = T))
boot.fn(Auto, sample(392, 392, replace = T))
```

Now, we use the `boot()` function to compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms.

```{r}
boot(Auto, boot.fn, 1000)
```

The results show that the bootstrap estimate for ${\rm SE}(\hat{\beta}_0)$ is $0.84$, and that the bootstrap estimate for ${\rm SE}(\hat{\beta}_1)$ is $0.0073$.  

How different are those estimates from those provided by fitting the model? Let's now compute the standard errors for the regression coefficients in a linear model (we use `summary` and then extract the coefficients using `$coef`)

```{r}
summary(lm(mpg ~ horsepower, data = Auto))$coef
```

The standard error estimates for $\hat{\beta}_0$ and $\hat{\beta}_1$ obtained from fitting the model using `lm()` are $0.717$ for the intercept and $0.0064$ for the slope. 

Interestingly, these are somewhat different from the estimates obtained using the bootstrap.  

Does this indicate a problem with the bootstrap? In fact, it suggests the opposite. Reflect on the formulae we covered earlier in the course and the assumptions on which these formulae rely. For example, they depend on the unknown parameter $\sigma^2$ (the noise variance) and so we estimate $\sigma^2$ using the RSS. Now although the formulas for the standard errors do not rely on the linear model being correct, the estimate for $\sigma^2$ does. If we create a scatterplot and examine the relationship between **mpg** and **horsepower**, we can see that there is a non-linear relationship and so the residuals from a linear fit will be inflated, and so will $\hat{\sigma}^2$.

```{r}
plot(Auto$mpg, Auto$horsepower)
```

Secondly, the standard formulas assume (somewhat unrealistically) that the $x_i$ are fixed, and all the variability comes from the variation in the errors $\epsilon_i$.   

The bootstrap approach **does not** rely on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of
$\hat{\beta}_0$ and $\hat{\beta}_1$ than is the `summary()` function.

Given the non-linear association betwen the two variables, let's now compute the bootstrap standard error estimates and the standard
linear regression estimates that result from fitting a *quadratic* model to the data.   

```{r}
boot.fn <- function(data, index)
  coef(
      lm(mpg ~ horsepower + I(horsepower^2), 
        data = data, subset = index)
    )

set.seed(1)
boot(Auto, boot.fn, 1000)
```

Since this model provides a good fit to the data, there is now a better correspondence between the bootstrap estimates and the standard estimates of ${\rm SE}(\hat{\beta}_0)$, ${\rm SE}(\hat{\beta}_1)$ and ${\rm SE}(\hat{\beta}_2)$.

```{r}
summary(lm(mpg ~ horsepower + I(horsepower^2), data = Auto))$coef
```


