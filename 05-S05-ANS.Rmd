---
editor_options:
  markdown:
    wrap: 72
---

# Answers {.unnumbered}

## Practical 1 {-}

*This practical is based on a demonstration created by Dr. Tatjana Kecojevic, Lecturer in Social Statistics.*

::: file
For the tasks below, you will require the **Salaries** dataset. This
dataset is part of the `carData` R package.

To access the dataset, load the `carData` package (make sure to first
install the package).  

You will also require the `GGally` package; please make sure to install it. 
:::

**Salaries** is a data frame with 397 observations. This dataset
consists of nine-month academic salary for Assistant Professors,
Associate Professors and Professors in a college in the U.S to monitor
salary differences between male and female faculty members. The data are
from 2008-09.

There are six variables:

|                   |                                                                              |
|-------------------|-----------------------------------------------------|
| **Variable Name** | **Variable Description**                                                     |
| rank              | a factor with levels = AssocProf, AsstProf, Prof                             |
| discipline        | a factor with levels A = theoretical departments) or B = applied departments |
| yrs.since.phd     | years since PhD                                                              |
| yrs.service       | years of service                                                             |
| sex               | a factor with levels Female and Male                                         |
| salary            | nine-month salary, in dollars.                                               |

Let's first load the packages:

```{r warning = FALSE, message = FALSE}
library(carData)
library(tidyverse)
library(GGally)
```

Once you load the `carData` package, the **Salaries** dataset will be
'loaded' too and can be accessed without needing to assign it to a
separate object.

```{r}
head(Salaries)
```

As usual, we can access variables within the dataset by indexing them.

```{r}
Salaries$salary
```

However, if we want to access variables within the dataset without
needing to index them we can use the base R `attach()` function.

```{r}
attach(Salaries)
```

So now, we can call on the variables from the dataset directly.

```{r}
salary
```

### Part I {-}


#### Exploring the data {-}

Let's begin by exploring the dataset.

```{r}
glimpse(Salaries)
```

We can see that **rank**, **discipline**, and **sex** are already coded
as factors. The variables **yrs.since.phd** and **yrs.service** are
coded as integers.

Our viewpoint states a belief that more years in service will cause
higher salary. Let us focus on the mechanics of fitting the model. First
we will examine the impact of each individual variable to see if our
view point is correct.

We start off with **salary** vs **yrs.since.phd**.

```{r}
summary(Salaries)
```

Both explanatory variables, **yrs.since.phd** and **yrs.service** have
mean and median values that are close to each other. However, the mean
and median for the **salary** variable are quite different.

We can better visualise this using boxplots.

```{r}
boxplot(Salaries[,3:4], col = c('brown1', 'steelblue'), main = "Distribution")
means <- sapply(Salaries[,3:4], mean)
points(means, col = "gray", pch = 22, lwd = 7)
```

```{r}
boxplot(salary, col = c('chartreuse4'), main = "Distributions")
means <- sapply(salary, mean)
points(means, col = "gray", pch = 22, lwd = 7)
```

:::question
What do the box plots indicate?
:::

:::answers
We notice that a number of observations are identified as the outliers
that are pulling the mean away from the median.
:::

#### Salary and Years since PhD {-}

Let's now consider the relationship between **yrs.since.phd** and **salary**
using a scatterplot onto which we add a line of best fit. Note that
since we 'attached' the dataset, we can call on the variables without
need to index or specify the dataset by name.

```{r}
plot(salary ~ yrs.since.phd, cex =.6, main = "The Relationship between Nine-month Salary and Years since PhD", xlab = "Years since PhD", ylab = "Nine-month Salary (dollars)")

model1 <- lm(salary ~ yrs.since.phd)

abline(model1, lty = 2, col = 2)
```


```{r}
summary(model1)
```

:::question
What do the results indicate?
:::

:::answers
The results show that there is a positive relationship between the
nine-month salary and years since PhD completion. The relationship is on
a weak side, with only 17.60% of variability in the response variable
**salary** being explained by the predictor **yrs.since.phd**.
:::

#### Salary and Years of Service {-}

Let's find out more about the relationship between nine-month salary and
years of service.

```{r}
plot(salary ~ yrs.service, cex =.6, main = "The Relationship between Nine-month Salary and Years of Service", xlab = "Years of Service", ylab = "Nine-month Salary (dollars)")

model2 <- lm(salary ~ yrs.service)

abline(model1, lty = 2, col = 2)
```

```{r}
summary(model2)
```

:::question
What do the plot and model results indicate?
:::

:::answers
The plot confirms our viewpoint and again we have a positive
relationship between salary and years of service. This variable explains
around 11 % of variability in the response variable.

Individually, the two variables do not seem to explain much of the
variability in the response.  
:::

#### The Model {-} 

Let's consider both variables (years of service and years since PhD) and whether these help explain salary. We define our multiple linear regression model as:  

$$y = b_0 + b_1x_1 + b_2x_2 + e$$

```{r}
mr_model <- lm(salary ~ yrs.since.phd + yrs.service)
summary(mr_model)
```

#### Test a): Does the fitted model make sense? {-}

*Do the estimated coefficients have the correct sign?*

The estimated model of best fit is:

$salary = 89912.2 + 1562.9yrs.since.phd ‚àí 629.1yrs.service$

We notice that when put together with the variable **yrs.since.phd**,
the **yrs.service** changes sign, which is not in line with our
previously drawn conclusion and the viewpoint. This is the result of
*collinearity*, which you already know happens when two predictors are
correlated with one another.

(Multi)collinearity can be identified when:

-   a regression coefficient $x_i$ is not significant even though,
    theoretically, it should be highly correlated with the response
    variable $y$;\
-   by adding or deleting an $x_i$ variable, the regression coefficients
    change dramatically;\
-   we get a negative regression coefficient when the response should
    increase along with $x_i$, or we get a positive regression
    coefficient when the response should decrease as $x_i$ increases;\
-   the explanatory variables have high pairwise correlations.

Removing one of the correlated explanatory variables usually doesn‚Äôt
drastically reduce the $R^2/R^2adj$.

With this model, using **yrs.since.phd** and **yrs.service** variables
we have managed to explain just over 18% of variation in the variable
**salary**.

#### Test b): Overall, is the model a good fit? {-}

$R^2adj$ is 18.42%, putting this model on the weaker side. However let
us go through the formal procedure and set the hypothesis below. The
null hypothesis of will be tested using the F-test:

-   $H_0:R^2=0$ (that is, the set of explanatory variables are
    insignificant, or in other words: useless)\
-   $H_1:R^2>0$ (that is, at least one explanatory variable is
    significant, or in other words: important)

The decision rule is:   

-   if $F_{calc} < F_{crit} => H_0$  
-   if $F_{calc} > F_{crit} => H_1$

Examining the sample evidence we get that $F_{calc} = 45.71$. The value
for $F_{crit}$ can be found in the statistical tables for $df1 = 2$ and
$df2 = 394$.

```{r}
qf(0.95, 2, 394)
```

Since $F_{crit} = 3.02 < F_{calc} => H_1$, this implies that this is a valid
model.

As pointed out earlier, this formal test involves a rather weak alternative hypothesis, which says only that $R^2$ is significantly bigger than 0. With $R^2$ of around 18% we can conclude that this is a useful model worthy of further investigation.  

#### Test c): Individually, are the explanatory variables important? {-}

Stage two of our model validation procedure is to examine the importance of any one single explanatory variable used in the fitted model. We have pointed out that just because a set of variables is important does not necessarily mean that each individual variable is contributing towards explaining the behaviour of $Y$.

We will conduct a set of t-tests to check the validity of each variable one at a time.   

**$b_1$: previously we concluded that the relationship between $x_1$ and $y$ is positive (in the fitted model parameter $b_1$  is positive). Consequently, we will use one tail t-test to assess the importance of $x_1$ in the model.**  

$H_0:b_1 = 0$ (explanatory variable $i$ is not important)  

$H_1:b_1 > 0$ (explanatory variable $i$ has a positive influence)

whereby: 

- If $t_{calc} < t_{crit} => H_0$   

- If $t_{calc} > t_{crit} => H_1$  

```{r}
qt(0.95, 394)
```

$t_{calc} = 6.09 > t_{crit} = 1.65 => H_1$, which implies that we need to keep x1 in the model. 


**$b_2$: previously we concluded that the relationship between $x_2$ and y is a positive relationship, but the model is suggesting that it is negative. We will stick to our belief and test if the coefficient should be positive:**

$H_0:b_2 = 0$ (explanatory variable $i$ is not important)  

$H_1:b_2 > 0$ (explanatory variable $i$ has a positive influence)  

whereby: 

- If $t_{calc} < t_{crit} => H_0$   

- If $t_{calc} > t_{crit} => H_1$  

```{r}
qt(0.95, 394)
```

$t_{calc} = ‚àí2.47 < t_{crit} = 1.65 => H_0$ therefore, the variable should be removed from the model.

The increase in the explain variation of around 1% is negligible in comparison to the best one factor model $salary = f(yrs.since.phd) + e$. Hence, we will put forward the model $salary = 91719 + 985yrs.since.phd$ as our best fitted model.  

Alternatively you could test for the coefficient not being equal to zero and make a conclusion for yourself if this would be a sensible thing to do.  

In this example, we have adopted a 'standard' regression approach that assumes modelling a relationship between quantitative response and only quantitative predictors. However, often when building multiple regression models, we do not want to be limited to just quantitative predictors.   

### Part II {-}

Now let's expand our multiple linear regression model with two *quantitative* variables to a model that also includes *categorical* variables. 

```{r}
# if you are starting a fresh R session, don't forget to:

# load the package
library(carData)

# attach the dataset
attach(Salaries)
```

In many datasets, categorical (attribute) variables are usually encoded numerically and are accompanied by information about the levels of the variable saved in the levels attribute.

Let's consider the **sex** variable.

```{r}
attributes(sex)
```

This variable is already coded as a factor with two levels, *Female* and *Male* (which you should already know from earlier in the demonstration). Now, what if we want to transform a variable of class `factor` into one of class `integer`? 

```{r}
unclass(sex)
```

We can easily do so with the `unclass()` function which removes the attributes of a factor variable and transforms the levels into numeric values. 

However, when using factor variable in a linear regression model, it would make no sense to treat it as a *quantitative* explanatory variable. In the context of linear modelling we need to code each category to represent factor levels. Two-level attribute variables are very easy to code. We simply create an indicator or dummy variable that takes on two possible dummy numerical values. Consider the **sex** variable.   

We can code this using a dummy variable $d$:\

$$
d = \begin{cases} 
0, & \text{if female} \\
1, & \text{if male} 
\end{cases}
$$
üí° This is the default coding used in R. A zero value is assigned to the level which is first alphabetically, unless it is changed by using the `releveld()` function for example, or by specifying the levels of the factor variable specifically.  


So, for a simple regression model predicting nine-month salary using one categorical variable:

$$salary = b_0 + b_1sex + e$$  
the model is specified as follows: 

$$salary_i = b_0 + b_1 sex_i + e_i = 
\begin{cases} 
b_0 + b_1 \times 1 + e_i = b_0 + b_1 + e_i, & \text{if the person is male} \\
b_0 + b_1 \times 0 + e_i = b_0 + e_i, & \text{if the person is female}
\end{cases}$$  

where $b_0$ can be interpreted as the average nine-month salary for females, and $b_0 + b_1$  as the nine-month average salary for males. The value of $b_1$ represents the average difference in nine-month salary between females and males.

We can conclude that dealing with an attribute variable with two levels in a linear model is straightforward. In this case, a dummy variable indicates whether an observation has a particular characteristic: yes/no. We can observe it as a 'switch' in a model, as this dummy variable can only assume the values $0$ and $1$, where $0$ indicates the absence of the effect, and $1$ indicates the presence. The values **0/1** can be seen as **off/on**.

The way in which R codes dummy variables is controlled by the `contrasts` option:

```{r}
options("contrasts")
```

The output points out the conversion of the factor into an appropriate set of contrasts. In particular, the first one: for unordered factors, and the second one: the ordered factors. The former is applicable in our context. To explicitly identify the coding of the factor, i.e. dummy variable used by R, we can use the `contrasts()` function.

```{r}
contrasts(sex)
```

Note that applied `contr.treatment` conversion takes only the value $0$ or $1$ and that for an attribute variable with $k$ levels it will create $k-1$ dummy variables. There are many different ways of coding attribute variables besides the dummy variable approach explained here. All of these different approaches lead to equivalent model fits. What differs are the coefficients (i.e. model parameters as they require different interpretations, arranged to measure particular contrasts). This 0/1 coding implemented in R‚Äôs default `contr.treatment` contrast offers straightforward interpretation of the associated parameter in the model, which often is not the case when implementing other contrasts.

#### Interpreting coefficients of attribute variables {-}

In the case of measured predictors, we are comfortable with the interpretation of the linear model coefficient as a slope, which tells us what a unit increase in the response variable is (i.e. outcome per unit increase in the explanatory variable). This is not necessarily the right interpretation for attribute predictors.

Let's consider average nine-month salary values for males and females separately. 

```{r}
Salaries %>% 
  select(salary, sex) %>%   
  group_by(sex) %>% 
  summarise(mean=mean(salary))
```

If we obtain the mean salary for each sex group we will find that for female professors the average salary is $ \$101,002$ and for male professors the average is $ \$115,090$. That is, a difference of $\$14,088$. 

If we now look at the parameters of the regression model for salary vs sex where females are coded as zero and males as one, we get exactly the same information, implying that the coefficient is the estimated difference in average between the two groups.

```{r}
lm(salary ~  sex)
```

#### Fitting a Multivariate Regression Model {-}

In Part I, we explored the extent to which variation in the response variable **salary** is associated with variation in years since PhD and years in service. 
Now, we extend the model to also include **sex**, **discipline** and **rank**. 
The overall goals of any model we construct is that it should contain enough to explain relations in the data and at the same time be simple enough to understand, explain to others, and use.  

For convenience we will adopt the following notation:  

$y$: salary  
$x_1$: yrs.since.phd  
$x_2$: yrs.service  
$x_3$: discipline  
$x_4$: sex  
$x_5$: rank  

Next, we need to specify the model that embodies our mechanistic understanding of the factors involved and the way that they are related to the response variable. It would make sense to expect that all of the available x variables may impact the behaviour of y, thus the model we wish to build should reflect our viewpoint, i.e. $y=f(x_1,x_2,x_3,x_4,x_5)$: 

$$y=b_0 + b_1x_1 + b_2x_2 + b_3x_3 + b_4x_4 + b_5x_5 + e$$
Our viewpoint states a belief that all explanatory variables have a positive impact on the response. For example, more years in service will cause a higher salary.  

Our objective now is to determine the values of the parameters in the model that lead to the best fit of the model to the data. That is, we are not only trying to estimate the parameters of the model, but we are also seeking the minimal adequate model to describe the data.  

The best model is the model that produces the least unexplained variation following the principle of parsimony rather than complexity. That is the model should have as few parameters as possible, subject to the constraint that the parameters in the model should all be statistically significant.   

For regression modelling in R we use the lm() function, that fits a linear model assuming normal errors and constant variance. We specify the model by a formula that uses arithmetic operators which enable different functionalities from their ordinary ones. But, before we dive into statistical modelling of the given data, we need to take a first step and conduct the most fundamental task of data analysis procedure: **Get to Know Our Data**.  

```{r}
ggpairs(Salaries)
```

::: question
What information can you extract from this visualisation?
:::

:::answers
This is an information rich visualisation that includes pairwise relationships of all the variables we want to consider for our model. By focusing on the last column of the plots, we can notice influence from all explanatory variables onto the response, except maybe for discipline and sex. We also notice unbalanced representation of the groups for the variables rank and sex, but for the purpose of our practice in fitting a multi-factor model this isn‚Äôt too problematic. We need to be especially concerned with the extent of correlations between the explanatory variables, and what is of particular interest to us is the high multicollinearity between rank, yrs.since.phd and yrs.service, which happens when the variables are highly linearly related. As a consequence, we will need to keep an eye on the significance of using all of these variables in the model.
:::

#### Fitting the Model {-}

There are no fixed rules when fitting linear models, but there are adopted standards that have proven to work well in practice. We start off by fitting a maximal model then we carry on simplifying it by removing non-significant explanatory variables. This needs to be done with caution, making sure that the simplifications make good scientific sense, and do not lead to significant reductions in explanatory power. Although this should be the adopted strategy for fitting a model, it is not a guarantee to finding all the important structures in a complex data frame.  

We can summarise our model building procedure algorithm as follows:   

1. Fit the maximal model that includes all the variables. Then, assess the overall significance of the model by checking how big the $R^2/\overline{R}^2$ is. If statistically significant, carry on with the model fitting procedure, otherwise stop (F-test).   
2. Remove the least significant terms one at a time. Then, check the $t_calculated$ for the variables values and perform a one tail or two tail t-test depending on your prior view. If the deletion causes an insignificant increase in $\overline{R}^2$, leave that term out of the model.  
3. Keep removing terms from the model until the model contains nothing but significant terms.   

Let's build the model. Now, if we plan to use all variables in a dataset, there is no need to write the names of each individual predictor. Instead, we can use a full stop which tell R to include all other variables in the data object that do not already appear in the formula.   

```{r}
model_1 <- lm(salary ~ ., data = Salaries)

summary(model_1)
```

:::question
**Overall, is the model a good fit? How big is the $R^2/\overline{R}^2$?**  
:::

::: answers
The $R^2 = 45.47%$ and the $\overline{R}^2= 44.63%$ are well above the value of zero allowing us to accept this as a valid model without having to formally test it to assess its statistical significance. It manages to explain almost half of the variability in the response variable **salary**.   
:::

:::question
**Individually, are the explanatory variables important? What steps are required given the results of the model?**
:::

We identify the **sex** variable as clearly not significant, which is in line with the conclusion we could draw from the boxplot in the pairwise comparison plot for **salary** vs. **sex**. We will remove it to begin the process of model simplification and remove the least significant term. We therefore re-fit (or 'update') the model without the **sex** variable.

```{r}
model_2 <- update(model_1,~. - sex) 

summary(model_2)
```

We note a slight reduction in $\overline{R}^2$ from $44.63%$ to $44.55%$ which we can regard as an insignificant decrease. The next step is to check the coefficients and assess for the effect of the remaining variables. We identify **yrs.since.phd** and **yrs.service** as the least influential in explaining the variability of salary. To illustrate how to formally assess their effect, we will conduct the t-test for the **yrs.since.phd** variable:  

$H_0:b_{ysp} = 0$  

$H_1:b_{ysp} > 0$

Therefore:   

If $t_{calc} < t_{crit} => H_0$  

If $t_{calc} > t_{crit} => H_1$

```{r}
qt(0.95, 391)
```

As $t_{calc} = 2.217 > t_{crit} = 1.64876 => H1$, we will keep the remaining variable and stop with the model simplification and focus on its interpretation. 

:::question
Specify the final fitted model.
:::
The structure of our final fitted model is:  

$$y=b_0 + b_1x_1 + b_2x_2 + b_3x_3 + b_4x_4 + e$$  
where: 
$y$: salary  
$x_1$: yrs.since.phd  
$x_2$: yrs.service  
$x_3$: discipline  
$x_4$: rank  

We can take a closer look at the coefficients of our fitted model:  

```{r}
coef(model_2)
```

Examining the output we realise that R has created three dummy variables for the variable **rank**:  
$$
dr_1 = \begin{cases} 
1 & \text{rank is AsstProf} \\
0 & \text{for rank is not AsstProf}
\end{cases}
$$

$$
dr_2 = \begin{cases} 
1 & \text{rank is AssocProf} \\
0 & \text{rank is not AssocProf}
\end{cases}
$$

$$
dr_3 = \begin{cases} 
1 & \text{rank is Prof} \\
0 & \text{rank is not Prof}
\end{cases}
$$
Therefore, R has chosen to use the model:  
$$y = b_0 + b_1dr_2 + b_2dr_3 + b_3d_1 + b_4x_1 + b_5x_2 + e$$  
where:
- $y$ is salary  
- $x_1$ is yrs.since.phd  
- $x_2$ is yrs.service  
- $dr_2$ and $dr_3$ are the dummy variables defined above for the purpose of coding variable rank  
- $d_1$ is a dummy variable used in the coding of variable discipline as explained earlier  

Note that R doesn‚Äôt need to use $dr_1$ to create three models; it only needs two dummy variables since it is using $dr_1$  as a reference level, also known as the base line. This subsequently allows R to create three models relating to the **rank** variable:  

- AsstProf: $y = b_0 + b_3d_1 + b_4x_1 + b_5x_2 + e$     
- AssocProf: $y = (b_0 + b_1) + b_3d_1 + b_4x_1 + b_5x_2 + e$     
- Prof: $y = (b_0 + b_2) + b_3d_1 + b_4x_1 + b_5x_2 + e$  

telling us that:  

- $b_0$ is the average salary for an Assistant Professor who works in a 'theoretical' department and $b_0 + b_3$ the average salary for an Assistant Professor who works in an 'applied' department.
- $(b_0 + b_1)$ is the average salary for an Associate Professor who works in a 'theoretical' department and $(b_0 + b_1) + b_3$ the average salary for an Associate Professor who works in an 'applied' department.  
- $(b_0 + b_2)$ is the average salary for a Professor who works in a 'theoretical' department and $(b_0 + b_2) + b_3$ the average salary for a Professor who works in an 'applied' department.  

:::question
Interpret the results
:::

Learning this we can make an interpretation of our final fitted model as follows:

For every year since PhD (yrs.since.phd) on average salary (salary) will go up by $534.63 assuming the rest of the variables are fixed in the model.   

For every year in service (yrs.service) on average salary (salary) will go down by $476.72 assuming the rest of the variables are fixed in the model.  

The average salary of an Assistant Professor (rank: AsstProf) who works in a ‚Äútheoretical‚Äù department is $\$69,869.01$ and who works in an ‚Äúapplied‚Äù department is $\$84,374.16$; this can vary for the number of years in service and since PhD.    
The average salary of an Associate Professor (rank: AssocProf) who works in a 'theoretical' department is $\$82,700.55$, and one who works in an 'applied' department is $\$97,205.70$; this can vary for the number of years in service and since PhD.   

The average salary of a Professor (rank: Prof) who works in a 'theoretical' department is $\$115,156.70$, and who works in an 'applied' department is $\$129,661.90$; this can vary for the number of years in service and since PhD. 

This model explains around 45% of the variability in the response variable **salary**.   

Adding `~ 0` to the `lm()` formula enables R to suppress the intercept. Note that if we remove the intercept, then we can directly obtain all ‚Äúthree intercepts‚Äù without a base level to fit the final fitted model:  

```{r}
model_2_1 <- lm(salary ~  0 + rank + discipline + yrs.since.phd + yrs.service)

summary(model_2_1)
```



## Practical 2 {-}

*This practical has been developed by Dr. Tatjana Kecojevic, Lecturer in Social Statistics.*

::: file
For the tasks below, you will require the **FDI** dataset.  

Click here to download the file:
<a href="data/FDI.csv" download="FDI.csv"> FDI.csv </a>.

Remember to place your data file in a separate subfolder within your R
project working directory.
:::

A business consultancy firm is compiling a major report about
globalisation. One aspect of this study concerns the determinants of FDI
undertaken by multi-national enterprises. Relevant information from a
sample of 60 multi-national companies that undertook significant
investment in overseas projects was made available as follows:

+---------------+----------------------------------------------------+
| Variable Name | Variable Description                               |
+:=============:+:==================================================:+
| FDI           | Value of FDI undertaken, in ¬£ millions, by          |
|               | investing company                                  |
+---------------+----------------------------------------------------+
| GDP_Cap       | GDP per capita, ¬£000s, in the country receiving    |
|               | the investment                                     |
+---------------+----------------------------------------------------+
| Gr_rate       | The economic growth rate, in %-terms, in the       |
|               | country receiving the investment                   |
+---------------+----------------------------------------------------+
| ROC           | The average return on capital invested, in         |
|               | %-terms, in the country receiving the investment   |
+---------------+----------------------------------------------------+
| Stable        | The political stability of the country receiving   |
|               | the investment as measured by the number of        |
|               | changes in government over the past 25 years       |
+---------------+----------------------------------------------------+
| Infra         | Infrastructure facilities (eg transport,           |
|               | communications) in the country receiving the       |
|               | investment                                         |
|               |                                                    |
|               | Coded: 1 = basic infrastructure 2 = good           |
|               | infrastructure                                     |
+---------------+----------------------------------------------------+
| Trade         | The openness to trade of the country receiving the |
|               | investment                                         |
|               |                                                    |
|               | Coded: 1 = trade tightly controlled 2 = some       |
|               | restrictions on trade 3 = free trade               |
+---------------+----------------------------------------------------+

This is a multiple regression type of the problem; FDI is the key
response variable as this study concerns the determinants of FDI
undertaken by multi-national enterprises.

The model is defined as $Y = b_0 + b_1x_1 + b_2x_2 + ... + b_kx_k + e$,
for the general $k$ explanatory variable model and where e is also known
as the error term $e ‚àº N(0,\sigma^2)$, with the error term from a normal
distribution with a mean of $0$, and a variance of $\sigma^2$.

Based on prior knowledge, we make the assumption that GDP_Cap, Gr_rate,
ROC, Infra, and Trade have positive relationships with FDI, whilst
Stable has a negative relationship with FDI.

We will use our best fit model to predict FDI for the following information:  *country X receiving the investment has GDP per capita of 11.1 and Gr_rate per capita of 3.05; The average return on capital invested is 20.5%; There were 11 changes of government over the past 25 years and country X has good infrastructure with some restrictions on trade.*

First, let's load the required packages: 

```{r warning = FALSE, message = FALSE}
library(tidyverse)
# you should have already installed this package as part of the previous Demonstration
library(GGally)
```

Let's import the data into R.

```{r}
mydataq1 <- read.csv("data/FDI.csv", header = T) 
```

Now let's get a glimpse of the data. As you know, there are many ways to
do that, such as, for example, using the tidyverse `glimpse` function.
This is quite a handy function because it also tells us more about the
class of each variable. We can see that although **Infra** and **Trade**
categorical, these are coded as integers.

```{r}
glimpse(mydataq1)
```

We therefore need to transform them into factors.

```{r}
mydataq1 <- mydataq1 %>%
  mutate(Infra = as_factor(Infra),
         Trade = as_factor(Trade)
         )
```

We can then explore all variables in the dataset as pairs using a matrix
of plots. Among many interesting features, we can note quite strong
correlations among pairs of variables which suggest the presence of
multicollinearity: GDP_Cap and ROC, Infra and GDP_Cap, Infra and ROC,
and Infra and Gr_rate.

```{r warning = FALSE, message = FALSE}
GGally::ggpairs(mydataq1)
```

Ok, so our initial model is:

$FDI = b_0 + b_1GDP\_Cap + b_2Gr\_rate + b_3ROC ‚Äì b_4Stable + b_5Infra + b_6Trade + e$

where **Infra** and **Stable** are dummy variables.

We can have a look at how these two dummy variables are used in the
model by using the `contrasts()` function from base R.

The **Infra** variable is coded as *1 = basic infrastructure* and *2 = good infrastructure*. Since this is a binary variable, there will be one
reference category and a single dummy variable.

```{r}
contrasts(mydataq1$Infra)
```

The **Trade** variable is coded as *1 = trade tightly* and *2 = some
restrictions on trade* and *2 = some restrictions on trade*. Since this
is variable with three categories, there will be one reference category
and two dummy variables.

```{r}
contrasts(mydataq1$Trade)
```

Let's now fit our multiple regression model.

```{r}
m1 <- lm(FDI ~ GDP_Cap + Gr_rate + ROC + Stable + Infra + Trade, data = mydataq1)
```

And explore the results.

```{r}
summary(m1)
```

:::question
What do the results indicate?   
How do they compare with our initial assumptions?  
How do you proceed with the analysis?
:::

The results provide us with several pieces of important information. We
initially assumed that the relationship between Gr_rate and FDI and
Infra and FDI are positive. However, we can see that the estimated
coefficients are negative. Also, there are several variables that are
not statistically significant. The **ROC** variable has the highest
p-value. Overall, the model appears to be a good fit given that 74.81% of variability is being explained. Also, make a note of the adjusted r-squared value which is 71.42 % (as the name implies, this measure adjusts the r-squared value according to the number of predictors in the model).  

Ok, so given the evidence of multicollinearity from earlier and given
the lack of statistical significance, we can proceed to remove the
variable with the largest p-value (so the **ROC**) variable and refit
the model.

```{r}
m2 <- lm(FDI ~ GDP_Cap + Gr_rate + Stable + Infra + Trade, data = mydataq1)

# or 

m2 <- update(m1,~. - ROC, data = mydataq1) 
```

Now, we see that the explained variability is the same as in model 1 (74.81%). However, the adjusted R-squared has increased slightly (from 71.4% to about 72%).

```{r}
summary(m2)
```

We can observe that the **Infra** variable has the largest p-value and,
as before with the **ROC** variable, we remove it and refit the model by
removing the least significant term.

```{r}
m3 <- update(m2,~. - Infra, data = mydataq1) 
```

The r-squared value has decreased slightly from 74.81% to 74.8. But, again, we see that the adjusted R-squared increased from about 72% in model 2 to about 72.5%.

```{r}
summary(m3)
```

The **Gr_rate** variable has the largest p-value and, as before, we
remove it and refit the model by removing the least significant term.

```{r}
m4 <- update(m3,~. - Gr_rate, data = mydataq1) 
```

Finally, we obtain a model where all coefficients are statistically
significant (although the **Stable** is significant at an $\alpha$ level
of 0.05). We see that the r-squared value has again decreased slightly to 74.79% but the explained variability is highest for this model, about 73% (the adjusted r-squared penalises the addition of predictors that are non-significant; since we removed these, the value increased). 

Overall, the explained variability is still about 75% (the decrease across the models was extremely small). 

```{r}
summary(m4)
```

Now, we can specify the model as:

$$
\begin{align*}
FDI &= 189.4274 + 0.9109 \cdot GDP\_Cap - 0.5411 \cdot stable + 0.0000 \cdot Trade1 \\
    &\phantom{= 189.4274 + 0.9109 \cdot GDP\_Cap - 0.5411 \cdot stable} + 4.8837 \cdot Trade2 \\
    &\phantom{= 189.4274 + 0.9109 \cdot GDP\_Cap - 0.5411 \cdot stable} + 5.9368 \cdot Trade3
\end{align*}
$$


Finally, we use our best fit model to predict FDI for the following information: *country X receiving the investment has GDP per capita of 11.1 and Gr_rate per capita of 3.05; The average return on capital invested is 20.5%; There were 11 changes of government over the past 25 years and country X has good infrastructure with some restrictions on trade.*   

$FDI = 189.4274 + 0.9109*11.1 - 0.5411*11 + 4.8837$

```{r}
189.4274 + 0.9109*11.1 - 0.5411*11 + 4.8837
```

Given that our final model (model 4) explains about 75% of the variability, we can conclude that our prediction of 198.47 is a fairly good one. 
