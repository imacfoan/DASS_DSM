<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>SOST70033 Data Science Modelling</title>
  <meta name="description" content="<p>Notebook hosting practical materials for SOST70033.</p>" />
  <meta name="generator" content="bookdown #bookdown:version# and GitBook 2.6.7" />

  <meta property="og:title" content="SOST70033 Data Science Modelling" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>Notebook hosting practical materials for SOST70033.</p>" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="SOST70033 Data Science Modelling" />
  
  <meta name="twitter:description" content="<p>Notebook hosting practical materials for SOST70033.</p>" />
  

<meta name="author" content="Dr. Ioana Macoveciuc" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



<!--bookdown:title:start-->
<div id="header">
<h1 class="title">SOST70033 Data Science Modelling</h1>
<p class="author"><em>Dr. Ioana Macoveciuc</em></p>
</div>
<!--bookdown:title:end-->

<!--bookdown:toc:start-->
  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">
<!--bookdown:toc2:start-->
<ul>
<li><a href="#about" id="toc-about">About</a></li>
<li><a href="#part-section-1" id="toc-part-section-1">(PART*) Section 1</a></li>
<li><a href="#overview" id="toc-overview">Overview</a></li>
<li><a href="#demonstration-a-more-in-depth-consideration-of-model-accuracy" id="toc-demonstration-a-more-in-depth-consideration-of-model-accuracy">Demonstration: A more in-depth consideration of model accuracy</a>
<ul>
<li><a href="#the-simulation" id="toc-the-simulation">The Simulation</a></li>
</ul></li>
<li><a href="#practical-1" id="toc-practical-1">Practical 1</a>
<ul>
<li><a href="#task-1" id="toc-task-1">Task 1</a></li>
<li><a href="#task-2" id="toc-task-2">Task 2</a></li>
<li><a href="#task-3" id="toc-task-3">Task 3</a></li>
<li><a href="#task-4" id="toc-task-4">Task 4</a></li>
<li><a href="#task-5" id="toc-task-5">Task 5</a></li>
<li><a href="#task-6" id="toc-task-6">Task 6</a></li>
<li><a href="#task-7" id="toc-task-7">Task 7</a></li>
<li><a href="#task-8" id="toc-task-8">Task 8</a></li>
<li><a href="#task-9" id="toc-task-9">Task 9</a></li>
<li><a href="#task-10" id="toc-task-10">Task 10</a></li>
<li><a href="#task-11" id="toc-task-11">Task 11</a></li>
</ul></li>
<li><a href="#practical-2" id="toc-practical-2">Practical 2</a>
<ul>
<li><a href="#task-1-1" id="toc-task-1-1">Task 1</a></li>
<li><a href="#task-2-1" id="toc-task-2-1">Task 2</a></li>
<li><a href="#task-3-1" id="toc-task-3-1">Task 3</a></li>
<li><a href="#task-4-1" id="toc-task-4-1">Task 4</a></li>
<li><a href="#task-5-1" id="toc-task-5-1">Task 5</a></li>
<li><a href="#task-6-1" id="toc-task-6-1">Task 6</a></li>
</ul></li>
<li><a href="#answers" id="toc-answers">Answers</a>
<ul>
<li><a href="#practical-1-1" id="toc-practical-1-1">Practical 1</a>
<ul>
<li><a href="#task-1-2" id="toc-task-1-2">Task 1</a></li>
<li><a href="#task-2-2" id="toc-task-2-2">Task 2</a></li>
<li><a href="#task-3-2" id="toc-task-3-2">Task 3</a></li>
<li><a href="#task-4-2" id="toc-task-4-2">Task 4</a></li>
<li><a href="#task-5-2" id="toc-task-5-2">Task 5</a></li>
<li><a href="#task-6-2" id="toc-task-6-2">Task 6</a></li>
<li><a href="#task-7-1" id="toc-task-7-1">Task 7</a></li>
<li><a href="#task-8-1" id="toc-task-8-1">Task 8</a></li>
<li><a href="#task-9-1" id="toc-task-9-1">Task 9</a></li>
<li><a href="#task-10-1" id="toc-task-10-1">Task 10</a></li>
<li><a href="#task-11-1" id="toc-task-11-1">Task 11</a></li>
</ul></li>
<li><a href="#practical-2-1" id="toc-practical-2-1">Practical 2</a>
<ul>
<li><a href="#task-1-3" id="toc-task-1-3">Task 1</a></li>
<li><a href="#task-2-3" id="toc-task-2-3">Task 2</a></li>
<li><a href="#task-3-3" id="toc-task-3-3">Task 3</a></li>
<li><a href="#task-4-3" id="toc-task-4-3">Task 4</a></li>
<li><a href="#task-5-3" id="toc-task-5-3">Task 5</a></li>
<li><a href="#task-6-3" id="toc-task-6-3">Task 6</a></li>
</ul></li>
</ul></li>
<li><a href="#part-section-2" id="toc-part-section-2">(PART*) Section 2</a></li>
<li><a href="#overview-1" id="toc-overview-1">Overview</a></li>
<li><a href="#demonstration-1" id="toc-demonstration-1">Demonstration 1</a>
<ul>
<li><a href="#simple-linear-models-without-intercept" id="toc-simple-linear-models-without-intercept">Simple Linear Models Without Intercept</a></li>
<li><a href="#simple-linear-models-with-intercept" id="toc-simple-linear-models-with-intercept">Simple Linear Models with Intercept</a></li>
</ul></li>
<li><a href="#demonstration-2" id="toc-demonstration-2">Demonstration 2</a>
<ul>
<li><a href="#population-parameters-and-estimated-coefficients" id="toc-population-parameters-and-estimated-coefficients">Population Parameters and Estimated Coefficients</a></li>
<li><a href="#what-happens-if-we-reduce-noise" id="toc-what-happens-if-we-reduce-noise">What happens if we <em>reduce</em> noise?</a></li>
<li><a href="#what-happens-if-we-increase-noise" id="toc-what-happens-if-we-increase-noise">What happens if we <em>increase</em> noise?</a></li>
<li><a href="#how-does-noise-affect-confidence-intervals-for-the-coefficients" id="toc-how-does-noise-affect-confidence-intervals-for-the-coefficients">How does noise affect confidence intervals for the coefficients?</a></li>
</ul></li>
<li><a href="#practical-1-2" id="toc-practical-1-2">Practical 1</a>
<ul>
<li><a href="#task-1-4" id="toc-task-1-4">Task 1</a></li>
<li><a href="#task-2-4" id="toc-task-2-4">Task 2</a></li>
<li><a href="#task-3-4" id="toc-task-3-4">Task 3</a></li>
<li><a href="#task-4-4" id="toc-task-4-4">Task 4</a></li>
<li><a href="#task-5-4" id="toc-task-5-4">Task 5</a></li>
<li><a href="#task-6-4" id="toc-task-6-4">Task 6</a></li>
<li><a href="#task-7-2" id="toc-task-7-2">Task 7</a></li>
<li><a href="#task-8-2" id="toc-task-8-2">Task 8</a></li>
<li><a href="#task-9-2" id="toc-task-9-2">Task 9</a></li>
<li><a href="#task-10-2" id="toc-task-10-2">Task 10</a></li>
<li><a href="#task-11-2" id="toc-task-11-2">Task 11</a></li>
<li><a href="#task-12" id="toc-task-12">Task 12</a></li>
<li><a href="#task-13" id="toc-task-13">Task 13</a></li>
<li><a href="#task-14" id="toc-task-14">Task 14</a></li>
<li><a href="#task-15" id="toc-task-15">Task 15</a></li>
<li><a href="#task-16" id="toc-task-16">Task 16</a></li>
</ul></li>
<li><a href="#practical-2-2" id="toc-practical-2-2">Practical 2</a>
<ul>
<li><a href="#task-1-5" id="toc-task-1-5">Task 1</a></li>
<li><a href="#task-2-5" id="toc-task-2-5">Task 2</a></li>
<li><a href="#task-3-5" id="toc-task-3-5">Task 3</a></li>
<li><a href="#task-4-5" id="toc-task-4-5">Task 4</a></li>
<li><a href="#task-5-5" id="toc-task-5-5">Task 5</a></li>
</ul></li>
<li><a href="#practical-3-the-quality-of-red-bordeaux-vintages" id="toc-practical-3-the-quality-of-red-bordeaux-vintages">Practical 3: The Quality of Red Bordeaux Vintages</a></li>
<li><a href="#answers-1" id="toc-answers-1">Answers</a>
<ul>
<li><a href="#practical-1-3" id="toc-practical-1-3">Practical 1</a>
<ul>
<li><a href="#task-1-6" id="toc-task-1-6">Task 1</a></li>
<li><a href="#task-2-6" id="toc-task-2-6">Task 2</a></li>
<li><a href="#task-3-6" id="toc-task-3-6">Task 3</a></li>
<li><a href="#task-4-6" id="toc-task-4-6">Task 4</a></li>
<li><a href="#task-5-6" id="toc-task-5-6">Task 5</a></li>
<li><a href="#task-6-5" id="toc-task-6-5">Task 6</a></li>
<li><a href="#task-7-3" id="toc-task-7-3">Task 7</a></li>
<li><a href="#task-8-3" id="toc-task-8-3">Task 8</a></li>
<li><a href="#task-9-3" id="toc-task-9-3">Task 9</a></li>
<li><a href="#task-10-3" id="toc-task-10-3">Task 10</a></li>
<li><a href="#task-11-3" id="toc-task-11-3">Task 11</a></li>
<li><a href="#task-12-1" id="toc-task-12-1">Task 12</a></li>
<li><a href="#task-13-1" id="toc-task-13-1">Task 13</a></li>
<li><a href="#task-14-1" id="toc-task-14-1">Task 14</a></li>
<li><a href="#task-15-1" id="toc-task-15-1">Task 15</a></li>
<li><a href="#task-16-1" id="toc-task-16-1">Task 16</a></li>
</ul></li>
<li><a href="#practical-2-3" id="toc-practical-2-3">Practical 2</a>
<ul>
<li><a href="#task-1-7" id="toc-task-1-7">Task 1</a></li>
<li><a href="#task-2-7" id="toc-task-2-7">Task 2</a></li>
<li><a href="#task-3-7" id="toc-task-3-7">Task 3</a></li>
<li><a href="#task-4-7" id="toc-task-4-7">Task 4</a></li>
<li><a href="#task-5-7" id="toc-task-5-7">Task 5</a></li>
</ul></li>
<li><a href="#practical-3-the-quality-of-red-bordeaux-vintages-1" id="toc-practical-3-the-quality-of-red-bordeaux-vintages-1">Practical 3: The Quality of Red Bordeaux Vintages</a></li>
</ul></li>
<li><a href="#part-section-3" id="toc-part-section-3">(PART*) Section 3</a></li>
<li><a href="#overview-2" id="toc-overview-2">Overview</a></li>
<li><a href="#demonstration-1-classification-problems" id="toc-demonstration-1-classification-problems">Demonstration 1: Classification Problems</a>
<ul>
<li><a href="#dataset-and-variables" id="toc-dataset-and-variables">Dataset and Variables</a></li>
<li><a href="#correlation-matrix-and-plot" id="toc-correlation-matrix-and-plot">Correlation Matrix and Plot</a></li>
<li><a href="#classic-logistic-regression" id="toc-classic-logistic-regression">“Classic” Logistic Regression</a>
<ul>
<li><a href="#confusion-matrix" id="toc-confusion-matrix">Confusion Matrix</a></li>
</ul></li>
<li><a href="#logistic-regression-in-statistical-learning" id="toc-logistic-regression-in-statistical-learning">Logistic Regression in Statistical Learning</a></li>
<li><a href="#linear-discriminant-analysis" id="toc-linear-discriminant-analysis">Linear Discriminant Analysis</a></li>
<li><a href="#quadratic-discriminant-analysis" id="toc-quadratic-discriminant-analysis">Quadratic Discriminant Analysis</a></li>
<li><a href="#k-nearest-neighbours" id="toc-k-nearest-neighbours"><span class="math inline">\(K\)</span>-nearest neighbours</a></li>
<li><a href="#naive-bayes" id="toc-naive-bayes">Naive Bayes</a></li>
</ul></li>
<li><a href="#demonstration-2-poisson-versus-linear-regression" id="toc-demonstration-2-poisson-versus-linear-regression">Demonstration 2: Poisson versus Linear Regression</a>
<ul>
<li><a href="#the-linear-model" id="toc-the-linear-model">The Linear Model</a>
<ul>
<li><a href="#approaches-to-variable-coding" id="toc-approaches-to-variable-coding">Approaches to variable coding</a></li>
<li><a href="#plotting-coefficient-estimates" id="toc-plotting-coefficient-estimates">Plotting Coefficient Estimates</a></li>
</ul></li>
<li><a href="#the-poisson-model" id="toc-the-poisson-model">The Poisson Model</a></li>
<li><a href="#linear-versus-poisson-regression" id="toc-linear-versus-poisson-regression">Linear versus Poisson Regression</a></li>
</ul></li>
<li><a href="#practical-predicting-a-companys-bankruptcy" id="toc-practical-predicting-a-companys-bankruptcy">Practical: Predicting a company’s bankruptcy</a>
<ul>
<li><a href="#data-and-variables" id="toc-data-and-variables">Data and Variables</a></li>
<li><a href="#importing-the-data" id="toc-importing-the-data">Importing the data</a></li>
<li><a href="#loading-required-packages" id="toc-loading-required-packages">Loading required packages</a></li>
<li><a href="#correlation-matrix-and-plot-1" id="toc-correlation-matrix-and-plot-1">Correlation Matrix and Plot</a></li>
<li><a href="#logistic-regression" id="toc-logistic-regression">Logistic Regression</a></li>
<li><a href="#explaining-the-logit" id="toc-explaining-the-logit">Explaining the Logit</a></li>
<li><a href="#tasks-linear-discriminant-analysis" id="toc-tasks-linear-discriminant-analysis">Tasks: Linear Discriminant Analysis</a>
<ul>
<li><a href="#task-1-8" id="toc-task-1-8">Task 1</a></li>
<li><a href="#task-2-8" id="toc-task-2-8">Task 2</a></li>
<li><a href="#task-3-8" id="toc-task-3-8">Task 3</a></li>
</ul></li>
<li><a href="#tasks-quadratic-discriminant-analysis" id="toc-tasks-quadratic-discriminant-analysis">Tasks: Quadratic Discriminant Analysis</a>
<ul>
<li><a href="#task-1-9" id="toc-task-1-9">Task 1</a></li>
<li><a href="#task-2-9" id="toc-task-2-9">Task 2</a></li>
</ul></li>
<li><a href="#tasks-k-nearest-neighbours" id="toc-tasks-k-nearest-neighbours">Tasks: <span class="math inline">\(K\)</span>-nearest neighbours</a>
<ul>
<li><a href="#task-1-10" id="toc-task-1-10">Task 1</a></li>
<li><a href="#task-2-10" id="toc-task-2-10">Task 2</a></li>
<li><a href="#task-3-9" id="toc-task-3-9">Task 3</a></li>
<li><a href="#task-4-8" id="toc-task-4-8">Task 4</a></li>
</ul></li>
<li><a href="#tasks-naive-bayes" id="toc-tasks-naive-bayes">Tasks: Naive Bayes</a>
<ul>
<li><a href="#task-1-11" id="toc-task-1-11">Task 1</a></li>
<li><a href="#task-2-11" id="toc-task-2-11">Task 2</a></li>
</ul></li>
</ul></li>
<li><a href="#answers-2" id="toc-answers-2">Answers</a>
<ul>
<li><a href="#practical-predicting-a-companys-bankruptcy-1" id="toc-practical-predicting-a-companys-bankruptcy-1">Practical: Predicting a company’s bankruptcy</a>
<ul>
<li><a href="#data-and-variables-1" id="toc-data-and-variables-1">Data and Variables</a></li>
<li><a href="#importing-the-data-1" id="toc-importing-the-data-1">Importing the data</a></li>
<li><a href="#loading-required-packages-1" id="toc-loading-required-packages-1">Loading required packages</a></li>
<li><a href="#correlation-matrix-and-plot-2" id="toc-correlation-matrix-and-plot-2">Correlation Matrix and Plot</a></li>
<li><a href="#logistic-regression-1" id="toc-logistic-regression-1">Logistic Regression</a></li>
<li><a href="#linear-discriminant-analysis-1" id="toc-linear-discriminant-analysis-1">Linear Discriminant Analysis</a></li>
<li><a href="#quadratic-discriminant-analysis-1" id="toc-quadratic-discriminant-analysis-1">Quadratic Discriminant Analysis</a></li>
<li><a href="#k-nearest-neighbours-1" id="toc-k-nearest-neighbours-1"><span class="math inline">\(K\)</span>-nearest neighbours</a></li>
<li><a href="#naive-bayes-1" id="toc-naive-bayes-1">Naive Bayes</a></li>
</ul></li>
</ul></li>
<li><a href="#part-section-4" id="toc-part-section-4">(PART*) Section 4</a></li>
<li><a href="#overview-3" id="toc-overview-3">Overview</a></li>
<li><a href="#demonstration-1-cross-validation" id="toc-demonstration-1-cross-validation">Demonstration 1: Cross-validation</a>
<ul>
<li><a href="#data-and-variables-2" id="toc-data-and-variables-2">Data and Variables</a></li>
<li><a href="#the-validation-set-approach" id="toc-the-validation-set-approach">The Validation Set Approach</a></li>
<li><a href="#leave-one-out-cross-validation" id="toc-leave-one-out-cross-validation">Leave-One-Out Cross-Validation</a></li>
<li><a href="#k-fold-cross-validation" id="toc-k-fold-cross-validation"><span class="math inline">\(k\)</span>-Fold Cross-Validation</a></li>
</ul></li>
<li><a href="#demonstration-2-bootstrapping" id="toc-demonstration-2-bootstrapping">Demonstration 2: Bootstrapping</a>
<ul>
<li><a href="#data-and-variables-3" id="toc-data-and-variables-3">Data and Variables</a></li>
<li><a href="#estimating-the-accuracy-of-a-statistic-of-interest" id="toc-estimating-the-accuracy-of-a-statistic-of-interest">Estimating the Accuracy of a Statistic of Interest</a></li>
<li><a href="#estimating-the-accuracy-of-a-linear-regression-model" id="toc-estimating-the-accuracy-of-a-linear-regression-model">Estimating the Accuracy of a Linear Regression Model</a></li>
</ul></li>
<li><a href="#practical" id="toc-practical">Practical</a>
<ul>
<li><a href="#part-i-the-validation-set-approach" id="toc-part-i-the-validation-set-approach">Part I: The Validation Set Approach</a>
<ul>
<li><a href="#task-1-16" id="toc-task-1-16">Task 1</a></li>
<li><a href="#task-2-16" id="toc-task-2-16">Task 2</a></li>
<li><a href="#task-3-12" id="toc-task-3-12">Task 3</a></li>
</ul></li>
<li><a href="#part-ii-leave-one-out-cross-validation" id="toc-part-ii-leave-one-out-cross-validation">Part II: Leave-one-out Cross-validation</a>
<ul>
<li><a href="#task-1-17" id="toc-task-1-17">Task 1</a></li>
<li><a href="#task-2-17" id="toc-task-2-17">Task 2</a></li>
<li><a href="#task-3-13" id="toc-task-3-13">Task 3</a></li>
<li><a href="#task-4-10" id="toc-task-4-10">Task 4</a></li>
</ul></li>
<li><a href="#part-iii-the-bootstrap" id="toc-part-iii-the-bootstrap">Part III: The Bootstrap</a>
<ul>
<li><a href="#task-1-18" id="toc-task-1-18">Task 1</a></li>
<li><a href="#task-2-18" id="toc-task-2-18">Task 2</a></li>
</ul></li>
<li><a href="#bonus" id="toc-bonus">Bonus</a>
<ul>
<li><a href="#task-1-19" id="toc-task-1-19">Task 1</a></li>
<li><a href="#task-2-19" id="toc-task-2-19">Task 2</a></li>
<li><a href="#task-3-14" id="toc-task-3-14">Task 3</a></li>
<li><a href="#task-4-11" id="toc-task-4-11">Task 4</a></li>
<li><a href="#task-5-8" id="toc-task-5-8">Task 5</a></li>
<li><a href="#task-6-6" id="toc-task-6-6">Task 6</a></li>
<li><a href="#task-7-4" id="toc-task-7-4">Task 7</a></li>
<li><a href="#task-8-4" id="toc-task-8-4">Task 8</a></li>
</ul></li>
</ul></li>
<li><a href="#answers-3" id="toc-answers-3">Answers</a>
<ul>
<li><a href="#part-i-the-validation-set-approach-1" id="toc-part-i-the-validation-set-approach-1">Part I: The Validation Set Approach</a>
<ul>
<li><a href="#task-1-20" id="toc-task-1-20">Task 1</a></li>
<li><a href="#task-2-20" id="toc-task-2-20">Task 2</a></li>
<li><a href="#task-3-15" id="toc-task-3-15">Task 3</a></li>
</ul></li>
<li><a href="#part-ii-leave-one-out-cross-validation-1" id="toc-part-ii-leave-one-out-cross-validation-1">Part II: Leave-one-out Cross-validation</a>
<ul>
<li><a href="#task-1-21" id="toc-task-1-21">Task 1</a></li>
<li><a href="#task-2-21" id="toc-task-2-21">Task 2</a></li>
<li><a href="#task-3-16" id="toc-task-3-16">Task 3</a></li>
<li><a href="#task-4-12" id="toc-task-4-12">Task 4</a></li>
</ul></li>
<li><a href="#part-iii-the-bootstrap-1" id="toc-part-iii-the-bootstrap-1">Part III: The Bootstrap</a>
<ul>
<li><a href="#task-1-22" id="toc-task-1-22">Task 1</a></li>
<li><a href="#task-2-22" id="toc-task-2-22">Task 2</a></li>
</ul></li>
<li><a href="#bonus-1" id="toc-bonus-1">Bonus</a>
<ul>
<li><a href="#task-1-23" id="toc-task-1-23">Task 1</a></li>
<li><a href="#task-2-23" id="toc-task-2-23">Task 2</a></li>
<li><a href="#task-3-17" id="toc-task-3-17">Task 3</a></li>
<li><a href="#task-4-13" id="toc-task-4-13">Task 4</a></li>
<li><a href="#task-5-9" id="toc-task-5-9">Task 5</a></li>
<li><a href="#task-6-7" id="toc-task-6-7">Task 6</a></li>
<li><a href="#task-7-5" id="toc-task-7-5">Task 7</a></li>
<li><a href="#task-8-5" id="toc-task-8-5">Task 8</a></li>
</ul></li>
</ul></li>
<li><a href="#part-section-5" id="toc-part-section-5">(PART*) Section 5</a></li>
<li><a href="#overview-4" id="toc-overview-4">Overview</a></li>
<li><a href="#practical-1-academic-salary" id="toc-practical-1-academic-salary">Practical 1: Academic Salary</a>
<ul>
<li><a href="#part-i" id="toc-part-i">Part I</a>
<ul>
<li><a href="#exploring-the-data" id="toc-exploring-the-data">Exploring the data</a></li>
<li><a href="#salary-and-years-since-phd" id="toc-salary-and-years-since-phd">Salary and Years since PhD</a></li>
<li><a href="#salary-and-years-of-service" id="toc-salary-and-years-of-service">Salary and Years of Service</a></li>
<li><a href="#the-model" id="toc-the-model">The Model</a></li>
<li><a href="#test-a-does-the-fitted-model-make-sense" id="toc-test-a-does-the-fitted-model-make-sense">Test a): Does the fitted model make sense?</a></li>
<li><a href="#test-b-overall-is-the-model-a-good-fit" id="toc-test-b-overall-is-the-model-a-good-fit">Test b): Overall, is the model a good fit?</a></li>
<li><a href="#test-c-individually-are-the-explanatory-variables-important" id="toc-test-c-individually-are-the-explanatory-variables-important">Test c): Individually, are the explanatory variables important?</a></li>
</ul></li>
<li><a href="#part-ii" id="toc-part-ii">Part II</a>
<ul>
<li><a href="#interpreting-coefficients-of-attribute-variables" id="toc-interpreting-coefficients-of-attribute-variables">Interpreting coefficients of attribute variables</a></li>
<li><a href="#fitting-a-multivariate-regression-model" id="toc-fitting-a-multivariate-regression-model">Fitting a Multivariate Regression Model</a></li>
<li><a href="#fitting-the-model" id="toc-fitting-the-model">Fitting the Model</a></li>
</ul></li>
</ul></li>
<li><a href="#practical-2-foreign-direct-investment-fdi-study" id="toc-practical-2-foreign-direct-investment-fdi-study">Practical 2: Foreign Direct Investment (FDI) Study</a></li>
<li><a href="#demonstration-1-propublicas-analysis-of-the-compas-tool" id="toc-demonstration-1-propublicas-analysis-of-the-compas-tool">Demonstration 1: ProPublica’s Analysis of the COMPAS Tool</a>
<ul>
<li><a href="#notes-on-the-data" id="toc-notes-on-the-data">Notes on the data</a></li>
<li><a href="#setup" id="toc-setup">Setup</a>
<ul>
<li><a href="#load-packages" id="toc-load-packages">Load packages</a></li>
<li><a href="#load-data" id="toc-load-data">Load data</a></li>
<li><a href="#inspect-data" id="toc-inspect-data">Inspect data</a></li>
</ul></li>
<li><a href="#preprocess-data" id="toc-preprocess-data">Preprocess data</a>
<ul>
<li><a href="#inspect-data-again" id="toc-inspect-data-again">Inspect data again</a></li>
</ul></li>
<li><a href="#exploratory-analysis" id="toc-exploratory-analysis">Exploratory analysis</a>
<ul>
<li><a href="#exercise" id="toc-exercise">👉 Exercise</a></li>
<li><a href="#risk-labels" id="toc-risk-labels">Risk labels</a></li>
</ul></li>
<li><a href="#bias-in-compas" id="toc-bias-in-compas">Bias in COMPAS</a>
<ul>
<li><a href="#preprocess-data-for-logistic-regression" id="toc-preprocess-data-for-logistic-regression">Preprocess data for logistic regression</a></li>
<li><a href="#estimate-the-logistic-regression-model" id="toc-estimate-the-logistic-regression-model">Estimate the logistic regression model</a></li>
<li><a href="#interpret-estimates" id="toc-interpret-estimates">Interpret estimates</a></li>
<li><a href="#exercise-1" id="toc-exercise-1">👉 Exercise</a></li>
</ul></li>
<li><a href="#predictive-accuracy" id="toc-predictive-accuracy">Predictive Accuracy</a>
<ul>
<li><a href="#exercise-2" id="toc-exercise-2">👉 Exercise</a></li>
<li><a href="#exercise-3" id="toc-exercise-3">👉 Exercise</a></li>
</ul></li>
</ul></li>
<li><a href="#demonstration-2-the-basics-of-decision-trees-and-related-methods" id="toc-demonstration-2-the-basics-of-decision-trees-and-related-methods">Demonstration 2: The Basics of Decision Trees and Related Methods</a>
<ul>
<li><a href="#decision-trees" id="toc-decision-trees">Decision Trees</a>
<ul>
<li><a href="#classification-trees" id="toc-classification-trees">Classification Trees</a></li>
<li><a href="#regression-trees" id="toc-regression-trees">Regression Trees</a></li>
</ul></li>
<li><a href="#ensemble-methods" id="toc-ensemble-methods">Ensemble Methods</a>
<ul>
<li><a href="#bagging" id="toc-bagging">Bagging</a></li>
<li><a href="#random-forests" id="toc-random-forests">Random Forests</a></li>
<li><a href="#boosting" id="toc-boosting">Boosting</a></li>
</ul></li>
</ul></li>
<li><a href="#answers-4" id="toc-answers-4">Answers</a>
<ul>
<li><a href="#practical-1-4" id="toc-practical-1-4">Practical 1</a>
<ul>
<li><a href="#part-i-1" id="toc-part-i-1">Part I</a></li>
<li><a href="#part-ii-1" id="toc-part-ii-1">Part II</a></li>
</ul></li>
<li><a href="#practical-2-4" id="toc-practical-2-4">Practical 2</a></li>
</ul></li>
<li><a href="#part-section-6" id="toc-part-section-6">(PART*) Section 6</a></li>
<li><a href="#overview-5" id="toc-overview-5">Overview</a></li>
<li><a href="#demonstration-1-subset-and-stepwise-selection" id="toc-demonstration-1-subset-and-stepwise-selection">Demonstration 1: Subset and Stepwise Selection</a>
<ul>
<li><a href="#best-subset-selection" id="toc-best-subset-selection">Best Subset Selection</a></li>
<li><a href="#stepwise-selection" id="toc-stepwise-selection">Stepwise Selection</a></li>
<li><a href="#choosing-among-models-using-the-validation-set-approach-and-cross-validation" id="toc-choosing-among-models-using-the-validation-set-approach-and-cross-validation">Choosing Among Models Using the Validation-Set Approach and Cross-Validation</a>
<ul>
<li><a href="#the-validation-set-approach-1" id="toc-the-validation-set-approach-1">The Validation Set Approach</a></li>
<li><a href="#cross-validation" id="toc-cross-validation">Cross-validation</a></li>
</ul></li>
</ul></li>
<li><a href="#demonstration-2-regularisation" id="toc-demonstration-2-regularisation">Demonstration 2: Regularisation</a>
<ul>
<li><a href="#ridge-regression" id="toc-ridge-regression">Ridge Regression</a></li>
<li><a href="#the-lasso" id="toc-the-lasso">The Lasso</a></li>
</ul></li>
<li><a href="#demonstration-3-pcr-and-pls-regression" id="toc-demonstration-3-pcr-and-pls-regression">Demonstration 3: PCR and PLS Regression</a>
<ul>
<li><a href="#principal-components-regression" id="toc-principal-components-regression">Principal Components Regression</a></li>
<li><a href="#partial-least-squares" id="toc-partial-least-squares">Partial Least Squares</a></li>
</ul></li>
<li><a href="#demonstration-4-beyond-linearity" id="toc-demonstration-4-beyond-linearity">Demonstration 4: Beyond Linearity</a>
<ul>
<li><a href="#polynomial-regression-and-step-functions" id="toc-polynomial-regression-and-step-functions">Polynomial Regression and Step Functions</a></li>
<li><a href="#step-functions" id="toc-step-functions">Step Functions</a></li>
<li><a href="#splines" id="toc-splines">Splines</a></li>
<li><a href="#local-regression" id="toc-local-regression">Local Regression</a></li>
<li><a href="#generalised-additive-models" id="toc-generalised-additive-models">Generalised Additive Models</a></li>
</ul></li>
<li><a href="#part-section-7" id="toc-part-section-7">(PART*) Section 7</a></li>
<li><a href="#overview-6" id="toc-overview-6">Overview</a></li>
<li><a href="#demonstration-1-k-means-clustering-in-r" id="toc-demonstration-1-k-means-clustering-in-r">Demonstration 1: K-means Clustering in R</a>
<ul>
<li><a href="#loading-the-necessary-packages" id="toc-loading-the-necessary-packages">Loading the necessary packages</a></li>
<li><a href="#profile-the-palmer-penguins-dataset" id="toc-profile-the-palmer-penguins-dataset">Profile the Palmer Penguins dataset</a></li>
<li><a href="#plot" id="toc-plot">Plot</a></li>
<li><a href="#preprocess-the-data-for-clustering" id="toc-preprocess-the-data-for-clustering">Preprocess the data for clustering</a></li>
<li><a href="#perform-k-means-clustering" id="toc-perform-k-means-clustering">Perform k-means clustering</a></li>
<li><a href="#visualise-the-clusters" id="toc-visualise-the-clusters">Visualise the clusters</a></li>
<li><a href="#tasks" id="toc-tasks">👉 TASKS</a>
<ul>
<li><a href="#task-1-how-well-did-k-means-clustering-perform" id="toc-task-1-how-well-did-k-means-clustering-perform">TASK 1: How well did k-means clustering perform?</a></li>
<li><a href="#task-2-how-many-clusters-should-we-use" id="toc-task-2-how-many-clusters-should-we-use">TASK 2: How many clusters should we use?</a></li>
<li><a href="#task-3-clustering-cars" id="toc-task-3-clustering-cars">TASK 3: Clustering cars</a></li>
</ul></li>
</ul></li>
<li><a href="#demonstration-2-principal-component-analysis-in-r" id="toc-demonstration-2-principal-component-analysis-in-r">Demonstration 2: Principal Component Analysis in R</a>
<ul>
<li><a href="#load-packages-1" id="toc-load-packages-1">Load packages</a></li>
<li><a href="#preprocess-the-world-happiness-report-data" id="toc-preprocess-the-world-happiness-report-data">Preprocess the world happiness report data</a></li>
<li><a href="#profile-the-happiness-data" id="toc-profile-the-happiness-data">Profile the happiness data</a></li>
<li><a href="#plot-happiness-score-against-feature" id="toc-plot-happiness-score-against-feature">Plot happiness score against feature</a></li>
<li><a href="#run-pca-using-five-features" id="toc-run-pca-using-five-features">Run PCA using five features</a></li>
<li><a href="#correlations-of-happiness-score-with-pc1-and-pc2" id="toc-correlations-of-happiness-score-with-pc1-and-pc2">Correlations of happiness score with PC1 and PC2</a></li>
<li><a href="#plot-first-two-principal-components" id="toc-plot-first-two-principal-components">Plot first two principal components</a></li>
<li><a href="#regress-happiness-on-the-five-indicators" id="toc-regress-happiness-on-the-five-indicators">Regress happiness on the five indicators</a></li>
<li><a href="#visualise-happiness-against-principal-components" id="toc-visualise-happiness-against-principal-components">Visualise happiness against principal components</a></li>
<li><a href="#regress-happiness-on-first-principal-component" id="toc-regress-happiness-on-first-principal-component">Regress happiness on first principal component</a></li>
<li><a href="#task" id="toc-task">👉 TASK</a></li>
</ul></li>
<li><a href="#part-section-8" id="toc-part-section-8">(PART*) Section 8</a></li>
<li><a href="#overview-7" id="toc-overview-7">Overview</a></li>
<li><a href="#demonstration" id="toc-demonstration">Demonstration</a>
<ul>
<li><a href="#data-set" id="toc-data-set">Data Set</a></li>
<li><a href="#project-planning-setting-the-stage-for-success" id="toc-project-planning-setting-the-stage-for-success">Project Planning: Setting the Stage for Success</a></li>
<li><a href="#data-preprocessing" id="toc-data-preprocessing">Data Preprocessing</a></li>
<li><a href="#exploratory-data-analysis-eda" id="toc-exploratory-data-analysis-eda">Exploratory Data Analysis (EDA)</a></li>
<li><a href="#model-selection-and-model-training" id="toc-model-selection-and-model-training">Model Selection and Model Training</a></li>
<li><a href="#model-evaluation" id="toc-model-evaluation">Model Evaluation</a></li>
<li><a href="#model-interpretation" id="toc-model-interpretation">Model Interpretation</a></li>
<li><a href="#report-conclusion" id="toc-report-conclusion">Report Conclusion</a></li>
<li><a href="#key-insights" id="toc-key-insights">Key Insights</a></li>
</ul></li>
<li><a href="#your-turn" id="toc-your-turn">👉 Your Turn!</a></li>
</ul>
<!--bookdown:toc2:end-->
      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">SOST70033 Data Science Modelling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<hr>
<center> 
  <div class="header">
   <img src="images/banners/DSM_banner.png" alt="Trulli">
  </div>
</center>
<!--bookdown:toc:end-->
<!--bookdown:body:start-->
<div id="about" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">About<a href="#about" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Welcome to <strong>SOST70033 Data Science Modelling</strong>! This notebook will host the materials for all R practicals and demonstrations for this course unit. The notebook follows the same section-based structure as the learning materials on Blackboard. To access this notebook, you can bookmark it like any other website in your favourite web browser.</p>
<p>For each section, you will have at least one practical and/or demonstration to complete and each of these can be accessed by using the sidebar menu on the left hand side of your screen. Clicking on the headings in each section will expand the menu and each task can be accessed individually.</p>
<p>The sidebar menu can be toggled on and off by clicking on the <strong>Toggle Sidebar</strong> button.
<img src="images/gifs/toggle_sidebar.gif" style="width: 50%; border: 4px solid #6D009D;" /></p>
<p>Other customisation options include changing the font, font size, and the appearance. You also have a handy search button.</p>
<p>This notebook is also designed to work well on tablet and mobile devices; to enhance your experience on these devices, it is recommended that you hide the sidebar menu and navigate across sections and subsections using the right and left arrows.</p>
<p><img src="images/gifs/other_options.gif" style="width: 50%; border: 4px solid #6D009D;" /></p>
<p>The code, as well as the output and answers are provided for the practicals at the end of each section. The R code can be copied and pasted directly in your R console or script by clicking on the following icon:</p>
<p><img src="images/snips/copy.png" style="width: 20%;" /></p>
<div class="attention">
<p><strong>1:</strong> Before beginning, it is recommended that you create a RStudio project for this course and work through the exercises and tasks in each section using this project.</p>
<p><strong>2:</strong> You should <strong>write</strong> and <strong>save</strong> your answers to the exercises and tasks in R scripts. You should have at least one R script for each course section.</p>
<p><strong>3:</strong> The recommended approach for a ‘clean’ working directory is to place all the data files you plan to use in a separate folder (e.g. a folder called <em>data</em>) within your R project working directory. You should always use simple names that allow you easy access to the contents when you want to either explore the folder on your machine or specify the path to these folders in R.</p>
<p><strong>4:</strong> To build a robust knowledge basis and adequately develop your practical programming skills, it is absolutely <strong>essential</strong> that you first attempt all practical tasks and exercises on your own before comparing your answers with those provided in this notebook. You must also follow along in all demonstrations that contain code on your own machines (<em>simply reading through the text and code will NOT be sufficient to develop a robust understanding of the method and its implementation in R</em>); note that you may also need to reflect on and answer questions throughout a demonstration as well.</p>
</div>
<!--chapter:end:index.Rmd-->
</div>
<div id="part-section-1" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART*) Section 1<a href="#part-section-1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="overview" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Overview<a href="#overview" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div style="color: #333; font-size: 24px; font-style: italic; text-align: justify;">
<p>Section 1: Introduction to Data Science - The Basics of Statistical
Learning</p>
</div>
<p>This section is comprised of a demonstration and two practicals.</p>
<p>The two practicals are adapted from exercises from the core textbook for
this course:</p>
<p>James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). <em>An
Introduction to Statistical Learning with Applications in R</em>. 2nd ed.
New York: Springer. <a href="https://www.statlearning.com/" class="uri">https://www.statlearning.com/</a></p>
<p>The demonstration has been developed by Dr. Tatjana Kecojevic, Lecturer
in Social Statistics.</p>
<div class="ilos">
<p><strong>Learning Outcomes:</strong></p>
<ul>
<li>appreciate the importance of the mean squared error;<br />
</li>
<li>indexing using base R;</li>
<li>creating scatterplot matrices;</li>
<li>creating new variables;</li>
<li>transforming existing variables;</li>
<li>using functionals;</li>
<li>‘calling’ on masked functions from specific packages;</li>
<li>translating base R code to <code>tidyverse</code> and vice versa.</li>
</ul>
</div>
<p><strong>In this section, you will practice using the functions below. It is
highly recommended that you explore these functions further using the
Help tab in your RStudio console. You can access the R documentation in
the Help tab using? (e.g. <code>?read.csv</code>)</strong></p>
<table style="width:96%;">
<colgroup>
<col width="29%" />
<col width="43%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Function</th>
<th align="center">Description</th>
<th align="center">Package</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><code>read.csv()</code></td>
<td align="center">read csv files</td>
<td align="center">utils</td>
</tr>
<tr class="even">
<td align="center"><code>read_csv()</code></td>
<td align="center">read csv files</td>
<td align="center">tidyverse</td>
</tr>
<tr class="odd">
<td align="center"><code>co l umn_to_rownames()</code></td>
<td align="center">convert column to row names</td>
<td align="center">tidyverse</td>
</tr>
<tr class="even">
<td align="center"><code>rownames()</code></td>
<td align="center">obtain names of rows</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>summary()</code></td>
<td align="center">produce summary results</td>
<td align="center">base</td>
</tr>
<tr class="even">
<td align="center"><code>summarise()</code></td>
<td align="center">object summaries</td>
<td align="center">tidyverse
(dplyr)</td>
</tr>
<tr class="odd">
<td align="center"><code>group_by()</code></td>
<td align="center">group by one or more
variables</td>
<td align="center">tidyverse
(dplyr)</td>
</tr>
<tr class="even">
<td align="center"><code>pairs()</code></td>
<td align="center">produce a matrix of
scatterplots</td>
<td align="center">graphics</td>
</tr>
<tr class="odd">
<td align="center"><code>plot()</code></td>
<td align="center">create a plot</td>
<td align="center">base</td>
</tr>
<tr class="even">
<td align="center"><code>ggplot()</code></td>
<td align="center">generic function for
creating a plot</td>
<td align="center">tidyverse
(ggplot2)</td>
</tr>
<tr class="odd">
<td align="center"><code>mutate()</code></td>
<td align="center">create, modify, and delete
columns</td>
<td align="center">tidyverse
(dplyr)</td>
</tr>
<tr class="even">
<td align="center"><code>if_else()</code></td>
<td align="center">condition-based function</td>
<td align="center">tidyverse
(dplyr)</td>
</tr>
<tr class="odd">
<td align="center"><code>as_factor()</code></td>
<td align="center">create factor using existing
levels</td>
<td align="center">tidyverse
(forcats)</td>
</tr>
<tr class="even">
<td align="center"><code>par()</code></td>
<td align="center">set graphical parameters
(e.g. <code>mfrow()</code>)</td>
<td align="center">graphics</td>
</tr>
<tr class="odd">
<td align="center"><code>slice_min()</code> and
<code>slice_max()</code></td>
<td align="center">index rows by location
(smallest and largest values
of a variable respectively)</td>
<td align="center">tidyverse
(dplyr)</td>
</tr>
<tr class="even">
<td align="center"><code>sapply()</code></td>
<td align="center">applying a function over
list or vector</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>select()</code></td>
<td align="center">keep or drop columns</td>
<td align="center"><p>tidyverse
(dplyr)</p>
<p><em>note that
this function
is also
available
through the
MASS package
(we will not
cover this in
this section)</em></p></td>
</tr>
<tr class="even">
<td align="center"><code>pivot_longer()</code></td>
<td align="center">lengthen data</td>
<td align="center">tidyverse
(tidyr)</td>
</tr>
<tr class="odd">
<td align="center"><code>where()</code></td>
<td align="center">selection helper</td>
<td align="center">tidyverse</td>
</tr>
<tr class="even">
<td align="center"><code>median()</code>,
<code>mean()</code>, <code>sd()</code></td>
<td align="center">median, mean, standard
deviation</td>
<td align="center">stats</td>
</tr>
</tbody>
</table>
<!--chapter:end:01-S01-overview.Rmd-->
</div>
<div id="demonstration-a-more-in-depth-consideration-of-model-accuracy" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Demonstration: A more in-depth consideration of model accuracy<a href="#demonstration-a-more-in-depth-consideration-of-model-accuracy" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>This demonstration was developed by Dr. Tatjana Kecojevic, Lecturer in Social Statistics.</em></p>
<p>So we discussed earlier that statistical modeling allows us to explain and dig deeper into understanding the nature of a process of a phenomenon of interest and that we can mathematically describe this phenomenon as an unknown function <span class="math inline">\(f\)</span>. In its related data, we annotate the information that can explain the problem’s
behaviour as <span class="math inline">\(x\)</span> (this could be a single value or a vector or matrix of values
or something more complex) and the results of its behaviour as <span class="math inline">\(y\)</span> (this
could also be a single value or something more complex).</p>
<p>Mathematically we present this as:</p>
<p><span class="math display">\[y_i = f(x_i)\]</span><br />
And of course, let’s not forget about the variation of the <span class="math inline">\(f\)</span>’s behaviour:</p>
<p><span class="math display">\[y_i = f(x_i) + \epsilon_i\]</span></p>
<p>The standard approach for the error/noise is to adopt the following distribution structure <span class="math inline">\(\epsilon \sim N(0,\sigma)\)</span>, meaning that the <span class="math inline">\(\epsilon\)</span> value is
considered to be normally distributed with a mean of <span class="math inline">\(0\)</span> and standard deviation <span class="math inline">\(\sigma\)</span>. Remember that this implies that the negative and positive impacts from the noise are considered equally likely, and that small errors are much more likely than extreme ones.</p>
<p>To evaluate the performance of a statistical model on a given data set, we “observe” the discrepancies between the predicted response values for given observations obtained by the chosen statistical model (<span class="math inline">\(\hat{f}(x_i)\)</span>) and the true response values for these observations (<span class="math inline">\(y_i\)</span>). As I already mentioned, the most commonly used performance measure for regression problems is the <strong>mean squared error</strong> (<strong>MSE</strong>):</p>
<p><span class="math display">\[MSE =  \frac{1}{n} \sum^{n}_{i=1}(y_i - \hat{f}(x_i))^2\]</span><br />
The <strong>MSE</strong> above is computed using the <em>training data</em>, used to fit the model, and as such it would be more correct to refer to it as the <strong>training</strong> <strong>MSE</strong>.</p>
<p>But we have already discussed that we are not really interested how well the model “works” on the training data, ie. <span class="math inline">\(\hat{f}(x_i) \approx y_i\)</span>. We are more interested in the accuracy of the predictions <span class="math inline">\(\hat{f}(x_0)\)</span> that are obtained when we apply the model to previously unseen test data <span class="math inline">\((x_0, y_0)\)</span>, ie. <span class="math inline">\(\hat{f}(x_0) \approx y_0\)</span>.</p>
<p>In other words, we want to chose the model with the lowest <span class="math inline">\(test\)</span> <span class="math inline">\(MSE\)</span> and to do so we need a large enough number of observations in the test data to calculate the <strong>mean square prediction error</strong> for the test observations (<span class="math inline">\(x_0\)</span>, <span class="math inline">\(y_0\)</span>), to which we can refer to as the <span class="math inline">\(test\)</span> <span class="math inline">\(MSE\)</span>.</p>
<p><span class="math display">\[mean(y_0 - \hat{f}(x_0))^2\]</span></p>
<p>In statistics nothing is black and white. In other words, nothing is straightforward and there are many considerations one needs to take into account when applying statistical modelling. The same applies in this situation. We need to realise that when a given model yields a small training MSE but a large test MSE, we are said to be <strong>overfitting</strong> the data. The statistical model is too ‘preoccupied’ to find patterns in the training data and consequently is modelling the patterns that are caused by random effects, rather than by true features of the unknown function <span class="math inline">\(f\)</span>. When the model overfits the training data, the test MSE will be large because the modelled features that the model identifies in the training data just <em>do not exist</em> in the test data. Saying that, regardless of overfitting occurring or not, we expect the training MSE to be smaller than the test MSE, as most of the statistical models either directly or indirectly seek to minimise the training MSE. We need to be aware that the chosen model needs to be flexible and not rigid and glued to the training data.</p>
<p>MSE is simple to calculate and yet, despite its simplicity, it can provide us with a vital insight into modelling. It consists of two intrinsic components that can provide greater enlightenment about how the model works:</p>
<ul>
<li><p>variance: degree to which <span class="math inline">\(\hat{f}\)</span> would differ if we estimated it using a different training dataset (ideally, it should <em>not</em> vary greatly).</p></li>
<li><p>bias: average difference between the estimator <span class="math inline">\(\hat{y}\)</span> and true value <span class="math inline">\(y\)</span>. Mathematically we write bias as:</p></li>
</ul>
<p><span class="math display">\[E[\hat{y} – y]\]</span></p>
<p>As it is not squared difference, it can be either positive or negative. Positive or negative bias implies that the model is over or under “predicting”, while the value of zero would indicate that the model is likely to predict too much as it is to predict too little. The latter implies that the model can be completely wrong in its prediction and still provide us with the bias of zero. This implies that bias on its own provides little information about how correct the model is in its
prediction.</p>
<p>Remember that <span class="math inline">\(y = f(x) + \epsilon\)</span> and therefore <span class="math inline">\(\hat{f}\)</span> is not directly approximating <span class="math inline">\(f\)</span>. <span class="math inline">\(\hat{f}\)</span> models <span class="math inline">\(y\)</span> that includes the noise. It can be challenging and in some cases even impossible to meaningfully capture the behaviour of <span class="math inline">\(f\)</span> itself when the noise term is very large. We have discussed earlier that we assess model accuracy using MSE which is calculated by:</p>
<ol style="list-style-type: decimal">
<li>obtaining the error (i.e. discrepancy between <span class="math inline">\(\hat{f}(x_i)\)</span> and
<span class="math inline">\(y_i\)</span>)</li>
<li>squaring this value (making negative into the positive same, and
greater error gets more severe penalty)</li>
<li>then averaging these results</li>
</ol>
<p>The mean of the squared error is the same as the expectation<span class="math inline">\(^*\)</span> of our squared error so we can go ahead and simplify this a slightly:</p>
<p><span class="math display">\[MSE=E[(y-\hat{f}(x))^2]\]</span></p>
<p>Now, we can break this further and write it as:</p>
<p><span class="math display">\[MSE = E[(f(x)+ \epsilon - \hat{f}(x))^2]\]</span><br />
Knowing that computing the expectation of adding two random variables is the same as computing the expectation of each random variable and then adding them up:</p>
<p><span class="math display">\[E[X+Y]=E[X] +E[Y]\]</span></p>
<p>and recalling that <span class="math inline">\(\sigma^2\)</span> represent the variance of <span class="math inline">\(\epsilon\)</span>, where the variance is calculated as:</p>
<p><span class="math display">\[E[X^2]-E[X]^2,\]</span></p>
<p>and therefore:<br />
<span class="math display">\[Var(\epsilon) = \sigma^2 = E[\epsilon^2] - E[\epsilon]^2,\]</span></p>
<p>with <span class="math inline">\(\epsilon \sim N(0,\sigma)\)</span></p>
<p><span class="math display">\[E[\epsilon]^2=\mu^2=0^2=0\]</span>
we get:</p>
<p><span class="math display">\[E[\epsilon^2] = \sigma^2\]</span></p>
<p>This helps us to rearranging MSE further and calculate it as:</p>
<p><span class="math display">\[MSE=σ^2+E[−2f(x)\hat{f}(x)+f(x)^2+\hat{f}(x)^2]\]</span></p>
<p>where <span class="math inline">\(\sigma^2\)</span> is the variance of the noise (i.e. <span class="math inline">\(\epsilon\)</span>). Therefore, the variance of the noise in data is an <strong><em>irreducible part</em></strong> of the MSE. Regardless of how good the model is, it can never reduce the MSE to being less than the variance related to the noise (i.e. <strong>error</strong>). This error represents the lack of information in data used to adequately explain everything that can be known about the phenomena being modelled. We should not look at it as a nuisance, as it can often guide us to further explore the problem and look into other factors that might be related to it.</p>
<p>Knowing that:
<span class="math display">\[Var(X) = E[X^2] - E[X]^2\]</span></p>
<p>we can apply further transformation and break MSE into:</p>
<p><span class="math display">\[MSE = \sigma^{2}+Var[f(x)-\hat{f}(x)]+E[f(x)-\hat{f}(x)]^2\]</span></p>
<p>The term <span class="math inline">\(Var[f(x)-\hat{f}(x)]\)</span> is the <em>variance</em> in the model predictions from
the true output values and the last term <span class="math inline">\(E[f(x)-\hat{f}(x)]^2\)</span> is just the <em>bias</em> squared. We mentioned earlier that that unlike variance, bias can be positive or negative, so we square this value in order to make sure it is always positive.</p>
<p>With this in mind, we realise that MSE consists of:</p>
<ol style="list-style-type: lower-roman">
<li>model variance</li>
<li>model bias and</li>
<li>irreducible error</li>
</ol>
<p><span class="math display">\[\text{Mean Squared Error}=\text{Model Variance} + \text{Model Bias}^2 + \text{Irreducible Error}\]</span></p>
<p>We come to the conclusion that in order to <strong><em>minimise</em></strong> the <strong>expected test error</strong>, we need to select a statistical model that simultaneously achieves low variance and low bias.</p>
<p>Note that in practice we will never know what the variance <span class="math inline">\(\sigma^2\)</span> of the error <span class="math inline">\(\epsilon\)</span> is, and therefore we will not be able to determine the variance and the bias of the model. However, since <span class="math inline">\(\sigma^2\)</span> is constant, we have to decrease either bias or variance to improve the model.</p>
<p>Testing the model using the test data and observing its bias and variance can help us address some important issues, allowing us to reason with the model. If the model fails to find the <span class="math inline">\(f\)</span> in data and is systematically over or under predicting, this will indicate <strong>underfitting</strong> and it will be reflected through high bias. However, high variance when working with test data indicates the issue of <strong>overfitting</strong>. What happens is that the model has learnt the training data really well and is too close to the data, so much so that it starts to mistake the <span class="math inline">\(f(x) + \epsilon\)</span> for true <span class="math inline">\(f(x)\)</span>.</p>
<div id="the-simulation" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">The Simulation<a href="#the-simulation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To better understand these concepts, let us run a small simulation study. We
will:</p>
<ol style="list-style-type: lower-roman">
<li>simulate a function <span class="math inline">\(f\)</span></li>
<li>apply the error, i.e. noise sampled from a distribution with a known
variance</li>
</ol>
<p>To make it very simple and illustrative we will use a linear function <span class="math inline">\(f(x) = 3 + 2x\)</span> to simulate response <span class="math inline">\(y\)</span> with the error <span class="math inline">\(e\thicksim N(\mu =0, \sigma^2 =4)\)</span>, where <span class="math inline">\(x\)</span> is going to be a sequence of numbers between <span class="math inline">\(0\)</span> and <span class="math inline">\(10\)</span> in steps of <span class="math inline">\(0.1\)</span>. We will examine the simulations for the models that over and under estimate the true <span class="math inline">\(f\)</span>, and since it is a linear function we will not have a problem identifying using simple linear regression modelling.</p>
<p>Let’s start with a simulation in which we will model the true function with <span class="math inline">\(\hat{f}_{1} = 4 + 2x\)</span> and <span class="math inline">\(\hat{f}_{2} = 1 + 2x\)</span>.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="do">## set the seed of R‘s random number generator</span></span>
<span id="cb1-2"><a href="#cb1-2" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" tabindex="-1"></a><span class="co"># simulate function f(x) = 3 + 2x</span></span>
<span id="cb1-4"><a href="#cb1-4" tabindex="-1"></a>f <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb1-5"><a href="#cb1-5" tabindex="-1"></a>  <span class="dv">3</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> x </span>
<span id="cb1-6"><a href="#cb1-6" tabindex="-1"></a>}</span>
<span id="cb1-7"><a href="#cb1-7" tabindex="-1"></a><span class="co"># generate vector X</span></span>
<span id="cb1-8"><a href="#cb1-8" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="at">by =</span> <span class="fl">0.05</span>)</span>
<span id="cb1-9"><a href="#cb1-9" tabindex="-1"></a><span class="co"># the error term coming from N(mean = 0, variance = 4)</span></span>
<span id="cb1-10"><a href="#cb1-10" tabindex="-1"></a>e <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="fu">length</span>(x), <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb1-11"><a href="#cb1-11" tabindex="-1"></a><span class="co"># simulate the response vector Y</span></span>
<span id="cb1-12"><a href="#cb1-12" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">f</span>(x) <span class="sc">+</span> e</span>
<span id="cb1-13"><a href="#cb1-13" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" tabindex="-1"></a><span class="co"># plot the simulated data </span></span>
<span id="cb1-15"><a href="#cb1-15" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">cex =</span> <span class="fl">0.75</span>, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">main =</span> <span class="st">&quot;Simulation: 1&quot;</span>) </span>
<span id="cb1-16"><a href="#cb1-16" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">3</span>, <span class="dv">2</span>, <span class="at">col =</span><span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">1</span>)</span>
<span id="cb1-17"><a href="#cb1-17" tabindex="-1"></a><span class="co"># model fitted to simulated data</span></span>
<span id="cb1-18"><a href="#cb1-18" tabindex="-1"></a>f_hat_1<span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb1-19"><a href="#cb1-19" tabindex="-1"></a>  <span class="dv">4</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> x</span>
<span id="cb1-20"><a href="#cb1-20" tabindex="-1"></a>}</span>
<span id="cb1-21"><a href="#cb1-21" tabindex="-1"></a>f_hat_2 <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb1-22"><a href="#cb1-22" tabindex="-1"></a>  <span class="dv">1</span> <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> x</span>
<span id="cb1-23"><a href="#cb1-23" tabindex="-1"></a>}</span>
<span id="cb1-24"><a href="#cb1-24" tabindex="-1"></a>y_bar <span class="ot">=</span> <span class="fu">mean</span>(y) <span class="co"># average value of the response variable y</span></span>
<span id="cb1-25"><a href="#cb1-25" tabindex="-1"></a>f_hat_3 <span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb1-26"><a href="#cb1-26" tabindex="-1"></a>  y_bar</span>
<span id="cb1-27"><a href="#cb1-27" tabindex="-1"></a>}</span>
<span id="cb1-28"><a href="#cb1-28" tabindex="-1"></a><span class="co"># add the line representing the fitted model</span></span>
<span id="cb1-29"><a href="#cb1-29" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb1-30"><a href="#cb1-30" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">4</span>, <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">1</span>)</span>
<span id="cb1-31"><a href="#cb1-31" tabindex="-1"></a><span class="fu">abline</span>(y_bar, <span class="dv">0</span>, <span class="at">col =</span> <span class="st">&quot;darkgreen&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">3</span>)</span>
<span id="cb1-32"><a href="#cb1-32" tabindex="-1"></a><span class="fu">legend</span>(<span class="fl">7.5</span>, <span class="dv">10</span>, <span class="at">legend=</span><span class="fu">c</span>(<span class="st">&quot;f_hat_1&quot;</span>, <span class="st">&quot;f_hat_2&quot;</span>, <span class="st">&quot;f_hat_3&quot;</span>, <span class="st">&quot;f&quot;</span>),</span>
<span id="cb1-33"><a href="#cb1-33" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;darkgreen&quot;</span>, <span class="st">&quot;gray&quot;</span>), </span>
<span id="cb1-34"><a href="#cb1-34" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">3</span>, <span class="dv">1</span>),</span>
<span id="cb1-35"><a href="#cb1-35" tabindex="-1"></a>       <span class="at">text.font =</span> <span class="dv">4</span>, <span class="at">bg =</span> <span class="st">&#39;lightyellow&#39;</span>)</span></code></pre></div>
<p><img src="01-S01-D1_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Observing the graph, we notice that <span class="math inline">\(\hat{f}_1\)</span> and <span class="math inline">\(\hat{f}_2\)</span>, depicted in blue and red lines respectively, follow the data nicely, but are also systematically over (in the case of <span class="math inline">\(\hat{f}_1\)</span> and under (in the case of <span class="math inline">\(\hat{f}_2\)</span>) estimating the values. In the simple model <span class="math inline">\(\hat{f}_3\)</span>, the line represents the value <span class="math inline">\(\bar{y}\)</span>, which cuts the data in half.</p>
<p>As we mentioned earlier, knowing the true function <span class="math inline">\(f\)</span> and the distribution of <span class="math inline">\(\epsilon\)</span> we can calculate: - the MSE using the simulated data and the estimated model, - the model’s bias and variance which will allow for the calculation of the “theoretical” MSE. This will allow for more detailed illustration about the information contained in the model’s bias and variance.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a><span class="co"># calculate MSE from data</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>MSE_data1 <span class="ot">=</span> <span class="fu">mean</span>((y <span class="sc">-</span> <span class="fu">f_hat_1</span>(x))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>MSE_data2 <span class="ot">=</span> <span class="fu">mean</span>((y <span class="sc">-</span> <span class="fu">f_hat_2</span>(x))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>MSE_data3 <span class="ot">=</span> <span class="fu">mean</span>((y <span class="sc">-</span> <span class="fu">f_hat_3</span>(x))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a><span class="co"># model bias </span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>bias_1 <span class="ot">=</span> <span class="fu">mean</span>(<span class="fu">f_hat_1</span>(x) <span class="sc">-</span> <span class="fu">f</span>(x))</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>bias_2 <span class="ot">=</span> <span class="fu">mean</span>(<span class="fu">f_hat_2</span>(x) <span class="sc">-</span> <span class="fu">f</span>(x))</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>bias_3 <span class="ot">=</span> <span class="fu">mean</span>(<span class="fu">f_hat_3</span>(x) <span class="sc">-</span> <span class="fu">f</span>(x))</span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a><span class="co"># model variance</span></span>
<span id="cb2-10"><a href="#cb2-10" tabindex="-1"></a>var_1 <span class="ot">=</span> <span class="fu">var</span>(<span class="fu">f</span>(x) <span class="sc">-</span> <span class="fu">f_hat_1</span>(x))</span>
<span id="cb2-11"><a href="#cb2-11" tabindex="-1"></a>var_2 <span class="ot">=</span> <span class="fu">var</span>(<span class="fu">f</span>(x) <span class="sc">-</span> <span class="fu">f_hat_2</span>(x))</span>
<span id="cb2-12"><a href="#cb2-12" tabindex="-1"></a>var_3 <span class="ot">=</span> <span class="fu">var</span>(<span class="fu">f</span>(x) <span class="sc">-</span> <span class="fu">f_hat_3</span>(x))</span>
<span id="cb2-13"><a href="#cb2-13" tabindex="-1"></a><span class="co"># calculate &#39;theoretical&#39; MSE</span></span>
<span id="cb2-14"><a href="#cb2-14" tabindex="-1"></a>MSE_1 <span class="ot">=</span> bias_1<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> var_1 <span class="sc">+</span> <span class="dv">2</span><span class="sc">^</span><span class="dv">2</span></span>
<span id="cb2-15"><a href="#cb2-15" tabindex="-1"></a>MSE_2 <span class="ot">=</span> bias_2<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> var_2 <span class="sc">+</span> <span class="dv">2</span><span class="sc">^</span><span class="dv">2</span></span>
<span id="cb2-16"><a href="#cb2-16" tabindex="-1"></a>MSE_3 <span class="ot">=</span> bias_3<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> var_3 <span class="sc">+</span> <span class="dv">2</span><span class="sc">^</span><span class="dv">2</span></span>
<span id="cb2-17"><a href="#cb2-17" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1</span>){</span>
<span id="cb2-18"><a href="#cb2-18" tabindex="-1"></a>  <span class="fu">cat</span> (<span class="fu">c</span>(<span class="st">&quot;==============================================&quot;</span>,<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>))</span>
<span id="cb2-19"><a href="#cb2-19" tabindex="-1"></a>  <span class="fu">cat</span> (<span class="fu">c</span>(<span class="st">&quot;=============== f_hat_1 ================&quot;</span>,<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>))</span>
<span id="cb2-20"><a href="#cb2-20" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;MSE_data1 = &quot;</span>, <span class="fu">round</span>(MSE_data1, <span class="dv">2</span>),  <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span>))</span>
<span id="cb2-21"><a href="#cb2-21" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;bias_1 = &quot;</span>, bias_1, <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span> ))   </span>
<span id="cb2-22"><a href="#cb2-22" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;variance_1 = &quot;</span>, <span class="fu">round</span>(var_1, <span class="dv">2</span>), <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span> ))  </span>
<span id="cb2-23"><a href="#cb2-23" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;MSE_1 = 4 + bias_1^2 + variance_1 = &quot;</span>, MSE_1, <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span> )) </span>
<span id="cb2-24"><a href="#cb2-24" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;==============================================&quot;</span>,<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>))</span>
<span id="cb2-25"><a href="#cb2-25" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;=============== f_hat_2 ================&quot;</span>,<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>))</span>
<span id="cb2-26"><a href="#cb2-26" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;MSE_data2 = &quot;</span>, <span class="fu">round</span>(MSE_data2, <span class="dv">2</span>),  <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span>))</span>
<span id="cb2-27"><a href="#cb2-27" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;bias_2 = &quot;</span>, bias_2, <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span> ))   </span>
<span id="cb2-28"><a href="#cb2-28" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;variance_2 = &quot;</span>, <span class="fu">round</span>(var_2, <span class="dv">2</span>), <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span> ))  </span>
<span id="cb2-29"><a href="#cb2-29" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;MSE_2 = 4 + bias_2^2 + variance_2 = &quot;</span>, MSE_2, <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span> ))</span>
<span id="cb2-30"><a href="#cb2-30" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;==============================================&quot;</span>,<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>))</span>
<span id="cb2-31"><a href="#cb2-31" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;=============== f_hat_3 ================&quot;</span>,<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>))</span>
<span id="cb2-32"><a href="#cb2-32" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;average y = &quot;</span>, <span class="fu">round</span>(y_bar, <span class="dv">2</span>),  <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span>))</span>
<span id="cb2-33"><a href="#cb2-33" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;MSE_data3 = &quot;</span>, <span class="fu">round</span>(MSE_data3, <span class="dv">2</span>),  <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span>))</span>
<span id="cb2-34"><a href="#cb2-34" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;bias_3 = &quot;</span>, <span class="fu">round</span>(bias_3, <span class="dv">2</span>), <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span> ))   </span>
<span id="cb2-35"><a href="#cb2-35" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;variance_3 = &quot;</span>, <span class="fu">round</span>(var_3, <span class="dv">2</span>), <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span> ))  </span>
<span id="cb2-36"><a href="#cb2-36" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;MSE_3 = 4 + bias_3^2 + variance_3 = &quot;</span>, <span class="fu">round</span>(MSE_3, <span class="dv">2</span>), <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span> ))</span>
<span id="cb2-37"><a href="#cb2-37" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;==============================================&quot;</span>,<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>))</span>
<span id="cb2-38"><a href="#cb2-38" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## ============================================== 
## =============== f_hat_1 ================ 
## MSE_data1 =  4.61 
## bias_1 =  1 
## variance_1 =  0 
## MSE_1 = 4 + bias_1^2 + variance_1 =  5 
## ============================================== 
## =============== f_hat_2 ================ 
## MSE_data2 =  7.64 
## bias_2 =  -2 
## variance_2 =  0 
## MSE_2 = 4 + bias_2^2 + variance_2 =  8 
## ============================================== 
## =============== f_hat_3 ================ 
## average y =  13 
## MSE_data3 =  36.7 
## bias_3 =  0 
## variance_3 =  33.84 
## MSE_3 = 4 + bias_3^2 + variance_3 =  37.84 
## ==============================================</code></pre>
<p><span class="math inline">\(\hat{f}_1\)</span> has a positive bias because it is overestimating data points more often than it is underestimating, but as it does it so consistently in comparison to <span class="math inline">\(f\)</span> that produces variance of zero. In contrast <span class="math inline">\(\hat{f}_2\)</span> has a negative bias as it is underestimating simulated data, but nonetheless it also does it consistently, resulting in zero variance with <span class="math inline">\(f\)</span>.</p>
<p>Unlike in the previous two model estimates which follow the data points, <span class="math inline">\(\hat{f}_3\)</span> predicts the mean value of data, resulting in no bias since it evenly underestimates and overestimates <span class="math inline">\(f(x)\)</span>. However, the variation in prediction between <span class="math inline">\(f\)</span> and <span class="math inline">\(\hat{f}_3\)</span> is obvious.</p>
<p>Given that the true function <span class="math inline">\(f\)</span> is linear, by applying simple regression modelling, we should be able to estimate it easily in R using the <span class="math inline">\(lm()\)</span> function.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="co"># model fitted to simulated data</span></span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>f_hat_4<span class="ot">&lt;-</span> <span class="cf">function</span>(x){</span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>    <span class="fu">lm</span>(y<span class="sc">~</span>x)</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>}</span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a><span class="co"># plot the simulated data </span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a><span class="fu">plot</span>(x, y, <span class="at">cex =</span> <span class="fl">0.75</span>, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">main =</span> <span class="st">&quot;Simulation: 2&quot;</span>) </span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">3</span>, <span class="dv">2</span>, <span class="at">col =</span><span class="st">&quot;gray&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">1</span>)</span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a><span class="co"># add the line representing the fitted model</span></span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a><span class="fu">abline</span>(<span class="fu">lm</span>(y<span class="sc">~</span>x), <span class="at">col =</span><span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">3</span>)</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a><span class="fu">legend</span>(<span class="fl">7.5</span>, <span class="dv">8</span>, <span class="at">legend=</span><span class="fu">c</span>(<span class="st">&quot;f_hat_4&quot;</span>, <span class="st">&quot;f&quot;</span>),</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;gray&quot;</span>), </span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>       <span class="at">lwd =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">1</span>),</span>
<span id="cb4-13"><a href="#cb4-13" tabindex="-1"></a>       <span class="at">text.font =</span> <span class="dv">4</span>, <span class="at">bg =</span> <span class="st">&#39;lightyellow&#39;</span>)</span></code></pre></div>
<p><img src="01-S01-D1_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>Since the true function <span class="math inline">\(f\)</span> is a linear model it is not surprising that <span class="math inline">\(\hat{f}_4\)</span> can learn it, resulting in zero values of the model’s bias and variance.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="co"># calculate MSE from data</span></span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>MSE_data4 <span class="ot">=</span> <span class="fu">mean</span>((y <span class="sc">-</span> <span class="fu">predict</span>(<span class="fu">f_hat_4</span>(x)))<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a><span class="co"># model bias </span></span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>bias_4 <span class="ot">=</span> <span class="fu">mean</span>(<span class="fu">predict</span>(<span class="fu">f_hat_4</span>(x)) <span class="sc">-</span> <span class="fu">f</span>(x))</span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a><span class="co"># model variance</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>var_4 <span class="ot">=</span> <span class="fu">var</span>(<span class="fu">f</span>(x) <span class="sc">-</span> <span class="fu">predict</span>(<span class="fu">f_hat_4</span>(x)))</span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a><span class="co"># calculate &#39;theoretical&#39; MSE</span></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>MSE_4 <span class="ot">=</span> bias_4<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> var_4 <span class="sc">+</span> <span class="dv">2</span><span class="sc">^</span><span class="dv">2</span></span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1</span>){</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>  </span>
<span id="cb5-11"><a href="#cb5-11" tabindex="-1"></a>  <span class="fu">cat</span> (<span class="fu">c</span>(<span class="st">&quot;==============================================&quot;</span>,<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>))</span>
<span id="cb5-12"><a href="#cb5-12" tabindex="-1"></a>  <span class="fu">cat</span> (<span class="fu">c</span>(<span class="st">&quot;=============== f_hat_4 ================&quot;</span>,<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>))</span>
<span id="cb5-13"><a href="#cb5-13" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;MSE_data4 = &quot;</span>, <span class="fu">round</span>(MSE_data4, <span class="dv">2</span>),  <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span>))</span>
<span id="cb5-14"><a href="#cb5-14" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;bias_4 = &quot;</span>, <span class="fu">round</span>(bias_4, <span class="dv">2</span>), <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span> ))   </span>
<span id="cb5-15"><a href="#cb5-15" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;variance_4 = &quot;</span>, <span class="fu">round</span>(var_4, <span class="dv">2</span>), <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span> ))  </span>
<span id="cb5-16"><a href="#cb5-16" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">c</span>(<span class="st">&quot;MSE_4 = 4 + bias_4^2 + variance_4 = &quot;</span>, <span class="fu">round</span>(MSE_4, <span class="dv">2</span>), <span class="at">sep =</span> <span class="st">&#39;</span><span class="sc">\n</span><span class="st">&#39;</span> ))</span>
<span id="cb5-17"><a href="#cb5-17" tabindex="-1"></a>  <span class="fu">cat</span> (<span class="fu">c</span>(<span class="st">&quot;==============================================&quot;</span>,<span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>))</span>
<span id="cb5-18"><a href="#cb5-18" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## ============================================== 
## =============== f_hat_4 ================ 
## MSE_data4 =  3.62 
## bias_4 =  0 
## variance_4 =  0 
## MSE_4 = 4 + bias_4^2 + variance_4 =  4 
## ==============================================</code></pre>
<p><em>We realise that the <span class="math inline">\(MSE\)</span> is more than just a simple error measurement. It is a tool that informs and educates the modeller about the performance of the model being used in the analysis of a problem. It is packed with information that when unwrapped can provide a greater insight into not just the fitted model, but the nature of the problem and its data.</em></p>
<hr />
<p><span class="math inline">\(^*\)</span> <em>The Expectation of a Random Variable</em> is the sum of its values
weighted by their probability. For example: What is the average toss of
a fair six-sided die?</p>
<p>If the random variable is the top face of a tossed, fair six-sided die,
then the probability of die landing on <span class="math inline">\(X\)</span> is:</p>
<p><span class="math display">\[f(x) = \frac{1}{6}\]</span> for <span class="math inline">\(x = 1, 2,... 6\)</span>. Therefore, the average
toss, i.e. <strong>the expected value</strong> of <span class="math inline">\(X\)</span> is:</p>
<p><span class="math display">\[E(X) = 1(\frac{1}{6}) + 2(\frac{1}{6}) + 3(\frac{1}{6}) + 4(\frac{1}{6}) + 5(\frac{1}{6}) + 6(\frac{1}{6}) = 3.5\]</span>
Of course, we do not expect to get a fraction when tossing a die, i.e.
we do not expect the toss to be 3.5, but rather an integer number between 1 to 6. So, what the expected value is really saying is what is the expected average of a large number of tosses will be. If we toss a fair, six-side die hundreds of times and calculate the average of the tosses we will not get the exact 3.5 figure, but we will expect it to be close to 3.5. This is a <strong>theoretical average</strong>, not the exact one that is realised.</p>
<!--chapter:end:01-S01-D1.Rmd-->
</div>
</div>
<div id="practical-1" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Practical 1<a href="#practical-1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="file">
<p>For the tasks below, you will require the <strong>College</strong> dataset from the
core textbook (James et. al 2021).</p>
<p>Click here to download the file:
<a href="data/College.csv" download="College.csv"> College.csv </a>.</p>
<p>Remember to place your data file in a separate subfolder within your R project working directory.</p>
</div>
<p>The <strong>College</strong> dataset contains statistics for a large number of US Colleges from the 1995 issue of US News and World Report. It is a data frame with 777 observations and 18 variables. The variables are:</p>
<ul>
<li>Private : Public/private indicator</li>
<li>Apps : Number of applications received</li>
<li>Accept : Number of applicants accepted</li>
<li>Enroll : Number of new students enrolled</li>
<li>Top10perc : New students from top 10% of high school class</li>
<li>Top25perc : New students from top 25% of high school class</li>
<li>F.Undergrad : Number of full-time undergraduates</li>
<li>P.Undergrad : Number of part-time undergraduates</li>
<li>Outstate : Out-of-state tuition</li>
<li>Room.Board : Room and board costs</li>
<li>Books : Estimated book costs</li>
<li>Personal : Estimated personal spending</li>
<li>PhD : Percent of faculty with Ph.D.’s</li>
<li>Terminal : Percent of faculty with terminal degree</li>
<li>S.F.Ratio: Student/faculty ratio</li>
<li>perc.alumni : Percent of alumni who donate</li>
<li>Expend : Instructional expenditure per student</li>
<li>Grad.Rate : Graduation rate</li>
</ul>
<div id="task-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Import the dataset in an object called <strong>college</strong> using the tidyverse <code>read_csv()</code> function.</p>
<p>If you then have a look at the contents of the data object using <code>View()</code>,
you will notice that the first column contains the names of all of the
universities in the dataset. You will also notice that it has a strange
name.</p>
<p><img src="images/snips/college-view.png"/></p>
<p>Actually, these data should not be treated as a variable (column) since
it is just a list of university names.</p>
</div>
<div id="task-2" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Keeping the list of names in the data object, transform this column such
that the university names in the column become row names. Hint: use the
<code>column_to_rownames()</code> function from <code>dplyr</code>.</p>
<div class="question">
<p>How would have your approach to this task differed if you would have
imported the dataset using base R? Try it!</p>
</div>
</div>
<div id="task-3" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 3<a href="#task-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Produce summary statistics for all variables in the data object.</p>
</div>
<div id="task-4" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 4<a href="#task-4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Create a scatterplot matrix of the first three numeric variables.</p>
</div>
<div id="task-5" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 5<a href="#task-5" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Produce side by side box plots of <code>Outstate</code> versus <code>Private</code> using base
R.</p>
<div class="question">
<p>Did this work? Why?</p>
</div>
</div>
<div id="task-6" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 6<a href="#task-6" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the <code>Top10perc</code> variable, create a new categorical variable called
<code>Elite</code> such that universities are divided into two groups based on
whether or not the proportion of students coming from the top 10% of
their high school classes exceeds 50%. Hint: use a combination of
<code>mutate()</code> and <code>if_else()</code>.</p>
</div>
<div id="task-7" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 7<a href="#task-7" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Produce side by side box plots of the new <code>Elite</code> variable and
<code>Outstate</code>.</p>
<div class="question">
<p>How would you produce a similar plot using base R?</p>
</div>
</div>
<div id="task-8" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 8<a href="#task-8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use base R to produce a multipanel plot that displays histograms of the
following variables: <code>Apps</code>, <code>perc.alumni</code>, <code>S.F.Ratio</code>, <code>Expend</code>. Hint:
use <code>par(mfrow=c(2,2))</code> to set up a 2x2 panel. Try to adjust the
specifications (e.g. breaks).</p>
</div>
<div id="task-9" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 9<a href="#task-9" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using <code>Accept</code> and <code>Apps</code>, create a new variable that describes
acceptance rate. Name this variable <code>acceptance_rate</code>. Hint: use
<code>mutate()</code>.</p>
</div>
<div id="task-10" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 10<a href="#task-10" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the <code>acceptance_rate</code> variable, find out which university has the
lowest acceptance rate. Hint: for a <code>tidyverse</code> approach, you can use
<code>slice_min()</code>.</p>
</div>
<div id="task-11" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 11<a href="#task-11" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the <code>acceptance_rate</code> variable, find out which university has the
highest acceptance rate.</p>
<!--chapter:end:01-S01-P1.Rmd-->
</div>
</div>
<div id="practical-2" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Practical 2<a href="#practical-2" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="file">
<p>For the tasks below, you will require the <strong>Boston</strong> dataset. This
dataset is part of the <code>MASS</code> R package.</p>
<p>To access the dataset, load the <code>MASS</code> package (install the package
first, if you have not done so previously).</p>
</div>
<div id="task-1-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Install and load the <code>MASS</code> package. You will also require <code>tidyverse</code>.</p>
<div class="question">
<p>Does R provide any message when loading <code>MASS</code>? Why does this matter?</p>
</div>
</div>
<div id="task-2-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Find out more about the <code>Boston</code> dataset variables by accessing the R
Documentation.</p>
<p>To explore the <code>Boston</code> dataset, simply type the name of the data object
into the console or use <code>View()</code></p>
<div class="question">
<p>What type of data structure is the <code>Boston</code> dataset? What are its
dimensions? How many categorical and quantitative variables are there?</p>
</div>
</div>
<div id="task-3-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 3<a href="#task-3-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Find the class of all 14 variables. Hint: use <code>sapply</code>.</p>
</div>
<div id="task-4-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 4<a href="#task-4-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using a <code>tidyverse</code> approach, calculate the mean, median, and standard
deviation of all variables of class <em>numeric</em>.</p>
<div class="question">
<p>What is the mean pupil-teacher ratio? What is the median and mean per
capita crime rate? Which value do you think is more suitable to describe
per capita crime rate?</p>
</div>
</div>
<div id="task-5-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 5<a href="#task-5-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using a base R approach, create a 2x2 multipanel plot of <code>crim</code> versus
<code>age</code>, <code>dis</code>, <code>rad</code>, <code>tax</code> and <code>ptratio</code>.</p>
<div class="question">
<p>What can you say about the relationships between <code>age</code>, <code>dis</code>, <code>rad</code>,
<code>tax</code>, and <code>crim</code>?</p>
</div>
</div>
<div id="task-6-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 6<a href="#task-6-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using a base R approach, create and display histograms of <code>crim</code>, <code>tax</code>
and <code>ptratio</code> in a 1x2 multipanel plot. Set the <code>breaks</code> argument to
<strong>25</strong> .</p>
<div class="question">
<p>What do these histograms indicate?</p>
</div>
<!--chapter:end:01-S01-P2.Rmd-->
</div>
</div>
<div id="answers" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Answers<a href="#answers" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="practical-1-1" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Practical 1<a href="#practical-1-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="file">
<p>For the tasks below, you will require the <strong>College</strong> dataset from the
core textbook (James et. al 2021).</p>
<p>Click here to download the file:
<a href="data/College.csv" download="College.csv"> College.csv </a>.</p>
<p><em>Remember to place your data file in a separate subfolder within your R
project working directory.</em></p>
</div>
<p>This data file contains 18 variables for 777 different universities and
colleges in the United States. The variables are:</p>
<ul>
<li>Private : Public/private indicator</li>
<li>Apps : Number of applications received</li>
<li>Accept : Number of applicants accepted</li>
<li>Enroll : Number of new students enrolled</li>
<li>Top10perc : New students from top 10% of high school class</li>
<li>Top25perc : New students from top 25% of high school class</li>
<li>F.Undergrad : Number of full-time undergraduates</li>
<li>P.Undergrad : Number of part-time undergraduates</li>
<li>Outstate : Out-of-state tuition</li>
<li>Room.Board : Room and board costs</li>
<li>Books : Estimated book costs</li>
<li>Personal : Estimated personal spending</li>
<li>PhD : Percent of faculty with Ph.D.’s</li>
<li>Terminal : Percent of faculty with terminal degree</li>
<li>S.F.Ratio: Student/faculty ratio</li>
<li>perc.alumni : Percent of alumni who donate</li>
<li>Expend : Instructional expenditure per student</li>
<li>Grad.Rate : Graduation rate</li>
</ul>
<div id="task-1-2" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Import the dataset in an object called <strong>college</strong> using the tidyverse <code>read_csv()</code> function.</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="co"># Remember to load tidyverse first</span></span>
<span id="cb7-2"><a href="#cb7-2" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb7-4"><a href="#cb7-4" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" tabindex="-1"></a>college <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;data/College.csv&quot;</span>)</span></code></pre></div>
<p>If you have a look at the contents of the data object using <code>View()</code>,
you will notice that the first column contains the names of all of the
universities in the dataset. You will also notice that it has a strange
name.</p>
<p><img src="images/snips/college-view.png"/></p>
<p>Actually, these data should not be treated as a variable (column) since
it is just a list of university names.</p>
</div>
<div id="task-2-2" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Keeping the list of names in the data object, transform this column such
that the university names in the column become row names. Hint: use the
<code>column_to_rownames()</code> function from <code>dplyr</code>.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a>college <span class="ot">&lt;-</span> college <span class="sc">%&gt;%</span> <span class="fu">column_to_rownames</span>(<span class="at">var =</span> <span class="st">&quot;...1&quot;</span>) </span></code></pre></div>
<div class="question">
<p>How would have your approach to this task differed if you would have
imported the dataset using base R? Try it!</p>
</div>
<div class="answers">
<p>The data file could have instead been imported using <code>read.csv()</code>:</p>
<p><code>college &lt;- read.csv("data/College.csv")</code></p>
<p>Using the base R approach, the first column containing the university
names would have been named “X”, as shown below using <code>View()</code>.</p>
<p><img src="images/snips/college-base-view.png" style="width: 60%;"/></p>
<p>Now, how would be go about transforming the contents of the first column
into row names?</p>
<p>This would require two steps.</p>
<p>First, we assign the column contents to rows names.</p>
<p><code>rownames(college) &lt;- college[, 1]</code></p>
<p>If you have another look at the data object, you will see that the rows
have now been renamed using the university names in the “X” column, but
the column is still part of the dataset. We therefore need to tell R to
delete the column.</p>
<p><code>college &lt;- college[, -1]</code></p>
</div>
</div>
<div id="task-3-2" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 3<a href="#task-3-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Produce summary statistics for all variables in the data object.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a><span class="fu">summary</span>(college)</span></code></pre></div>
<pre><code>##    Private               Apps           Accept          Enroll    
##  Length:777         Min.   :   81   Min.   :   72   Min.   :  35  
##  Class :character   1st Qu.:  776   1st Qu.:  604   1st Qu.: 242  
##  Mode  :character   Median : 1558   Median : 1110   Median : 434  
##                     Mean   : 3002   Mean   : 2019   Mean   : 780  
##                     3rd Qu.: 3624   3rd Qu.: 2424   3rd Qu.: 902  
##                     Max.   :48094   Max.   :26330   Max.   :6392  
##    Top10perc       Top25perc      F.Undergrad     P.Undergrad     
##  Min.   : 1.00   Min.   :  9.0   Min.   :  139   Min.   :    1.0  
##  1st Qu.:15.00   1st Qu.: 41.0   1st Qu.:  992   1st Qu.:   95.0  
##  Median :23.00   Median : 54.0   Median : 1707   Median :  353.0  
##  Mean   :27.56   Mean   : 55.8   Mean   : 3700   Mean   :  855.3  
##  3rd Qu.:35.00   3rd Qu.: 69.0   3rd Qu.: 4005   3rd Qu.:  967.0  
##  Max.   :96.00   Max.   :100.0   Max.   :31643   Max.   :21836.0  
##     Outstate       Room.Board       Books           Personal   
##  Min.   : 2340   Min.   :1780   Min.   :  96.0   Min.   : 250  
##  1st Qu.: 7320   1st Qu.:3597   1st Qu.: 470.0   1st Qu.: 850  
##  Median : 9990   Median :4200   Median : 500.0   Median :1200  
##  Mean   :10441   Mean   :4358   Mean   : 549.4   Mean   :1341  
##  3rd Qu.:12925   3rd Qu.:5050   3rd Qu.: 600.0   3rd Qu.:1700  
##  Max.   :21700   Max.   :8124   Max.   :2340.0   Max.   :6800  
##       PhD            Terminal       S.F.Ratio      perc.alumni   
##  Min.   :  8.00   Min.   : 24.0   Min.   : 2.50   Min.   : 0.00  
##  1st Qu.: 62.00   1st Qu.: 71.0   1st Qu.:11.50   1st Qu.:13.00  
##  Median : 75.00   Median : 82.0   Median :13.60   Median :21.00  
##  Mean   : 72.66   Mean   : 79.7   Mean   :14.09   Mean   :22.74  
##  3rd Qu.: 85.00   3rd Qu.: 92.0   3rd Qu.:16.50   3rd Qu.:31.00  
##  Max.   :103.00   Max.   :100.0   Max.   :39.80   Max.   :64.00  
##      Expend        Grad.Rate     
##  Min.   : 3186   Min.   : 10.00  
##  1st Qu.: 6751   1st Qu.: 53.00  
##  Median : 8377   Median : 65.00  
##  Mean   : 9660   Mean   : 65.46  
##  3rd Qu.:10830   3rd Qu.: 78.00  
##  Max.   :56233   Max.   :118.00</code></pre>
</div>
<div id="task-4-2" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 4<a href="#task-4-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Create a scatterplot matrix of the first three numeric variables.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a><span class="fu">pairs</span>(college[,<span class="dv">2</span><span class="sc">:</span><span class="dv">4</span>])</span></code></pre></div>
<p><img src="01-S01-ANS_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="task-5-2" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 5<a href="#task-5-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Produce side by side box plots of <code>Outstate</code> versus <code>Private</code> using base
R.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a><span class="fu">plot</span>(college<span class="sc">$</span>Private, college<span class="sc">$</span>Outstate, <span class="at">xlab =</span> <span class="st">&quot;Private&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Outstate&quot;</span>)</span></code></pre></div>
<div class="question">
<p>Did this work? Why?</p>
</div>
<div class="answers">
<p>Using the <code>plot()</code> base R function to produce a box plot would produce
an error since the <code>Private</code> variable is of class character. Most
statistical functions will not work with character vectors.</p>
<p><code>Error in plot.window(...) : need finite 'xlim' values</code><br />
<code>In addition: Warning messages:</code><br />
<code>1: In xy.coords(x, y, xlabel, ylabel, log) : NAs introduced by coercion</code><br />
<code>2: In min(x) : no non-missing arguments to min; returning Inf</code><br />
<code>3: In max(x) : no non-missing arguments to max; returning -Inf</code></p>
<p>Creating a box plot with <code>tidyverse</code> would work.</p>
<p><code>college %&gt;%       ggplot(aes(x = Private, y = Outstate)) +        geom_boxplot()</code></p>
<p><img src="images/snips/private.png" style="width: 60%;"/></p>
<p>However, it is important to note that if a variable is not of the right
class, then this might have unintended consequences for example, when
building models. In this case, the <code>Private</code> variable must be
transformed into a factor.</p>
</div>
</div>
<div id="task-6-2" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 6<a href="#task-6-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the <code>Top10perc</code> variable, create a new categorical variable called
<code>Elite</code> such that universities are divided into two groups based on
whether or not the proportion of students coming from the top 10% of
their high school classes exceeds 50%. Hint: use a combination of
<code>mutate()</code> and <code>if_else()</code>.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>college <span class="ot">&lt;-</span> college <span class="sc">%&gt;%</span></span>
<span id="cb13-2"><a href="#cb13-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Elite =</span> <span class="fu">if_else</span>(Top10perc <span class="sc">&gt;</span> <span class="dv">50</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>),</span>
<span id="cb13-3"><a href="#cb13-3" tabindex="-1"></a>         <span class="at">Elite =</span> <span class="fu">as_factor</span>(Elite))</span>
<span id="cb13-4"><a href="#cb13-4" tabindex="-1"></a><span class="co">#do not forget the factor transformation step (categorical variables are factors in R)</span></span></code></pre></div>
</div>
<div id="task-7-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 7<a href="#task-7-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Produce side by side box plots of the new <code>Elite</code> variable and
<code>Outstate</code>.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>college <span class="sc">%&gt;%</span>  </span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Elite, <span class="at">y =</span> Outstate)) <span class="sc">+</span>   </span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>()</span></code></pre></div>
<p><img src="01-S01-ANS_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<div class="question">
<p>How would you produce a similar plot using base R?</p>
</div>
<div class="answers">
<p><code>plot(college$Elite, college$Outstate,  xlab = "Elite", ylab = "Outstate")</code></p>
</div>
</div>
<div id="task-8-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 8<a href="#task-8-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use base R to produce a multipanel plot that displays histograms of the
following variables: <code>Apps</code>, <code>perc.alumni</code>, <code>S.F.Ratio</code>, <code>Expend</code>. Hint:
use <code>par(mfrow=c(2,2))</code> to set up a 2x2 panel. Try to adjust the
specifications (e.g. breaks).</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="co"># An example is shown below. Note that the purpose of the mfrow parameter is </span></span>
<span id="cb15-2"><a href="#cb15-2" tabindex="-1"></a><span class="co"># to change the default way in which R displays plots.</span></span>
<span id="cb15-3"><a href="#cb15-3" tabindex="-1"></a><span class="co"># Once applied, all plots you create later will also be displayed in a 2x2 grid. </span></span>
<span id="cb15-4"><a href="#cb15-4" tabindex="-1"></a><span class="co"># To revert back, you need to enter par(mfrow=c(1,1)) into the console.</span></span>
<span id="cb15-5"><a href="#cb15-5" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb15-7"><a href="#cb15-7" tabindex="-1"></a><span class="fu">hist</span>(college<span class="sc">$</span>Apps)</span>
<span id="cb15-8"><a href="#cb15-8" tabindex="-1"></a><span class="fu">hist</span>(college<span class="sc">$</span>perc.alumni, <span class="at">col=</span><span class="dv">2</span>)</span>
<span id="cb15-9"><a href="#cb15-9" tabindex="-1"></a><span class="fu">hist</span>(college<span class="sc">$</span>S.F.Ratio, <span class="at">col=</span><span class="dv">3</span>, <span class="at">breaks=</span><span class="dv">10</span>)</span>
<span id="cb15-10"><a href="#cb15-10" tabindex="-1"></a><span class="fu">hist</span>(college<span class="sc">$</span>Expend, <span class="at">breaks=</span><span class="dv">100</span>)</span></code></pre></div>
<p><img src="01-S01-ANS_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
<div id="task-9-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 9<a href="#task-9-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using <code>Accept</code> and <code>Apps</code>, create a new variable that describes
acceptance rate. Name this variable <code>acceptance_rate</code>. Hint: use
<code>mutate()</code>.</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" tabindex="-1"></a>college <span class="ot">&lt;-</span> college <span class="sc">%&gt;%</span></span>
<span id="cb16-2"><a href="#cb16-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">acceptance_rate =</span> Accept <span class="sc">/</span> Apps)</span></code></pre></div>
</div>
<div id="task-10-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 10<a href="#task-10-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the <code>acceptance_rate</code> variable, find out which university has the
lowest acceptance rate. Hint: for a <code>tidyverse</code> approach, you can use
<code>slice_min()</code>.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" tabindex="-1"></a>college <span class="sc">%&gt;%</span></span>
<span id="cb17-2"><a href="#cb17-2" tabindex="-1"></a>  <span class="fu">slice_min</span>(<span class="at">order_by =</span> acceptance_rate, <span class="at">n =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>##                      Private  Apps Accept Enroll Top10perc Top25perc
## Princeton University     Yes 13218   2042   1153        90        98
##                      F.Undergrad P.Undergrad Outstate Room.Board Books Personal
## Princeton University        4540         146    19900       5910   675     1575
##                      PhD Terminal S.F.Ratio perc.alumni Expend Grad.Rate Elite
## Princeton University  91       96       8.4          54  28320        99   Yes
##                      acceptance_rate
## Princeton University       0.1544863</code></pre>
</div>
<div id="task-11-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 11<a href="#task-11-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the <code>acceptance_rate</code> variable, find out which university has the
highest acceptance rate.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" tabindex="-1"></a>college <span class="sc">%&gt;%</span></span>
<span id="cb19-2"><a href="#cb19-2" tabindex="-1"></a>  <span class="fu">slice_max</span>(<span class="at">order_by =</span> acceptance_rate, <span class="at">n =</span> <span class="dv">1</span>)</span></code></pre></div>
<pre><code>##                                  Private Apps Accept Enroll Top10perc Top25perc
## Emporia State University              No 1256   1256    853        43        79
## Mayville State University             No  233    233    153         5        12
## MidAmerica Nazarene College          Yes  331    331    225        15        36
## Southwest Baptist University         Yes 1093   1093    642        12        32
## University of Wisconsin-Superior      No  910    910    342        14        53
## Wayne State College                   No 1373   1373    724         6        21
##                                  F.Undergrad P.Undergrad Outstate Room.Board
## Emporia State University                3957         588     5401       3144
## Mayville State University                658          58     4486       2516
## MidAmerica Nazarene College             1100         166     6840       3720
## Southwest Baptist University            1770         967     7070       2500
## University of Wisconsin-Superior        1434         417     7032       2780
## Wayne State College                     2754         474     2700       2660
##                                  Books Personal PhD Terminal S.F.Ratio
## Emporia State University           450     1888  72       75      19.3
## Mayville State University          600     1900  68       68      15.7
## MidAmerica Nazarene College       1100     4913  33       33      15.4
## Southwest Baptist University       400     1000  52       54      15.9
## University of Wisconsin-Superior   550     1960  75       81      15.2
## Wayne State College                540     1660  60       68      20.3
##                                  perc.alumni Expend Grad.Rate Elite
## Emporia State University                   4   5527        50    No
## Mayville State University                 11   6971        51    No
## MidAmerica Nazarene College               20   5524        49    No
## Southwest Baptist University              13   4718        71    No
## University of Wisconsin-Superior          15   6490        36    No
## Wayne State College                       29   4550        52    No
##                                  acceptance_rate
## Emporia State University                       1
## Mayville State University                      1
## MidAmerica Nazarene College                    1
## Southwest Baptist University                   1
## University of Wisconsin-Superior               1
## Wayne State College                            1</code></pre>
</div>
</div>
<div id="practical-2-1" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Practical 2<a href="#practical-2-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="file">
<p>For the tasks below, you will require the <strong>Boston</strong> dataset. This
dataset is part of the <code>MASS</code> R package.</p>
<p>To access the dataset, load the <code>MASS</code> package (install the package
first, if you have not done so previously).</p>
</div>
<div id="task-1-3" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Install and load the <code>MASS</code> package. You will also require <code>tidyverse</code>.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" tabindex="-1"></a><span class="co"># if you haven&#39;t already, install the MASS package before loading</span></span>
<span id="cb21-2"><a href="#cb21-2" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;MASS&quot;</span>)</span>
<span id="cb21-3"><a href="#cb21-3" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb21-4"><a href="#cb21-4" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
<div class="question">
<p>Does R provide any message when loading <code>MASS</code>? Why does this matter?</p>
</div>
<div class="answers">
<p>One important message that R provides when loading <code>MASS</code> is that this
package masks the <code>select()</code> function from <code>tidyverse</code>.</p>
<p><code>Attaching package: ‘MASS’</code></p>
<p><code>The following object is masked from ‘package:dplyr’:</code></p>
<pre><code>`select`  </code></pre>
<p>When masking occurs, this means that both packages contain the same
function. If you were to use the <code>select()</code> function, R will call the
function from the <code>MASS</code> package, rather than from <code>tidyverse (dplyr)</code>
package. This is because the <code>MASS</code> package is the one masking the
function. If you intend to use the <code>select()</code> function as defined by the
<code>tidyverse</code> package, it may not work as intended and/or you may be
prompted by an error message such as:</p>
<p><code>Error in select(...): unused argument (...)</code></p>
<p>To avoid such issues, you must ‘call’ on the package from which you want
R to use the masked function (e.g. <code>dplyr::select()</code>). This is why it is
important to read through all warnings and messages provided in the
console.</p>
</div>
</div>
<div id="task-2-3" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Find out more about the <code>Boston</code> dataset variables by accessing the R
Documentation.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" tabindex="-1"></a>?Boston</span></code></pre></div>
<p>To explore the <code>Boston</code> dataset, simply type the name of the data object
into the console or use <code>View()</code></p>
<div class="question">
<p>What type of data structure is the <code>Boston</code> dataset? What are its
dimensions? How many categorical and quantitative variables are there?</p>
</div>
<div class="answers">
<p>The <code>Boston</code> dataset is a data frame with 506 rows (observations) and 14
columns (variables). There is one categorical variable (<code>chas</code>), and 13
quantitative variables.</p>
</div>
</div>
<div id="task-3-3" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 3<a href="#task-3-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Find the class of all 14 variables. Hint: use <code>sapply</code>.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb24-1"><a href="#cb24-1" tabindex="-1"></a><span class="fu">sapply</span>(Boston, class)</span></code></pre></div>
<pre><code>##      crim        zn     indus      chas       nox        rm       age       dis 
## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;integer&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; 
##       rad       tax   ptratio     black     lstat      medv 
## &quot;integer&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot;</code></pre>
</div>
<div id="task-4-3" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 4<a href="#task-4-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using a <code>tidyverse</code> approach, calculate the mean, median, and standard
deviation of all variables of class <em>numeric</em>.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb26-1"><a href="#cb26-1" tabindex="-1"></a>Boston <span class="sc">%&gt;%</span></span>
<span id="cb26-2"><a href="#cb26-2" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(dplyr<span class="sc">::</span><span class="fu">where</span>(is.numeric)) <span class="sc">%&gt;%</span></span>
<span id="cb26-3"><a href="#cb26-3" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="fu">everything</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb26-4"><a href="#cb26-4" tabindex="-1"></a>  <span class="fu">group_by</span>(name) <span class="sc">%&gt;%</span></span>
<span id="cb26-5"><a href="#cb26-5" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">Mean =</span> <span class="fu">mean</span>(value, <span class="at">na.rm =</span> <span class="cn">TRUE</span>),</span>
<span id="cb26-6"><a href="#cb26-6" tabindex="-1"></a>            <span class="at">SD =</span> <span class="fu">sd</span>(value, <span class="at">na.rm =</span> <span class="cn">TRUE</span>), </span>
<span id="cb26-7"><a href="#cb26-7" tabindex="-1"></a>            <span class="at">Median =</span> <span class="fu">median</span>(value, <span class="at">na.rm =</span> <span class="cn">TRUE</span>))</span></code></pre></div>
<pre><code>## # A tibble: 14 × 4
##    name        Mean      SD  Median
##    &lt;chr&gt;      &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;
##  1 age      68.6     28.1    77.5  
##  2 black   357.      91.3   391.   
##  3 chas      0.0692   0.254   0    
##  4 crim      3.61     8.60    0.257
##  5 dis       3.80     2.11    3.21 
##  6 indus    11.1      6.86    9.69 
##  7 lstat    12.7      7.14   11.4  
##  8 medv     22.5      9.20   21.2  
##  9 nox       0.555    0.116   0.538
## 10 ptratio  18.5      2.16   19.0  
## 11 rad       9.55     8.71    5    
## 12 rm        6.28     0.703   6.21 
## 13 tax     408.     169.    330    
## 14 zn       11.4     23.3     0</code></pre>
<div class="question">
<p>What is the mean pupil-teacher ratio? What is the median and mean per
capita crime rate? Which value do you think is more suitable to describe
per capita crime rate?</p>
</div>
<div class="answers">
<p>The mean pupil-teacher ratio is about 19. The median crime rate is 0.257
whilst the mean is larger at 3.61. Given the difference between the
median and the mean, a skewed distribution is expected, therefore, the
median may be a more a suitable summary statistic to describe crime rate
(a histogram would be needed).</p>
</div>
</div>
<div id="task-5-3" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 5<a href="#task-5-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using a base R approach, create a 2x2 multipanel plot of <code>crim</code> versus
<code>age</code>, <code>dis</code>, <code>rad</code>, <code>tax</code> and <code>ptratio</code>.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb28-2"><a href="#cb28-2" tabindex="-1"></a><span class="fu">plot</span>(Boston<span class="sc">$</span>age, Boston<span class="sc">$</span>crim)</span>
<span id="cb28-3"><a href="#cb28-3" tabindex="-1"></a><span class="fu">plot</span>(Boston<span class="sc">$</span>dis, Boston<span class="sc">$</span>crim)</span>
<span id="cb28-4"><a href="#cb28-4" tabindex="-1"></a><span class="fu">plot</span>(Boston<span class="sc">$</span>rad, Boston<span class="sc">$</span>crim)</span>
<span id="cb28-5"><a href="#cb28-5" tabindex="-1"></a><span class="fu">plot</span>(Boston<span class="sc">$</span>tax, Boston<span class="sc">$</span>crim)</span></code></pre></div>
<p><img src="01-S01-ANS_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<div class="question">
<p>What can you say about the relationships between <code>age</code>, <code>dis</code>, <code>rad</code>,
<code>tax</code>, and <code>crim</code>?</p>
</div>
<div class="answers">
<p>As the age of the home increases (<code>age</code>), crime also increases. There is
also higher crime around employment centers (<code>dis</code>). With very high
index of accessibility to radial highways (<code>rad</code>), and tax rates (<code>tax</code>)
there also appears to be high crime rates.</p>
</div>
</div>
<div id="task-6-3" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 6<a href="#task-6-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using a base R approach, create and display histograms of <code>crim</code>, <code>tax</code>
and <code>ptratio</code> in a 1x2 multipanel plot. Set the <code>breaks</code> argument to
<strong>25</strong> .</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow=</span><span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</span>
<span id="cb29-2"><a href="#cb29-2" tabindex="-1"></a><span class="fu">hist</span>(Boston<span class="sc">$</span>crim, <span class="at">breaks=</span><span class="dv">25</span>)</span>
<span id="cb29-3"><a href="#cb29-3" tabindex="-1"></a><span class="fu">hist</span>(Boston<span class="sc">$</span>tax, <span class="at">breaks=</span><span class="dv">25</span>)</span></code></pre></div>
<p><img src="01-S01-ANS_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<div class="question">
<p>What do these histograms indicate?</p>
</div>
<div class="answers">
<p>Most areas have low crime rates, but there is a rather long tail showing
high crime rates (although the frequency seems to be very low). Given
the degree of skew, the mean would not be a good measure of central
tendency. With respect to tax rates, there appears to be a large divide
between low taxation and high taxation, with the highest peak at around
670.</p>
<p><em>Remember to revert back to single panel display.</em></p>
</div>
<!--chapter:end:01-S01-ANS.Rmd-->
</div>
</div>
</div>
<div id="part-section-2" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART*) Section 2<a href="#part-section-2" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="overview-1" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Overview<a href="#overview-1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div style="color: #333; font-size: 24px; font-style: italic; text-align: justify;">
<p>Section 2: Linear Regression and Prediction</p>
</div>
<p>This section is comprised of two demonstrations and three practicals.</p>
<p>The two demonstrations and Practicals 1 and 2 are adapted from exercises
from the core textbook for this course: James, G., Witten, D., Hastie,
T. and Tibshirani, R. (2021). <em>An Introduction to Statistical Learning
with Applications in R</em>. 2nd ed. New York: Springer.
<a href="https://www.statlearning.com/" class="uri">https://www.statlearning.com/</a></p>
<p>Practical 3 is based on a demonstration developed by Dr. Tatjana
Kecojevic, Lecturer in Social Statistics.</p>
<div class="ilos">
<p><strong>Learning Outcomes:</strong></p>
<ul>
<li>explain the relevance of the intercept;<br />
</li>
<li>appreciate the impact of noise on coefficient estimates;<br />
</li>
<li>produce and interpret diagnostic plots with base R;<br />
</li>
<li>include non-linear transformations and interactions;<br />
</li>
<li>compare model fit;<br />
</li>
<li>compute and interpret confidence intervals.<br />
</li>
</ul>
</div>
<p><strong>In this section, you will practice using the functions below. It is
highly recommended that you explore these functions further using the
Help tab in your RStudio console.</strong></p>
<table style="width:96%;">
<colgroup>
<col width="19%" />
<col width="61%" />
<col width="15%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Function</th>
<th align="center">Description</th>
<th align="center">Package</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><code>lm()</code></td>
<td align="center">fit linear models</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>predict()</code></td>
<td align="center">generic function for predictions from
results of different models (e.g.
<code>predict.lm()</code>)</td>
<td align="center">stats</td>
</tr>
<tr class="odd">
<td align="center"><code>confint()</code></td>
<td align="center">compute confidence intervals</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>plot()</code></td>
<td align="center">generic function for plotting</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>legend()</code></td>
<td align="center"><p>add legend (to <code>plot()</code>)</p>
<p>arguments such as <code>col</code>, <code>lty</code>, and <code>cex</code>
control colour, line type, and font size
respectively</p></td>
<td align="center">graphics</td>
</tr>
<tr class="even">
<td align="center"><code>abline()</code></td>
<td align="center">adding one or more straight lines to plot</td>
<td align="center">graphics</td>
</tr>
<tr class="odd">
<td align="center"><code>cor()</code></td>
<td align="center">computes correlation between variables</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>rnorm()</code></td>
<td align="center">generates normal distribution</td>
<td align="center">stats</td>
</tr>
<tr class="odd">
<td align="center"><code>poly()</code></td>
<td align="center">returns or evaluates polynomials</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>par()</code></td>
<td align="center">set graphical parameters (e.g. <code>mfrow()</code>
)</td>
<td align="center">graphics</td>
</tr>
<tr class="odd">
<td align="center"><code>subset()</code></td>
<td align="center">return subset of a data object (vector,
matrix, or dataframe) according to
condition(s)</td>
<td align="center">base</td>
</tr>
<tr class="even">
<td align="center"><code>anova()</code></td>
<td align="center">compute analysis of variance for</td>
<td align="center">stats</td>
</tr>
<tr class="odd">
<td align="center"><code>rnorm()</code></td>
<td align="center">density, distribution function, quantile
function and random generation for the
normal distribution</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>sqrt()</code></td>
<td align="center">compute square root</td>
<td align="center">base</td>
</tr>
</tbody>
</table>
<!--chapter:end:02-S02-overview.Rmd-->
</div>
<div id="demonstration-1" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Demonstration 1<a href="#demonstration-1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="simple-linear-models-without-intercept" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Simple Linear Models Without Intercept<a href="#simple-linear-models-without-intercept" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s investigate the <em>t-statistic</em> for the null hypothesis <span class="math inline">\(H_{0}:\beta = 0\)</span> in simple linear regression <strong>without</strong> an intercept. The equation for a model without the intercept would therefore be <span class="math inline">\(Y = \beta X\)</span>.</p>
<p>By excluding the intercept, the model is constrained to pass through the origin <span class="math inline">\((0,0)\)</span>, allowing the relationship between the response and predictor to be interpreted as proportional. In other words, the removal of the intercept forces the regression line to start at <span class="math inline">\((0,0)\)</span>, so when <span class="math inline">\(x = 0\)</span>, then <span class="math inline">\(y = 0\)</span>.</p>
<p>Let’s first generate some data for a predictor <span class="math inline">\(x\)</span> and a response <span class="math inline">\(y\)</span>.</p>
<p>We select a seed value to ensure that we generate the same data every time. To generate values, we use the <code>rnorm</code> function to produce 100 data values drawn from a normal distribution (hence <code>rnorm()</code>).</p>
<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb30-2"><a href="#cb30-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb30-3"><a href="#cb30-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span></code></pre></div>
<p>Now that we generated our predictor and our response variable, let’s run a simple linear regression without an intercept using <span class="math inline">\(y\)</span> as the response and <span class="math inline">\(x\)</span> as a predictor. One way to do so is by adding <span class="math inline">\(0\)</span> into the formula.</p>
<div class="sourceCode" id="cb31"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x <span class="sc">+</span> <span class="dv">0</span>)</span></code></pre></div>
<p>And now, let’s have a look at the results.</p>
<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(fit))</span></code></pre></div>
<pre><code>##   Estimate Std. Error  t value     Pr(&gt;|t|)
## x 1.993876  0.1064767 18.72593 2.642197e-34</code></pre>
<p>We can see a significant positive relationship between y and x. The coefficient estimate for <span class="math inline">\(x\)</span> is <span class="math inline">\(1.993876\)</span>, and since the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is proportional, we interpret the estimate as the <span class="math inline">\(y\)</span> values being predicted to be (a little below) twice the <span class="math inline">\(x\)</span> values.</p>
<p>But what happens if we swap <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> and run a model using <span class="math inline">\(y\)</span> as the predictor and <span class="math inline">\(x\)</span> as the response?</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="#cb34-1" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(x <span class="sc">~</span> y <span class="sc">+</span> <span class="dv">0</span>)</span>
<span id="cb34-2"><a href="#cb34-2" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(fit2))</span></code></pre></div>
<pre><code>##    Estimate Std. Error  t value     Pr(&gt;|t|)
## y 0.3911145 0.02088625 18.72593 2.642197e-34</code></pre>
<p>We again observe a significant positive relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, except that the <span class="math inline">\(x\)</span> values are predicted to be (a little below) half the <span class="math inline">\(y\)</span> values (since the coefficient estimate is <span class="math inline">\(0.3911145\)</span>).</p>
<p>Note also the t-statistic values for the two models. They are identical and of course so is the p-value (therefore, there is a significant relationships between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>).</p>
<p>Therefore, the results of the models of <span class="math inline">\(y\)</span> onto <span class="math inline">\(x\)</span> and <span class="math inline">\(x\)</span> onto <span class="math inline">\(y\)</span> indicate that the coefficients would be the inverse of each other (2 and 1/2) whilst the t-statistic values (and p-values) remain the same.</p>
<p><strong>Why are the t-statistic values identical?</strong></p>
<p>For each coefficient, the t statistic is calculated by dividing the coefficient estimate by its standard error. For example, for the <em>fit2</em> model, we have a coefficient estimate of <span class="math inline">\(0.3911145\)</span> and a standard error of <span class="math inline">\(0.02088625\)</span> and so dividing <span class="math inline">\(0.3911145\)</span> by <span class="math inline">\(0.02088625\)</span> gives us <span class="math inline">\(18.72593\)</span>.</p>
<p>You’ll also remember that the correlation coefficient between two variables is symmetric and so the correlation between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is the same as for <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. This is the reason why it is incorrect to state that “<span class="math inline">\(X\)</span> causes a change in <span class="math inline">\(Y\)</span>”.</p>
<p>In a linear model, we are testing whether there is a linear association between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> but not if X causes Y or Y causes X. Therefore, irrespective of whether we are regressing <span class="math inline">\(y\)</span> onto <span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span> onto <span class="math inline">\(y\)</span>, the t-statistic is testing the same null hypothesis <span class="math inline">\(H_{0} : \beta = 0\)</span> (i.e. fundamentally, it is testing whether there is a linear correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>).</p>
<p><strong>So what exactly is the role of the intercept?</strong></p>
<p>As you already know, the intercept represents the value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x = 0\)</span> which can be thought of as the initial value effect that exists independently of <span class="math inline">\(x\)</span>. This not only applies to the simple linear regression model but also to the multiple linear regression model (i.e. the intercept is the value of <span class="math inline">\(y\)</span> when all predictors are zero). In other words, the intercept adjusts the starting point of regression line and allows for the line to shift up or down on the y-axis thus reflecting a “baseline” level of <span class="math inline">\(y\)</span> that is not dependent on <span class="math inline">\(x\)</span>. With an intercept, the slope coefficient still tells us how much <span class="math inline">\(Y\)</span> changes with a one-unit change in <span class="math inline">\(x\)</span>, but this change is relative to the value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x = 0\)</span> and this is important when <span class="math inline">\(x\)</span> can take a value of <span class="math inline">\(0\)</span> that is meaningful to the model.</p>
<p><em>Without the intercept</em>, the line is forced to pass through the origin <span class="math inline">\((0,0)\)</span>, which may not be suitable unless the data naturally begin at zero (or there are some other theoretical or practical reasons which warrant the line passing through the origin).</p>
<p><em>With an intercept</em>, the regression line is no longer forced to pass through zero (and will only do so if the data naturally begin at zero). The intercept therefore allows for the regression line to better fit the data, particularly when the data do not actually begin at zero. In this way, the model can capture the average outcome when the predictor(s) is/are zero.</p>
</div>
<div id="simple-linear-models-with-intercept" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Simple Linear Models with Intercept<a href="#simple-linear-models-with-intercept" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Do you think that the t-statistic will be the same for both regression of Y onto X and X onto Y if we were to include the intercept?</em></p>
<p>We use the same data as before and run a regression with <span class="math inline">\(y\)</span> as response and <span class="math inline">\(x\)</span> as predictor and include the intercept.</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="#cb36-1" tabindex="-1"></a>fit3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span></code></pre></div>
<p>We then extract the model coefficients from the summary results of the model.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(fit3))</span></code></pre></div>
<pre><code>##                Estimate Std. Error    t value     Pr(&gt;|t|)
## (Intercept) -0.03769261 0.09698729 -0.3886346 6.983896e-01
## x            1.99893961 0.10772703 18.5555993 7.723851e-34</code></pre>
<p>How does coefficient for <strong>fit3</strong> compare to <strong>fit</strong>? How about the t-statistic value?</p>
<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(fit))</span></code></pre></div>
<pre><code>##   Estimate Std. Error  t value     Pr(&gt;|t|)
## x 1.993876  0.1064767 18.72593 2.642197e-34</code></pre>
<p>As you can see, the coefficient for the model with the intercept (<strong>fit3</strong>) is very similar to the coefficient for the model without the intercept (<strong>fit</strong>). The t-statistic is also very close (<span class="math inline">\(18.72593\)</span> for the model without intercept and <span class="math inline">\(18.5555993\)</span> for the model with the intercept).</p>
<p>Now we run a regression with <span class="math inline">\(x\)</span> <em>as response</em> and <span class="math inline">\(y\)</span> <em>as predictor</em>.</p>
<div class="sourceCode" id="cb41"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" tabindex="-1"></a>fit4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(x <span class="sc">~</span> y)</span></code></pre></div>
<p>We then extract the model coefficients from the summary results of the model.</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="#cb42-1" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(fit4))</span></code></pre></div>
<pre><code>##               Estimate Std. Error    t value     Pr(&gt;|t|)
## (Intercept) 0.03880394 0.04266144  0.9095787 3.652764e-01
## y           0.38942451 0.02098690 18.5555993 7.723851e-34</code></pre>
<p>How does the coefficient for <strong>fit4</strong> compare to <strong>fit2</strong>? How about the t-statistic value? Are the t-statistic values different between the <strong>fit3</strong> and <strong>fit4</strong> models?</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="#cb44-1" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(fit2))</span></code></pre></div>
<pre><code>##    Estimate Std. Error  t value     Pr(&gt;|t|)
## y 0.3911145 0.02088625 18.72593 2.642197e-34</code></pre>
<p>The slope coefficient for the model with the intercept (<span class="math inline">\(0.38942451\)</span>) is very similar to the coefficient for the model without the intercept (<span class="math inline">\(0.3911145\)</span>) and so is the t-statistic.</p>
<p>Also, as expected, the t-statistic value for the two models for which we included the intercept are identical. Therefore, irrespective of whether we include the intercept or not, the t-statistic value for the regression of <span class="math inline">\(y\)</span> onto <span class="math inline">\(x\)</span> will be identical to the t-statistic value for the regression of <span class="math inline">\(x\)</span> onto <span class="math inline">\(y\)</span>.</p>
<!--chapter:end:02-S02-D1.Rmd-->
</div>
</div>
<div id="demonstration-2" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Demonstration 2<a href="#demonstration-2" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="population-parameters-and-estimated-coefficients" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Population Parameters and Estimated Coefficients<a href="#population-parameters-and-estimated-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s explore the differences between population parameters and estimated coefficients.</p>
<p>We will generate a “true” population dataset.</p>
<p>We create a variable <span class="math inline">\(X\)</span> with 100 observations drawn from a normal distribution. To be more specific about the characteristic of our variable <span class="math inline">\(X\)</span>, we will not only specify the total number of observations (100), but also the mean (0), and standard deviation (1). This will be our <em>predictor</em>.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="#cb46-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb46-2"><a href="#cb46-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<p>We now create another vector called <strong>eps</strong> containing 100 observations drawn from a normal distribution with a mean of zero and a variance of 0.25. This will be the our error term <span class="math inline">\(\epsilon\)</span>.</p>
<div class="sourceCode" id="cb47"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1" tabindex="-1"></a>eps <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">0.25</span>))</span></code></pre></div>
<p>Using <span class="math inline">\(X\)</span> and <span class="math inline">\(\epsilon\)</span>, we now generate a vector <span class="math inline">\(Y\)</span> according to the following formula: <span class="math inline">\(Y = -1 + 0.5X + \epsilon\)</span>. Essentially, we specify our intercept, the slope coefficient, the predictor variable and the error to obtain our <em>response</em> variable.</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="#cb48-1" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> x <span class="sc">+</span> eps</span></code></pre></div>
<p>The values <span class="math inline">\(-1\)</span> and <span class="math inline">\(0.5\)</span> represent the “true” population values for the intercept <span class="math inline">\(\beta_{0}\)</span> and slope <span class="math inline">\(\beta_{1}\)</span> respectively.</p>
<p>Now we can create a scatterplot to observe the association between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1" tabindex="-1"></a><span class="fu">plot</span>(x, y)</span></code></pre></div>
<p><img src="02-S02-D2_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>The plot indicates a linear relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. The relationship is clearly not perfectly linear due to noise.</p>
<p><strong>If we were to instead estimate the intercept and slope, to what degree do you think these estimated coefficients will differ from the true population values?</strong></p>
<p>Ok, so we have the variables we generated, so our predictor <strong>X</strong> and our response <strong>X</strong> and we run a regression model.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="#cb50-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb50-2"><a href="#cb50-2" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.93842 -0.30688 -0.06975  0.26970  1.17309 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.01885    0.04849 -21.010  &lt; 2e-16 ***
## x            0.49947    0.05386   9.273 4.58e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4814 on 98 degrees of freedom
## Multiple R-squared:  0.4674, Adjusted R-squared:  0.4619 
## F-statistic: 85.99 on 1 and 98 DF,  p-value: 4.583e-15</code></pre>
<p>The results of the model show an estimated slope coefficient (<span class="math inline">\(\hat{\beta_{1}}\)</span>) for <span class="math inline">\(x\)</span> of <span class="math inline">\(0.49947\)</span>. This is very close to the population value (<span class="math inline">\(\beta_{1}\)</span>) which is <span class="math inline">\(0.5\)</span>.</p>
<p>We see a similar estimated value for the intercept (<span class="math inline">\(\hat{\beta_{0}}\)</span>) which is <span class="math inline">\(-1.01885\)</span>, again very close to the true value for the intercept (<span class="math inline">\(\beta_{0}\)</span>) which is <span class="math inline">\(-1\)</span>.</p>
<p>Therefore, if we were to plot the population regression line and the estimated regression line, we would see that the two are difficult to distinguish (given the similarity of the estimated and true values for the coefficients).</p>
<div class="sourceCode" id="cb52"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb52-1"><a href="#cb52-1" tabindex="-1"></a><span class="fu">plot</span>(x, y)</span>
<span id="cb52-2"><a href="#cb52-2" tabindex="-1"></a><span class="fu">abline</span>(fit)</span>
<span id="cb52-3"><a href="#cb52-3" tabindex="-1"></a><span class="fu">abline</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="fl">0.5</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb52-4"><a href="#cb52-4" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>,</span>
<span id="cb52-5"><a href="#cb52-5" tabindex="-1"></a>  <span class="fu">c</span>(<span class="st">&quot;model fit&quot;</span>, <span class="st">&quot;population regression&quot;</span>),</span>
<span id="cb52-6"><a href="#cb52-6" tabindex="-1"></a>  <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>),</span>
<span id="cb52-7"><a href="#cb52-7" tabindex="-1"></a>  <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), </span>
<span id="cb52-8"><a href="#cb52-8" tabindex="-1"></a>  <span class="at">cex =</span> <span class="fl">0.72</span>,</span>
<span id="cb52-9"><a href="#cb52-9" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="02-S02-D2_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>What if we were to fit a polynomial regression model? Would there be any evidence that adding a quadratic term improves the model fit?</p>
<p>To add a polynomial term of degree two, we can use the <code>poly</code> base R function directly in the code for the model.</p>
<p>Since the F-test is not statistically significant, there is no evidence that adding a quadratic term improves the model fit.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> <span class="fu">poly</span>(x, <span class="dv">2</span>))</span>
<span id="cb53-2"><a href="#cb53-2" tabindex="-1"></a><span class="fu">anova</span>(fit, fit2)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: y ~ x
## Model 2: y ~ poly(x, 2)
##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
## 1     98 22.709                           
## 2     97 22.257  1   0.45163 1.9682 0.1638</code></pre>
</div>
<div id="what-happens-if-we-reduce-noise" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">What happens if we <em>reduce</em> noise?<a href="#what-happens-if-we-reduce-noise" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For our first model (<strong>fit</strong>), we specified a variance of <span class="math inline">\(0.5\)</span> (standard deviation 0.25) for <span class="math inline">\(\epsilon\)</span> and we noted an <span class="math inline">\(R^2\)</span> value of <span class="math inline">\(0.4674\)</span>.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="#cb55-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb55-2"><a href="#cb55-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb55-3"><a href="#cb55-3" tabindex="-1"></a>eps <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">0.25</span>))</span>
<span id="cb55-4"><a href="#cb55-4" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> x <span class="sc">+</span> eps</span>
<span id="cb55-5"><a href="#cb55-5" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb55-6"><a href="#cb55-6" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.93842 -0.30688 -0.06975  0.26970  1.17309 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.01885    0.04849 -21.010  &lt; 2e-16 ***
## x            0.49947    0.05386   9.273 4.58e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.4814 on 98 degrees of freedom
## Multiple R-squared:  0.4674, Adjusted R-squared:  0.4619 
## F-statistic: 85.99 on 1 and 98 DF,  p-value: 4.583e-15</code></pre>
<p>Now, let’s observe what happens to the <span class="math inline">\(R^2\)</span> value if we <em>reduce</em> noise from 0.25 to 0.05. We can do so directly when we generate data for our variable <span class="math inline">\(y\)</span> without needing to create a new <strong>eps</strong> object.</p>
<p>The results show that the <span class="math inline">\(R^{2}\)</span> value for <strong>fit3</strong> is much higher than the <span class="math inline">\(R^{2}\)</span> value for <strong>fit</strong>.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb57-2"><a href="#cb57-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="fu">sqrt</span>(<span class="fl">0.05</span>))</span>
<span id="cb57-3"><a href="#cb57-3" tabindex="-1"></a></span>
<span id="cb57-4"><a href="#cb57-4" tabindex="-1"></a>fit3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb57-5"><a href="#cb57-5" tabindex="-1"></a><span class="fu">summary</span>(fit3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.61308 -0.12553 -0.00391  0.15199  0.41332 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -0.98917    0.02216  -44.64   &lt;2e-16 ***
## x            0.52375    0.02152   24.33   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2215 on 98 degrees of freedom
## Multiple R-squared:  0.858,  Adjusted R-squared:  0.8565 
## F-statistic: 592.1 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>By plotting the data we can clearly see that the data points are less dispersed than before and therefore, the association between x and y appears more linear. We can also observe that the estimated regression line deviates slightly from the population regression line (particularly at lowest and highest values).</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1" tabindex="-1"></a><span class="fu">plot</span>(x, y)</span>
<span id="cb59-2"><a href="#cb59-2" tabindex="-1"></a><span class="fu">abline</span>(fit3)</span>
<span id="cb59-3"><a href="#cb59-3" tabindex="-1"></a><span class="fu">abline</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="fl">0.5</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb59-4"><a href="#cb59-4" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>,</span>
<span id="cb59-5"><a href="#cb59-5" tabindex="-1"></a>  <span class="fu">c</span>(<span class="st">&quot;model fit&quot;</span>, <span class="st">&quot;population regression&quot;</span>),</span>
<span id="cb59-6"><a href="#cb59-6" tabindex="-1"></a>  <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>),</span>
<span id="cb59-7"><a href="#cb59-7" tabindex="-1"></a>  <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), </span>
<span id="cb59-8"><a href="#cb59-8" tabindex="-1"></a>  <span class="at">cex =</span> <span class="fl">0.72</span></span>
<span id="cb59-9"><a href="#cb59-9" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="02-S02-D2_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="what-happens-if-we-increase-noise" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">What happens if we <em>increase</em> noise?<a href="#what-happens-if-we-increase-noise" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Using the same approach as before, we now increase the standard deviation to 1.</p>
<p>Now, the <span class="math inline">\(R^{2}\)</span> value for <strong>fit4</strong> is much <em>lower</em> than that of either <strong>fit3</strong> or <strong>fit</strong>.</p>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="#cb60-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb60-2"><a href="#cb60-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">1</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">100</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb60-3"><a href="#cb60-3" tabindex="-1"></a></span>
<span id="cb60-4"><a href="#cb60-4" tabindex="-1"></a>fit4 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb60-5"><a href="#cb60-5" tabindex="-1"></a><span class="fu">summary</span>(fit4)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.51014 -0.60549  0.02065  0.70483  2.08980 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -1.04745    0.09676 -10.825  &lt; 2e-16 ***
## x            0.42505    0.08310   5.115 1.56e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9671 on 98 degrees of freedom
## Multiple R-squared:  0.2107, Adjusted R-squared:  0.2027 
## F-statistic: 26.16 on 1 and 98 DF,  p-value: 1.56e-06</code></pre>
<p>If we plot the data, we can observe that the data points are more dispersed and therefore, the estimated regression line deviates to a greater extent from the population regression line.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="#cb62-1" tabindex="-1"></a><span class="fu">plot</span>(x, y)</span>
<span id="cb62-2"><a href="#cb62-2" tabindex="-1"></a><span class="fu">abline</span>(fit4)</span>
<span id="cb62-3"><a href="#cb62-3" tabindex="-1"></a><span class="fu">abline</span>(<span class="sc">-</span><span class="dv">1</span>, <span class="fl">0.5</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb62-4"><a href="#cb62-4" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>,</span>
<span id="cb62-5"><a href="#cb62-5" tabindex="-1"></a>  <span class="fu">c</span>(<span class="st">&quot;model fit&quot;</span>, <span class="st">&quot;population regression&quot;</span>),</span>
<span id="cb62-6"><a href="#cb62-6" tabindex="-1"></a>  <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>),</span>
<span id="cb62-7"><a href="#cb62-7" tabindex="-1"></a>  <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), </span>
<span id="cb62-8"><a href="#cb62-8" tabindex="-1"></a>  <span class="at">cex =</span> <span class="fl">0.72</span>,</span>
<span id="cb62-9"><a href="#cb62-9" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="02-S02-D2_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="how-does-noise-affect-confidence-intervals-for-the-coefficients" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">How does noise affect confidence intervals for the coefficients?<a href="#how-does-noise-affect-confidence-intervals-for-the-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <strong>fit3</strong> model is the model with the lowest amount of noise (standard deviation of 0.05), whilst <strong>fit4</strong> is the model with the largest amount of noise (standard deviation of 1).</p>
<p>The confidence interval for the coefficient for the model with the <em>highest</em> noise is the widest. The larger the amount of noise, the wider the interval and therefore, the less precise the coefficient estimates will be.</p>
<p>Conversely, the narrowest interval is that of the model with the <em>lowest</em> noise which yielded the most precise estimates.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1" tabindex="-1"></a><span class="fu">confint</span>(fit)</span></code></pre></div>
<pre><code>##                  2.5 %     97.5 %
## (Intercept) -1.1150804 -0.9226122
## x            0.3925794  0.6063602</code></pre>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1" tabindex="-1"></a><span class="fu">confint</span>(fit3)</span></code></pre></div>
<pre><code>##                 2.5 %     97.5 %
## (Intercept) -1.033141 -0.9451916
## x            0.481037  0.5664653</code></pre>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1" tabindex="-1"></a><span class="fu">confint</span>(fit4)</span></code></pre></div>
<pre><code>##                  2.5 %     97.5 %
## (Intercept) -1.2394772 -0.8554276
## x            0.2601391  0.5899632</code></pre>
<!--chapter:end:02-S02-D2.Rmd-->
</div>
</div>
<div id="practical-1-2" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Practical 1<a href="#practical-1-2" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="file">
<p>For the tasks below, you will require the <strong>Auto</strong> dataset from the core
textbook (James et. al 2021).</p>
<p>This dataset is part of the <code>ISRL2</code> package. By loading the package, the
<strong>Auto</strong> dataset loads automatically:</p>
<p><code>library(ISLR2)</code></p>
</div>
<p>The <strong>Auto</strong> dataset contains information such as engine horsepower, gas mileage, model year, and origin of car, for 392 vehicles. It is a dataframe with 392 observations and 9 variables. The variables are:</p>
<ul>
<li>mpg: miles per gallon</li>
<li>cylinders: Number of cylinders between 4 and 8</li>
<li>displacement: Engine displacement (cu. inches)</li>
<li>horsepower: Engine horsepower</li>
<li>weight: Vehicle weight (lbs.)</li>
<li>acceleration: Time to accelerate from 0 to 60 mph (sec.)</li>
<li>year: Model year</li>
<li>origin: Origin of car (1. American, 2. European, 3. Japanese)</li>
<li>name: Vehicle name</li>
</ul>
<div id="task-1-4" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use the <code>lm()</code> function to perform simple linear regression with <strong>mpg</strong>
as the response and <strong>horsepower</strong> as the predictor. Store the output in
an object called <strong>fit</strong>.</p>
</div>
<div id="task-2-4" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Have a look at the results of the model.</p>
<div class="question">
<p>Is there a relationship between the predictor and the response?</p>
</div>
</div>
<div id="task-3-4" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 3<a href="#task-3-4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>What is the associated 95% confidence intervals for predicted miles per
gallon associated with an engine horsepower of 98? Hint: use the
<code>predict()</code> function. For confidence intervals, set the <code>interval</code>
argument to <code>confidence</code>.</p>
</div>
<div id="task-4-4" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 4<a href="#task-4-4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How about the prediction interval for the same value?</p>
<div class="question">
<p>Are the two intervals different? Why?</p>
</div>
</div>
<div id="task-5-4" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 5<a href="#task-5-4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using base R, plot the response and the predictor as well as the least
squares regression line. Add suitable labels to the X and Y axes.</p>
</div>
<div id="task-6-4" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 6<a href="#task-6-4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use base R to produce diagnostic plots of the least squares regression
fit. Display these in a 2X2 grid.</p>
</div>
<div id="task-7-2" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 7<a href="#task-7-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Subset the <strong>Auto</strong> dataset such that it excludes the <strong>name</strong> and
<strong>origin</strong> variables and store this subsetted dataset in a new object
called <strong>quant_vars</strong>.</p>
</div>
<div id="task-8-2" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 8<a href="#task-8-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Compute a correlation matrix of all variables.</p>
<div class="question">
<p>Did you use the <strong>Auto</strong> dataset or the <strong>quant_vars</strong> object? Why does
it matter which data object you use?</p>
</div>
</div>
<div id="task-9-2" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 9<a href="#task-9-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the <strong>quant_vars</strong> object, perform multiple linear regression with
miles per gallon as the response and all other variables as the
predictors.</p>
<p>Store the results in an object called <strong>fit2</strong>.</p>
</div>
<div id="task-10-2" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 10<a href="#task-10-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Have a look at the results of the multiple regression model.</p>
<div class="question">
<p>Is there a relationship between the predictors and the response? Which
predictors appear to have a statistically significant relationship to
the response? What does the coefficient for the year variable suggest?</p>
</div>
</div>
<div id="task-11-2" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 11<a href="#task-11-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Produce diagnostic plots of the multiple linear regression fit in a 2x2
grid.</p>
<div class="question">
<p>Do the residual plots suggest any unusually large outliers? Does the
leverage plot identify any observations with unusually high leverage?</p>
</div>
</div>
<div id="task-12" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 12<a href="#task-12" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Fit separate linear regression models with interaction effect terms for:
weight and horsepower, acceleration and horsepower, and cylinders and
weight.</p>
<div class="question">
<p>Are any of the interaction terms statistically significant?</p>
</div>
</div>
<div id="task-13" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 13<a href="#task-13" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the <strong>Auto</strong> data object, apply transformations for the
<strong>horsepower</strong> variable and plot the relationship between <strong>horsepower</strong>
and <strong>mpg</strong> in a 2x2 grid.</p>
<ul>
<li>First plot: use the original variable;<br />
</li>
<li>Second plot: apply log transform;<br />
</li>
<li>Third plot: raise it to the power of two.</li>
</ul>
<div class="question">
<p>Which of these transformations is most suitable?</p>
</div>
</div>
<div id="task-14" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 14<a href="#task-14" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now run a multiple regression model with all variables as before but
this time, apply a log transformation of the <strong>horsepower</strong> variable.</p>
</div>
<div id="task-15" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 15<a href="#task-15" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Have a look at the results.</p>
<div class="question">
<p>How do the results of model <strong>fit3</strong> differ from those of model
<strong>fit2</strong>?</p>
</div>
</div>
<div id="task-16" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 16<a href="#task-16" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Produce diagnostic plots for the <strong>fit3</strong> object and display them in a
2x2 grid.</p>
<div class="question">
<p>How do the diagnostic plots differ?</p>
</div>
<!--chapter:end:02-S02-P1.Rmd-->
</div>
</div>
<div id="practical-2-2" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Practical 2<a href="#practical-2-2" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="file">
<p>For the tasks below, you will require the <strong>Carseats</strong> dataset from the
core textbook (James et. al 2021).</p>
<p>This dataset is part of the <code>ISRL2</code> package. By loading the package, the
<strong>Carseats</strong> dataset loads automatically:</p>
<p><code>library(ISLR2)</code></p>
</div>
<p><strong>Carseats</strong> is a simulated dataset comprising of sales of child car seats at 400 different stores. It is a datafrate with 400 observations and 11 variables. The variables are:</p>
<ul>
<li>Sales: Unit sales (thousands of dollars) at each location<br />
</li>
<li>CompPrice: Price charged by competitor at each location<br />
</li>
<li>Income: Community income level (thousands of dollars)<br />
</li>
<li>Advertising: Local advertising budget for company at each location
(thousands of dollars)<br />
</li>
<li>Population: Population size in region (thousands of dollars)<br />
</li>
<li>Price: Price company charges for car seats at each site<br />
</li>
<li>ShelveLoc: Quality of the shelving location for the car seats at
each site<br />
</li>
<li>Age: Average age of the local population<br />
</li>
<li>Education: Education level at each location<br />
</li>
<li>Urban: Whether the store is in an urban or rural location<br />
</li>
<li>US: Whether the store is in the US or not</li>
</ul>
<div id="task-1-5" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Task 1<a href="#task-1-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Fit a multiple regression model to predict unit sales at each location
based on price company charges for car seats, whether the store is in an
urban or rural location, and whether the store is in the US or not.</p>
</div>
<div id="task-2-5" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Task 2<a href="#task-2-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Have a look at the results and interpret the coefficients.</p>
<div class="question">
<p>Which coefficients are statistically significant? What do they indicate?</p>
</div>
</div>
<div id="task-3-5" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Task 3<a href="#task-3-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Based on the conclusions you have drawn in Task 2, now fit a smaller
model that only uses the predictors for which there is evidence of
association with sales.</p>
</div>
<div id="task-4-5" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Task 4<a href="#task-4-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Compare the two models (<em>fit</em> and <em>fit2</em>).</p>
<div class="question">
<p>Which model is the better fit?</p>
</div>
</div>
<div id="task-5-5" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Task 5<a href="#task-5-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Produce diagnostic plots for <em>fit2</em> and display these in a 2x2 grid.</p>
<div class="question">
<p>Is there evidence of outliers or high leverage observations in the
<em>fit2</em> model?</p>
</div>
<!--chapter:end:02-S02-P2.Rmd-->
</div>
</div>
<div id="practical-3-the-quality-of-red-bordeaux-vintages" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Practical 3: The Quality of Red Bordeaux Vintages<a href="#practical-3-the-quality-of-red-bordeaux-vintages" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>This practical was developed by Dr. Tatjana Kecojevic, Lecturer in Social Statistics.</em></p>
<p>We would like to analyse the quality of a vintage that has been quantified as the price and make the interpretation of our statistical findings.</p>
<p>We are going to use <code>wine.csv</code> available from Eduardo García Portugués’s book: <em>Notes for Predictive Modelling; <a href="https://bookdown.org/egarpor/PM-UC3M/" class="uri">https://bookdown.org/egarpor/PM-UC3M/</a></em>.</p>
<p>The <strong>wine</strong> dataset is formed by the auction price of 27 red Bordeaux vintages, five vintage descriptors (WinterRain, AGST, HarvestRain, Age, Year), and the population of France in the year of the vintage, FrancePop.</p>
<ul>
<li>Year: year in which grapes were harvested to make wine<br />
</li>
<li>Price: logarithm of the average market price for Bordeaux vintages according to 1990–1991 auctions. The price is relative to the price of the 1961 vintage, regarded as the best one ever recorded<br />
</li>
<li>WinterRain: winter rainfall (in mm)<br />
</li>
<li>AGST: Average Growing Season Temperature (in degrees Celsius)<br />
</li>
<li>HarvestRain: harvest rainfall (in mm)<br />
</li>
<li>Age: age of the wine measured as the number of years stored in a cask<br />
</li>
<li>FrancePop: population of France at Year (in thousands)</li>
</ul>
<p>You will require the <code>GGally</code> package; please make sure to install it first.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1" tabindex="-1"></a><span class="fu">library</span>(GGally)</span></code></pre></div>
<p>And now let’s import the data.</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="#cb70-1" tabindex="-1"></a>wine <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/egarpor/handy/master/datasets/wine.csv&quot;</span>)</span></code></pre></div>
<p>Let’s first obtain some summary statistics.</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1" tabindex="-1"></a><span class="fu">summary</span>(wine)</span></code></pre></div>
<pre><code>##       Year          Price         WinterRain         AGST        HarvestRain   
##  Min.   :1952   Min.   :6.205   Min.   :376.0   Min.   :14.98   Min.   : 38.0  
##  1st Qu.:1960   1st Qu.:6.508   1st Qu.:543.5   1st Qu.:16.15   1st Qu.: 88.0  
##  Median :1967   Median :6.984   Median :600.0   Median :16.42   Median :123.0  
##  Mean   :1967   Mean   :7.042   Mean   :608.4   Mean   :16.48   Mean   :144.8  
##  3rd Qu.:1974   3rd Qu.:7.441   3rd Qu.:705.5   3rd Qu.:17.01   3rd Qu.:185.5  
##  Max.   :1980   Max.   :8.494   Max.   :830.0   Max.   :17.65   Max.   :292.0  
##       Age          FrancePop    
##  Min.   : 3.00   Min.   :43184  
##  1st Qu.: 9.50   1st Qu.:46856  
##  Median :16.00   Median :50650  
##  Mean   :16.19   Mean   :50085  
##  3rd Qu.:22.50   3rd Qu.:53511  
##  Max.   :31.00   Max.   :55110</code></pre>
<p>And a matrix of plots. The <code>ggpairs()</code> function which is part of the <code>GGally</code> package produces an information rich visualisation that includes pairwise relationships of all the variables in the dataset.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1" tabindex="-1"></a><span class="fu">ggpairs</span>(wine)</span></code></pre></div>
<p><img src="02-S02-P3_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<div class="question">
<p>What conclusions can you draw based on the above information?</p>
</div>
<p>We are going to investigate possible interactions between the rainfall (WinterRain) and the growing season temperature (AGST). We will start with the most complicated model that includes the highest-order interaction. In R we will specify the three-way interaction, which will automatically add all combinations of two-way interactions.</p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="#cb74-1" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Price <span class="sc">~</span> WinterRain <span class="sc">+</span> AGST <span class="sc">+</span> HarvestRain <span class="sc">+</span> Age <span class="sc">+</span> WinterRain <span class="sc">*</span> AGST <span class="sc">*</span> HarvestRain, <span class="at">data =</span> wine)</span>
<span id="cb74-2"><a href="#cb74-2" tabindex="-1"></a></span>
<span id="cb74-3"><a href="#cb74-3" tabindex="-1"></a><span class="fu">summary</span>(model1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Price ~ WinterRain + AGST + HarvestRain + Age + 
##     WinterRain * AGST * HarvestRain, data = wine)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.35058 -0.19462 -0.02645  0.17194  0.52079 
## 
## Coefficients:
##                               Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)                  8.582e+00  1.924e+01   0.446   0.6609   
## WinterRain                  -1.858e-02  2.896e-02  -0.642   0.5292   
## AGST                        -1.748e-01  1.137e+00  -0.154   0.8795   
## HarvestRain                 -4.713e-02  1.540e-01  -0.306   0.7631   
## Age                          2.476e-02  8.288e-03   2.987   0.0079 **
## WinterRain:AGST              1.272e-03  1.712e-03   0.743   0.4671   
## WinterRain:HarvestRain       7.836e-05  2.600e-04   0.301   0.7665   
## AGST:HarvestRain             3.059e-03  9.079e-03   0.337   0.7401   
## WinterRain:AGST:HarvestRain -5.446e-06  1.540e-05  -0.354   0.7278   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2833 on 18 degrees of freedom
## Multiple R-squared:  0.8621, Adjusted R-squared:  0.8007 
## F-statistic: 14.06 on 8 and 18 DF,  p-value: 2.675e-06</code></pre>
<div class="question">
<p>What can you say about the explained variability of the model? Which coefficients are statistically significant? Simplify the model in stages and decide on the final model.</p>
</div>
<!--chapter:end:02-S02-P3.Rmd-->
</div>
<div id="answers-1" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Answers<a href="#answers-1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="practical-1-3" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Practical 1<a href="#practical-1-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="file">
<p>For the tasks below, you will require the <strong>Auto</strong> dataset from the core
textbook (James et. al 2021).</p>
<p>This dataset is part of the <code>ISRL2</code> package. By loading the package, the
<strong>Auto</strong> dataset loads automatically:</p>
<p><code>library(ISLR2)</code></p>
<p>Remember to install it first</p>
<p><code>install.packages("ISLR2")</code></p>
</div>
<p>This data file (text format) contains 398 observations of 9 variables.
The variables are:</p>
<ul>
<li>mpg: miles per gallon</li>
<li>cylinders: Number of cylinders between 4 and 8</li>
<li>displacement: Engine displacement (cu. inches)</li>
<li>horsepower: Engine horsepower</li>
<li>weight: Vehicle weight (lbs.)</li>
<li>acceleration: Time to accelerate from 0 to 60 mph (sec.)</li>
<li>year: Model year</li>
<li>origin: Origin of car (1. American, 2. European, 3. Japanese)</li>
<li>name: Vehicle name</li>
</ul>
<div id="task-1-6" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-6" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use the <code>lm()</code> function to perform simple linear regression with <strong>mpg</strong>
as the response and <strong>horsepower</strong> as the predictor. Store the output in
an object called <strong>fit</strong>.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="#cb76-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data =</span> Auto)</span></code></pre></div>
</div>
<div id="task-2-6" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-6" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Have a look at the results of the model.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ horsepower, data = Auto)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -13.5710  -3.2592  -0.3435   2.7630  16.9240 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 39.935861   0.717499   55.66   &lt;2e-16 ***
## horsepower  -0.157845   0.006446  -24.49   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.906 on 390 degrees of freedom
## Multiple R-squared:  0.6059, Adjusted R-squared:  0.6049 
## F-statistic: 599.7 on 1 and 390 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="question">
<p>Is there a relationship between the predictor and the response?</p>
</div>
<div class="answers">
<p>The slope coefficient (<code>-0.157845</code>) is statistically significant
(<code>&lt;2e-16 ***</code>). We can conclude that there is evidence to suggest a
negative relationship between miles per gallon and engine horsepower.
For a one-unit increase in engine horsepower, miles per gallon are
reduced by 0.16.</p>
</div>
</div>
<div id="task-3-6" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 3<a href="#task-3-6" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>What is the associated 95% confidence intervals for predicted miles per
gallon associated with an engine horsepower of 98? Hint: use the
<code>predict()</code> function. For confidence intervals, set the <code>interval</code>
argument to <code>confidence</code>.</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1" tabindex="-1"></a><span class="fu">predict</span>(fit, <span class="fu">data.frame</span>(<span class="at">horsepower =</span> <span class="dv">98</span>), <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 24.46708 23.97308 24.96108</code></pre>
</div>
<div id="task-4-6" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 4<a href="#task-4-6" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>How about the prediction interval for the same value?</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1" tabindex="-1"></a><span class="fu">predict</span>(fit, <span class="fu">data.frame</span>(<span class="at">horsepower =</span> <span class="dv">98</span>), <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>)</span></code></pre></div>
<pre><code>##        fit     lwr      upr
## 1 24.46708 14.8094 34.12476</code></pre>
<div class="question">
<p>Are the two intervals different? Why?</p>
</div>
<div class="answers">
<p>The prediction interval (lower limit 14.8094 and upper limit 34.12476)
is wider (and therefore less precise) than the confidence interval
(lower limit 23.97308 and upper limit 24.96108). The confidence interval
measures the uncertainty around the estimate of the conditional mean
whilst the prediction interval takes into account not only uncertainty
but also the variability of the conditional distribution.</p>
</div>
</div>
<div id="task-5-6" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 5<a href="#task-5-6" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using base R, plot the response and the predictor as well as the least
squares regression line. Add suitable labels to the X and Y axes.</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1" tabindex="-1"></a><span class="fu">plot</span>(Auto<span class="sc">$</span>horsepower, Auto<span class="sc">$</span>mpg, <span class="at">xlab =</span> <span class="st">&quot;horsepower&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;mpg&quot;</span>)</span>
<span id="cb83-2"><a href="#cb83-2" tabindex="-1"></a><span class="fu">abline</span>(fit)</span></code></pre></div>
<p><img src="02-S02-ANS_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
</div>
<div id="task-6-5" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 6<a href="#task-6-5" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use base R to produce diagnostic plots of the least squares regression
fit. Display these in a 2X2 grid.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="#cb84-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb84-2"><a href="#cb84-2" tabindex="-1"></a><span class="fu">plot</span>(fit, <span class="at">cex =</span> <span class="fl">0.2</span>)</span></code></pre></div>
<p><img src="02-S02-ANS_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
</div>
<div id="task-7-3" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 7<a href="#task-7-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Subset the <strong>Auto</strong> dataset such that it excludes the <strong>name</strong> and
<strong>origin</strong> variables and store this subsetted dataset in a new object
called <strong>quant_vars</strong>.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1" tabindex="-1"></a>quant_vars <span class="ot">&lt;-</span> <span class="fu">subset</span>(Auto, <span class="at">select =</span> <span class="sc">-</span><span class="fu">c</span>(name, origin))</span></code></pre></div>
</div>
<div id="task-8-3" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 8<a href="#task-8-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Compute a correlation matrix of all variables.</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="#cb86-1" tabindex="-1"></a><span class="fu">cor</span>(quant_vars)</span></code></pre></div>
<pre><code>##                     mpg  cylinders displacement horsepower     weight
## mpg           1.0000000 -0.7776175   -0.8051269 -0.7784268 -0.8322442
## cylinders    -0.7776175  1.0000000    0.9508233  0.8429834  0.8975273
## displacement -0.8051269  0.9508233    1.0000000  0.8972570  0.9329944
## horsepower   -0.7784268  0.8429834    0.8972570  1.0000000  0.8645377
## weight       -0.8322442  0.8975273    0.9329944  0.8645377  1.0000000
## acceleration  0.4233285 -0.5046834   -0.5438005 -0.6891955 -0.4168392
## year          0.5805410 -0.3456474   -0.3698552 -0.4163615 -0.3091199
##              acceleration       year
## mpg             0.4233285  0.5805410
## cylinders      -0.5046834 -0.3456474
## displacement   -0.5438005 -0.3698552
## horsepower     -0.6891955 -0.4163615
## weight         -0.4168392 -0.3091199
## acceleration    1.0000000  0.2903161
## year            0.2903161  1.0000000</code></pre>
<div class="question">
<p>Did you use the <strong>Auto</strong> dataset or the <strong>quant_vars</strong> object? Why does
it matter which data object you use?</p>
</div>
<div class="answers">
<p>To compute the correlation matrix using all variables of a data object,
these variables must all be numeric. In the <strong>Auto</strong> data object, the
<strong>name</strong> variable is coded as a factor.</p>
<p><code>class(Auto$name)</code><br />
<code>[1] "factor"</code></p>
<p>Therefore, if you try to use the <code>cor()</code> function with <strong>Auto</strong> dataset
without excluding the <strong>name</strong> variable, you will get an error.</p>
<p><code>cor(Auto)</code><br />
<code>Error in cor(Auto) : 'x' must be numeric</code>.</p>
<p>Also, whilst the <strong>origin</strong> variable is of class integer and will not
pose a problem when you apply the <code>cor()</code> function, you’ll remember from
the variable description list that this is a nominal variable with its
categories numerically labelled.</p>
<p>Compute the correlation matrix using <strong>quant_vars</strong>.</p>
</div>
</div>
<div id="task-9-3" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 9<a href="#task-9-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the <strong>quant_vars</strong> object, perform multiple linear regression with
miles per gallon as the response and all other variables as the
predictors.</p>
<p>Store the results in an object called <strong>fit2</strong>.</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="#cb88-1" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> quant_vars)</span></code></pre></div>
</div>
<div id="task-10-3" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 10<a href="#task-10-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Have a look at the results of the multiple regression model.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1" tabindex="-1"></a><span class="fu">summary</span>(fit2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ ., data = quant_vars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.6927 -2.3864 -0.0801  2.0291 14.3607 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -1.454e+01  4.764e+00  -3.051  0.00244 ** 
## cylinders    -3.299e-01  3.321e-01  -0.993  0.32122    
## displacement  7.678e-03  7.358e-03   1.044  0.29733    
## horsepower   -3.914e-04  1.384e-02  -0.028  0.97745    
## weight       -6.795e-03  6.700e-04 -10.141  &lt; 2e-16 ***
## acceleration  8.527e-02  1.020e-01   0.836  0.40383    
## year          7.534e-01  5.262e-02  14.318  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.435 on 385 degrees of freedom
## Multiple R-squared:  0.8093, Adjusted R-squared:  0.8063 
## F-statistic: 272.2 on 6 and 385 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="question">
<p>Is there a relationship between the predictors and the response? Which
predictors appear to have a statistically significant relationship to
the response? What does the coefficient for the year variable suggest?</p>
</div>
<div class="answers">
<p>Two of the predictors are statistically significant: <strong>weight</strong> and
<strong>year</strong>. The relationship between <strong>weight</strong> and <strong>mpg</strong> is negative
which suggests that for a one pound increase in weight of vehicle, the
number of miles per gallon the vehicle can travel decreases, whilst that
of <strong>mpg</strong> and <strong>year</strong> is positive which suggests that the more recent
the vehicle is, the higher the number of miles per gallon it can travel.</p>
</div>
</div>
<div id="task-11-3" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 11<a href="#task-11-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Produce diagnostic plots of the multiple linear regression fit in a 2x2
grid.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb91-2"><a href="#cb91-2" tabindex="-1"></a><span class="fu">plot</span>(fit2, <span class="at">cex =</span> <span class="fl">0.2</span>)</span></code></pre></div>
<p><img src="02-S02-ANS_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<div class="question">
<p>Do the residual plots suggest any unusually large outliers? Does the
leverage plot identify any observations with unusually high leverage?</p>
</div>
<div class="answers">
<p>One point has high leverage, the residuals also show a trend with fitted
values.</p>
</div>
</div>
<div id="task-12-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 12<a href="#task-12-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Fit separate linear regression models with interaction effect terms for:
weight and horsepower, acceleration and horsepower, and cylinders and
weight.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="#cb92-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(mpg <span class="sc">~</span> . <span class="sc">+</span> weight<span class="sc">:</span>horsepower, <span class="at">data =</span> quant_vars))</span>
<span id="cb92-2"><a href="#cb92-2" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(mpg <span class="sc">~</span> . <span class="sc">+</span> acceleration<span class="sc">:</span>horsepower, <span class="at">data =</span> quant_vars))</span>
<span id="cb92-3"><a href="#cb92-3" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(mpg <span class="sc">~</span> . <span class="sc">+</span> cylinders<span class="sc">:</span>weight, <span class="at">data =</span> quant_vars))</span></code></pre></div>
<div class="question">
<p>Are any of the interaction terms statistically significant?</p>
</div>
<div class="answers">
<p>For each model, the interaction term is statistically significant.</p>
</div>
</div>
<div id="task-13-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 13<a href="#task-13-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the <strong>Auto</strong> data object, apply transformations for the
<strong>horsepower</strong> variable and plot the relationship between <strong>horsepower</strong>
and <strong>mpg</strong> in a 2x2 grid.</p>
<ul>
<li>First plot: use the original variable;<br />
</li>
<li>Second plot: apply log transform;<br />
</li>
<li>Third plot: raise it to the power of two.</li>
</ul>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb93-2"><a href="#cb93-2" tabindex="-1"></a><span class="fu">plot</span>(Auto<span class="sc">$</span>horsepower, Auto<span class="sc">$</span>mpg, <span class="at">cex =</span> <span class="fl">0.2</span>)</span>
<span id="cb93-3"><a href="#cb93-3" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">log</span>(Auto<span class="sc">$</span>horsepower), Auto<span class="sc">$</span>mpg, <span class="at">cex =</span> <span class="fl">0.2</span>)</span>
<span id="cb93-4"><a href="#cb93-4" tabindex="-1"></a><span class="fu">plot</span>(Auto<span class="sc">$</span>horsepower <span class="sc">^</span> <span class="dv">2</span>, Auto<span class="sc">$</span>mpg, <span class="at">cex =</span> <span class="fl">0.2</span>)</span></code></pre></div>
<div class="question">
<p>Which of these transformations is most suitable?</p>
</div>
<div class="answers">
<p>The relationship between horsepower and miles per gallon is clearly
non-linear (plot 1). The log transform seems to address this best.</p>
</div>
</div>
<div id="task-14-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 14<a href="#task-14-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now run a multiple regression model with all variables as before but
this time, apply a log transformation of the <strong>horsepower</strong> variable.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="#cb94-1" tabindex="-1"></a>quant_vars<span class="sc">$</span>horsepower <span class="ot">&lt;-</span> <span class="fu">log</span>(quant_vars<span class="sc">$</span>horsepower)</span>
<span id="cb94-2"><a href="#cb94-2" tabindex="-1"></a>fit3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> ., <span class="at">data =</span> quant_vars)</span></code></pre></div>
</div>
<div id="task-15-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 15<a href="#task-15-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Have a look at the results.</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1" tabindex="-1"></a><span class="fu">summary</span>(fit3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ ., data = quant_vars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -8.6778 -2.0080 -0.3142  1.9262 14.0979 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  29.1713000  8.9291383   3.267  0.00118 ** 
## cylinders    -0.3563199  0.3181815  -1.120  0.26347    
## displacement  0.0088277  0.0068866   1.282  0.20066    
## horsepower   -8.7568129  1.5958761  -5.487 7.42e-08 ***
## weight       -0.0044304  0.0007213  -6.142 2.03e-09 ***
## acceleration -0.3317439  0.1077476  -3.079  0.00223 ** 
## year          0.6979715  0.0503916  13.851  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.308 on 385 degrees of freedom
## Multiple R-squared:  0.8231, Adjusted R-squared:  0.8203 
## F-statistic: 298.5 on 6 and 385 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="question">
<p>How do the results of model <strong>fit3</strong> differ from those of model
<strong>fit2</strong>?</p>
</div>
<div class="answers">
<p>The <strong>fit2</strong> model results showed that only two predictors were
statistically significant: <strong>weight</strong> and <strong>year</strong>. The <strong>fit3</strong> model
has two additional predictors that are statistically significant:
<strong>acceleration</strong> as well as <strong>horsepower</strong>.Also, the coefficient values
can now be interpreted more easily.</p>
</div>
</div>
<div id="task-16-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 16<a href="#task-16-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Produce diagnostic plots for the <strong>fit3</strong> object and display them in a
2x2 grid.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb97-2"><a href="#cb97-2" tabindex="-1"></a><span class="fu">plot</span>(fit3, <span class="at">cex =</span> <span class="fl">0.2</span>)</span></code></pre></div>
<p><img src="02-S02-ANS_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<div class="question">
<p>How do the diagnostic plots differ?</p>
</div>
<div class="answers">
<p>A log transformation of <strong>horsepower</strong> appears to give a more linear
relationship with <strong>mpg</strong> but this difference does not seem to be
substantial.</p>
</div>
</div>
</div>
<div id="practical-2-3" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Practical 2<a href="#practical-2-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="file">
<p>For the tasks below, you will require the <strong>Carseats</strong> dataset from the
core textbook (James et. al 2021).</p>
<p>This dataset is part of the <code>ISRL2</code> package. By loading the package, the
<strong>Carseats</strong> dataset loads automatically:</p>
<p><code>library(ISLR2)</code></p>
</div>
<p>This dataframe object contains a simulated dataset of sales of child car
seats at 400 different stores.</p>
<p>The 9 variables are:</p>
<ul>
<li>Sales: Unit sales (thousands of dollars) at each location<br />
</li>
<li>CompPrice: Price charged by competitor at each location<br />
</li>
<li>Income: Community income level (thousands of dollars)<br />
</li>
<li>Advertising: Local advertising budget for company at each location
(thousands of dollars)<br />
</li>
<li>Population: Population size in region (thousands of dollars)<br />
</li>
<li>Price: Price company charges for car seats at each site<br />
</li>
<li>ShelveLoc: Quality of the shelving location for the car seats at
each site<br />
</li>
<li>Age: Average age of the local population<br />
</li>
<li>Education: Education level at each location<br />
</li>
<li>Urban: Whether the store is in an urban or rural location<br />
</li>
<li>US: Whether the store is in the US or not</li>
</ul>
<div id="task-1-7" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-7" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Fit a multiple regression model to predict unit sales at each location
based on price company charges for car seats, whether the store is in an
urban or rural location, and whether the store is in the US or not.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="#cb98-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Sales <span class="sc">~</span> Price <span class="sc">+</span> Urban <span class="sc">+</span> US, <span class="at">data =</span> Carseats)</span></code></pre></div>
</div>
<div id="task-2-7" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-7" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Have a look at the results and interpret the coefficients.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Sales ~ Price + Urban + US, data = Carseats)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.9206 -1.6220 -0.0564  1.5786  7.0581 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 13.043469   0.651012  20.036  &lt; 2e-16 ***
## Price       -0.054459   0.005242 -10.389  &lt; 2e-16 ***
## UrbanYes    -0.021916   0.271650  -0.081    0.936    
## USYes        1.200573   0.259042   4.635 4.86e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.472 on 396 degrees of freedom
## Multiple R-squared:  0.2393, Adjusted R-squared:  0.2335 
## F-statistic: 41.52 on 3 and 396 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="question">
<p>Which coefficients are statistically significant? What do they indicate?</p>
</div>
<div class="answers">
<p>The null hypothesis for the slope being zero is rejected for the
<strong>Price</strong> and <strong>US</strong> variables. The coefficient for <strong>Price</strong> is
statistically significant; since it is negative, as price increases by a
thousand dollars (i.e. one unit increase), the sales of child decrease
by about 0.05. The slope for the <strong>US</strong> variable is also statistically
significant but it is positive. Also, this is a binary factor variable
coded as Yes and No (No is the reference category). Therefore, sales of
child car seats are higher by 1.2 for car seat products that are
produced in the US than for car seat products not produced in the US.</p>
</div>
</div>
<div id="task-3-7" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 3<a href="#task-3-7" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Based on the conclusions you have drawn in Task 2, now fit a smaller
model that only uses the predictors for which there is evidence of
association with sales.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Sales <span class="sc">~</span> Price <span class="sc">+</span> US, <span class="at">data =</span> Carseats)</span></code></pre></div>
</div>
<div id="task-4-7" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 4<a href="#task-4-7" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Compare the two models (<em>fit</em> and <em>fit2</em>).</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="#cb102-1" tabindex="-1"></a><span class="fu">anova</span>(fit, fit2)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: Sales ~ Price + Urban + US
## Model 2: Sales ~ Price + US
##   Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)
## 1    396 2420.8                           
## 2    397 2420.9 -1  -0.03979 0.0065 0.9357</code></pre>
<div class="question">
<p>Which model is the better fit?</p>
</div>
<div class="answers">
<p>They have similar r-squared values, and the <em>fit</em> model (containing the
extra variable <strong>Urban</strong>) is non-significantly better.</p>
</div>
</div>
<div id="task-5-7" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 5<a href="#task-5-7" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Produce diagnostic plots for <em>fit2</em> and display these in a 2x2 grid.</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="#cb104-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb104-2"><a href="#cb104-2" tabindex="-1"></a><span class="fu">plot</span>(fit2, <span class="at">cex =</span> <span class="fl">0.2</span>)</span></code></pre></div>
<p><img src="02-S02-ANS_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<div class="question">
<p>Is there evidence of outliers or high leverage observations in the
<em>fit2</em> model?</p>
</div>
<div class="answers">
<p>Yes, there is evidence of outliers and high leverage observations for
<em>fit2</em>.</p>
</div>
</div>
</div>
<div id="practical-3-the-quality-of-red-bordeaux-vintages-1" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Practical 3: The Quality of Red Bordeaux Vintages<a href="#practical-3-the-quality-of-red-bordeaux-vintages-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>This demonstration was developed by Dr. Tatjana Kecojevic, Lecturer in Social Statistics.</em></p>
<p>We would like to analyse the quality of a vintage that has been quantified as the price and make the interpretation of our statistical findings.</p>
<p>We are going to use <code>wine.csv</code> available from Eduardo García Portugués’s book: <em>Notes for Predictive Modelling; <a href="https://bookdown.org/egarpor/PM-UC3M/" class="uri">https://bookdown.org/egarpor/PM-UC3M/</a></em>.</p>
<p>The <strong>wine</strong> dataset is formed by the auction price of 27 red Bordeaux vintages, five vintage descriptors (WinterRain, AGST, HarvestRain, Age, Year), and the population of France in the year of the vintage, FrancePop.</p>
<ul>
<li>Year: year in which grapes were harvested to make wine<br />
</li>
<li>Price: logarithm of the average market price for Bordeaux vintages according to 1990–1991 auctions. The price is relative to the price of the 1961 vintage, regarded as the best one ever recorded<br />
</li>
<li>WinterRain: winter rainfall (in mm)<br />
</li>
<li>AGST: Average Growing Season Temperature (in degrees Celsius)<br />
</li>
<li>HarvestRain: harvest rainfall (in mm)<br />
</li>
<li>Age: age of the wine measured as the number of years stored in a cask<br />
</li>
<li>FrancePop: population of France at Year (in thousands)</li>
</ul>
<p>You will require the <code>GGally</code> package; please make sure to install it first.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1" tabindex="-1"></a><span class="fu">library</span>(GGally)</span></code></pre></div>
<p>And now let’s import the data.</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="#cb106-1" tabindex="-1"></a>wine <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;https://raw.githubusercontent.com/egarpor/handy/master/datasets/wine.csv&quot;</span>)</span></code></pre></div>
<p>Let’s first obtain some summary statistics.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1" tabindex="-1"></a><span class="fu">summary</span>(wine)</span></code></pre></div>
<pre><code>##       Year          Price         WinterRain         AGST        HarvestRain   
##  Min.   :1952   Min.   :6.205   Min.   :376.0   Min.   :14.98   Min.   : 38.0  
##  1st Qu.:1960   1st Qu.:6.508   1st Qu.:543.5   1st Qu.:16.15   1st Qu.: 88.0  
##  Median :1967   Median :6.984   Median :600.0   Median :16.42   Median :123.0  
##  Mean   :1967   Mean   :7.042   Mean   :608.4   Mean   :16.48   Mean   :144.8  
##  3rd Qu.:1974   3rd Qu.:7.441   3rd Qu.:705.5   3rd Qu.:17.01   3rd Qu.:185.5  
##  Max.   :1980   Max.   :8.494   Max.   :830.0   Max.   :17.65   Max.   :292.0  
##       Age          FrancePop    
##  Min.   : 3.00   Min.   :43184  
##  1st Qu.: 9.50   1st Qu.:46856  
##  Median :16.00   Median :50650  
##  Mean   :16.19   Mean   :50085  
##  3rd Qu.:22.50   3rd Qu.:53511  
##  Max.   :31.00   Max.   :55110</code></pre>
<p>And a matrix of plots.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1" tabindex="-1"></a><span class="fu">ggpairs</span>(wine)</span></code></pre></div>
<p><img src="02-S02-ANS_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<div class="question">
<p>What conclusions can you draw based on the above information?</p>
</div>
<div class="answers">
<p>We can notice a perfect relationship between the variables Year and Age. This is to be expected since this data was collected in 1983 and Age was calculated as: Age = 1983 - Year. Knowing this, we are going to remove Year from the analysis and use Age as it will be easier to interpret.</p>
<p>There is a strong relationship between Year, ie. Age and FrancePop and since we want to impose our viewpoint that the total population does not influence the quality of the wine we will not consider this variable in the model.</p>
</div>
<p>We are going to investigate possible interactions between the rainfall (WinterRain) and the growing season temperature (AGST). We will start with the most complicated model that includes the highest-order interaction. In R we will specify the three-way interaction, which will automatically add all combinations of two-way interactions.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="#cb110-1" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Price <span class="sc">~</span> WinterRain <span class="sc">+</span> AGST <span class="sc">+</span> HarvestRain <span class="sc">+</span> Age <span class="sc">+</span> WinterRain <span class="sc">*</span> AGST <span class="sc">*</span> HarvestRain, <span class="at">data =</span> wine)</span>
<span id="cb110-2"><a href="#cb110-2" tabindex="-1"></a></span>
<span id="cb110-3"><a href="#cb110-3" tabindex="-1"></a><span class="fu">summary</span>(model1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Price ~ WinterRain + AGST + HarvestRain + Age + 
##     WinterRain * AGST * HarvestRain, data = wine)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.35058 -0.19462 -0.02645  0.17194  0.52079 
## 
## Coefficients:
##                               Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)                  8.582e+00  1.924e+01   0.446   0.6609   
## WinterRain                  -1.858e-02  2.896e-02  -0.642   0.5292   
## AGST                        -1.748e-01  1.137e+00  -0.154   0.8795   
## HarvestRain                 -4.713e-02  1.540e-01  -0.306   0.7631   
## Age                          2.476e-02  8.288e-03   2.987   0.0079 **
## WinterRain:AGST              1.272e-03  1.712e-03   0.743   0.4671   
## WinterRain:HarvestRain       7.836e-05  2.600e-04   0.301   0.7665   
## AGST:HarvestRain             3.059e-03  9.079e-03   0.337   0.7401   
## WinterRain:AGST:HarvestRain -5.446e-06  1.540e-05  -0.354   0.7278   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2833 on 18 degrees of freedom
## Multiple R-squared:  0.8621, Adjusted R-squared:  0.8007 
## F-statistic: 14.06 on 8 and 18 DF,  p-value: 2.675e-06</code></pre>
<div class="question">
<p>What can you say about the explained variability of the model? Which coefficients are statistically significant? Simplify the model in stages and decide on the final model.</p>
</div>
<p>The model explains well over 80% of variability and is clearly a strong model, but the key question is whether we can simplify it.</p>
<p>We will start the process of this model simplification by removing the three-way interaction as it is clearly not significant.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="#cb112-1" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">update</span>(model1, <span class="sc">~</span>. <span class="sc">-</span>WinterRain<span class="sc">:</span>AGST<span class="sc">:</span>HarvestRain, <span class="at">data =</span> wine)</span>
<span id="cb112-2"><a href="#cb112-2" tabindex="-1"></a><span class="fu">summary</span>(model2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Price ~ WinterRain + AGST + HarvestRain + Age + 
##     WinterRain:AGST + WinterRain:HarvestRain + AGST:HarvestRain, 
##     data = wine)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.35245 -0.19452  0.01643  0.17289  0.51420 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)             2.980e+00  1.066e+01   0.279  0.78293   
## WinterRain             -9.699e-03  1.408e-02  -0.689  0.49930   
## AGST                    1.542e-01  6.383e-01   0.242  0.81168   
## HarvestRain             6.496e-03  2.610e-02   0.249  0.80610   
## Age                     2.441e-02  8.037e-03   3.037  0.00678 **
## WinterRain:AGST         7.490e-04  8.420e-04   0.890  0.38484   
## WinterRain:HarvestRain -1.350e-05  7.338e-06  -1.840  0.08144 . 
## AGST:HarvestRain       -1.032e-04  1.520e-03  -0.068  0.94656   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2767 on 19 degrees of freedom
## Multiple R-squared:  0.8611, Adjusted R-squared:  0.8099 
## F-statistic: 16.83 on 7 and 19 DF,  p-value: 6.523e-07</code></pre>
<p>The <span class="math inline">\(\overline{R}^2\)</span> has slightly increased in value. Next, we remove the least significant two-way interaction term.</p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="#cb114-1" tabindex="-1"></a>model3 <span class="ot">&lt;-</span> <span class="fu">update</span>(model2, <span class="sc">~</span>. <span class="sc">-</span>AGST<span class="sc">:</span>HarvestRain, <span class="at">data =</span> wine)</span>
<span id="cb114-2"><a href="#cb114-2" tabindex="-1"></a><span class="fu">summary</span>(model3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Price ~ WinterRain + AGST + HarvestRain + Age + 
##     WinterRain:AGST + WinterRain:HarvestRain, data = wine)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.35424 -0.19343  0.01176  0.17161  0.51218 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)             3.518e+00  6.946e+00   0.507  0.61802   
## WinterRain             -1.017e-02  1.195e-02  -0.851  0.40488   
## AGST                    1.218e-01  4.138e-01   0.294  0.77147   
## HarvestRain             4.752e-03  4.553e-03   1.044  0.30901   
## Age                     2.451e-02  7.710e-03   3.179  0.00472 **
## WinterRain:AGST         7.769e-04  7.166e-04   1.084  0.29119   
## WinterRain:HarvestRain -1.342e-05  7.059e-06  -1.902  0.07174 . 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2697 on 20 degrees of freedom
## Multiple R-squared:  0.8611, Adjusted R-squared:  0.8194 
## F-statistic: 20.66 on 6 and 20 DF,  p-value: 1.35e-07</code></pre>
<p>Again, it is reassuring to notice an increase in the <span class="math inline">\(\overline{R}^2\)</span>, but we can still simplify the model further by removing another least significant two-way interaction term.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="#cb116-1" tabindex="-1"></a>model4 <span class="ot">&lt;-</span> <span class="fu">update</span>(model3, <span class="sc">~</span>. <span class="sc">-</span>WinterRain<span class="sc">:</span>AGST, <span class="at">data =</span> wine)</span>
<span id="cb116-2"><a href="#cb116-2" tabindex="-1"></a><span class="fu">summary</span>(model4)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Price ~ WinterRain + AGST + HarvestRain + Age + 
##     WinterRain:HarvestRain, data = wine)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.5032 -0.1934  0.0109  0.1771  0.4621 
## 
## Coefficients:
##                          Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)            -3.812e+00  1.598e+00  -2.386 0.026553 *  
## WinterRain              2.747e-03  9.471e-04   2.900 0.008560 ** 
## AGST                    5.586e-01  9.495e-02   5.883 7.71e-06 ***
## HarvestRain             4.717e-03  4.572e-03   1.032 0.313877    
## Age                     2.785e-02  7.094e-03   3.926 0.000774 ***
## WinterRain:HarvestRain -1.349e-05  7.088e-06  -1.903 0.070835 .  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2708 on 21 degrees of freedom
## Multiple R-squared:  0.8529, Adjusted R-squared:  0.8179 
## F-statistic: 24.35 on 5 and 21 DF,  p-value: 4.438e-08</code></pre>
<p>There is an insignificant decrease in <span class="math inline">\(\overline{R}^2\)</span>. We notice HarvestRain is now the least significant term, but it is used for the WinterRain:HarvestRain interaction, which is significant at <span class="math inline">\(\alpha = 0.05\)</span> and therefore we should keep it. However, as the concept of parsimony prefers a model without interactions to a model containing interactions between variables, we will remove the remaining interaction term and see if it significantly affects the explanatory power of the model.</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="#cb118-1" tabindex="-1"></a>model5 <span class="ot">&lt;-</span> <span class="fu">update</span>(model4, <span class="sc">~</span>. <span class="sc">-</span>WinterRain<span class="sc">:</span>HarvestRain, <span class="at">data =</span> wine)</span>
<span id="cb118-2"><a href="#cb118-2" tabindex="-1"></a><span class="fu">summary</span>(model5)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Price ~ WinterRain + AGST + HarvestRain + Age, data = wine)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.46024 -0.23862  0.01347  0.18601  0.53443 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -3.6515703  1.6880876  -2.163  0.04167 *  
## WinterRain   0.0011667  0.0004820   2.420  0.02421 *  
## AGST         0.6163916  0.0951747   6.476 1.63e-06 ***
## HarvestRain -0.0038606  0.0008075  -4.781 8.97e-05 ***
## Age          0.0238480  0.0071667   3.328  0.00305 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.2865 on 22 degrees of freedom
## Multiple R-squared:  0.8275, Adjusted R-squared:  0.7962 
## F-statistic: 26.39 on 4 and 22 DF,  p-value: 4.057e-08</code></pre>
<p>The <span class="math inline">\(\overline{R}^2\)</span> is reduced by around 2%, but it has all the significant terms and it is easier to interpret. For those reasons and in the spirit of parsimony that argues that a model should be as simple as possible, we can suggest that this should be regarded as the best final fitted model.</p>
<p>We realise that for the large numbers of explanatory variables, and many interactions and non-linear terms, the process of model simplification can take a very long time. There are many algorithms for automatic variable selection that can help us to choose the variables to include in a regression model. <em>Stepwise</em> regression and <em>Best Subsets</em> regression are two of the more common variable selection methods.</p>
<p>The <em>stepwise</em> procedure starts from the saturated model (or the maximal model, whichever is appropriate) through a series of simplifications to the minimal adequate model. This progression is made on the basis of deletion tests: F tests, AIC, t-tests or chi-squared tests that assess the significance of the increase in deviance that results when a given term is removed from the current model.</p>
<p>The best subset regression (BREG), also known as “all possible regressions”, as the name of the procedure indicates, fits a separate least squares regression for each possible combination of the p predictors, i.e. explanatory variables. After fitting all of the models, BREG then displays the best fitted models with one explanatory variable, two explanatory variables, three explanatory variables, and so on. Usually, either adjusted R-squared or Mallows Cp is the criterion for picking the best fitting models for this process. The result is a display of the best fitted models of different sizes up to the full/maximal model and the final fitted model can be selected by comparing displayed models based on the criteria of parsimony. <strong>You will learn more about variable selection later in the course</strong>.</p>
<p>“These methods are frequently abused by naive researchers who seek to interpret the order of entry of variables into the regression equation as an index of their ‘importance’. This practice is potentially misleading.” <em>J. Fox and S. Weisberg’s book: An R Companion to Applied Regression, Third Edition, Sage (2019)</em></p>
<p><em>’Parsimony says that, other things being equal, we prefer:</em></p>
<ul>
<li><em>a model with n−1 parameters to a model with n parameters</em><br />
</li>
<li><em>a model with k−1 explanatory variables to a model with k explanatory variables a linear model to a model which is curved</em></li>
<li><em>a model without a hump to a model with a hump</em></li>
<li><em>a model without interactions to a model containing interactions between factors’</em></li>
</ul>
<p><em>Crawley, M.J. 2013, The R Book. 2nd Edition. John Wiley, New York</em></p>
<!--chapter:end:02-S02-ANS.Rmd-->
</div>
</div>
<div id="part-section-3" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART*) Section 3<a href="#part-section-3" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="overview-2" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Overview<a href="#overview-2" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div style="color: #333; font-size: 24px; font-style: italic; text-align: justify;">
<p>Section 3: Classification</p>
</div>
<p>This section is comprised of two demonstrations and one practical.</p>
<p>The two demonstrations are adapted from the core textbook for this
course:</p>
<p>James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). <em>An
Introduction to Statistical Learning with Applications in R</em>. 2nd ed.
New York: Springer. <a href="https://www.statlearning.com/" class="uri">https://www.statlearning.com/</a></p>
<p>The practical is adapted from a demonstration developed by Dr. Tatjana
Kecojevic, Lecturer in Social Statistics.</p>
<div class="ilos">
<p><strong>Learning Outcomes:</strong></p>
<ul>
<li><p>appreciate the difference between <em>classic</em> modelling and supervised
learning;</p></li>
<li><p>compute and plot a correlation matrix;</p></li>
<li><p>compute and interpret a confusion matrix and overall prediction
accuracy;</p></li>
<li><p>apply a variety of classification methods and appreciate their
advantages and limitations;</p></li>
<li><p>understand the differences between modelling count data using a
linear versus a Poisson model;</p></li>
<li><p>interpret the results of a Poisson regression model.</p></li>
</ul>
</div>
<p><strong>In this section, you will practice using the functions below. It is
highly recommended that you explore these functions further using the
Help tab in your RStudio console.</strong></p>
<table style="width:93%;">
<colgroup>
<col width="23%" />
<col width="45%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Function</th>
<th align="center">Description</th>
<th align="center">Package</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><code>str()</code></td>
<td align="center">display structure of object</td>
<td align="center">utils</td>
</tr>
<tr class="even">
<td align="center"><code>head()</code></td>
<td align="center">return first part of a data
object</td>
<td align="center">utils</td>
</tr>
<tr class="odd">
<td align="center"><code>attach()</code></td>
<td align="center">attach database to R search
path (access simplified to
providing names without
indexing)</td>
<td align="center">base</td>
</tr>
<tr class="even">
<td align="center"><code>contrasts()</code></td>
<td align="center">set/view contrasts of factor</td>
<td align="center">stats</td>
</tr>
<tr class="odd">
<td align="center"><code>contr.sum()</code></td>
<td align="center">return matrix of contrasts</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>summary()</code></td>
<td align="center">produce summary results</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>lm()</code></td>
<td align="center">fit linear models</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>coef()</code></td>
<td align="center">extract model coefficients</td>
<td align="center">stats</td>
</tr>
<tr class="odd">
<td align="center"><code>plot()</code></td>
<td align="center">plotting objects</td>
<td align="center">base</td>
</tr>
<tr class="even">
<td align="center"><code>axis()</code></td>
<td align="center">add axis to current plot
(accompanies <code>plot()</code>)</td>
<td align="center">graphics</td>
</tr>
<tr class="odd">
<td align="center"><code>abline()</code></td>
<td align="center">add one or more straight lines
to plot (accompanies <code>plot()</code>)</td>
<td align="center">graphics</td>
</tr>
<tr class="even">
<td align="center"><code>cor()</code></td>
<td align="center">compute correlation</td>
<td align="center">stats</td>
</tr>
<tr class="odd">
<td align="center"><code>corrplot()</code></td>
<td align="center"><p>plot correlation matrix</p>
<p>arguments such as <code>type</code> and
<code>diag</code> control the structure
of the plot</p></td>
<td align="center">corrplot</td>
</tr>
<tr class="even">
<td align="center"><code>glm()</code></td>
<td align="center"><p>fit a generalised linear model</p>
<p>arguments such as <code>family</code>
control the distribution</p></td>
<td align="center">stats</td>
</tr>
<tr class="odd">
<td align="center"><code>predict()</code></td>
<td align="center"><p>generic function for
predictions from results of
models</p>
<p>arguments such as <code>type</code>
specify the type of
predictions required</p></td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>diag()</code></td>
<td align="center">extract/replace diagonal of a
matrix</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>sum()</code></td>
<td align="center">compute the sum of values</td>
<td align="center">base</td>
</tr>
<tr class="even">
<td align="center"><code>lda()</code></td>
<td align="center">perform LDA</td>
<td align="center">MASS</td>
</tr>
<tr class="odd">
<td align="center"><code>table()</code></td>
<td align="center">build contingency table</td>
<td align="center">base</td>
</tr>
<tr class="even">
<td align="center"><code>qda()</code></td>
<td align="center">perform QDA</td>
<td align="center">MASS</td>
</tr>
<tr class="odd">
<td align="center"><code>knn()</code></td>
<td align="center"><p>perform KNN classification</p>
<p>arguments such as <code>k</code> control
number of neighbours</p></td>
<td align="center">class</td>
</tr>
<tr class="even">
<td align="center"><code>naiveBayes()</code></td>
<td align="center">apply the Naive Bayes
classifier</td>
<td align="center">e1071</td>
</tr>
</tbody>
</table>
<!--chapter:end:03-S03-overview.Rmd-->
</div>
<div id="demonstration-1-classification-problems" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Demonstration 1: Classification Problems<a href="#demonstration-1-classification-problems" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this demonstration, you will learn how to address classification problems using logistic regression, discriminant analysis, KNN, and Naive Bayes.</p>
<p>You will need the <strong>Weekly</strong> dataset, part of the <code>ISRL2</code> package. By loading the package, the <strong>Weekly</strong> dataset loads automatically.</p>
<p>In addition to the <code>ISLR2</code> package, will also require the following:</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="#cb120-1" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb120-2"><a href="#cb120-2" tabindex="-1"></a><span class="fu">library</span>(class)</span>
<span id="cb120-3"><a href="#cb120-3" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb120-4"><a href="#cb120-4" tabindex="-1"></a><span class="fu">library</span>(corrplot)</span>
<span id="cb120-5"><a href="#cb120-5" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb120-6"><a href="#cb120-6" tabindex="-1"></a><span class="fu">library</span>(e1071)</span></code></pre></div>
<div id="dataset-and-variables" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Dataset and Variables<a href="#dataset-and-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <strong>Weekly</strong> dataset contains weekly percentage returns for the S&amp;P 500 stock index between 1990 and 2010. It is a dataframe with 1098 observations and 9 variables. The variables are:</p>
<ul>
<li>Year: year observation was recorded<br />
</li>
<li>Lag1: Percentage return for previous week<br />
</li>
<li>Lag2: Percentage return for 2 weeks previous<br />
</li>
<li>Lag3: Percentage return for 3 weeks previous<br />
</li>
<li>Lag4: Percentage return for 4 weeks previous<br />
</li>
<li>Lag5: Percentage return for 5 weeks previous<br />
</li>
<li>Volume: Volume of shares traded (average number of daily shares traded in billions)<br />
</li>
<li>Today: percentage return for current week<br />
</li>
<li>Direction: whether the market had a positive (up) or negative (down) return on a given week.</li>
</ul>
<p>In this demonstration, the goal is to predict whether the market had a positive (up) or negative (down) return on a given week. Therefore, <strong>Direction</strong> will be our response variable.</p>
<p>Before we consider the model, let’s first explore our dataset.</p>
<p>By exploring the structure of the dataframe, we find out that all variables are numeric, with the exception of the <strong>Direction</strong> variable.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="#cb121-1" tabindex="-1"></a><span class="fu">str</span>(Weekly)</span></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    1089 obs. of  9 variables:
##  $ Year     : num  1990 1990 1990 1990 1990 1990 1990 1990 1990 1990 ...
##  $ Lag1     : num  0.816 -0.27 -2.576 3.514 0.712 ...
##  $ Lag2     : num  1.572 0.816 -0.27 -2.576 3.514 ...
##  $ Lag3     : num  -3.936 1.572 0.816 -0.27 -2.576 ...
##  $ Lag4     : num  -0.229 -3.936 1.572 0.816 -0.27 ...
##  $ Lag5     : num  -3.484 -0.229 -3.936 1.572 0.816 ...
##  $ Volume   : num  0.155 0.149 0.16 0.162 0.154 ...
##  $ Today    : num  -0.27 -2.576 3.514 0.712 1.178 ...
##  $ Direction: Factor w/ 2 levels &quot;Down&quot;,&quot;Up&quot;: 1 1 2 2 2 1 2 2 2 1 ...</code></pre>
</div>
<div id="correlation-matrix-and-plot" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Correlation Matrix and Plot<a href="#correlation-matrix-and-plot" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s now produce a correlation plot between all pairs of <em>numeric</em> variables in the dataset.</p>
<p>Using the base R <code>cor()</code> function, we exclude the 9th variable (which is a factor) and compute the correlation among all pairs of numeric variables.</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb123-1"><a href="#cb123-1" tabindex="-1"></a><span class="fu">cor</span>(Weekly[, <span class="sc">-</span><span class="dv">9</span>])</span></code></pre></div>
<pre><code>##               Year         Lag1        Lag2        Lag3         Lag4
## Year    1.00000000 -0.032289274 -0.03339001 -0.03000649 -0.031127923
## Lag1   -0.03228927  1.000000000 -0.07485305  0.05863568 -0.071273876
## Lag2   -0.03339001 -0.074853051  1.00000000 -0.07572091  0.058381535
## Lag3   -0.03000649  0.058635682 -0.07572091  1.00000000 -0.075395865
## Lag4   -0.03112792 -0.071273876  0.05838153 -0.07539587  1.000000000
## Lag5   -0.03051910 -0.008183096 -0.07249948  0.06065717 -0.075675027
## Volume  0.84194162 -0.064951313 -0.08551314 -0.06928771 -0.061074617
## Today  -0.03245989 -0.075031842  0.05916672 -0.07124364 -0.007825873
##                Lag5      Volume        Today
## Year   -0.030519101  0.84194162 -0.032459894
## Lag1   -0.008183096 -0.06495131 -0.075031842
## Lag2   -0.072499482 -0.08551314  0.059166717
## Lag3    0.060657175 -0.06928771 -0.071243639
## Lag4   -0.075675027 -0.06107462 -0.007825873
## Lag5    1.000000000 -0.05851741  0.011012698
## Volume -0.058517414  1.00000000 -0.033077783
## Today   0.011012698 -0.03307778  1.000000000</code></pre>
<p>We store the computed correlation matrix in a new object which we will then use to create a correlation matrix plot with the <code>corrplot()</code> function from the <code>corrplot</code> package.</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="#cb125-1" tabindex="-1"></a>cor_matrix <span class="ot">&lt;-</span> <span class="fu">cor</span>(Weekly[, <span class="sc">-</span><span class="dv">9</span>])</span>
<span id="cb125-2"><a href="#cb125-2" tabindex="-1"></a></span>
<span id="cb125-3"><a href="#cb125-3" tabindex="-1"></a><span class="fu">corrplot</span>(cor_matrix)</span></code></pre></div>
<p><img src="03-S03-D1_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>By using the default arguments, we obtain the correlation matrix in full, with each circle representing the correlation between each variable. The size of the circle represents the magnitude of the correlation, whilst the shade corresponds to both strength and direction of the correlation. As the legend illustrates, a perfect negative correlation (-1) is represented by dark red and a perfect positive correlation (+1) in dark blue. As you already know, the correlation matrix is symmetric around its diagonal. The diagonal area represents the correlation of each variable with itself, and therefore, this corresponds to a perfect correlation (dark blue).</p>
<p>To facilitate interpretation, particularly when we are dealing with a large number of variables, we can set the <code>diag</code> argument to <code>FALSE</code> to remove the correlation of each variable with itself from the plot. Because of its symmetric property, we can also display just half of the square since the parts on either side of the diagonal are mirror images. We can achieve this using the <code>type</code> argument. We can either choose to display the area above the diagonal or the area below the diagonal, as I did below. There are many other options if you want to further customise your correlation plot such using a different visualisation method of the direction and strength of the correlation using the <code>method</code> argument. Have a look at the documentaion of the <code>corrplot</code> function using ?.</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="#cb126-1" tabindex="-1"></a><span class="fu">corrplot</span>(cor_matrix, <span class="at">type =</span> <span class="st">&quot;lower&quot;</span>, <span class="at">diag =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="03-S03-D1_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Now, the correlation plot only displays the correlations between the 8 numeric variables. We observe a strong positive correlation between volume of daily shares traded and the year the observation was recorded (dark blue). The correlations between other variables are quite weak but notably, we see that Lag1 and Lag3, Lag 2 and Lag4, Lag3 and Lag5, and Today and Lag2 are positively correlated with one another (albeit weakly). Other variables also appear weakly negatively correlated, such as Lag1 and Lag2.</p>
<p><em>Ok, so what was the purpose of computing the correlation matrix? You’ll remember that multicollinearity is an issue in model building which can lead to inflated variances of the estimated coefficients. As a result of the shared variance between two highly correlated predictors, our ability to adequately evaluate the effect of the predictors on the outcome will be affected (e.g. increased risk of overfitting). One way to deal with multicollinearity is to eliminate one of the highly correlated predictors.</em></p>
</div>
<div id="classic-logistic-regression" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">“Classic” Logistic Regression<a href="#classic-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now let’s build our logistic regression model the classic way.</p>
<p>Given the high correlation between <strong>Year</strong> and <strong>Volume</strong>, we have to reflect on how we want to address this. Assuming we have knowledge of important factors that can predict market movement (<strong>Direction</strong>), we decide to drop the <strong>Year</strong> variable rather than <strong>Volume</strong>* since the latter measures average number of daily shares traded (in billions).</p>
<p>You may remember that in R, these models are built using the base R <code>glm</code> function within which the <code>family</code> argument must be set to <code>binomial</code>.</p>
<p>Note that in this dataset, the <strong>Direction</strong> variable is already a factor so there is no need to perform any recoding/transformations but remember to ALWAYS explore your data in detail before you build any model.</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="#cb127-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2 <span class="sc">+</span> Lag3 <span class="sc">+</span> Lag4 <span class="sc">+</span> Lag5 <span class="sc">+</span> Volume,</span>
<span id="cb127-2"><a href="#cb127-2" tabindex="-1"></a>           <span class="at">data =</span> Weekly, <span class="at">family =</span> binomial)</span>
<span id="cb127-3"><a href="#cb127-3" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + 
##     Volume, family = binomial, data = Weekly)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.6949  -1.2565   0.9913   1.0849   1.4579  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)  0.26686    0.08593   3.106   0.0019 **
## Lag1        -0.04127    0.02641  -1.563   0.1181   
## Lag2         0.05844    0.02686   2.175   0.0296 * 
## Lag3        -0.01606    0.02666  -0.602   0.5469   
## Lag4        -0.02779    0.02646  -1.050   0.2937   
## Lag5        -0.01447    0.02638  -0.549   0.5833   
## Volume      -0.02274    0.03690  -0.616   0.5377   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1496.2  on 1088  degrees of freedom
## Residual deviance: 1486.4  on 1082  degrees of freedom
## AIC: 1500.4
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>The results show that only <strong>Lag2</strong> is significant at an alpha level of 0.05.</p>
<p>Ok, so let’s see now how well our model predicts whether the market had a positive or negative return in a given week.</p>
<div id="confusion-matrix" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Confusion Matrix<a href="#confusion-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now, let’s assess the effectiveness of our model by comparing the actual values with those predicted by the model. We can do so using a <em>confusion matrix</em> which compares the model predictions with the true values.</p>
<p>Using <code>predict(fit, type = "response")</code>, we generate predictions from our model (<code>fit</code>) on the scale of the response variable (<code>type = "response"</code>). In this case, our response variable is measured on a probability scale.</p>
<p>We choose the standard threshold of 0.5 such that we label an observation as belonging to the <strong>Up</strong> category if its posterior probability is above 0.5 or as <strong>Down</strong> if the posterior probability is below 0.5. Hence, in this context, the <code>&gt; 0.5</code> argument transforms the predicted probabilities into a binary form such that predictions greater than 0.5 are labelled TRUE (so representing upward market movement), whilst the rest are labelled FALSE (representing downward market movement).</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="#cb129-1" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">&gt;</span> <span class="fl">0.5</span></span></code></pre></div>
<p>Now that we have the frequencies of the TRUE and FALSE instances, let’s build our two-way confusion matrix using the base R <code>table()</code> function. The <code>ifelse</code> function nested inside <code>table()</code> converts the logical TRUE and FALSE values in the <strong>pred</strong> object intro descriptive labels to facilitate interpretation; so, if <strong>pred</strong> is TRUE (&gt; 0.5), it becomes labelled as <code>Up (pred)</code>, whilst it is is FALSE, it is labelled as <code>"Down (pred)"</code>. To also include the actual values of market movement from the dataset, we also need to add <code>Weekly$Direction</code>as our argument.</p>
<p>Finally to ‘force’ R to display the <code>conf_matrix</code> object we just created, we can place the entire code in single parentheses.</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="#cb130-1" tabindex="-1"></a>(conf_matrix <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="fu">ifelse</span>(pred, <span class="st">&quot;Up (pred)&quot;</span>, <span class="st">&quot;Down (pred)&quot;</span>), Weekly<span class="sc">$</span>Direction))</span></code></pre></div>
<pre><code>##              
##               Down  Up
##   Down (pred)   54  48
##   Up (pred)    430 557</code></pre>
<p>Now let’s interpret the results. The results in diagonal represent correct predictions of market movement whilst those in the off-diagonal represent misclassified observations. We can see that our model incorrectly classified 430 instances of market movement as upward movement when in fact they represented downward movement and 48 instances as downward movement when in fact they represented upward movement. Overall, our logistic regression model correctly predicts upwards movements well but it performs poorly at predicting downward movements.</p>
<p>We can also compute the overall fraction of correct predictions by dividing the number of correct predictions by total number of predictions. We therefore divide the sum of the diagonal values in our confusion matrix (numerator) by the sum of all elements of the matrix (denominator). We extract the diagonal values from the matrix using the base R <code>diag()</code> function.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="#cb132-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">diag</span>(conf_matrix)) <span class="sc">/</span> <span class="fu">sum</span>(conf_matrix)</span></code></pre></div>
<pre><code>## [1] 0.5610652</code></pre>
<p>The overall fraction of correct predictions is 0.561 (so our model makes correct predictions about 56.1% of the time).</p>
<p><strong>Right, so does that mean that this model will make correct predictions 56% of the time on a new, unseen dataset?</strong></p>
<p>You already know the answer :)! We used our entire dataset to fit our model. This means that we cannot say anything about how our model will perform on a different dataset and we no longer have any ‘unseen’ data left to test this out.</p>
</div>
</div>
<div id="logistic-regression-in-statistical-learning" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Logistic Regression in Statistical Learning<a href="#logistic-regression-in-statistical-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now, let’s consider logistic regression as applied in statistical learning.</p>
<p>We will again consider a basic fixed split. In this example, we will fit our model using data from 1990 up to 2008 and set the data from 2009 and 2010 aside; this will be our test dataset.</p>
<p>This is easily achieved by creating a vector of logical values from the data according to our <strong>Year</strong> criterion.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="#cb134-1" tabindex="-1"></a>train <span class="ot">&lt;-</span> Weekly<span class="sc">$</span>Year <span class="sc">&lt;</span> <span class="dv">2009</span></span></code></pre></div>
<p>In our previous model, we observed that <strong>Lag2</strong> was the only statistically significant predictor. To exemplify how we can use logistic regression in statistical learning, we will build a simple simple with only one predictor.</p>
<p>The approach to building the model is the same as the one with which you are already familiar. The exception, of course, is that we will only use part of the dataset to build our model (which in this case is referred to as <em>training the model</em>). To subset our dataset to only include data from years previous to 2009, we use the logical vector <strong>train</strong> we just created.</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb135-1"><a href="#cb135-1" tabindex="-1"></a>fit_log_SL <span class="ot">&lt;-</span> <span class="fu">glm</span>(Direction <span class="sc">~</span> Lag2, <span class="at">data =</span> Weekly[train, ], <span class="at">family =</span> binomial)</span></code></pre></div>
<p>Now let’s generate predictions; the function and the overall structure of the code is the same as discussed earlier in the demonstration. The exception is that we used the trained model <code>fit_log_SL</code> to make predictions on the <em>test</em> dataset (<code>Weekly[!train, ]</code>). Using <code>!</code>, we tell R to not include the training data when generating predictions.</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb136-1"><a href="#cb136-1" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_log_SL, Weekly[<span class="sc">!</span>train, ], <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">&gt;</span> <span class="fl">0.5</span></span></code></pre></div>
<p>Now let’s compute the confusion matrix such that we can compare our predictions on the test data (<code>pred</code>) against the actual values in our dataset (<code>Weekly[!train, ]$Direction)</code>).</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb137-1"><a href="#cb137-1" tabindex="-1"></a>(t <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="fu">ifelse</span>(pred, <span class="st">&quot;Up (pred)&quot;</span>, <span class="st">&quot;Down (pred)&quot;</span>), Weekly[<span class="sc">!</span>train, ]<span class="sc">$</span>Direction))</span></code></pre></div>
<pre><code>##              
##               Down Up
##   Down (pred)    9  5
##   Up (pred)     34 56</code></pre>
<p>If we then compute the overall fraction of correct predictions for the test data we can see that this is higher than the value we obtained using the classical approach (<span class="math inline">\(0.561\)</span>).</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb139-1"><a href="#cb139-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">diag</span>(t)) <span class="sc">/</span> <span class="fu">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.625</code></pre>
</div>
<div id="linear-discriminant-analysis" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Linear Discriminant Analysis<a href="#linear-discriminant-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>How well would linear discriminant analysis address our binary classification problem?</p>
<p>In R, LDA can be performed using the <code>lda()</code> function from the <code>MASS</code> package. The basic structure of the function is similar to the other models you have built.</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb141-1"><a href="#cb141-1" tabindex="-1"></a>fit_lda <span class="ot">&lt;-</span> <span class="fu">lda</span>(Direction <span class="sc">~</span> Lag2, <span class="at">data =</span> Weekly[train, ])</span></code></pre></div>
<p>The output is, of course, different.</p>
<ul>
<li>prior probabilities of groups: these tells us the way in which the two classes are distributed in our <em>training data</em> (i.e. 44.8 % of the observations correspond to downward market movement whilst 55.2% to upward market movement).<br />
</li>
<li>group means: the average of our single predictor <strong>Lag2</strong> within each class, and are used by LDA as an estimate of <span class="math inline">\(μ_{k}\)</span>.<br />
</li>
<li>coefficient(s) of linear discriminants: tells us how our predictor(s) influence the score that is used to classify the observations into one of the two categories. Here, the coefficient is positive 0.44 and so this indicates that higher values for <strong>Lag2</strong> will make the modelmore likely classify an observation as belonging to the <strong>Up</strong> class; also, the larger the absolute value of the coefficient, the stronger the influence on the model.</li>
</ul>
<div class="sourceCode" id="cb142"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb142-1"><a href="#cb142-1" tabindex="-1"></a>fit_lda</span></code></pre></div>
<pre><code>## Call:
## lda(Direction ~ Lag2, data = Weekly[train, ])
## 
## Prior probabilities of groups:
##      Down        Up 
## 0.4477157 0.5522843 
## 
## Group means:
##             Lag2
## Down -0.03568254
## Up    0.26036581
## 
## Coefficients of linear discriminants:
##            LD1
## Lag2 0.4414162</code></pre>
<p>Now let’s consider what the <code>predict()</code> function does when applied in the context of LDA.</p>
<div class="sourceCode" id="cb144"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb144-1"><a href="#cb144-1" tabindex="-1"></a>result_lda <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_lda, Weekly[<span class="sc">!</span>train, ])</span></code></pre></div>
<p>The output will contain three components: <code>class</code>, <code>posterior</code>, and <code>x</code>, each of which can be accessed using <code>$</code>.</p>
<p>The <code>class</code> component is a factor that contains the predictions for market movement (up/down).</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="#cb145-1" tabindex="-1"></a>result_lda<span class="sc">$</span>class</span></code></pre></div>
<pre><code>##   [1] Up   Up   Down Down Up   Up   Up   Down Down Down Down Up   Up   Up   Up  
##  [16] Up   Up   Up   Up   Up   Down Up   Up   Up   Up   Up   Up   Up   Up   Up  
##  [31] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down
##  [46] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Up   Up  
##  [61] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Down Up   Down Up  
##  [76] Up   Up   Up   Down Down Up   Up   Up   Up   Up   Down Up   Up   Up   Up  
##  [91] Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up   Up  
## Levels: Down Up</code></pre>
<p>The <code>posterior</code> component is matrix that contains the posterior probability that the corresponding observation belongs to a given class.</p>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb147-1"><a href="#cb147-1" tabindex="-1"></a>result_lda<span class="sc">$</span>posterior</span></code></pre></div>
<pre><code>##           Down        Up
## 986  0.4736555 0.5263445
## 987  0.3558617 0.6441383
## 988  0.5132860 0.4867140
## 989  0.5142948 0.4857052
## 990  0.4799727 0.5200273
## 991  0.4597586 0.5402414
## 992  0.3771117 0.6228883
## 993  0.5184724 0.4815276
## 994  0.5480397 0.4519603
## 995  0.5146118 0.4853882
## 996  0.5504246 0.4495754
## 997  0.3055404 0.6944596
## 998  0.4268160 0.5731840
## 999  0.3637275 0.6362725
## 1000 0.4034316 0.5965684
## 1001 0.4256310 0.5743690
## 1002 0.4277053 0.5722947
## 1003 0.4548626 0.5451374
## 1004 0.4308002 0.5691998
## 1005 0.3674066 0.6325934
## 1006 0.5210641 0.4789359
## 1007 0.4426627 0.5573373
## 1008 0.3983332 0.6016668
## 1009 0.4170520 0.5829480
## 1010 0.4400457 0.5599543
## 1011 0.4872186 0.5127814
## 1012 0.4529323 0.5470677
## 1013 0.4844231 0.5155769
## 1014 0.4769786 0.5230214
## 1015 0.3531293 0.6468707
## 1016 0.3912903 0.6087097
## 1017 0.4373753 0.5626247
## 1018 0.4163510 0.5836490
## 1019 0.4583549 0.5416451
## 1020 0.4182305 0.5817695
## 1021 0.4454253 0.5545747
## 1022 0.4667580 0.5332420
## 1023 0.4126831 0.5873169
## 1024 0.4146279 0.5853721
## 1025 0.4814414 0.5185586
## 1026 0.4756405 0.5243595
## 1027 0.3860819 0.6139181
## 1028 0.4278606 0.5721394
## 1029 0.4599449 0.5400551
## 1030 0.5071309 0.4928691
## 1031 0.4042648 0.5957352
## 1032 0.4173045 0.5826955
## 1033 0.4520606 0.5479394
## 1034 0.4491759 0.5508241
## 1035 0.4304467 0.5695533
## 1036 0.4487621 0.5512379
## 1037 0.4544049 0.5455951
## 1038 0.4184691 0.5815309
## 1039 0.4637729 0.5362271
## 1040 0.4114393 0.5885607
## 1041 0.4605038 0.5394962
## 1042 0.5053429 0.4946571
## 1043 0.4728071 0.5271929
## 1044 0.4595437 0.5404563
## 1045 0.4368785 0.5631215
## 1046 0.4051682 0.5948318
## 1047 0.4553490 0.5446510
## 1048 0.4056270 0.5943730
## 1049 0.4352188 0.5647812
## 1050 0.4370488 0.5629512
## 1051 0.4410978 0.5589022
## 1052 0.4352756 0.5647244
## 1053 0.4296973 0.5703027
## 1054 0.4520034 0.5479966
## 1055 0.4194240 0.5805760
## 1056 0.4853885 0.5146115
## 1057 0.5411727 0.4588273
## 1058 0.4177113 0.5822887
## 1059 0.5100863 0.4899137
## 1060 0.4470646 0.5529354
## 1061 0.4816287 0.5183713
## 1062 0.4138300 0.5861700
## 1063 0.4157203 0.5842797
## 1064 0.5017234 0.4982766
## 1065 0.5216975 0.4783025
## 1066 0.3738247 0.6261753
## 1067 0.4666863 0.5333137
## 1068 0.3993705 0.6006295
## 1069 0.4506892 0.5493108
## 1070 0.4235170 0.5764830
## 1071 0.5036414 0.4963586
## 1072 0.4593288 0.5406712
## 1073 0.4587988 0.5412012
## 1074 0.3965787 0.6034213
## 1075 0.4428192 0.5571808
## 1076 0.4287787 0.5712213
## 1077 0.4202670 0.5797330
## 1078 0.4523464 0.5476536
## 1079 0.4258989 0.5741011
## 1080 0.4358286 0.5641714
## 1081 0.4409698 0.5590302
## 1082 0.4491046 0.5508954
## 1083 0.3986650 0.6013350
## 1084 0.4804910 0.5195090
## 1085 0.4487050 0.5512950
## 1086 0.4616361 0.5383639
## 1087 0.4074084 0.5925916
## 1088 0.4311115 0.5688885
## 1089 0.4452828 0.5547172</code></pre>
<p>The <code>x</code> component contains the linear discriminants.</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="#cb149-1" tabindex="-1"></a>result_lda<span class="sc">$</span>x</span></code></pre></div>
<pre><code>##              LD1
## 986  -0.80594669
## 987   2.92755168
## 988  -2.01984129
## 989  -2.05074043
## 990  -0.99972841
## 991  -0.37865579
## 992   2.22702414
## 993  -2.17875113
## 994  -3.08806854
## 995  -2.06045158
## 996  -3.16178505
## 997   4.66982149
## 998   0.64322275
## 999   2.66623327
## 1000  1.38038783
## 1001  0.68030171
## 1002  0.61541353
## 1003 -0.22769145
## 1004  0.51874338
## 1005  2.54484381
## 1006 -2.25820605
## 1007  0.14971942
## 1008  1.54282900
## 1009  0.94956560
## 1010  0.23094000
## 1011 -1.22176077
## 1012 -0.16810026
## 1013 -1.13612602
## 1014 -0.90791384
## 1015  3.01892483
## 1016  1.76839269
## 1017  0.31392625
## 1018  0.97163642
## 1019 -0.33539700
## 1020  0.91248664
## 1021  0.06408467
## 1022 -0.59406691
## 1023  1.08728746
## 1024  1.02593061
## 1025 -1.04475287
## 1026 -0.86686213
## 1027  1.93613085
## 1028  0.61055795
## 1029 -0.38439421
## 1030 -1.83135657
## 1031  1.35390286
## 1032  0.94162011
## 1033 -0.14117387
## 1034 -0.05200779
## 1035  0.52977878
## 1036 -0.03920672
## 1037 -0.21356613
## 1038  0.90498257
## 1039 -0.50225234
## 1040  1.12657351
## 1041 -0.40160944
## 1042 -1.77662096
## 1043 -0.77990314
## 1044 -0.37203455
## 1045  0.32937582
## 1046  1.32521081
## 1047 -0.24269960
## 1048  1.31064407
## 1049  0.38102152
## 1050  0.32407882
## 1051  0.19827520
## 1052  0.37925585
## 1053  0.55317384
## 1054 -0.13940820
## 1055  0.87496626
## 1056 -1.16570091
## 1057 -2.87618875
## 1058  0.92881904
## 1059 -1.92184689
## 1060  0.01332181
## 1061 -1.05049128
## 1062  1.05109133
## 1063  0.99150015
## 1064 -1.66582548
## 1065 -2.27762836
## 1066  2.33428828
## 1067 -0.59185983
## 1068  1.50972278
## 1069 -0.09879791
## 1070  0.74651414
## 1071 -1.72453384
## 1072 -0.36541331
## 1073 -0.34908091
## 1074  1.59888886
## 1075  0.14486384
## 1076  0.58186590
## 1077  0.84848129
## 1078 -0.15000219
## 1079  0.67191480
## 1080  0.36204062
## 1081  0.20224795
## 1082 -0.04980071
## 1083  1.53223501
## 1084 -1.01561940
## 1085 -0.03744106
## 1086 -0.43648132
## 1087  1.25414279
## 1088  0.50903222
## 1089  0.06849883</code></pre>
<p>To obtain our predictions, we can simply extract the <code>class</code> element.</p>
<p>Alternatively, if we want to directly extract just the predictions from the <code>class</code> element, we can use the <code>predict</code> function as we did earlier in the demonstration.</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb151-1"><a href="#cb151-1" tabindex="-1"></a><span class="co">#either </span></span>
<span id="cb151-2"><a href="#cb151-2" tabindex="-1"></a>pred_lda <span class="ot">&lt;-</span> result_lda<span class="sc">$</span>class</span>
<span id="cb151-3"><a href="#cb151-3" tabindex="-1"></a></span>
<span id="cb151-4"><a href="#cb151-4" tabindex="-1"></a><span class="co">#or</span></span>
<span id="cb151-5"><a href="#cb151-5" tabindex="-1"></a>pred_lda <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_lda, Weekly[<span class="sc">!</span>train, ], <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)<span class="sc">$</span>class</span></code></pre></div>
<p>Now let’s compute the confusion matrix for our LDA classifier such that we can compare our predictions on the test data against the actual values in our dataset.</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="#cb152-1" tabindex="-1"></a>(t <span class="ot">&lt;-</span> <span class="fu">table</span>(pred_lda, Weekly[<span class="sc">!</span>train, ]<span class="sc">$</span>Direction))</span></code></pre></div>
<pre><code>##         
## pred_lda Down Up
##     Down    9  5
##     Up     34 56</code></pre>
<p>And now the fraction of correct predictions which, we can see is identical to that obtained for logistic regression.</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="#cb154-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">diag</span>(t)) <span class="sc">/</span> <span class="fu">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.625</code></pre>
</div>
<div id="quadratic-discriminant-analysis" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Quadratic Discriminant Analysis<a href="#quadratic-discriminant-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s now consider how quadratic discriminant analysis would address our binary classification problem. The code syntax is identical to that for linear discriminant analysis and the <code>qda</code> function is also part of the <code>MASS</code> package.</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="#cb156-1" tabindex="-1"></a>fit_qda <span class="ot">&lt;-</span> <span class="fu">qda</span>(Direction <span class="sc">~</span> Lag2, <span class="at">data =</span> Weekly[train, ])</span></code></pre></div>
<p>In terms of prior probabilities and group means, the output is identical to that of linear discriminant analysis. However, the output does not include the coefficients of the <em>linear</em> discriminants for obvious reasons.</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="#cb157-1" tabindex="-1"></a>fit_qda</span></code></pre></div>
<pre><code>## Call:
## qda(Direction ~ Lag2, data = Weekly[train, ])
## 
## Prior probabilities of groups:
##      Down        Up 
## 0.4477157 0.5522843 
## 
## Group means:
##             Lag2
## Down -0.03568254
## Up    0.26036581</code></pre>
<p>The prediction function works in the same way as for LDA, except that it will produce only two elements (<code>class</code>, and <code>posterior</code>); again, the <code>x</code> element will not be included since we are dealing with a quadratic function.</p>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="#cb159-1" tabindex="-1"></a>pred_qda <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_qda, Weekly[<span class="sc">!</span>train, ], <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)<span class="sc">$</span>class</span></code></pre></div>
<p>The confusion matrix is computed in the same way.</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="#cb160-1" tabindex="-1"></a>(t <span class="ot">&lt;-</span> <span class="fu">table</span>(pred_qda, Weekly[<span class="sc">!</span>train, ]<span class="sc">$</span>Direction))</span></code></pre></div>
<pre><code>##         
## pred_qda Down Up
##     Down    0  0
##     Up     43 61</code></pre>
<p>The fraction of correct predictions is lower than that for logistic regression and for LDA. We therefore conclude that in this context, QDA does not perform well in comparison to the previous two approaches.</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="#cb162-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">diag</span>(t)) <span class="sc">/</span> <span class="fu">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.5865385</code></pre>
</div>
<div id="k-nearest-neighbours" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor"><span class="math inline">\(K\)</span>-nearest neighbours<a href="#k-nearest-neighbours" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Earlier in the course, we covered K-nearest neighbour classification. Let’s now explore how this approach is used in R and how it performs in the context of our market movement problem.</p>
<p>To implement KNN in R, the most commonly used package is <code>class</code>.</p>
<div class="attention">
<p>Note that it is possible to be confronted with the following error when loading the package:</p>
<p><code>Error: (converted from warning) package ‘class’ was built under R version ...</code></p>
<p>This will occur if you are using an older version of R than that under which the package was built. The best option is to update RStudio. If that is not possible (e.g. due to system requirements), then another option is to suppress it using <code>suppressWarnings(library(class))</code> in your console. This should allow you to use the functions from the package.</p>
</div>
<p>In R, we build our model using the <code>knn()</code> function from the <code>class</code> package. This function works differently to those we have covered so far for linear and logistic regression, and for LDA and QDA.</p>
<p>This is because the <code>knn()</code> function both fits the model AND generates predictions. There are four arguments required:</p>
<ul>
<li>argument 1: predictors in our <em>training</em> data,<br />
</li>
<li>argument 2: predictors in our <em>test</em> data,<br />
</li>
<li>argument 3: outcome variable in our <em>training</em> data,<br />
</li>
<li>argument 4: the value for <span class="math inline">\(k\)</span>; note the function specifies 1 nearest neighbour by default but I added it here for illustration purposes (this value needs to be added only when using a value other than 1).</li>
</ul>
<p>Now, you may wonder why I also included the <code>drop = FALSE</code> argument when I subsetting the dataset.</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="#cb164-1" tabindex="-1"></a>fit_knn <span class="ot">&lt;-</span> <span class="fu">knn</span>(Weekly[train, <span class="st">&quot;Lag2&quot;</span>, <span class="at">drop =</span> <span class="cn">FALSE</span>],</span>
<span id="cb164-2"><a href="#cb164-2" tabindex="-1"></a>               Weekly[<span class="sc">!</span>train, <span class="st">&quot;Lag2&quot;</span>, <span class="at">drop =</span> <span class="cn">FALSE</span>],</span>
<span id="cb164-3"><a href="#cb164-3" tabindex="-1"></a>               Weekly<span class="sc">$</span>Direction[train], </span>
<span id="cb164-4"><a href="#cb164-4" tabindex="-1"></a>               <span class="at">k =</span> <span class="dv">1</span></span>
<span id="cb164-5"><a href="#cb164-5" tabindex="-1"></a>               )</span></code></pre></div>
<p>Before we proceed to interpret the results, let’s see what output we would produce if we subsetted our dataset such that we extract our predictor from the training data without the <code>drop = FALSE</code> argument. This looks like a vector, right?</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="#cb165-1" tabindex="-1"></a>Weekly[train, <span class="st">&quot;Lag2&quot;</span>]</span></code></pre></div>
<pre><code>##   [1]   1.572   0.816  -0.270  -2.576   3.514   0.712   1.178  -1.372   0.807
##  [10]   0.041   1.253  -2.678  -1.793   2.820   4.022   0.750  -0.017   2.420
##  [19]  -1.225   1.171  -2.061   0.729   0.112   2.480  -1.552  -2.259  -2.428
##  [28]  -2.708  -2.292  -4.978   3.547   0.260  -2.032  -1.739  -1.693   1.781
##  [37]  -3.682   4.150  -2.487   2.343   0.606   1.077  -0.637   2.260   1.716
##  [46]  -0.284   1.508  -0.913  -2.349  -1.798   5.393   1.156   2.077   4.751
##  [55]   2.702  -0.924   1.318   1.209  -0.363  -1.635   2.106   0.037   1.343
##  [64]   0.999  -1.348   0.470  -1.329  -0.892   1.370   3.269  -2.668   0.754
##  [73]  -1.188  -1.745   0.787   1.649   1.044  -0.856   1.641  -0.015  -0.398
##  [82]   2.228   0.320  -1.601  -1.416   1.129  -0.521  -1.205   0.052   2.897
##  [91]  -2.115   1.853   0.401  -2.614  -1.694  -0.245   1.034   1.417   0.668
## [100]   5.018   3.169  -1.011   0.906  -0.807  -1.613   0.565   0.338  -0.255
## [109]   0.309  -2.001   0.346   1.345  -1.896  -0.483   0.682   2.906  -1.687
## [118]   0.858   0.853  -1.433   0.958   0.321  -0.450  -0.900  -1.486  -0.054
## [127]   2.062   0.692   0.241  -0.967   3.064  -1.256   0.246  -1.205  -0.002
## [136]   0.540   0.599   0.798  -2.029  -0.936  -1.903   2.253   0.576   1.106
## [145]  -0.263   1.161   0.999   0.823   0.442   0.387   1.741  -0.342  -0.923
## [154]  -1.529   1.888  -0.238   0.612   2.313  -0.969  -2.330   2.110   0.616
## [163]   0.834   0.078  -0.533  -1.427   0.102   1.607  -2.653   0.723   0.482
## [172]  -0.622   1.429   0.976  -0.029  -0.622  -0.800   0.884  -0.393   0.509
## [181]  -0.527   0.303   0.230   0.123   0.325   1.337   0.960   0.174   0.082
## [190]  -0.626  -0.262   0.798  -0.210   1.996  -1.327   0.984  -1.766   1.266
## [199]  -0.599   0.099   0.395  -0.207   0.528   0.214  -0.199   0.740   1.066
## [208]  -0.040   0.838  -1.857   0.079  -0.530  -0.346  -0.285   0.366   0.990
## [217]  -2.225  -3.216   0.298  -0.206   0.325   0.733  -0.685  -0.822   2.427
## [226]   0.530   0.612  -0.317  -0.048  -3.414   0.768   0.751   1.025  -0.231
## [235]   1.137  -0.255   1.061   0.377   2.183  -0.593  -0.597   0.643  -2.445
## [244]   0.661  -1.645   3.076  -0.897   1.910  -2.425   0.015  -0.190  -1.989
## [253]   0.223  -1.399   2.649   0.224  -0.122   0.307   1.148  -0.255   1.207
## [262]   1.756   0.587   0.106   1.274  -0.551   0.855   1.215   1.100  -0.052
## [271]   1.140   0.555  -0.145   1.223   1.051   1.044  -1.210   0.859   1.692
## [280]  -0.858   2.252   1.830  -0.902   2.133   0.633  -1.120   1.682  -0.709
## [289]  -0.685   0.739   0.159   0.668   1.568   1.863  -0.278   0.461  -0.329
## [298]   0.345   0.506  -1.321   1.875   0.364   1.240  -0.017   1.168   1.730
## [307]  -0.185  -0.712   0.650   0.127  -2.416   1.665   1.600   2.288   3.229
## [316]  -1.278   1.713  -2.232  -1.687   1.252   1.433  -0.787   1.605  -2.920
## [325]   1.313   1.301  -1.810   1.630   2.579   1.435  -1.384   0.626  -1.108
## [334]   0.149   0.568  -1.967  -1.711  -1.154  -0.443   4.181  -0.059   0.470
## [343]   0.274  -2.255   0.566   3.791   0.954  -0.122   2.225  -0.114   1.450
## [352]  -1.393   0.407   3.844   0.930   1.506   1.107  -2.301  -1.482   2.776
## [361]   1.058  -1.158   1.533   2.195  -0.728   2.030   0.432   2.396  -0.830
## [370]  -1.366   1.789  -1.466  -1.144  -1.303  -2.065  -2.672   3.889  -0.127
## [379]   6.219   1.453   0.603   2.083   0.148   1.147   4.110   0.608  -1.268
## [388]   3.338  -0.026  -0.151   2.566   0.889  -1.436  -3.506   2.523  -2.606
## [397]   3.289  -0.553   2.879  -0.557   2.096   0.202  -2.360  -0.267  -2.869
## [406]   1.409   0.091   3.742  -0.798   2.972  -3.090  -0.693  -1.090   4.120
## [415]  -4.856   3.646  -0.408   2.369   3.283   0.754   1.384   1.463   0.605
## [424]   1.224   2.859  -0.338   2.488  -1.072   1.085  -1.320   1.182  -1.147
## [433]   0.053   0.157  -1.770   2.112  -1.348   0.165   2.957   1.167   1.562
## [442]   1.926  -3.872  -1.765  -2.786  -2.451   1.740  -5.004  -5.184   3.611
## [451]   1.093   2.417  -4.034  -1.816   7.317   1.349   2.615   3.854  -1.340
## [460]   3.361   2.473  -1.308  -0.874   1.849   3.219   0.241   3.731  -2.496
## [469]  -1.453   4.444  -3.145  -0.748   0.739  -0.072   2.999   1.499   0.363
## [478]  -1.269   0.851   4.223  -2.177   2.870  -1.597   0.735  -0.535  -0.561
## [487]  -2.139   1.990  -2.569   3.803  -2.050   5.771   0.867   1.105  -4.359
## [496]  -2.080  -2.140   2.106   0.673   0.872   0.665  -0.411  -1.201  -4.348
## [505]   0.427   4.148  -6.632   4.348   4.708   0.536   1.885   1.858  -0.378
## [514]   1.177  -1.134   0.282   2.626   0.748  -1.891   1.643  -1.624  -5.634
## [523]   4.721  -2.615  -2.958  -0.946   5.686  -1.001   4.975   4.301  -1.891
## [532]   1.186 -10.538   5.748   1.247  -1.363  -0.815  -0.986  -2.056   7.202
## [541]  -1.375   0.515  -1.569   0.910   1.671   2.102  -1.973  -4.074   3.031
## [550]   0.609   1.351   0.987   0.951  -1.727  -1.920  -1.166  -0.843  -1.916
## [559]  -2.471   1.656  -1.242   3.415  -4.255   0.127  -1.897  -1.978   4.156
## [568]  -4.215  -0.473   1.097  -1.661   1.556   1.819   0.924  -0.404  -2.572
## [577]  -1.006  -4.277  -0.938  -0.062  -6.720  -0.930   1.799  -2.749   4.880
## [586]   5.026   0.810   1.082  -1.653   3.716  -1.089  -1.348   0.340  -4.000
## [595]   0.905  -0.079  -2.760   2.107  -0.397  -0.415   0.707  -1.992  -2.369
## [604]   1.976  -4.334  -4.217 -11.050   7.780   2.924   1.892  -1.664   2.900
## [613]  -1.576   3.045   1.637   1.027  -0.947   1.655  -3.041   1.941   1.409
## [622]   0.990  -2.295  -1.573   0.506  -0.978  -2.315   0.726  -1.299   3.848
## [631]   2.874   0.159  -1.497  -0.114  -2.149  -1.044   1.275  -4.342  -0.269
## [640]  -1.718   4.891  -2.058  -1.539  -3.712  -1.972  -1.800   0.069  -0.080
## [649]  -6.839  -7.992   0.600   1.337   5.137   2.215   1.302  -2.635  -2.418
## [658]  -0.460  -4.992  -2.132  -3.238   4.339   5.874   1.499   0.369  -0.690
## [667]   1.687   2.277   0.619  -2.572  -2.494   0.706  -2.273   3.791   2.089
## [676]  -2.780  -4.478  -0.662  -3.040   0.627   1.591  -0.828  -1.458   0.528
## [685]   7.503  -3.605   1.778  -1.200   2.911   0.585   3.479   0.358   1.167
## [694]  -1.173   3.254   2.508   0.086   0.716  -1.955   0.971   1.262  -0.483
## [703]   0.540  -1.855  -0.261   1.338   0.241   1.505   1.327  -0.270   1.735
## [712]  -3.807   3.310   0.797   0.121  -1.002   2.119   0.238  -0.272  -1.435
## [721]   2.214   0.312   1.191   1.352   0.664   1.149   1.207   1.602   0.151
## [730]  -0.913   1.028   0.267  -0.148   0.073   1.041  -3.137  -0.963  -0.155
## [739]   3.046  -0.218  -0.413   0.528  -2.920  -0.777  -0.273  -0.195   2.480
## [748]   0.162   1.245  -0.128  -0.052  -0.798  -1.117  -1.026  -1.379   1.429
## [757]  -3.426   0.078   3.151   0.858   0.529   0.924   0.412  -1.634   1.927
## [766]  -0.827  -1.242  -1.124   3.145   3.183   1.544  -1.168   1.052   0.720
## [775]  -0.266   0.522   1.334   0.148  -2.123  -0.141  -1.406   0.299   2.704
## [784]   0.189  -0.308   0.814   0.887  -1.803  -0.869  -1.532   0.128   0.706
## [793]  -3.266   0.831   0.411   1.253  -1.477   3.053   0.799  -0.230   0.175
## [802]   1.573  -2.086   0.241   1.458   1.325   0.469   0.041  -0.629   0.324
## [811]  -0.868  -1.198   1.072   1.926  -0.288  -1.827   1.112  -2.678  -0.780
## [820]  -0.588   1.595   1.813   1.195   1.097   1.601  -0.250  -0.451   0.631
## [829]   0.106  -1.606   2.977   0.168  -2.029   1.762  -1.534   0.234   1.598
## [838]   0.170  -0.171  -0.451   2.016  -0.329  -0.620   0.049  -0.492   1.719
## [847]  -0.051   1.156  -2.604  -1.875   1.036   0.630  -2.788  -0.061  -0.563
## [856]   2.065  -0.372  -2.314   0.331   3.085   0.063  -0.986   2.807  -0.554
## [865]   1.229  -0.922   1.597  -0.370   1.603   1.029   1.188   0.218   0.639
## [874]  -0.947   1.217   1.470  -0.018  -0.303   0.940   1.224  -1.144   0.534
## [883]  -0.606   1.491  -0.016  -0.582   1.843  -0.713   1.216  -0.299  -4.412
## [892]   1.130  -1.133   3.544  -1.062   1.612   0.630   2.168   0.655   0.773
## [901]   0.015   1.122  -0.461   1.360  -1.866   1.674  -1.980   0.053   1.802
## [910]   1.441  -1.185  -4.899  -1.775   1.436  -0.530   2.312  -0.364  -1.387
## [919]   2.112   2.796   0.066   2.020   0.270  -3.917   2.309  -1.669  -3.706
## [928]   0.347  -1.237   2.807   1.588  -2.440   1.125  -0.402  -4.522  -0.752
## [937]  -5.412   0.409   4.871  -4.596   1.405   0.231  -1.661  -2.800  -0.404
## [946]   3.212  -1.075   4.195  -2.742   4.314   0.540   1.149  -1.812   2.670
## [955]  -3.467   1.777  -2.835  -0.048  -3.096  -3.001  -1.211  -1.854   1.710
## [964]  -0.232   0.203   2.857   0.145  -0.462  -0.725  -3.159   0.756   0.270
## [973]  -3.331  -9.399 -18.195   4.596  -6.781  10.491  -3.898  -6.198  -8.389
## [982]  12.026  -2.251   0.418   0.926</code></pre>
<p>If you wrap the code within the <code>class</code> function you can indeed confirm that the output is a vector.</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="#cb167-1" tabindex="-1"></a><span class="fu">class</span>(Weekly[train, <span class="st">&quot;Lag2&quot;</span>])</span></code></pre></div>
<pre><code>## [1] &quot;numeric&quot;</code></pre>
<p>The <code>knn()</code> requires that the training and test data be specified as either a matrix or a dataframe.</p>
<p>If we set the <code>drop</code> argument to <code>FALSE</code>, then we are essentially telling R NOT to delete the dimensions of our object when we are subsetting it such that it keeps the row numbers, thereby producing a dataframe.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="#cb169-1" tabindex="-1"></a><span class="fu">class</span>(Weekly[train, <span class="st">&quot;Lag2&quot;</span>, <span class="at">drop =</span> <span class="cn">FALSE</span>])</span></code></pre></div>
<pre><code>## [1] &quot;data.frame&quot;</code></pre>
<div class="attention">
<p>You must pay close attention to the requirements and specifications for the functions with which you build your models. Often, you will be prompted by error messages in the console.</p>
<p>For example, the <code>knn()</code> function expects either a matrix or a dataframe. If you do not set <code>drop = FALSE</code> you will not be able to proceed:</p>
<p><code>Error in knn(Weekly[train, "Lag2"], Weekly[!train, "Lag2"], Weekly$Direction[train], : dims of 'test' and 'train' differ</code></p>
<p>Other times, the error may be not so severe as to impede the function from working, but incorrectly structured data or improperly coded variables will lead to the function producing invalid results. Remember to carefully explore the arguments using the Help tab (e.g. ?knn).</p>
</div>
<p>Now let’s return to our results and produce a confusion matrix.</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="#cb171-1" tabindex="-1"></a>(t <span class="ot">&lt;-</span> <span class="fu">table</span>(fit_knn, Weekly[<span class="sc">!</span>train, ]<span class="sc">$</span>Direction))</span></code></pre></div>
<pre><code>##        
## fit_knn Down Up
##    Down   21 29
##    Up     22 32</code></pre>
<p>Our overall fraction of correct predictions is 0.5. Therefore, the KNN classifier (<span class="math inline">\(k = 1\)</span>) performs the worst out of all other classifiers we have explored so far (but only slightly worse than QDA).</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="#cb173-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">diag</span>(t)) <span class="sc">/</span> <span class="fu">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.5096154</code></pre>
<p>But before we move on to our next classifier, let’s consider other values for <span class="math inline">\(k\)</span> for illustration purposes. To ensure consistent results, we also set the seed (to 1 in this case).</p>
<p>We fit KNN for up to <span class="math inline">\(k = 30\)</span> by using the base R <code>sapply</code> to apply the <code>knn()</code> function to every integer from 1 to 30 and to then calculate the overall fraction of correct prediction.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="#cb175-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb175-2"><a href="#cb175-2" tabindex="-1"></a>knn_k <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">30</span>, <span class="cf">function</span>(k) {</span>
<span id="cb175-3"><a href="#cb175-3" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">knn</span>(Weekly[train, <span class="st">&quot;Lag2&quot;</span>, <span class="at">drop =</span> <span class="cn">FALSE</span>],</span>
<span id="cb175-4"><a href="#cb175-4" tabindex="-1"></a>             Weekly[<span class="sc">!</span>train, <span class="st">&quot;Lag2&quot;</span>, <span class="at">drop =</span> <span class="cn">FALSE</span>],</span>
<span id="cb175-5"><a href="#cb175-5" tabindex="-1"></a>             Weekly<span class="sc">$</span>Direction[train],</span>
<span id="cb175-6"><a href="#cb175-6" tabindex="-1"></a>             <span class="at">k =</span> k</span>
<span id="cb175-7"><a href="#cb175-7" tabindex="-1"></a>             )</span>
<span id="cb175-8"><a href="#cb175-8" tabindex="-1"></a>  <span class="fu">mean</span>(fit <span class="sc">==</span> Weekly[<span class="sc">!</span>train, ]<span class="sc">$</span>Direction)</span>
<span id="cb175-9"><a href="#cb175-9" tabindex="-1"></a>  })</span></code></pre></div>
<p>We can then create a plot to observe at what value for <code>k</code> the overall fraction of correct predictions is highest. This fraction stabilises at a value for <span class="math inline">\(k\)</span> somewhere between <span class="math inline">\(k = 10\)</span> and <span class="math inline">\(k = 15\)</span>.</p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="#cb176-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">30</span>, knn_k, <span class="at">type =</span> <span class="st">&quot;o&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;k&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Fraction correct&quot;</span>)</span></code></pre></div>
<p><img src="03-S03-D1_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
<p>We can find this out directly by asking R the index of the first time a maximum value among all other values appears. Our classifier appears to perform best when <span class="math inline">\(k = 12\)</span>.</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="#cb177-1" tabindex="-1"></a>(k <span class="ot">&lt;-</span> <span class="fu">which.max</span>(knn_k))</span></code></pre></div>
<pre><code>## [1] 12</code></pre>
<p>Now let’s re-evaluate our KNN classifier on the test data using `<span class="math inline">\(k = 12\)</span> and compute the confusion matrix.</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="#cb179-1" tabindex="-1"></a>fit_knn <span class="ot">&lt;-</span> <span class="fu">knn</span>(Weekly[train, <span class="st">&quot;Lag2&quot;</span>, <span class="at">drop =</span> <span class="cn">FALSE</span>],</span>
<span id="cb179-2"><a href="#cb179-2" tabindex="-1"></a>               Weekly[<span class="sc">!</span>train, <span class="st">&quot;Lag2&quot;</span>, <span class="at">drop =</span> <span class="cn">FALSE</span>],</span>
<span id="cb179-3"><a href="#cb179-3" tabindex="-1"></a>               Weekly<span class="sc">$</span>Direction[train], </span>
<span id="cb179-4"><a href="#cb179-4" tabindex="-1"></a>               <span class="at">k =</span> <span class="dv">12</span></span>
<span id="cb179-5"><a href="#cb179-5" tabindex="-1"></a>               )</span>
<span id="cb179-6"><a href="#cb179-6" tabindex="-1"></a></span>
<span id="cb179-7"><a href="#cb179-7" tabindex="-1"></a><span class="fu">table</span>(fit_knn , Weekly[<span class="sc">!</span>train, ]<span class="sc">$</span>Direction)</span></code></pre></div>
<pre><code>##        
## fit_knn Down Up
##    Down   18 18
##    Up     25 43</code></pre>
<p>Now, the overall fraction of correct predictions is higher than it was for <span class="math inline">\(k = 1\)</span> but this fraction still does not outperform logistic regression and LDA.</p>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="#cb181-1" tabindex="-1"></a><span class="fu">mean</span>(fit_knn <span class="sc">==</span> Weekly[<span class="sc">!</span>train, ]<span class="sc">$</span>Direction)</span></code></pre></div>
<pre><code>## [1] 0.5865385</code></pre>
</div>
<div id="naive-bayes" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Naive Bayes<a href="#naive-bayes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Finally, let’s evaluate the performance of Naive Bayes and conclude which approach performs best for our market movement classification problem.</p>
<p>A useful package for Naive Bayes is <code>e1071</code>. Like the <code>class</code> package, do note that you may be prompted with a similar error when loading the package (this can also be addressed by either updating RStudio or suppressing the warning).</p>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="#cb183-1" tabindex="-1"></a>fit_NBayes <span class="ot">&lt;-</span> <span class="fu">naiveBayes</span>(Direction <span class="sc">~</span> Lag2, <span class="at">data =</span> Weekly, <span class="at">subset =</span> train)</span></code></pre></div>
<p>Before generating the predictions, let’s explore the output of the fit. There are two important components:
- A-priori probabilities: i.e. prior probabilities (distribution of the classes for the response variable)<br />
- Conditional probabilities: parameters of the model for the predictor by class. For a numeric variable (as is our predictor in this case), the parameters shown are the mean <code>[,1]</code> and standard deviation <code>[,2]</code> for the predictor values in each class; for a categorical variable, these would be conditional probabilities for the predictor in each class.</p>
<p>The a priori probabilities can be extracted by specifying <code>fit_NBayes$apriori</code> and the conditional probabilities can be extracted using <code>fit_NBayes$tables</code>.</p>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb184-1"><a href="#cb184-1" tabindex="-1"></a>fit_NBayes</span></code></pre></div>
<pre><code>## 
## Naive Bayes Classifier for Discrete Predictors
## 
## Call:
## naiveBayes.default(x = X, y = Y, laplace = laplace)
## 
## A-priori probabilities:
## Y
##      Down        Up 
## 0.4477157 0.5522843 
## 
## Conditional probabilities:
##       Lag2
## Y             [,1]     [,2]
##   Down -0.03568254 2.199504
##   Up    0.26036581 2.317485</code></pre>
<p>Now let’s predict market movement.</p>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb186-1"><a href="#cb186-1" tabindex="-1"></a>pred_NBayes <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_NBayes, Weekly[<span class="sc">!</span>train, ], <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span></code></pre></div>
<p>And finally, generate our confusion matrix.</p>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="#cb187-1" tabindex="-1"></a>(t <span class="ot">&lt;-</span> <span class="fu">table</span>(pred_NBayes, Weekly[<span class="sc">!</span>train, ]<span class="sc">$</span>Direction))</span></code></pre></div>
<pre><code>##            
## pred_NBayes Down Up
##        Down    0  0
##        Up     43 61</code></pre>
<p>Our overall fraction of correct predictions is <span class="math inline">\(0.5865385\)</span>. Naive Bayes performs slightly better than KNN with <span class="math inline">\(k = 1\)</span> (<span class="math inline">\(0.5\)</span>) and the same as QDA (<span class="math inline">\(0.5865385\)</span>).</p>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb189-1"><a href="#cb189-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">diag</span>(t)) <span class="sc">/</span> <span class="fu">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.5865385</code></pre>
<p><em>Based on the approaches we have implemented in this demonstration, logistic regression and LDA perform best.</em></p>
<!--chapter:end:03-S03-D1.Rmd-->
</div>
</div>
<div id="demonstration-2-poisson-versus-linear-regression" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Demonstration 2: Poisson versus Linear Regression<a href="#demonstration-2-poisson-versus-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this demonstration, we will cover the basics of building and
interpreting a Poisson regression model.</p>
<p>You will need the <strong>Bikeshare</strong> dataset, part of the <code>ISRL2</code> package. By
loading the package, the <strong>Weekly</strong> dataset loads automatically.</p>
<p>This dataset measures the number of bike rentals (<code>bikers</code>) per hour in
Washington, DC. It contains 8645 observations on several variables. The
variables we will use in this practical and their descriptions are
listed below. For further information on additional variables, type
<code>?Bikeshare</code> in your R console after loading the <code>ISLR2</code> package.</p>
<ul>
<li><p>mnth: Month of the year, coded as a factor.</p></li>
<li><p>hr: Hour of the day, coded as a factor from 0 to 23.</p></li>
<li><p>workingday: Is it a work day? Yes=1, No=0.</p></li>
<li><p>temp: Normalised temperature in Celsius. The values are derived via
(t-t_min)/(t_max-t_min), t_min=-8, t_max=+39;</p></li>
<li><p>weathersit: Weather, coded as a factor.</p></li>
</ul>
<p>Loading the necessary packages:</p>
<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb191-1"><a href="#cb191-1" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span></code></pre></div>
<p>Once you load the <code>ISLR2</code> package, the <strong>Bikeshare</strong> dataset will be
‘loaded’ too and can be accessed without needing to assign it to a
separate object.</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="#cb192-1" tabindex="-1"></a><span class="fu">head</span>(Bikeshare)</span></code></pre></div>
<pre><code>##   season mnth day hr holiday weekday workingday   weathersit temp  atemp  hum
## 1      1  Jan   1  0       0       6          0        clear 0.24 0.2879 0.81
## 2      1  Jan   1  1       0       6          0        clear 0.22 0.2727 0.80
## 3      1  Jan   1  2       0       6          0        clear 0.22 0.2727 0.80
## 4      1  Jan   1  3       0       6          0        clear 0.24 0.2879 0.75
## 5      1  Jan   1  4       0       6          0        clear 0.24 0.2879 0.75
## 6      1  Jan   1  5       0       6          0 cloudy/misty 0.24 0.2576 0.75
##   windspeed casual registered bikers
## 1    0.0000      3         13     16
## 2    0.0000      8         32     40
## 3    0.0000      5         27     32
## 4    0.0000      3         10     13
## 5    0.0000      0          1      1
## 6    0.0896      0          1      1</code></pre>
<p>As usual, we can access variables within the dataset by indexing them.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="#cb194-1" tabindex="-1"></a>Bikeshare<span class="sc">$</span>temp</span></code></pre></div>
<pre><code>##    [1] 0.24 0.22 0.22 0.24 0.24 0.24 0.22 0.20 0.24 0.32 0.38 0.36 0.42 0.46
##   [15] 0.46 0.44 0.42 0.44 0.42 0.42 0.40 0.40 0.40 0.46 0.46 0.44 0.42 0.46
##   [29] 0.46 0.42 0.40 0.40 0.38 0.36 0.36 0.36 0.36 0.36 0.34 0.34 0.34 0.36
##   [43] 0.32 0.30 0.26 0.24 0.22 0.22 0.20 0.16 0.16 0.14 0.14 0.14 0.16 0.18
##   [57] 0.20 0.22 0.24 0.26 0.26 0.26 0.24 0.24 0.20 0.20 0.18 0.14 0.18 0.16
##   [71] 0.16 0.14 0.14 0.12 0.12 0.12 0.14 0.16 0.16 0.22 0.22 0.24 0.26 0.28
##   [85] 0.30 0.28 0.26 0.24 0.24 0.22 0.22 0.20 0.20 0.16 0.16 0.24 0.22 0.20
##   [99] 0.18 0.20 0.22 0.22 0.26 0.26 0.28 0.30 0.30 0.30 0.24 0.24 0.24 0.22
##  [113] 0.20 0.18 0.20 0.18 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.18 0.20 0.22
##  [127] 0.26 0.26 0.28 0.28 0.26 0.22 0.22 0.22 0.20 0.22 0.22 0.20 0.20 0.20
##  [141] 0.20 0.20 0.22 0.20 0.20 0.20 0.20 0.22 0.20 0.20 0.20 0.20 0.20 0.20
##  [155] 0.20 0.20 0.16 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.16 0.16
##  [169] 0.16 0.16 0.16 0.18 0.20 0.20 0.20 0.20 0.20 0.18 0.16 0.14 0.14 0.12
##  [183] 0.12 0.12 0.10 0.10 0.10 0.10 0.10 0.08 0.08 0.10 0.08 0.10 0.12 0.14
##  [197] 0.16 0.18 0.20 0.22 0.22 0.20 0.18 0.16 0.16 0.14 0.14 0.14 0.12 0.12
##  [211] 0.12 0.12 0.12 0.10 0.10 0.12 0.12 0.12 0.14 0.14 0.16 0.20 0.20 0.20
##  [225] 0.20 0.20 0.20 0.20 0.16 0.16 0.14 0.14 0.14 0.14 0.14 0.16 0.16 0.16
##  [239] 0.16 0.18 0.18 0.20 0.20 0.20 0.20 0.20 0.16 0.16 0.16 0.16 0.16 0.16
##  [253] 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.12 0.14 0.16 0.16 0.18 0.20 0.20
##  [267] 0.22 0.20 0.20 0.22 0.20 0.20 0.18 0.16 0.16 0.16 0.14 0.14 0.14 0.14
##  [281] 0.14 0.14 0.14 0.12 0.12 0.14 0.14 0.16 0.20 0.20 0.22 0.22 0.24 0.24
##  [295] 0.20 0.20 0.16 0.16 0.14 0.14 0.12 0.12 0.10 0.10 0.10 0.10 0.10 0.10
##  [309] 0.12 0.14 0.18 0.18 0.20 0.22 0.22 0.24 0.22 0.22 0.20 0.16 0.18 0.16
##  [323] 0.16 0.18 0.18 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.14 0.16 0.18 0.20
##  [337] 0.24 0.28 0.30 0.32 0.34 0.32 0.30 0.32 0.32 0.32 0.30 0.30 0.26 0.26
##  [351] 0.26 0.22 0.26 0.26 0.26 0.24 0.22 0.22 0.22 0.24 0.24 0.26 0.28 0.26
##  [365] 0.24 0.22 0.20 0.18 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.18 0.18 0.18
##  [379] 0.18 0.16 0.16 0.16 0.16 0.16 0.18 0.18 0.18 0.20 0.20 0.20 0.18 0.18
##  [393] 0.16 0.16 0.14 0.16 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22
##  [407] 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.24 0.24 0.24 0.26 0.28 0.30
##  [421] 0.40 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.32 0.30 0.30 0.26 0.26 0.26
##  [435] 0.26 0.26 0.24 0.22 0.22 0.22 0.24 0.26 0.28 0.30 0.28 0.30 0.32 0.30
##  [449] 0.30 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.22 0.22 0.24 0.22
##  [463] 0.20 0.20 0.20 0.20 0.22 0.22 0.20 0.20 0.16 0.16 0.14 0.12 0.12 0.10
##  [477] 0.08 0.06 0.06 0.04 0.04 0.04 0.04 0.02 0.02 0.02 0.02 0.04 0.04 0.06
##  [491] 0.06 0.08 0.10 0.12 0.12 0.12 0.08 0.08 0.06 0.06 0.06 0.04 0.04 0.04
##  [505] 0.02 0.02 0.04 0.04 0.08 0.06 0.10 0.14 0.14 0.16 0.14 0.16 0.16 0.16
##  [519] 0.14 0.12 0.12 0.10 0.10 0.08 0.06 0.06 0.04 0.04 0.02 0.02 0.02 0.02
##  [533] 0.04 0.06 0.10 0.10 0.12 0.14 0.14 0.16 0.16 0.14 0.14 0.14 0.14 0.14
##  [547] 0.14 0.16 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.16 0.20 0.22 0.24 0.26
##  [561] 0.26 0.30 0.32 0.32 0.30 0.30 0.26 0.24 0.24 0.22 0.22 0.22 0.24 0.22
##  [575] 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.20 0.22
##  [589] 0.22 0.20 0.20 0.18 0.18 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.18 0.16
##  [603] 0.16 0.18 0.18 0.18 0.18 0.18 0.22 0.20 0.22 0.24 0.24 0.24 0.24 0.22
##  [617] 0.24 0.24 0.22 0.22 0.22 0.20 0.16 0.16 0.16 0.18 0.18 0.18 0.18 0.20
##  [631] 0.22 0.22 0.22 0.24 0.24 0.22 0.22 0.18 0.18 0.16 0.16 0.16 0.14 0.16
##  [645] 0.14 0.14 0.14 0.14 0.14 0.16 0.18 0.22 0.30 0.28 0.28 0.30 0.30 0.30
##  [659] 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.18 0.16 0.16
##  [673] 0.16 0.16 0.16 0.16 0.18 0.16 0.18 0.16 0.16 0.16 0.16 0.30 0.16 0.16
##  [687] 0.16 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.16 0.16 0.18 0.20
##  [701] 0.20 0.22 0.24 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.22 0.22 0.22
##  [715] 0.22 0.22 0.22 0.22 0.22 0.22 0.24 0.22 0.24 0.24 0.34 0.38 0.38 0.36
##  [729] 0.36 0.34 0.28 0.24 0.22 0.22 0.20 0.20 0.20 0.18 0.18 0.16 0.16 0.14
##  [743] 0.14 0.16 0.18 0.18 0.20 0.20 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.18
##  [757] 0.18 0.20 0.20 0.16 0.14 0.14 0.14 0.16 0.14 0.14 0.16 0.20 0.22 0.24
##  [771] 0.26 0.28 0.28 0.30 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24
##  [785] 0.24 0.22 0.20 0.20 0.22 0.20 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22
##  [799] 0.24 0.28 0.28 0.30 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26
##  [813] 0.24 0.24 0.28 0.30 0.32 0.34 0.34 0.34 0.34 0.34 0.34 0.30 0.28 0.28
##  [827] 0.26 0.26 0.24 0.24 0.22 0.20 0.20 0.20 0.20 0.18 0.18 0.16 0.22 0.24
##  [841] 0.30 0.32 0.36 0.36 0.38 0.36 0.32 0.34 0.32 0.32 0.32 0.28 0.30 0.28
##  [855] 0.28 0.26 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.22 0.22 0.24 0.24 0.22
##  [869] 0.22 0.22 0.22 0.20 0.16 0.16 0.14 0.12 0.12 0.10 0.10 0.08 0.06 0.06
##  [883] 0.06 0.06 0.10 0.12 0.14 0.14 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.14
##  [897] 0.14 0.14 0.16 0.16 0.14 0.14 0.14 0.14 0.12 0.12 0.10 0.10 0.12 0.12
##  [911] 0.14 0.16 0.18 0.20 0.20 0.20 0.18 0.16 0.14 0.14 0.14 0.12 0.12 0.10
##  [925] 0.10 0.10 0.08 0.10 0.08 0.10 0.12 0.14 0.22 0.22 0.24 0.30 0.32 0.30
##  [939] 0.30 0.28 0.26 0.22 0.20 0.20 0.18 0.16 0.14 0.14 0.12 0.12 0.12 0.12
##  [953] 0.12 0.14 0.16 0.22 0.30 0.30 0.30 0.34 0.34 0.34 0.32 0.28 0.28 0.26
##  [967] 0.26 0.24 0.22 0.20 0.20 0.20 0.20 0.20 0.20 0.22 0.22 0.24 0.30 0.32
##  [981] 0.36 0.38 0.40 0.40 0.42 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38
##  [995] 0.36 0.34 0.32 0.32 0.34 0.34 0.38 0.40 0.44 0.52 0.56 0.58 0.60 0.56
## [1009] 0.52 0.46 0.40 0.38 0.36 0.36 0.34 0.32 0.30 0.30 0.28 0.22 0.22 0.20
## [1023] 0.20 0.20 0.22 0.24 0.26 0.28 0.32 0.34 0.34 0.34 0.32 0.30 0.28 0.26
## [1037] 0.24 0.24 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.20 0.22 0.24 0.26 0.34
## [1051] 0.38 0.42 0.46 0.46 0.46 0.46 0.40 0.34 0.38 0.36 0.34 0.38 0.34 0.34
## [1065] 0.34 0.34 0.32 0.32 0.30 0.32 0.32 0.36 0.38 0.44 0.48 0.54 0.60 0.60
## [1079] 0.56 0.58 0.54 0.48 0.48 0.52 0.50 0.46 0.44 0.44 0.44 0.46 0.46 0.46
## [1093] 0.44 0.42 0.42 0.42 0.44 0.44 0.50 0.60 0.66 0.66 0.66 0.66 0.64 0.62
## [1107] 0.60 0.58 0.54 0.52 0.48 0.46 0.44 0.42 0.40 0.40 0.40 0.38 0.38 0.40
## [1121] 0.42 0.44 0.44 0.44 0.46 0.44 0.44 0.42 0.36 0.34 0.32 0.32 0.30 0.28
## [1135] 0.26 0.24 0.24 0.22 0.22 0.20 0.18 0.20 0.22 0.26 0.30 0.30 0.34 0.36
## [1149] 0.36 0.36 0.34 0.34 0.34 0.34 0.32 0.32 0.30 0.34 0.34 0.34 0.34 0.32
## [1163] 0.34 0.42 0.42 0.32 0.32 0.32 0.32 0.32 0.30 0.32 0.30 0.28 0.28 0.24
## [1177] 0.24 0.24 0.22 0.20 0.20 0.12 0.12 0.12 0.14 0.16 0.16 0.20 0.22 0.22
## [1191] 0.24 0.22 0.22 0.22 0.20 0.20 0.20 0.16 0.16 0.14 0.14 0.12 0.12 0.12
## [1205] 0.12 0.12 0.14 0.18 0.20 0.24 0.26 0.30 0.32 0.34 0.34 0.34 0.32 0.30
## [1219] 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.24 0.24 0.26
## [1233] 0.32 0.36 0.38 0.40 0.40 0.38 0.36 0.34 0.34 0.34 0.34 0.34 0.32 0.32
## [1247] 0.32 0.32 0.32 0.32 0.34 0.34 0.36 0.34 0.42 0.52 0.54 0.54 0.56 0.46
## [1261] 0.32 0.32 0.32 0.30 0.30 0.28 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22
## [1275] 0.22 0.22 0.24 0.26 0.30 0.30 0.32 0.34 0.34 0.36 0.36 0.36 0.34 0.32
## [1289] 0.30 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.24 0.24 0.24 0.26 0.28 0.30
## [1303] 0.36 0.40 0.42 0.44 0.46 0.48 0.42 0.40 0.40 0.40 0.38 0.38 0.36 0.36
## [1317] 0.34 0.32 0.34 0.34 0.36 0.34 0.42 0.52 0.56 0.56 0.46 0.42 0.42 0.42
## [1331] 0.40 0.46 0.44 0.44 0.38 0.34 0.32 0.30 0.26 0.24 0.22 0.22 0.20 0.20
## [1345] 0.20 0.20 0.22 0.24 0.28 0.30 0.32 0.32 0.34 0.34 0.34 0.32 0.30 0.30
## [1359] 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.20 0.22 0.22 0.22 0.24 0.28 0.32
## [1373] 0.34 0.40 0.50 0.52 0.54 0.54 0.50 0.46 0.40 0.36 0.34 0.30 0.26 0.24
## [1387] 0.24 0.20 0.20 0.16 0.14 0.14 0.12 0.14 0.16 0.18 0.20 0.22 0.22 0.24
## [1401] 0.24 0.26 0.26 0.24 0.20 0.20 0.18 0.20 0.18 0.20 0.18 0.18 0.18 0.18
## [1415] 0.16 0.16 0.16 0.18 0.22 0.24 0.28 0.32 0.34 0.36 0.36 0.36 0.36 0.34
## [1429] 0.32 0.30 0.30 0.30 0.30 0.28 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30
## [1443] 0.32 0.34 0.40 0.44 0.46 0.48 0.46 0.48 0.48 0.48 0.46 0.44 0.44 0.42
## [1457] 0.44 0.42 0.42 0.40 0.42 0.42 0.42 0.42 0.40 0.42 0.42 0.42 0.46 0.46
## [1471] 0.44 0.44 0.36 0.34 0.32 0.30 0.28 0.24 0.22 0.22 0.20 0.20 0.20 0.20
## [1485] 0.20 0.20 0.20 0.20 0.22 0.24 0.26 0.30 0.32 0.32 0.34 0.34 0.34 0.32
## [1499] 0.30 0.30 0.28 0.26 0.28 0.26 0.24 0.24 0.24 0.22 0.20 0.20 0.18 0.22
## [1513] 0.26 0.30 0.36 0.36 0.38 0.38 0.36 0.38 0.36 0.34 0.34 0.32 0.30 0.30
## [1527] 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.26 0.30 0.32 0.32
## [1541] 0.32 0.34 0.36 0.34 0.34 0.34 0.34 0.32 0.32 0.32 0.34 0.34 0.34 0.34
## [1555] 0.36 0.36 0.38 0.38 0.40 0.40 0.40 0.42 0.42 0.44 0.44 0.42 0.44 0.44
## [1569] 0.44 0.36 0.36 0.34 0.34 0.34 0.34 0.34 0.32 0.30 0.26 0.26 0.28 0.30
## [1583] 0.32 0.32 0.34 0.36 0.36 0.34 0.34 0.34 0.32 0.30 0.30 0.30 0.30 0.30
## [1597] 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.24 0.26 0.28 0.32 0.34 0.34 0.36
## [1611] 0.40 0.42 0.46 0.46 0.42 0.42 0.40 0.38 0.36 0.38 0.38 0.36 0.34 0.34
## [1625] 0.36 0.34 0.36 0.40 0.40 0.42 0.44 0.46 0.46 0.46 0.48 0.46 0.44 0.40
## [1639] 0.36 0.32 0.30 0.30 0.26 0.26 0.26 0.26 0.26 0.24 0.26 0.28 0.30 0.32
## [1653] 0.34 0.36 0.38 0.38 0.38 0.38 0.40 0.38 0.36 0.36 0.34 0.34 0.32 0.32
## [1667] 0.32 0.30 0.30 0.24 0.24 0.22 0.24 0.26 0.30 0.32 0.34 0.36 0.36 0.38
## [1681] 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.30
## [1695] 0.30 0.30 0.32 0.32 0.36 0.36 0.40 0.40 0.40 0.40 0.40 0.44 0.44 0.44
## [1709] 0.42 0.42 0.40 0.40 0.38 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.36 0.40
## [1723] 0.44 0.44 0.50 0.52 0.50 0.52 0.50 0.50 0.46 0.44 0.42 0.42 0.40 0.42
## [1737] 0.42 0.40 0.40 0.36 0.38 0.40 0.40 0.42 0.46 0.52 0.54 0.56 0.64 0.66
## [1751] 0.68 0.68 0.70 0.68 0.66 0.62 0.62 0.62 0.60 0.60 0.58 0.56 0.54 0.52
## [1765] 0.52 0.44 0.40 0.42 0.42 0.44 0.46 0.46 0.50 0.50 0.50 0.50 0.48 0.46
## [1779] 0.44 0.42 0.40 0.40 0.38 0.34 0.32 0.30 0.28 0.26 0.26 0.26 0.24 0.28
## [1793] 0.30 0.32 0.34 0.36 0.38 0.40 0.40 0.42 0.40 0.38 0.36 0.36 0.34 0.34
## [1807] 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.30 0.30 0.34 0.38 0.42 0.44 0.50
## [1821] 0.54 0.56 0.54 0.54 0.52 0.58 0.56 0.46 0.46 0.46 0.46 0.42 0.44 0.44
## [1835] 0.42 0.40 0.40 0.40 0.40 0.40 0.44 0.44 0.46 0.50 0.50 0.50 0.50 0.50
## [1849] 0.48 0.44 0.44 0.42 0.40 0.40 0.36 0.34 0.34 0.34 0.32 0.34 0.32 0.32
## [1863] 0.32 0.34 0.34 0.34 0.34 0.36 0.38 0.40 0.40 0.38 0.38 0.36 0.32 0.32
## [1877] 0.32 0.32 0.30 0.30 0.28 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.30 0.30
## [1891] 0.32 0.30 0.30 0.30 0.30 0.30 0.30 0.28 0.26 0.26 0.24 0.24 0.20 0.20
## [1905] 0.20 0.18 0.18 0.18 0.20 0.22 0.24 0.26 0.30 0.32 0.32 0.36 0.34 0.34
## [1919] 0.32 0.30 0.30 0.30 0.30 0.28 0.26 0.26 0.26 0.22 0.20 0.22 0.20 0.18
## [1933] 0.20 0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.34 0.34 0.32 0.32 0.28 0.28
## [1947] 0.26 0.28 0.26 0.26 0.24 0.22 0.20 0.18 0.16 0.16 0.20 0.22 0.22 0.24
## [1961] 0.26 0.30 0.32 0.32 0.34 0.32 0.30 0.30 0.28 0.26 0.26 0.26 0.22 0.22
## [1975] 0.22 0.22 0.18 0.18 0.20 0.20 0.22 0.24 0.26 0.26 0.30 0.32 0.32 0.34
## [1989] 0.34 0.32 0.30 0.32 0.32 0.30 0.28 0.26 0.24 0.24 0.24 0.20 0.22 0.22
## [2003] 0.22 0.24 0.28 0.30 0.34 0.34 0.34 0.36 0.38 0.38 0.40 0.36 0.34 0.36
## [2017] 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30 0.32 0.32 0.32 0.34
## [2031] 0.34 0.34 0.36 0.36 0.28 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24
## [2045] 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.26 0.26 0.28 0.28 0.30 0.30
## [2059] 0.30 0.30 0.30 0.30 0.30 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.24 0.24
## [2073] 0.24 0.24 0.24 0.26 0.32 0.32 0.32 0.34 0.36 0.36 0.34 0.34 0.34 0.34
## [2087] 0.34 0.32 0.32 0.30 0.30 0.30 0.26 0.24 0.24 0.24 0.24 0.26 0.26 0.30
## [2101] 0.34 0.36 0.40 0.32 0.34 0.32 0.34 0.38 0.38 0.38 0.36 0.34 0.32 0.32
## [2115] 0.32 0.30 0.30 0.26 0.30 0.28 0.28 0.28 0.32 0.34 0.36 0.40 0.42 0.44
## [2129] 0.44 0.46 0.46 0.46 0.46 0.46 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.38
## [2143] 0.38 0.38 0.40 0.42 0.44 0.46 0.50 0.54 0.60 0.64 0.68 0.74 0.76 0.76
## [2157] 0.74 0.72 0.70 0.70 0.70 0.68 0.64 0.62 0.62 0.54 0.54 0.50 0.46 0.48
## [2171] 0.48 0.38 0.36 0.34 0.32 0.34 0.36 0.36 0.40 0.38 0.42 0.38 0.36 0.34
## [2185] 0.34 0.32 0.30 0.30 0.26 0.24 0.26 0.24 0.24 0.24 0.26 0.32 0.36 0.40
## [2199] 0.42 0.44 0.46 0.50 0.52 0.54 0.52 0.52 0.50 0.46 0.46 0.46 0.46 0.46
## [2213] 0.42 0.42 0.36 0.34 0.34 0.32 0.34 0.36 0.40 0.42 0.46 0.46 0.52 0.56
## [2227] 0.60 0.60 0.52 0.48 0.46 0.44 0.44 0.40 0.38 0.36 0.34 0.34 0.34 0.34
## [2241] 0.32 0.34 0.34 0.34 0.36 0.36 0.40 0.38 0.36 0.34 0.34 0.32 0.32 0.32
## [2255] 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.30 0.30 0.30 0.30 0.30 0.32
## [2269] 0.34 0.34 0.36 0.36 0.36 0.36 0.36 0.38 0.38 0.38 0.38 0.38 0.36 0.36
## [2283] 0.38 0.38 0.38 0.38 0.38 0.36 0.36 0.36 0.36 0.38 0.38 0.40 0.40 0.42
## [2297] 0.46 0.50 0.50 0.52 0.52 0.50 0.50 0.46 0.44 0.44 0.46 0.48 0.46 0.46
## [2311] 0.46 0.46 0.46 0.50 0.52 0.56 0.56 0.60 0.60 0.64 0.72 0.74 0.74 0.74
## [2325] 0.72 0.72 0.68 0.66 0.64 0.58 0.62 0.62 0.60 0.58 0.56 0.54 0.54 0.54
## [2339] 0.48 0.46 0.50 0.52 0.56 0.54 0.50 0.48 0.48 0.44 0.44 0.42 0.42 0.42
## [2353] 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.38 0.38 0.40 0.42
## [2367] 0.42 0.44 0.42 0.44 0.46 0.46 0.44 0.44 0.44 0.42 0.42 0.40 0.38 0.38
## [2381] 0.36 0.34 0.34 0.34 0.34 0.38 0.42 0.46 0.50 0.52 0.54 0.56 0.56 0.60
## [2395] 0.60 0.60 0.56 0.54 0.50 0.46 0.48 0.46 0.44 0.44 0.40 0.40 0.38 0.36
## [2409] 0.36 0.40 0.44 0.50 0.50 0.52 0.52 0.54 0.54 0.54 0.52 0.50 0.46 0.42
## [2423] 0.40 0.40 0.38 0.36 0.36 0.36 0.36 0.36 0.36 0.38 0.40 0.40 0.40 0.40
## [2437] 0.42 0.42 0.46 0.46 0.52 0.52 0.50 0.50 0.50 0.52 0.44 0.44 0.42 0.44
## [2451] 0.44 0.42 0.40 0.40 0.36 0.36 0.36 0.36 0.38 0.40 0.42 0.46 0.46 0.50
## [2465] 0.52 0.54 0.54 0.56 0.56 0.56 0.52 0.50 0.50 0.44 0.46 0.46 0.42 0.42
## [2479] 0.40 0.40 0.40 0.46 0.46 0.50 0.52 0.54 0.56 0.56 0.58 0.60 0.60 0.58
## [2493] 0.64 0.56 0.60 0.56 0.52 0.50 0.50 0.46 0.46 0.48 0.46 0.46 0.48 0.52
## [2507] 0.50 0.52 0.50 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.54 0.50 0.50 0.50
## [2521] 0.48 0.46 0.44 0.42 0.42 0.42 0.40 0.40 0.42 0.44 0.62 0.66 0.62 0.62
## [2535] 0.70 0.70 0.74 0.76 0.76 0.74 0.74 0.70 0.68 0.66 0.62 0.60 0.56 0.52
## [2549] 0.50 0.46 0.44 0.42 0.40 0.42 0.40 0.42 0.42 0.44 0.46 0.48 0.50 0.52
## [2563] 0.52 0.50 0.50 0.46 0.44 0.42 0.42 0.40 0.36 0.36 0.36 0.36 0.34 0.34
## [2577] 0.34 0.34 0.34 0.34 0.36 0.34 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.32
## [2591] 0.32 0.30 0.30 0.32 0.32 0.32 0.32 0.32 0.34 0.34 0.34 0.34 0.36 0.38
## [2605] 0.42 0.46 0.52 0.52 0.58 0.60 0.60 0.60 0.58 0.56 0.54 0.56 0.58 0.54
## [2619] 0.52 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.52 0.56 0.60 0.66 0.68 0.70
## [2633] 0.70 0.66 0.74 0.66 0.64 0.60 0.60 0.54 0.54 0.54 0.52 0.54 0.54 0.50
## [2647] 0.52 0.46 0.50 0.52 0.56 0.60 0.64 0.64 0.66 0.70 0.72 0.74 0.70 0.70
## [2661] 0.68 0.66 0.66 0.62 0.60 0.58 0.62 0.62 0.56 0.54 0.56 0.54 0.56 0.58
## [2675] 0.58 0.64 0.66 0.68 0.70 0.74 0.72 0.70 0.70 0.68 0.68 0.64 0.64 0.62
## [2689] 0.60 0.60 0.60 0.60 0.58 0.58 0.56 0.56 0.56 0.58 0.58 0.60 0.62 0.64
## [2703] 0.66 0.64 0.68 0.70 0.70 0.66 0.66 0.62 0.64 0.62 0.62 0.62 0.64 0.62
## [2717] 0.62 0.64 0.62 0.62 0.64 0.64 0.66 0.62 0.62 0.62 0.62 0.62 0.62 0.66
## [2731] 0.62 0.64 0.64 0.62 0.58 0.56 0.54 0.54 0.52 0.50 0.46 0.46 0.46 0.46
## [2745] 0.50 0.52 0.54 0.54 0.56 0.60 0.56 0.56 0.60 0.56 0.54 0.52 0.52 0.46
## [2759] 0.48 0.46 0.42 0.44 0.44 0.44 0.44 0.42 0.42 0.42 0.40 0.40 0.40 0.44
## [2773] 0.48 0.50 0.52 0.54 0.54 0.56 0.58 0.56 0.54 0.54 0.44 0.44 0.44 0.44
## [2787] 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.42 0.44 0.46 0.48 0.46 0.48 0.50
## [2801] 0.50 0.50 0.50 0.48 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.44 0.44
## [2815] 0.44 0.44 0.44 0.46 0.48 0.50 0.54 0.58 0.62 0.62 0.64 0.66 0.66 0.66
## [2829] 0.64 0.62 0.62 0.60 0.60 0.56 0.56 0.56 0.56 0.54 0.52 0.52 0.52 0.54
## [2843] 0.56 0.60 0.64 0.66 0.68 0.70 0.70 0.70 0.72 0.70 0.70 0.68 0.66 0.64
## [2857] 0.58 0.56 0.52 0.50 0.50 0.42 0.38 0.36 0.34 0.34 0.34 0.34 0.34 0.40
## [2871] 0.44 0.46 0.50 0.48 0.50 0.40 0.42 0.42 0.40 0.40 0.38 0.36 0.36 0.34
## [2885] 0.34 0.34 0.34 0.34 0.34 0.38 0.42 0.46 0.50 0.52 0.52 0.54 0.54 0.56
## [2899] 0.58 0.56 0.56 0.54 0.50 0.50 0.48 0.46 0.44 0.40 0.38 0.36 0.36 0.34
## [2913] 0.36 0.40 0.42 0.46 0.54 0.54 0.56 0.58 0.60 0.60 0.60 0.58 0.54 0.54
## [2927] 0.52 0.48 0.46 0.44 0.42 0.42 0.42 0.42 0.40 0.46 0.42 0.48 0.52 0.54
## [2941] 0.56 0.56 0.60 0.60 0.60 0.60 0.60 0.58 0.58 0.56 0.56 0.54 0.54 0.50
## [2955] 0.50 0.52 0.48 0.46 0.42 0.44 0.44 0.46 0.52 0.56 0.58 0.58 0.60 0.60
## [2969] 0.60 0.60 0.60 0.58 0.58 0.56 0.52 0.52 0.50 0.46 0.46 0.44 0.44 0.46
## [2983] 0.42 0.42 0.44 0.48 0.52 0.54 0.56 0.60 0.60 0.62 0.62 0.62 0.64 0.62
## [2997] 0.62 0.58 0.54 0.52 0.52 0.50 0.48 0.46 0.44 0.44 0.42 0.40 0.42 0.44
## [3011] 0.50 0.52 0.56 0.56 0.60 0.62 0.62 0.64 0.66 0.64 0.64 0.60 0.54 0.54
## [3025] 0.52 0.52 0.52 0.50 0.52 0.50 0.48 0.46 0.46 0.48 0.48 0.52 0.54 0.56
## [3039] 0.60 0.62 0.62 0.64 0.66 0.64 0.62 0.56 0.54 0.54 0.50 0.46 0.46 0.46
## [3053] 0.44 0.44 0.42 0.42 0.44 0.46 0.48 0.50 0.54 0.58 0.58 0.62 0.62 0.64
## [3067] 0.64 0.64 0.62 0.60 0.60 0.56 0.54 0.54 0.52 0.52 0.50 0.50 0.50 0.50
## [3081] 0.50 0.50 0.50 0.50 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52
## [3095] 0.52 0.52 0.52 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.48 0.50 0.52
## [3109] 0.52 0.52 0.52 0.52 0.54 0.54 0.54 0.56 0.56 0.54 0.54 0.54 0.54 0.52
## [3123] 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.54 0.58 0.58 0.60 0.62 0.62
## [3137] 0.64 0.66 0.64 0.56 0.56 0.56 0.54 0.54 0.56 0.54 0.52 0.52 0.50 0.50
## [3151] 0.50 0.50 0.52 0.52 0.56 0.60 0.62 0.64 0.66 0.68 0.68 0.72 0.60 0.58
## [3165] 0.58 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.56 0.54 0.54 0.52 0.52 0.52
## [3179] 0.52 0.54 0.54 0.56 0.56 0.56 0.62 0.62 0.62 0.62 0.60 0.60 0.58 0.54
## [3193] 0.54 0.54 0.54 0.54 0.52 0.52 0.52 0.52 0.52 0.54 0.56 0.56 0.54 0.54
## [3207] 0.56 0.56 0.58 0.60 0.60 0.60 0.60 0.56 0.56 0.52 0.52 0.52 0.52 0.50
## [3221] 0.50 0.48 0.48 0.48 0.50 0.50 0.52 0.54 0.54 0.54 0.58 0.58 0.54 0.56
## [3235] 0.58 0.56 0.60 0.58 0.54 0.54 0.50 0.48 0.46 0.46 0.44 0.44 0.44 0.44
## [3249] 0.46 0.50 0.54 0.54 0.56 0.58 0.60 0.60 0.62 0.60 0.60 0.62 0.60 0.58
## [3263] 0.58 0.56 0.54 0.52 0.52 0.52 0.52 0.48 0.46 0.46 0.50 0.54 0.56 0.60
## [3277] 0.62 0.64 0.66 0.70 0.72 0.72 0.72 0.72 0.70 0.68 0.62 0.62 0.60 0.58
## [3291] 0.54 0.52 0.52 0.50 0.50 0.50 0.52 0.54 0.60 0.62 0.64 0.70 0.72 0.66
## [3305] 0.62 0.66 0.68 0.70 0.66 0.66 0.64 0.62 0.60 0.58 0.56 0.56 0.56 0.54
## [3319] 0.54 0.54 0.54 0.56 0.60 0.60 0.66 0.68 0.68 0.74 0.72 0.72 0.72 0.72
## [3333] 0.70 0.70 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.58 0.60 0.58 0.58 0.64
## [3347] 0.62 0.64 0.70 0.74 0.76 0.78 0.78 0.74 0.66 0.70 0.70 0.66 0.66 0.64
## [3361] 0.62 0.66 0.60 0.58 0.56 0.54 0.54 0.56 0.56 0.62 0.64 0.68 0.70 0.74
## [3375] 0.74 0.74 0.74 0.76 0.74 0.74 0.72 0.70 0.70 0.66 0.66 0.64 0.64 0.64
## [3389] 0.62 0.60 0.60 0.60 0.60 0.62 0.66 0.72 0.70 0.74 0.78 0.82 0.82 0.82
## [3403] 0.80 0.80 0.80 0.78 0.72 0.72 0.70 0.70 0.68 0.70 0.68 0.66 0.64 0.64
## [3417] 0.64 0.64 0.66 0.70 0.72 0.74 0.76 0.74 0.76 0.78 0.76 0.74 0.72 0.60
## [3431] 0.62 0.60 0.60 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.58 0.60 0.62 0.64
## [3445] 0.70 0.70 0.72 0.72 0.74 0.74 0.76 0.74 0.72 0.70 0.70 0.66 0.66 0.64
## [3459] 0.64 0.62 0.62 0.62 0.60 0.60 0.62 0.62 0.62 0.62 0.66 0.66 0.70 0.72
## [3473] 0.74 0.74 0.74 0.74 0.72 0.72 0.70 0.68 0.66 0.66 0.64 0.64 0.64 0.64
## [3487] 0.62 0.62 0.64 0.64 0.66 0.72 0.80 0.82 0.86 0.86 0.88 0.88 0.88 0.86
## [3501] 0.86 0.52 0.76 0.74 0.72 0.70 0.70 0.68 0.66 0.64 0.66 0.66 0.68 0.74
## [3515] 0.78 0.80 0.82 0.86 0.86 0.90 0.90 0.90 0.90 0.84 0.84 0.78 0.78 0.76
## [3529] 0.74 0.72 0.70 0.70 0.70 0.68 0.68 0.66 0.68 0.70 0.72 0.74 0.76 0.82
## [3543] 0.86 0.90 0.90 0.90 0.86 0.86 0.82 0.74 0.74 0.74 0.74 0.74 0.74 0.72
## [3557] 0.70 0.66 0.66 0.66 0.66 0.70 0.70 0.72 0.72 0.74 0.76 0.80 0.78 0.80
## [3571] 0.80 0.76 0.74 0.72 0.70 0.66 0.64 0.62 0.62 0.60 0.56 0.54 0.52 0.52
## [3585] 0.54 0.54 0.56 0.58 0.62 0.64 0.66 0.68 0.70 0.70 0.72 0.72 0.70 0.68
## [3599] 0.64 0.64 0.62 0.58 0.56 0.54 0.54 0.52 0.52 0.50 0.54 0.56 0.60 0.62
## [3613] 0.64 0.66 0.70 0.74 0.74 0.74 0.72 0.72 0.74 0.70 0.70 0.66 0.64 0.64
## [3627] 0.64 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.64 0.64 0.66
## [3641] 0.70 0.70 0.72 0.74 0.70 0.68 0.66 0.64 0.64 0.62 0.62 0.60 0.58 0.58
## [3655] 0.56 0.56 0.58 0.62 0.64 0.70 0.72 0.74 0.76 0.78 0.78 0.80 0.76 0.78
## [3669] 0.76 0.72 0.68 0.66 0.66 0.64 0.64 0.62 0.60 0.60 0.58 0.58 0.60 0.64
## [3683] 0.68 0.72 0.76 0.76 0.80 0.80 0.80 0.82 0.80 0.80 0.78 0.76 0.74 0.72
## [3697] 0.70 0.68 0.66 0.66 0.64 0.64 0.62 0.62 0.64 0.66 0.76 0.76 0.82 0.84
## [3711] 0.88 0.90 0.92 0.92 0.92 0.92 0.90 0.82 0.80 0.80 0.76 0.76 0.74 0.74
## [3725] 0.72 0.72 0.72 0.70 0.72 0.72 0.76 0.84 0.86 0.90 0.92 0.90 0.92 0.94
## [3739] 0.92 0.90 0.88 0.84 0.80 0.76 0.74 0.74 0.70 0.70 0.68 0.68 0.66 0.66
## [3753] 0.66 0.72 0.74 0.76 0.78 0.82 0.84 0.84 0.86 0.84 0.82 0.82 0.80 0.78
## [3767] 0.76 0.76 0.72 0.72 0.70 0.70 0.70 0.68 0.68 0.68 0.70 0.72 0.74 0.74
## [3781] 0.74 0.76 0.80 0.80 0.82 0.82 0.80 0.74 0.72 0.70 0.68 0.66 0.66 0.66
## [3795] 0.66 0.64 0.64 0.64 0.62 0.62 0.62 0.64 0.70 0.72 0.76 0.78 0.82 0.78
## [3809] 0.82 0.80 0.68 0.68 0.70 0.70 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.60
## [3823] 0.56 0.54 0.54 0.56 0.58 0.62 0.64 0.66 0.66 0.70 0.70 0.70 0.70 0.70
## [3837] 0.70 0.66 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.58 0.54 0.54 0.54 0.56
## [3851] 0.58 0.62 0.62 0.64 0.64 0.64 0.64 0.64 0.68 0.64 0.62 0.64 0.60 0.60
## [3865] 0.58 0.56 0.56 0.54 0.52 0.50 0.50 0.48 0.50 0.54 0.58 0.60 0.64 0.68
## [3879] 0.70 0.74 0.74 0.76 0.76 0.74 0.72 0.70 0.66 0.64 0.62 0.62 0.60 0.58
## [3893] 0.60 0.56 0.56 0.60 0.58 0.56 0.60 0.62 0.66 0.68 0.72 0.72 0.72 0.70
## [3907] 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.56 0.56 0.56 0.56 0.54 0.54
## [3921] 0.56 0.60 0.60 0.70 0.70 0.68 0.70 0.74 0.76 0.78 0.76 0.76 0.76 0.64
## [3935] 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.62 0.64 0.66 0.70
## [3949] 0.72 0.72 0.74 0.76 0.80 0.82 0.76 0.76 0.74 0.74 0.72 0.72 0.72 0.72
## [3963] 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.66 0.68 0.68 0.70 0.72 0.74 0.74
## [3977] 0.74 0.74 0.76 0.74 0.74 0.72 0.70 0.68 0.66 0.66 0.66 0.64 0.64 0.64
## [3991] 0.62 0.62 0.60 0.56 0.56 0.60 0.60 0.60 0.62 0.64 0.66 0.66 0.70 0.70
## [4005] 0.70 0.66 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.60
## [4019] 0.62 0.62 0.64 0.66 0.70 0.74 0.76 0.78 0.80 0.78 0.76 0.74 0.74 0.72
## [4033] 0.70 0.70 0.66 0.66 0.66 0.66 0.64 0.64 0.66 0.70 0.72 0.72 0.74 0.74
## [4047] 0.80 0.82 0.82 0.82 0.82 0.80 0.80 0.80 0.74 0.74 0.72 0.72 0.72 0.72
## [4061] 0.72 0.72 0.70 0.72 0.72 0.72 0.74 0.74 0.76 0.76 0.76 0.76 0.76 0.76
## [4075] 0.74 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.64
## [4089] 0.66 0.66 0.70 0.74 0.80 0.80 0.80 0.80 0.78 0.82 0.80 0.76 0.76 0.74
## [4103] 0.72 0.70 0.70 0.68 0.68 0.66 0.66 0.64 0.64 0.62 0.64 0.64 0.70 0.72
## [4117] 0.72 0.74 0.74 0.74 0.74 0.74 0.76 0.74 0.72 0.72 0.70 0.68 0.68 0.66
## [4131] 0.64 0.64 0.62 0.60 0.60 0.60 0.60 0.62 0.64 0.66 0.72 0.72 0.74 0.74
## [4145] 0.74 0.74 0.74 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.64 0.62 0.62 0.62
## [4159] 0.60 0.62 0.62 0.64 0.66 0.72 0.72 0.70 0.72 0.72 0.72 0.74 0.74 0.74
## [4173] 0.74 0.72 0.70 0.70 0.68 0.68 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.68
## [4187] 0.70 0.74 0.80 0.82 0.84 0.84 0.86 0.86 0.86 0.82 0.80 0.74 0.74 0.74
## [4201] 0.70 0.70 0.68 0.68 0.66 0.66 0.66 0.64 0.66 0.70 0.70 0.72 0.74 0.76
## [4215] 0.76 0.80 0.80 0.82 0.82 0.82 0.80 0.76 0.74 0.72 0.70 0.68 0.66 0.64
## [4229] 0.62 0.60 0.58 0.58 0.60 0.62 0.68 0.70 0.72 0.74 0.76 0.76 0.78 0.78
## [4243] 0.80 0.78 0.76 0.74 0.74 0.72 0.70 0.66 0.66 0.66 0.62 0.64 0.62 0.60
## [4257] 0.62 0.66 0.70 0.74 0.76 0.80 0.80 0.80 0.82 0.82 0.82 0.82 0.80 0.78
## [4271] 0.72 0.70 0.70 0.68 0.68 0.66 0.64 0.64 0.62 0.58 0.62 0.64 0.68 0.74
## [4285] 0.80 0.80 0.82 0.82 0.84 0.86 0.88 0.84 0.82 0.80 0.76 0.74 0.72 0.72
## [4299] 0.70 0.70 0.70 0.68 0.68 0.62 0.62 0.64 0.68 0.70 0.74 0.76 0.80 0.80
## [4313] 0.82 0.84 0.84 0.80 0.80 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.64
## [4327] 0.66 0.64 0.64 0.66 0.70 0.72 0.76 0.76 0.78 0.78 0.80 0.82 0.82 0.80
## [4341] 0.80 0.78 0.76 0.74 0.74 0.72 0.70 0.70 0.66 0.66 0.66 0.66 0.66 0.70
## [4355] 0.72 0.74 0.76 0.78 0.80 0.82 0.80 0.82 0.82 0.82 0.80 0.80 0.76 0.78
## [4369] 0.76 0.74 0.72 0.70 0.70 0.70 0.68 0.68 0.70 0.72 0.72 0.70 0.70 0.72
## [4383] 0.74 0.74 0.76 0.76 0.76 0.78 0.76 0.74 0.72 0.70 0.70 0.68 0.66 0.66
## [4397] 0.66 0.64 0.64 0.64 0.64 0.66 0.70 0.74 0.78 0.82 0.84 0.86 0.86 0.86
## [4411] 0.86 0.86 0.84 0.82 0.76 0.74 0.74 0.72 0.74 0.72 0.70 0.68 0.70 0.68
## [4425] 0.70 0.70 0.72 0.76 0.74 0.74 0.76 0.78 0.80 0.80 0.68 0.66 0.66 0.66
## [4439] 0.66 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.64 0.66 0.70 0.74
## [4453] 0.76 0.78 0.80 0.82 0.84 0.84 0.82 0.84 0.82 0.76 0.76 0.74 0.72 0.72
## [4467] 0.70 0.70 0.68 0.66 0.66 0.66 0.64 0.70 0.72 0.74 0.76 0.78 0.82 0.80
## [4481] 0.82 0.84 0.84 0.84 0.82 0.80 0.76 0.74 0.74 0.72 0.70 0.70 0.70 0.68
## [4495] 0.66 0.66 0.68 0.70 0.74 0.78 0.80 0.82 0.84 0.86 0.86 0.86 0.86 0.86
## [4509] 0.86 0.84 0.72 0.70 0.72 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.74 0.76
## [4523] 0.76 0.78 0.82 0.82 0.86 0.86 0.90 0.90 0.86 0.88 0.86 0.84 0.82 0.82
## [4537] 0.80 0.78 0.76 0.76 0.76 0.74 0.74 0.74 0.74 0.76 0.80 0.82 0.82 0.84
## [4551] 0.84 0.82 0.82 0.64 0.66 0.70 0.72 0.70 0.70 0.70 0.68 0.66 0.66 0.66
## [4565] 0.64 0.62 0.62 0.60 0.60 0.62 0.64 0.68 0.70 0.72 0.74 0.76 0.74 0.76
## [4579] 0.76 0.74 0.72 0.72 0.70 0.66 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.60
## [4593] 0.60 0.64 0.64 0.68 0.66 0.70 0.70 0.70 0.70 0.72 0.72 0.74 0.72 0.70
## [4607] 0.70 0.66 0.66 0.64 0.62 0.60 0.60 0.60 0.60 0.58 0.60 0.62 0.66 0.70
## [4621] 0.72 0.74 0.76 0.76 0.74 0.76 0.76 0.76 0.76 0.74 0.72 0.70 0.70 0.68
## [4635] 0.66 0.64 0.64 0.64 0.62 0.64 0.62 0.64 0.68 0.72 0.74 0.76 0.76 0.80
## [4649] 0.80 0.82 0.80 0.80 0.80 0.76 0.76 0.74 0.72 0.70 0.70 0.70 0.66 0.66
## [4663] 0.64 0.64 0.64 0.68 0.70 0.74 0.76 0.80 0.80 0.82 0.82 0.84 0.84 0.84
## [4677] 0.82 0.80 0.78 0.74 0.76 0.74 0.74 0.74 0.72 0.72 0.72 0.70 0.72 0.74
## [4691] 0.76 0.80 0.82 0.82 0.84 0.86 0.86 0.88 0.90 0.80 0.80 0.76 0.74 0.74
## [4705] 0.74 0.72 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.70 0.74 0.74 0.76 0.80
## [4719] 0.82 0.84 0.86 0.86 0.84 0.84 0.84 0.82 0.80 0.80 0.78 0.76 0.76 0.74
## [4733] 0.74 0.74 0.72 0.72 0.72 0.74 0.76 0.80 0.82 0.86 0.88 0.88 0.90 0.90
## [4747] 0.92 0.92 0.90 0.86 0.84 0.82 0.82 0.80 0.82 0.80 0.78 0.78 0.76 0.74
## [4761] 0.76 0.80 0.84 0.86 0.90 0.90 0.94 0.94 0.96 0.94 0.90 0.88 0.90 0.86
## [4775] 0.84 0.82 0.84 0.80 0.82 0.82 0.82 0.78 0.76 0.76 0.80 0.80 0.84 0.84
## [4789] 0.86 0.90 0.92 0.94 0.92 0.94 0.94 0.94 0.92 0.82 0.82 0.82 0.80 0.80
## [4803] 0.80 0.78 0.80 0.80 0.78 0.78 0.80 0.80 0.82 0.82 0.86 0.84 0.90 0.86
## [4817] 0.86 0.90 0.90 0.90 0.88 0.86 0.84 0.80 0.78 0.76 0.76 0.76 0.74 0.74
## [4831] 0.72 0.72 0.72 0.74 0.76 0.80 0.82 0.84 0.84 0.74 0.72 0.70 0.70 0.72
## [4845] 0.74 0.74 0.72 0.70 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.66 0.68 0.72
## [4859] 0.74 0.76 0.80 0.82 0.84 0.84 0.86 0.88 0.86 0.86 0.84 0.82 0.80 0.78
## [4873] 0.76 0.76 0.78 0.78 0.76 0.72 0.72 0.70 0.70 0.72 0.74 0.76 0.78 0.80
## [4887] 0.82 0.84 0.84 0.86 0.86 0.84 0.82 0.80 0.76 0.74 0.72 0.74 0.70 0.72
## [4901] 0.72 0.72 0.70 0.70 0.70 0.72 0.74 0.78 0.80 0.84 0.84 0.86 0.84 0.86
## [4915] 0.86 0.84 0.84 0.82 0.80 0.78 0.76 0.76 0.74 0.74 0.74 0.74 0.72 0.72
## [4929] 0.72 0.74 0.76 0.86 0.90 0.92 0.96 0.94 0.96 0.96 0.96 0.96 0.92 0.90
## [4943] 0.86 0.82 0.80 0.78 0.76 0.76 0.76 0.74 0.72 0.72 0.72 0.76 0.78 0.82
## [4957] 0.82 0.84 0.84 0.88 0.90 0.90 0.90 0.90 0.88 0.74 0.82 0.80 0.78 0.76
## [4971] 0.76 0.74 0.74 0.74 0.72 0.72 0.74 0.74 0.76 0.80 0.84 0.86 0.90 0.90
## [4985] 0.90 0.92 0.92 0.92 0.86 0.80 0.80 0.78 0.74 0.74 0.72 0.72 0.70 0.70
## [4999] 0.66 0.66 0.66 0.74 0.80 0.82 0.86 0.88 0.90 0.90 0.92 0.90 0.90 0.76
## [5013] 0.78 0.74 0.72 0.70 0.70 0.68 0.66 0.66 0.68 0.66 0.66 0.66 0.68 0.72
## [5027] 0.74 0.78 0.82 0.84 0.86 0.86 0.90 0.90 0.90 0.90 0.86 0.86 0.82 0.80
## [5041] 0.78 0.80 0.80 0.78 0.78 0.76 0.76 0.76 0.72 0.74 0.74 0.74 0.74 0.74
## [5055] 0.76 0.76 0.76 0.70 0.68 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.68 0.66
## [5069] 0.66 0.66 0.68 0.68 0.68 0.68 0.70 0.72 0.72 0.74 0.76 0.76 0.80 0.80
## [5083] 0.76 0.76 0.70 0.70 0.70 0.70 0.68 0.66 0.66 0.64 0.66 0.64 0.64 0.64
## [5097] 0.64 0.66 0.70 0.72 0.74 0.76 0.76 0.78 0.78 0.76 0.80 0.78 0.76 0.74
## [5111] 0.72 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.66 0.64 0.64 0.68 0.68 0.74
## [5125] 0.74 0.78 0.80 0.80 0.82 0.84 0.74 0.74 0.72 0.72 0.72 0.70 0.70 0.70
## [5139] 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.72 0.76 0.80 0.82 0.90 0.90
## [5153] 0.86 0.72 0.72 0.74 0.72 0.72 0.72 0.72 0.70 0.70 0.70 0.68 0.66 0.66
## [5167] 0.66 0.70 0.70 0.72 0.74 0.76 0.80 0.82 0.82 0.84 0.82 0.84 0.86 0.86
## [5181] 0.84 0.82 0.80 0.76 0.76 0.74 0.72 0.72 0.72 0.72 0.70 0.70 0.72 0.72
## [5195] 0.74 0.78 0.80 0.80 0.82 0.84 0.86 0.86 0.86 0.80 0.80 0.80 0.80 0.78
## [5209] 0.78 0.76 0.74 0.72 0.72 0.70 0.70 0.68 0.68 0.70 0.74 0.76 0.80 0.80
## [5223] 0.82 0.82 0.84 0.86 0.86 0.84 0.82 0.80 0.76 0.76 0.74 0.74 0.70 0.70
## [5237] 0.66 0.64 0.64 0.64 0.62 0.66 0.70 0.72 0.74 0.76 0.78 0.76 0.80 0.80
## [5251] 0.80 0.80 0.78 0.74 0.72 0.70 0.70 0.66 0.64 0.64 0.62 0.62 0.62 0.60
## [5265] 0.62 0.64 0.68 0.72 0.74 0.76 0.80 0.78 0.80 0.80 0.82 0.82 0.76 0.74
## [5279] 0.72 0.70 0.68 0.68 0.68 0.68 0.68 0.66 0.64 0.64 0.64 0.66 0.70 0.70
## [5293] 0.72 0.74 0.66 0.74 0.74 0.68 0.70 0.72 0.70 0.68 0.68 0.68 0.68 0.66
## [5307] 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.66 0.66 0.70 0.70 0.70 0.72
## [5321] 0.74 0.74 0.74 0.76 0.74 0.72 0.70 0.60 0.60 0.60 0.60 0.60 0.60 0.60
## [5335] 0.60 0.60 0.60 0.60 0.64 0.64 0.68 0.72 0.74 0.78 0.74 0.74 0.74 0.74
## [5349] 0.70 0.70 0.66 0.66 0.66 0.64 0.64 0.64 0.62 0.62 0.64 0.62 0.62 0.64
## [5363] 0.70 0.68 0.70 0.74 0.76 0.76 0.76 0.80 0.80 0.76 0.76 0.74 0.74 0.72
## [5377] 0.70 0.66 0.66 0.64 0.66 0.64 0.64 0.64 0.62 0.66 0.70 0.74 0.76 0.78
## [5391] 0.80 0.80 0.82 0.80 0.82 0.80 0.76 0.76 0.74 0.72 0.70 0.70 0.68 0.66
## [5405] 0.66 0.66 0.64 0.64 0.64 0.64 0.66 0.72 0.74 0.76 0.80 0.80 0.82 0.80
## [5419] 0.80 0.80 0.76 0.66 0.66 0.68 0.70 0.70 0.68 0.64 0.64 0.64 0.64 0.64
## [5433] 0.64 0.66 0.66 0.70 0.72 0.72 0.74 0.76 0.78 0.80 0.80 0.76 0.76 0.62
## [5447] 0.62 0.62 0.60 0.60 0.60 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.66 0.70
## [5461] 0.72 0.72 0.74 0.76 0.80 0.80 0.80 0.80 0.76 0.74 0.74 0.72 0.70 0.70
## [5475] 0.70 0.68 0.68 0.66 0.68 0.66 0.66 0.68 0.70 0.72 0.74 0.80 0.82 0.80
## [5489] 0.70 0.70 0.72 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.68 0.68 0.70 0.70
## [5503] 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.72 0.74 0.74 0.74 0.74 0.74 0.74
## [5517] 0.72 0.70 0.66 0.64 0.64 0.62 0.60 0.58 0.56 0.56 0.54 0.54 0.54 0.60
## [5531] 0.62 0.66 0.70 0.70 0.70 0.72 0.72 0.72 0.72 0.72 0.72 0.66 0.64 0.62
## [5545] 0.62 0.62 0.62 0.60 0.58 0.58 0.56 0.56 0.56 0.60 0.62 0.64 0.70 0.72
## [5559] 0.74 0.76 0.76 0.76 0.76 0.76 0.74 0.74 0.72 0.70 0.70 0.68 0.68 0.68
## [5573] 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.72 0.74 0.70 0.70 0.70 0.72 0.74
## [5587] 0.72 0.72 0.72 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.60 0.62 0.62
## [5601] 0.62 0.64 0.66 0.70 0.74 0.76 0.76 0.78 0.80 0.80 0.78 0.74 0.74 0.72
## [5615] 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70
## [5629] 0.70 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.62 0.66 0.70 0.70 0.74
## [5643] 0.76 0.78 0.78 0.80 0.76 0.74 0.72 0.72 0.66 0.64 0.62 0.62 0.60 0.60
## [5657] 0.60 0.56 0.56 0.56 0.60 0.62 0.62 0.66 0.66 0.68 0.70 0.70 0.70 0.72
## [5671] 0.70 0.66 0.66 0.64 0.64 0.62 0.60 0.56 0.56 0.54 0.54 0.52 0.52 0.54
## [5685] 0.56 0.62 0.66 0.70 0.72 0.72 0.74 0.74 0.74 0.74 0.72 0.70 0.66 0.66
## [5699] 0.64 0.62 0.62 0.60 0.60 0.56 0.56 0.56 0.54 0.54 0.60 0.62 0.64 0.70
## [5713] 0.72 0.74 0.74 0.74 0.74 0.74 0.72 0.70 0.84 0.66 0.66 0.64 0.60 0.60
## [5727] 0.60 0.58 0.58 0.56 0.56 0.60 0.60 0.62 0.64 0.68 0.72 0.72 0.72 0.72
## [5741] 0.72 0.74 0.72 0.72 0.70 0.66 0.66 0.66 0.64 0.64 0.62 0.62 0.60 0.60
## [5755] 0.60 0.60 0.60 0.62 0.62 0.64 0.66 0.66 0.68 0.70 0.70 0.70 0.68 0.68
## [5769] 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.64
## [5783] 0.66 0.66 0.66 0.70 0.70 0.72 0.72 0.72 0.72 0.72 0.70 0.70 0.68 0.68
## [5797] 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.66 0.66 0.66 0.70 0.74 0.76
## [5811] 0.78 0.78 0.78 0.80 0.76 0.76 0.74 0.74 0.72 0.72 0.72 0.70 0.68 0.68
## [5825] 0.68 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.70 0.72 0.74 0.74 0.68 0.68
## [5839] 0.66 0.66 0.66 0.66 0.66 0.60 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [5853] 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [5867] 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.56 0.60 0.60 0.62 0.60 0.60 0.60
## [5881] 0.56 0.60 0.60 0.62 0.64 0.64 0.64 0.64 0.64 0.64 0.62 0.62 0.60 0.62
## [5895] 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.64 0.64 0.66 0.68 0.70 0.66 0.64
## [5909] 0.64 0.64 0.62 0.62 0.64 0.62 0.62 0.64 0.62 0.62 0.62 0.62 0.62 0.62
## [5923] 0.62 0.62 0.62 0.62 0.62 0.64 0.70 0.72 0.74 0.74 0.70 0.70 0.66 0.64
## [5937] 0.66 0.62 0.62 0.62 0.62 0.60 0.58 0.58 0.58 0.58 0.60 0.62 0.64 0.70
## [5951] 0.72 0.72 0.74 0.74 0.74 0.74 0.74 0.72 0.70 0.66 0.64 0.64 0.62 0.62
## [5965] 0.62 0.62 0.62 0.60 0.60 0.58 0.60 0.64 0.66 0.70 0.70 0.70 0.74 0.72
## [5979] 0.74 0.72 0.72 0.68 0.64 0.62 0.64 0.62 0.58 0.56 0.56 0.56 0.54 0.56
## [5993] 0.56 0.58 0.60 0.64 0.68 0.70 0.72 0.72 0.74 0.74 0.72 0.72 0.70 0.68
## [6007] 0.66 0.64 0.62 0.62 0.60 0.58 0.60 0.58 0.56 0.56 0.56 0.58 0.60 0.64
## [6021] 0.68 0.70 0.72 0.74 0.74 0.74 0.74 0.74 0.70 0.68 0.66 0.64 0.64 0.64
## [6035] 0.62 0.62 0.60 0.60 0.60 0.58 0.58 0.60 0.62 0.64 0.70 0.72 0.74 0.76
## [6049] 0.78 0.78 0.76 0.76 0.72 0.70 0.72 0.66 0.66 0.64 0.64 0.64 0.62 0.62
## [6063] 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.68 0.66 0.64 0.64 0.60 0.54 0.48
## [6077] 0.48 0.46 0.46 0.46 0.44 0.44 0.42 0.42 0.40 0.40 0.40 0.38 0.38 0.40
## [6091] 0.42 0.46 0.50 0.50 0.52 0.54 0.54 0.54 0.52 0.52 0.52 0.50 0.50 0.50
## [6105] 0.48 0.50 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.48 0.50 0.52
## [6119] 0.52 0.52 0.52 0.52 0.54 0.52 0.52 0.52 0.52 0.50 0.50 0.46 0.46 0.46
## [6133] 0.44 0.44 0.44 0.44 0.44 0.46 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.58
## [6147] 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.52 0.52 0.52 0.50 0.50 0.50
## [6161] 0.50 0.50 0.52 0.54 0.56 0.58 0.58 0.60 0.60 0.60 0.60 0.58 0.56 0.56
## [6175] 0.56 0.56 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [6189] 0.56 0.56 0.56 0.56 0.58 0.62 0.60 0.60 0.60 0.58 0.56 0.56 0.56 0.56
## [6203] 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.56 0.60 0.62 0.64 0.66
## [6217] 0.66 0.66 0.66 0.66 0.64 0.62 0.62 0.60 0.62 0.60 0.60 0.60 0.60 0.60
## [6231] 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.66 0.66 0.68 0.68 0.66
## [6245] 0.64 0.64 0.62 0.64 0.62 0.62 0.62 0.60 0.60 0.60 0.60 0.62 0.62 0.62
## [6259] 0.62 0.62 0.62 0.62 0.62 0.60 0.60 0.62 0.62 0.60 0.60 0.62 0.60 0.60
## [6273] 0.60 0.58 0.58 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.58 0.60 0.60 0.62
## [6287] 0.62 0.64 0.66 0.66 0.64 0.66 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60
## [6301] 0.60 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.66 0.68 0.66 0.66
## [6315] 0.70 0.68 0.68 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62
## [6329] 0.62 0.62 0.62 0.64 0.64 0.68 0.68 0.70 0.70 0.72 0.70 0.70 0.66 0.66
## [6343] 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.64 0.64
## [6357] 0.64 0.66 0.66 0.70 0.68 0.68 0.66 0.66 0.64 0.64 0.62 0.60 0.60 0.60
## [6371] 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.62 0.62 0.62 0.66 0.68 0.70
## [6385] 0.72 0.70 0.70 0.70 0.66 0.66 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60
## [6399] 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.62 0.66 0.64 0.68 0.68 0.68 0.66
## [6413] 0.64 0.62 0.60 0.58 0.56 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52
## [6427] 0.54 0.56 0.60 0.64 0.64 0.66 0.64 0.64 0.62 0.62 0.58 0.54 0.54 0.52
## [6441] 0.52 0.52 0.50 0.48 0.46 0.46 0.44 0.44 0.42 0.42 0.40 0.40 0.40 0.38
## [6455] 0.40 0.40 0.42 0.42 0.40 0.40 0.38 0.38 0.36 0.36 0.36 0.36 0.36 0.34
## [6469] 0.34 0.34 0.34 0.32 0.32 0.34 0.34 0.36 0.36 0.38 0.40 0.40 0.36 0.36
## [6483] 0.36 0.36 0.36 0.38 0.36 0.36 0.36 0.36 0.34 0.36 0.36 0.36 0.34 0.36
## [6497] 0.36 0.36 0.36 0.40 0.40 0.40 0.40 0.42 0.40 0.40 0.40 0.40 0.40 0.40
## [6511] 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.42 0.44 0.46
## [6525] 0.48 0.54 0.56 0.56 0.58 0.58 0.58 0.56 0.54 0.52 0.52 0.50 0.50 0.48
## [6539] 0.48 0.48 0.46 0.46 0.44 0.44 0.44 0.46 0.52 0.54 0.56 0.60 0.62 0.64
## [6553] 0.64 0.64 0.64 0.64 0.60 0.58 0.52 0.50 0.52 0.50 0.50 0.48 0.46 0.44
## [6567] 0.44 0.42 0.40 0.42 0.44 0.46 0.52 0.54 0.56 0.56 0.58 0.58 0.58 0.56
## [6581] 0.54 0.52 0.50 0.46 0.46 0.44 0.44 0.44 0.42 0.40 0.40 0.42 0.42 0.42
## [6595] 0.46 0.48 0.52 0.56 0.60 0.60 0.62 0.66 0.64 0.60 0.56 0.56 0.54 0.50
## [6609] 0.50 0.50 0.48 0.46 0.46 0.44 0.42 0.42 0.42 0.42 0.46 0.50 0.52 0.58
## [6623] 0.62 0.62 0.62 0.66 0.64 0.62 0.60 0.54 0.52 0.52 0.50 0.48 0.46 0.46
## [6637] 0.46 0.44 0.44 0.44 0.44 0.44 0.46 0.50 0.56 0.62 0.64 0.66 0.68 0.68
## [6651] 0.66 0.62 0.64 0.56 0.56 0.54 0.52 0.50 0.50 0.48 0.48 0.46 0.46 0.46
## [6665] 0.44 0.46 0.52 0.54 0.56 0.62 0.70 0.72 0.74 0.72 0.70 0.66 0.64 0.58
## [6679] 0.60 0.56 0.56 0.54 0.54 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.56 0.60
## [6693] 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.58 0.58 0.58 0.56 0.56
## [6707] 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [6721] 0.54 0.54 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [6735] 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.62 0.62 0.64 0.66 0.66 0.66 0.62
## [6749] 0.62 0.62 0.62 0.62 0.62 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.56 0.56
## [6763] 0.56 0.56 0.54 0.52 0.52 0.56 0.62 0.62 0.60 0.58 0.54 0.54 0.52 0.50
## [6777] 0.46 0.46 0.46 0.46 0.46 0.44 0.42 0.42 0.40 0.40 0.46 0.52 0.54 0.56
## [6791] 0.58 0.60 0.60 0.62 0.62 0.58 0.54 0.50 0.50 0.50 0.50 0.48 0.46 0.44
## [6805] 0.42 0.42 0.42 0.42 0.38 0.40 0.44 0.50 0.54 0.56 0.58 0.60 0.60 0.62
## [6819] 0.60 0.58 0.56 0.54 0.54 0.54 0.56 0.56 0.54 0.56 0.56 0.56 0.52 0.50
## [6833] 0.50 0.50 0.50 0.50 0.52 0.56 0.56 0.56 0.58 0.56 0.58 0.56 0.56 0.54
## [6847] 0.52 0.50 0.50 0.48 0.48 0.46 0.44 0.44 0.44 0.46 0.46 0.46 0.50 0.52
## [6861] 0.56 0.60 0.62 0.64 0.62 0.62 0.62 0.60 0.56 0.56 0.54 0.54 0.52 0.52
## [6875] 0.52 0.52 0.52 0.50 0.50 0.52 0.52 0.54 0.52 0.52 0.52 0.52 0.54 0.54
## [6889] 0.54 0.56 0.56 0.56 0.60 0.60 0.58 0.58 0.58 0.56 0.56 0.56 0.50 0.50
## [6903] 0.48 0.44 0.42 0.44 0.44 0.46 0.48 0.48 0.50 0.48 0.48 0.48 0.48 0.46
## [6917] 0.46 0.46 0.44 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.34 0.36 0.36 0.40
## [6931] 0.42 0.46 0.50 0.50 0.48 0.52 0.50 0.50 0.46 0.44 0.44 0.44 0.42 0.42
## [6945] 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.36 0.40 0.42 0.44 0.44 0.46
## [6959] 0.50 0.48 0.48 0.50 0.46 0.44 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.34
## [6973] 0.34 0.34 0.32 0.32 0.34 0.36 0.40 0.42 0.46 0.50 0.52 0.52 0.52 0.52
## [6987] 0.50 0.50 0.46 0.44 0.44 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.38 0.40
## [7001] 0.38 0.42 0.44 0.46 0.50 0.52 0.54 0.54 0.56 0.54 0.52 0.54 0.48 0.48
## [7015] 0.46 0.48 0.46 0.44 0.44 0.42 0.40 0.38 0.38 0.38 0.40 0.44 0.48 0.50
## [7029] 0.52 0.54 0.56 0.56 0.56 0.56 0.56 0.52 0.48 0.46 0.44 0.46 0.44 0.44
## [7043] 0.44 0.44 0.44 0.42 0.42 0.42 0.42 0.44 0.48 0.52 0.52 0.58 0.56 0.52
## [7057] 0.52 0.52 0.52 0.52 0.50 0.50 0.52 0.48 0.48 0.46 0.46 0.46 0.46 0.48
## [7071] 0.48 0.50 0.46 0.48 0.50 0.50 0.50 0.50 0.50 0.52 0.52 0.56 0.50 0.48
## [7085] 0.44 0.42 0.40 0.36 0.34 0.34 0.32 0.32 0.30 0.30 0.30 0.28 0.28 0.30
## [7099] 0.32 0.34 0.36 0.38 0.38 0.36 0.36 0.36 0.36 0.36 0.36 0.34 0.32 0.30
## [7113] 0.30 0.28 0.30 0.30 0.30 0.30 0.26 0.26 0.26 0.28 0.28 0.26 0.26 0.24
## [7127] 0.24 0.24 0.22 0.22 0.22 0.22 0.24 0.24 0.24 0.22 0.22 0.22 0.22 0.22
## [7141] 0.24 0.22 0.24 0.24 0.24 0.26 0.30 0.32 0.36 0.38 0.40 0.42 0.42 0.42
## [7155] 0.40 0.36 0.56 0.34 0.32 0.30 0.26 0.26 0.26 0.24 0.24 0.24 0.22 0.24
## [7169] 0.24 0.28 0.32 0.36 0.40 0.42 0.44 0.44 0.44 0.42 0.42 0.40 0.40 0.40
## [7183] 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.34 0.32 0.32 0.34 0.36 0.40 0.44
## [7197] 0.46 0.50 0.48 0.50 0.50 0.48 0.44 0.42 0.42 0.40 0.36 0.36 0.34 0.32
## [7211] 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.34 0.40 0.42 0.46 0.48 0.50
## [7225] 0.48 0.48 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32
## [7239] 0.32 0.34 0.32 0.34 0.36 0.40 0.44 0.50 0.52 0.52 0.52 0.52 0.48 0.46
## [7253] 0.44 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.38 0.38 0.40 0.40
## [7267] 0.42 0.42 0.44 0.48 0.44 0.44 0.46 0.46 0.42 0.42 0.40 0.36 0.34 0.34
## [7281] 0.32 0.32 0.32 0.30 0.30 0.28 0.26 0.26 0.26 0.28 0.30 0.32 0.36 0.36
## [7295] 0.40 0.42 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.30 0.28 0.28 0.26 0.26
## [7309] 0.24 0.24 0.24 0.26 0.26 0.28 0.30 0.36 0.42 0.44 0.46 0.46 0.48 0.46
## [7323] 0.44 0.42 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.30 0.28
## [7337] 0.28 0.30 0.34 0.36 0.42 0.46 0.54 0.56 0.56 0.52 0.50 0.46 0.46 0.40
## [7351] 0.38 0.36 0.36 0.34 0.32 0.32 0.32 0.30 0.30 0.30 0.30 0.32 0.36 0.42
## [7365] 0.46 0.52 0.54 0.56 0.58 0.56 0.52 0.48 0.46 0.40 0.40 0.36 0.36 0.36
## [7379] 0.34 0.32 0.32 0.32 0.30 0.30 0.30 0.32 0.34 0.40 0.46 0.50 0.50 0.52
## [7393] 0.52 0.52 0.46 0.44 0.44 0.44 0.40 0.40 0.38 0.40 0.40 0.38 0.38 0.38
## [7407] 0.36 0.36 0.38 0.40 0.42 0.44 0.46 0.42 0.36 0.36 0.36 0.36 0.36 0.36
## [7421] 0.36 0.36 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.30 0.30 0.28 0.28 0.30
## [7435] 0.32 0.32 0.34 0.34 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.32
## [7449] 0.30 0.24 0.24 0.24 0.24 0.20 0.22 0.22 0.22 0.26 0.30 0.34 0.38 0.44
## [7463] 0.48 0.50 0.52 0.52 0.50 0.42 0.42 0.42 0.42 0.42 0.40 0.40 0.36 0.36
## [7477] 0.36 0.36 0.34 0.34 0.34 0.34 0.40 0.44 0.46 0.52 0.52 0.54 0.50 0.54
## [7491] 0.52 0.52 0.50 0.50 0.48 0.48 0.46 0.46 0.46 0.46 0.44 0.44 0.44 0.44
## [7505] 0.44 0.46 0.48 0.50 0.54 0.56 0.60 0.62 0.64 0.62 0.62 0.56 0.60 0.60
## [7519] 0.60 0.58 0.56 0.56 0.56 0.56 0.54 0.56 0.54 0.56 0.54 0.54 0.56 0.56
## [7533] 0.56 0.54 0.54 0.54 0.54 0.52 0.50 0.50 0.50 0.48 0.50 0.46 0.46 0.46
## [7547] 0.46 0.46 0.46 0.44 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46
## [7561] 0.48 0.48 0.48 0.46 0.46 0.44 0.44 0.42 0.42 0.42 0.42 0.42 0.42 0.40
## [7575] 0.34 0.36 0.34 0.34 0.34 0.34 0.32 0.34 0.34 0.34 0.34 0.32 0.32 0.32
## [7589] 0.30 0.30 0.30 0.26 0.26 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.22
## [7603] 0.26 0.26 0.30 0.32 0.34 0.34 0.34 0.34 0.32 0.30 0.28 0.28 0.28 0.26
## [7617] 0.26 0.26 0.26 0.24 0.26 0.26 0.24 0.24 0.24 0.26 0.28 0.32 0.36 0.40
## [7631] 0.42 0.42 0.42 0.42 0.40 0.38 0.34 0.36 0.36 0.38 0.38 0.38 0.40 0.40
## [7645] 0.40 0.40 0.42 0.42 0.42 0.42 0.44 0.44 0.50 0.50 0.54 0.52 0.52 0.52
## [7659] 0.52 0.54 0.50 0.52 0.48 0.46 0.46 0.46 0.46 0.44 0.44 0.44 0.44 0.42
## [7673] 0.46 0.46 0.48 0.52 0.52 0.50 0.50 0.48 0.44 0.44 0.42 0.42 0.40 0.40
## [7687] 0.40 0.40 0.40 0.38 0.40 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.40 0.40
## [7701] 0.40 0.40 0.42 0.42 0.44 0.44 0.44 0.44 0.46 0.46 0.46 0.50 0.48 0.48
## [7715] 0.48 0.50 0.50 0.52 0.46 0.44 0.46 0.48 0.52 0.52 0.50 0.48 0.44 0.42
## [7729] 0.42 0.40 0.38 0.40 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.28 0.30
## [7743] 0.30 0.30 0.26 0.30 0.34 0.36 0.42 0.46 0.48 0.50 0.50 0.50 0.48 0.42
## [7757] 0.40 0.36 0.36 0.36 0.34 0.34 0.34 0.28 0.28 0.30 0.28 0.26 0.26 0.26
## [7771] 0.32 0.36 0.40 0.46 0.50 0.52 0.52 0.50 0.50 0.46 0.42 0.40 0.36 0.34
## [7785] 0.34 0.34 0.32 0.30 0.30 0.30 0.30 0.30 0.26 0.32 0.34 0.36 0.40 0.44
## [7799] 0.48 0.48 0.50 0.46 0.46 0.42 0.40 0.42 0.38 0.38 0.36 0.36 0.36 0.34
## [7813] 0.34 0.34 0.36 0.38 0.38 0.40 0.46 0.46 0.50 0.54 0.54 0.62 0.62 0.56
## [7827] 0.54 0.50 0.48 0.50 0.48 0.48 0.48 0.46 0.46 0.44 0.44 0.40 0.42 0.42
## [7841] 0.42 0.44 0.48 0.52 0.56 0.58 0.60 0.58 0.56 0.58 0.56 0.54 0.54 0.54
## [7855] 0.52 0.52 0.52 0.52 0.50 0.50 0.52 0.50 0.52 0.52 0.54 0.56 0.56 0.50
## [7869] 0.42 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.38 0.36 0.36 0.34
## [7883] 0.32 0.32 0.30 0.28 0.26 0.26 0.28 0.30 0.34 0.38 0.36 0.38 0.38 0.36
## [7897] 0.36 0.34 0.34 0.32 0.32 0.32 0.30 0.28 0.28 0.26 0.26 0.26 0.26 0.26
## [7911] 0.24 0.24 0.26 0.30 0.32 0.34 0.36 0.40 0.40 0.40 0.40 0.36 0.36 0.34
## [7925] 0.34 0.30 0.30 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.24 0.28
## [7939] 0.30 0.34 0.36 0.40 0.42 0.42 0.42 0.42 0.40 0.34 0.38 0.34 0.32 0.32
## [7953] 0.30 0.26 0.26 0.24 0.22 0.24 0.24 0.22 0.24 0.26 0.32 0.32 0.36 0.36
## [7967] 0.36 0.38 0.38 0.36 0.34 0.30 0.30 0.30 0.32 0.30 0.30 0.30 0.26 0.28
## [7981] 0.26 0.26 0.24 0.26 0.26 0.30 0.32 0.34 0.36 0.40 0.42 0.42 0.42 0.38
## [7995] 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.32 0.32 0.30 0.30 0.30 0.32
## [8009] 0.32 0.36 0.36 0.36 0.40 0.42 0.46 0.46 0.50 0.42 0.44 0.46 0.46 0.44
## [8023] 0.44 0.46 0.50 0.46 0.46 0.46 0.46 0.46 0.44 0.46 0.46 0.46 0.46 0.46
## [8037] 0.46 0.46 0.48 0.50 0.46 0.46 0.46 0.46 0.46 0.44 0.46 0.46 0.44 0.46
## [8051] 0.46 0.46 0.46 0.46 0.48 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.42
## [8065] 0.42 0.40 0.40 0.34 0.34 0.30 0.24 0.24 0.26 0.26 0.28 0.26 0.24 0.22
## [8079] 0.22 0.22 0.22 0.24 0.26 0.28 0.28 0.30 0.32 0.32 0.32 0.30 0.28 0.26
## [8093] 0.26 0.28 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.24 0.26
## [8107] 0.30 0.32 0.36 0.38 0.38 0.38 0.36 0.34 0.34 0.34 0.30 0.30 0.28 0.28
## [8121] 0.26 0.26 0.28 0.28 0.26 0.26 0.24 0.24 0.26 0.28 0.32 0.32 0.32 0.34
## [8135] 0.34 0.34 0.32 0.28 0.26 0.26 0.24 0.22 0.22 0.20 0.20 0.16 0.18 0.16
## [8149] 0.16 0.18 0.16 0.18 0.16 0.20 0.24 0.26 0.26 0.30 0.30 0.30 0.30 0.28
## [8163] 0.24 0.22 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.18 0.18 0.16 0.16 0.14
## [8177] 0.16 0.18 0.22 0.26 0.28 0.30 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.26
## [8191] 0.26 0.24 0.22 0.20 0.20 0.18 0.20 0.18 0.20 0.16 0.20 0.26 0.30 0.34
## [8205] 0.36 0.40 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.30 0.30 0.26 0.26 0.26
## [8219] 0.26 0.28 0.26 0.26 0.26 0.28 0.26 0.30 0.32 0.34 0.36 0.36 0.38 0.38
## [8233] 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.32
## [8247] 0.32 0.32 0.34 0.36 0.40 0.40 0.46 0.50 0.52 0.52 0.46 0.52 0.52 0.52
## [8261] 0.52 0.50 0.52 0.52 0.50 0.50 0.48 0.46 0.50 0.48 0.44 0.38 0.36 0.36
## [8275] 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30
## [8289] 0.30 0.30 0.28 0.26 0.24 0.26 0.26 0.26 0.26 0.26 0.26 0.28 0.28 0.28
## [8303] 0.28 0.28 0.28 0.26 0.26 0.24 0.22 0.20 0.20 0.20 0.20 0.20 0.22 0.22
## [8317] 0.22 0.20 0.20 0.20 0.20 0.22 0.24 0.24 0.26 0.30 0.30 0.32 0.28 0.28
## [8331] 0.28 0.26 0.24 0.22 0.22 0.20 0.20 0.18 0.18 0.16 0.16 0.14 0.16 0.18
## [8345] 0.20 0.22 0.24 0.26 0.30 0.34 0.36 0.38 0.40 0.38 0.36 0.36 0.40 0.36
## [8359] 0.36 0.36 0.36 0.36 0.34 0.34 0.36 0.36 0.36 0.36 0.42 0.36 0.42 0.40
## [8373] 0.44 0.44 0.44 0.44 0.44 0.40 0.38 0.38 0.36 0.36 0.36 0.38 0.34 0.36
## [8387] 0.36 0.36 0.36 0.38 0.36 0.36 0.36 0.40 0.48 0.48 0.46 0.44 0.48 0.48
## [8401] 0.44 0.44 0.44 0.50 0.50 0.50 0.50 0.50 0.50 0.44 0.48 0.44 0.38 0.38
## [8415] 0.36 0.34 0.36 0.38 0.40 0.44 0.48 0.46 0.46 0.48 0.46 0.44 0.44 0.44
## [8429] 0.42 0.42 0.36 0.40 0.40 0.40 0.38 0.38 0.38 0.36 0.38 0.38 0.40 0.40
## [8443] 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.36 0.34 0.32 0.32 0.32 0.32
## [8457] 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.30 0.32 0.34 0.34
## [8471] 0.34 0.32 0.32 0.28 0.30 0.30 0.26 0.26 0.24 0.24 0.24 0.22 0.22 0.22
## [8485] 0.20 0.20 0.22 0.20 0.24 0.26 0.30 0.30 0.32 0.34 0.34 0.36 0.34 0.32
## [8499] 0.32 0.32 0.30 0.28 0.26 0.22 0.28 0.34 0.34 0.34 0.32 0.32 0.32 0.34
## [8513] 0.34 0.34 0.36 0.38 0.38 0.38 0.36 0.34 0.32 0.30 0.30 0.26 0.26 0.26
## [8527] 0.26 0.26 0.26 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.32 0.32 0.30 0.30
## [8541] 0.42 0.42 0.44 0.40 0.38 0.34 0.32 0.32 0.32 0.30 0.32 0.32 0.32 0.32
## [8555] 0.32 0.32 0.32 0.32 0.32 0.34 0.36 0.34 0.34 0.32 0.32 0.30 0.28 0.26
## [8569] 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.18 0.20
## [8583] 0.20 0.22 0.24 0.26 0.28 0.30 0.30 0.30 0.30 0.30 0.30 0.28 0.28 0.28
## [8597] 0.30 0.28 0.26 0.24 0.24 0.24 0.22 0.24 0.24 0.24 0.26 0.30 0.32 0.32
## [8611] 0.36 0.40 0.42 0.42 0.38 0.36 0.34 0.34 0.36 0.34 0.36 0.38 0.40 0.40
## [8625] 0.40 0.38 0.36 0.40 0.38 0.34 0.38 0.40 0.42 0.52 0.50 0.46 0.46 0.44
## [8639] 0.42 0.42 0.42 0.42 0.40 0.38 0.36</code></pre>
<p>However, if we want to access variables within the dataset without
needing to index them we can use the base R <code>attach()</code> function.</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="#cb196-1" tabindex="-1"></a><span class="fu">attach</span>(Bikeshare)</span></code></pre></div>
<p>So now, we can call on the variables from the dataset directly (if we do
not have other datasets loaded in the R environment with variables named
in the same way).</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="#cb197-1" tabindex="-1"></a>temp</span></code></pre></div>
<pre><code>##    [1] 0.24 0.22 0.22 0.24 0.24 0.24 0.22 0.20 0.24 0.32 0.38 0.36 0.42 0.46
##   [15] 0.46 0.44 0.42 0.44 0.42 0.42 0.40 0.40 0.40 0.46 0.46 0.44 0.42 0.46
##   [29] 0.46 0.42 0.40 0.40 0.38 0.36 0.36 0.36 0.36 0.36 0.34 0.34 0.34 0.36
##   [43] 0.32 0.30 0.26 0.24 0.22 0.22 0.20 0.16 0.16 0.14 0.14 0.14 0.16 0.18
##   [57] 0.20 0.22 0.24 0.26 0.26 0.26 0.24 0.24 0.20 0.20 0.18 0.14 0.18 0.16
##   [71] 0.16 0.14 0.14 0.12 0.12 0.12 0.14 0.16 0.16 0.22 0.22 0.24 0.26 0.28
##   [85] 0.30 0.28 0.26 0.24 0.24 0.22 0.22 0.20 0.20 0.16 0.16 0.24 0.22 0.20
##   [99] 0.18 0.20 0.22 0.22 0.26 0.26 0.28 0.30 0.30 0.30 0.24 0.24 0.24 0.22
##  [113] 0.20 0.18 0.20 0.18 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.18 0.20 0.22
##  [127] 0.26 0.26 0.28 0.28 0.26 0.22 0.22 0.22 0.20 0.22 0.22 0.20 0.20 0.20
##  [141] 0.20 0.20 0.22 0.20 0.20 0.20 0.20 0.22 0.20 0.20 0.20 0.20 0.20 0.20
##  [155] 0.20 0.20 0.16 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.18 0.16 0.16
##  [169] 0.16 0.16 0.16 0.18 0.20 0.20 0.20 0.20 0.20 0.18 0.16 0.14 0.14 0.12
##  [183] 0.12 0.12 0.10 0.10 0.10 0.10 0.10 0.08 0.08 0.10 0.08 0.10 0.12 0.14
##  [197] 0.16 0.18 0.20 0.22 0.22 0.20 0.18 0.16 0.16 0.14 0.14 0.14 0.12 0.12
##  [211] 0.12 0.12 0.12 0.10 0.10 0.12 0.12 0.12 0.14 0.14 0.16 0.20 0.20 0.20
##  [225] 0.20 0.20 0.20 0.20 0.16 0.16 0.14 0.14 0.14 0.14 0.14 0.16 0.16 0.16
##  [239] 0.16 0.18 0.18 0.20 0.20 0.20 0.20 0.20 0.16 0.16 0.16 0.16 0.16 0.16
##  [253] 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.12 0.14 0.16 0.16 0.18 0.20 0.20
##  [267] 0.22 0.20 0.20 0.22 0.20 0.20 0.18 0.16 0.16 0.16 0.14 0.14 0.14 0.14
##  [281] 0.14 0.14 0.14 0.12 0.12 0.14 0.14 0.16 0.20 0.20 0.22 0.22 0.24 0.24
##  [295] 0.20 0.20 0.16 0.16 0.14 0.14 0.12 0.12 0.10 0.10 0.10 0.10 0.10 0.10
##  [309] 0.12 0.14 0.18 0.18 0.20 0.22 0.22 0.24 0.22 0.22 0.20 0.16 0.18 0.16
##  [323] 0.16 0.18 0.18 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.14 0.16 0.18 0.20
##  [337] 0.24 0.28 0.30 0.32 0.34 0.32 0.30 0.32 0.32 0.32 0.30 0.30 0.26 0.26
##  [351] 0.26 0.22 0.26 0.26 0.26 0.24 0.22 0.22 0.22 0.24 0.24 0.26 0.28 0.26
##  [365] 0.24 0.22 0.20 0.18 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.18 0.18 0.18
##  [379] 0.18 0.16 0.16 0.16 0.16 0.16 0.18 0.18 0.18 0.20 0.20 0.20 0.18 0.18
##  [393] 0.16 0.16 0.14 0.16 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22
##  [407] 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.24 0.24 0.24 0.26 0.28 0.30
##  [421] 0.40 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.32 0.30 0.30 0.26 0.26 0.26
##  [435] 0.26 0.26 0.24 0.22 0.22 0.22 0.24 0.26 0.28 0.30 0.28 0.30 0.32 0.30
##  [449] 0.30 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.22 0.22 0.24 0.22
##  [463] 0.20 0.20 0.20 0.20 0.22 0.22 0.20 0.20 0.16 0.16 0.14 0.12 0.12 0.10
##  [477] 0.08 0.06 0.06 0.04 0.04 0.04 0.04 0.02 0.02 0.02 0.02 0.04 0.04 0.06
##  [491] 0.06 0.08 0.10 0.12 0.12 0.12 0.08 0.08 0.06 0.06 0.06 0.04 0.04 0.04
##  [505] 0.02 0.02 0.04 0.04 0.08 0.06 0.10 0.14 0.14 0.16 0.14 0.16 0.16 0.16
##  [519] 0.14 0.12 0.12 0.10 0.10 0.08 0.06 0.06 0.04 0.04 0.02 0.02 0.02 0.02
##  [533] 0.04 0.06 0.10 0.10 0.12 0.14 0.14 0.16 0.16 0.14 0.14 0.14 0.14 0.14
##  [547] 0.14 0.16 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.16 0.20 0.22 0.24 0.26
##  [561] 0.26 0.30 0.32 0.32 0.30 0.30 0.26 0.24 0.24 0.22 0.22 0.22 0.24 0.22
##  [575] 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.22 0.20 0.22
##  [589] 0.22 0.20 0.20 0.18 0.18 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.18 0.16
##  [603] 0.16 0.18 0.18 0.18 0.18 0.18 0.22 0.20 0.22 0.24 0.24 0.24 0.24 0.22
##  [617] 0.24 0.24 0.22 0.22 0.22 0.20 0.16 0.16 0.16 0.18 0.18 0.18 0.18 0.20
##  [631] 0.22 0.22 0.22 0.24 0.24 0.22 0.22 0.18 0.18 0.16 0.16 0.16 0.14 0.16
##  [645] 0.14 0.14 0.14 0.14 0.14 0.16 0.18 0.22 0.30 0.28 0.28 0.30 0.30 0.30
##  [659] 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.18 0.16 0.16
##  [673] 0.16 0.16 0.16 0.16 0.18 0.16 0.18 0.16 0.16 0.16 0.16 0.30 0.16 0.16
##  [687] 0.16 0.16 0.16 0.16 0.16 0.16 0.14 0.14 0.16 0.16 0.16 0.16 0.18 0.20
##  [701] 0.20 0.22 0.24 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.22 0.22 0.22
##  [715] 0.22 0.22 0.22 0.22 0.22 0.22 0.24 0.22 0.24 0.24 0.34 0.38 0.38 0.36
##  [729] 0.36 0.34 0.28 0.24 0.22 0.22 0.20 0.20 0.20 0.18 0.18 0.16 0.16 0.14
##  [743] 0.14 0.16 0.18 0.18 0.20 0.20 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.18
##  [757] 0.18 0.20 0.20 0.16 0.14 0.14 0.14 0.16 0.14 0.14 0.16 0.20 0.22 0.24
##  [771] 0.26 0.28 0.28 0.30 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24
##  [785] 0.24 0.22 0.20 0.20 0.22 0.20 0.20 0.20 0.22 0.22 0.22 0.22 0.22 0.22
##  [799] 0.24 0.28 0.28 0.30 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26 0.26
##  [813] 0.24 0.24 0.28 0.30 0.32 0.34 0.34 0.34 0.34 0.34 0.34 0.30 0.28 0.28
##  [827] 0.26 0.26 0.24 0.24 0.22 0.20 0.20 0.20 0.20 0.18 0.18 0.16 0.22 0.24
##  [841] 0.30 0.32 0.36 0.36 0.38 0.36 0.32 0.34 0.32 0.32 0.32 0.28 0.30 0.28
##  [855] 0.28 0.26 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.22 0.22 0.24 0.24 0.22
##  [869] 0.22 0.22 0.22 0.20 0.16 0.16 0.14 0.12 0.12 0.10 0.10 0.08 0.06 0.06
##  [883] 0.06 0.06 0.10 0.12 0.14 0.14 0.18 0.18 0.20 0.20 0.20 0.20 0.18 0.14
##  [897] 0.14 0.14 0.16 0.16 0.14 0.14 0.14 0.14 0.12 0.12 0.10 0.10 0.12 0.12
##  [911] 0.14 0.16 0.18 0.20 0.20 0.20 0.18 0.16 0.14 0.14 0.14 0.12 0.12 0.10
##  [925] 0.10 0.10 0.08 0.10 0.08 0.10 0.12 0.14 0.22 0.22 0.24 0.30 0.32 0.30
##  [939] 0.30 0.28 0.26 0.22 0.20 0.20 0.18 0.16 0.14 0.14 0.12 0.12 0.12 0.12
##  [953] 0.12 0.14 0.16 0.22 0.30 0.30 0.30 0.34 0.34 0.34 0.32 0.28 0.28 0.26
##  [967] 0.26 0.24 0.22 0.20 0.20 0.20 0.20 0.20 0.20 0.22 0.22 0.24 0.30 0.32
##  [981] 0.36 0.38 0.40 0.40 0.42 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38
##  [995] 0.36 0.34 0.32 0.32 0.34 0.34 0.38 0.40 0.44 0.52 0.56 0.58 0.60 0.56
## [1009] 0.52 0.46 0.40 0.38 0.36 0.36 0.34 0.32 0.30 0.30 0.28 0.22 0.22 0.20
## [1023] 0.20 0.20 0.22 0.24 0.26 0.28 0.32 0.34 0.34 0.34 0.32 0.30 0.28 0.26
## [1037] 0.24 0.24 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.20 0.22 0.24 0.26 0.34
## [1051] 0.38 0.42 0.46 0.46 0.46 0.46 0.40 0.34 0.38 0.36 0.34 0.38 0.34 0.34
## [1065] 0.34 0.34 0.32 0.32 0.30 0.32 0.32 0.36 0.38 0.44 0.48 0.54 0.60 0.60
## [1079] 0.56 0.58 0.54 0.48 0.48 0.52 0.50 0.46 0.44 0.44 0.44 0.46 0.46 0.46
## [1093] 0.44 0.42 0.42 0.42 0.44 0.44 0.50 0.60 0.66 0.66 0.66 0.66 0.64 0.62
## [1107] 0.60 0.58 0.54 0.52 0.48 0.46 0.44 0.42 0.40 0.40 0.40 0.38 0.38 0.40
## [1121] 0.42 0.44 0.44 0.44 0.46 0.44 0.44 0.42 0.36 0.34 0.32 0.32 0.30 0.28
## [1135] 0.26 0.24 0.24 0.22 0.22 0.20 0.18 0.20 0.22 0.26 0.30 0.30 0.34 0.36
## [1149] 0.36 0.36 0.34 0.34 0.34 0.34 0.32 0.32 0.30 0.34 0.34 0.34 0.34 0.32
## [1163] 0.34 0.42 0.42 0.32 0.32 0.32 0.32 0.32 0.30 0.32 0.30 0.28 0.28 0.24
## [1177] 0.24 0.24 0.22 0.20 0.20 0.12 0.12 0.12 0.14 0.16 0.16 0.20 0.22 0.22
## [1191] 0.24 0.22 0.22 0.22 0.20 0.20 0.20 0.16 0.16 0.14 0.14 0.12 0.12 0.12
## [1205] 0.12 0.12 0.14 0.18 0.20 0.24 0.26 0.30 0.32 0.34 0.34 0.34 0.32 0.30
## [1219] 0.24 0.24 0.24 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.24 0.24 0.26
## [1233] 0.32 0.36 0.38 0.40 0.40 0.38 0.36 0.34 0.34 0.34 0.34 0.34 0.32 0.32
## [1247] 0.32 0.32 0.32 0.32 0.34 0.34 0.36 0.34 0.42 0.52 0.54 0.54 0.56 0.46
## [1261] 0.32 0.32 0.32 0.30 0.30 0.28 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22
## [1275] 0.22 0.22 0.24 0.26 0.30 0.30 0.32 0.34 0.34 0.36 0.36 0.36 0.34 0.32
## [1289] 0.30 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.24 0.24 0.24 0.26 0.28 0.30
## [1303] 0.36 0.40 0.42 0.44 0.46 0.48 0.42 0.40 0.40 0.40 0.38 0.38 0.36 0.36
## [1317] 0.34 0.32 0.34 0.34 0.36 0.34 0.42 0.52 0.56 0.56 0.46 0.42 0.42 0.42
## [1331] 0.40 0.46 0.44 0.44 0.38 0.34 0.32 0.30 0.26 0.24 0.22 0.22 0.20 0.20
## [1345] 0.20 0.20 0.22 0.24 0.28 0.30 0.32 0.32 0.34 0.34 0.34 0.32 0.30 0.30
## [1359] 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.20 0.22 0.22 0.22 0.24 0.28 0.32
## [1373] 0.34 0.40 0.50 0.52 0.54 0.54 0.50 0.46 0.40 0.36 0.34 0.30 0.26 0.24
## [1387] 0.24 0.20 0.20 0.16 0.14 0.14 0.12 0.14 0.16 0.18 0.20 0.22 0.22 0.24
## [1401] 0.24 0.26 0.26 0.24 0.20 0.20 0.18 0.20 0.18 0.20 0.18 0.18 0.18 0.18
## [1415] 0.16 0.16 0.16 0.18 0.22 0.24 0.28 0.32 0.34 0.36 0.36 0.36 0.36 0.34
## [1429] 0.32 0.30 0.30 0.30 0.30 0.28 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.30
## [1443] 0.32 0.34 0.40 0.44 0.46 0.48 0.46 0.48 0.48 0.48 0.46 0.44 0.44 0.42
## [1457] 0.44 0.42 0.42 0.40 0.42 0.42 0.42 0.42 0.40 0.42 0.42 0.42 0.46 0.46
## [1471] 0.44 0.44 0.36 0.34 0.32 0.30 0.28 0.24 0.22 0.22 0.20 0.20 0.20 0.20
## [1485] 0.20 0.20 0.20 0.20 0.22 0.24 0.26 0.30 0.32 0.32 0.34 0.34 0.34 0.32
## [1499] 0.30 0.30 0.28 0.26 0.28 0.26 0.24 0.24 0.24 0.22 0.20 0.20 0.18 0.22
## [1513] 0.26 0.30 0.36 0.36 0.38 0.38 0.36 0.38 0.36 0.34 0.34 0.32 0.30 0.30
## [1527] 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24 0.24 0.26 0.30 0.32 0.32
## [1541] 0.32 0.34 0.36 0.34 0.34 0.34 0.34 0.32 0.32 0.32 0.34 0.34 0.34 0.34
## [1555] 0.36 0.36 0.38 0.38 0.40 0.40 0.40 0.42 0.42 0.44 0.44 0.42 0.44 0.44
## [1569] 0.44 0.36 0.36 0.34 0.34 0.34 0.34 0.34 0.32 0.30 0.26 0.26 0.28 0.30
## [1583] 0.32 0.32 0.34 0.36 0.36 0.34 0.34 0.34 0.32 0.30 0.30 0.30 0.30 0.30
## [1597] 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.24 0.26 0.28 0.32 0.34 0.34 0.36
## [1611] 0.40 0.42 0.46 0.46 0.42 0.42 0.40 0.38 0.36 0.38 0.38 0.36 0.34 0.34
## [1625] 0.36 0.34 0.36 0.40 0.40 0.42 0.44 0.46 0.46 0.46 0.48 0.46 0.44 0.40
## [1639] 0.36 0.32 0.30 0.30 0.26 0.26 0.26 0.26 0.26 0.24 0.26 0.28 0.30 0.32
## [1653] 0.34 0.36 0.38 0.38 0.38 0.38 0.40 0.38 0.36 0.36 0.34 0.34 0.32 0.32
## [1667] 0.32 0.30 0.30 0.24 0.24 0.22 0.24 0.26 0.30 0.32 0.34 0.36 0.36 0.38
## [1681] 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.30
## [1695] 0.30 0.30 0.32 0.32 0.36 0.36 0.40 0.40 0.40 0.40 0.40 0.44 0.44 0.44
## [1709] 0.42 0.42 0.40 0.40 0.38 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.36 0.40
## [1723] 0.44 0.44 0.50 0.52 0.50 0.52 0.50 0.50 0.46 0.44 0.42 0.42 0.40 0.42
## [1737] 0.42 0.40 0.40 0.36 0.38 0.40 0.40 0.42 0.46 0.52 0.54 0.56 0.64 0.66
## [1751] 0.68 0.68 0.70 0.68 0.66 0.62 0.62 0.62 0.60 0.60 0.58 0.56 0.54 0.52
## [1765] 0.52 0.44 0.40 0.42 0.42 0.44 0.46 0.46 0.50 0.50 0.50 0.50 0.48 0.46
## [1779] 0.44 0.42 0.40 0.40 0.38 0.34 0.32 0.30 0.28 0.26 0.26 0.26 0.24 0.28
## [1793] 0.30 0.32 0.34 0.36 0.38 0.40 0.40 0.42 0.40 0.38 0.36 0.36 0.34 0.34
## [1807] 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.30 0.30 0.34 0.38 0.42 0.44 0.50
## [1821] 0.54 0.56 0.54 0.54 0.52 0.58 0.56 0.46 0.46 0.46 0.46 0.42 0.44 0.44
## [1835] 0.42 0.40 0.40 0.40 0.40 0.40 0.44 0.44 0.46 0.50 0.50 0.50 0.50 0.50
## [1849] 0.48 0.44 0.44 0.42 0.40 0.40 0.36 0.34 0.34 0.34 0.32 0.34 0.32 0.32
## [1863] 0.32 0.34 0.34 0.34 0.34 0.36 0.38 0.40 0.40 0.38 0.38 0.36 0.32 0.32
## [1877] 0.32 0.32 0.30 0.30 0.28 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.30 0.30
## [1891] 0.32 0.30 0.30 0.30 0.30 0.30 0.30 0.28 0.26 0.26 0.24 0.24 0.20 0.20
## [1905] 0.20 0.18 0.18 0.18 0.20 0.22 0.24 0.26 0.30 0.32 0.32 0.36 0.34 0.34
## [1919] 0.32 0.30 0.30 0.30 0.30 0.28 0.26 0.26 0.26 0.22 0.20 0.22 0.20 0.18
## [1933] 0.20 0.22 0.24 0.26 0.28 0.30 0.32 0.34 0.34 0.34 0.32 0.32 0.28 0.28
## [1947] 0.26 0.28 0.26 0.26 0.24 0.22 0.20 0.18 0.16 0.16 0.20 0.22 0.22 0.24
## [1961] 0.26 0.30 0.32 0.32 0.34 0.32 0.30 0.30 0.28 0.26 0.26 0.26 0.22 0.22
## [1975] 0.22 0.22 0.18 0.18 0.20 0.20 0.22 0.24 0.26 0.26 0.30 0.32 0.32 0.34
## [1989] 0.34 0.32 0.30 0.32 0.32 0.30 0.28 0.26 0.24 0.24 0.24 0.20 0.22 0.22
## [2003] 0.22 0.24 0.28 0.30 0.34 0.34 0.34 0.36 0.38 0.38 0.40 0.36 0.34 0.36
## [2017] 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30 0.32 0.32 0.32 0.34
## [2031] 0.34 0.34 0.36 0.36 0.28 0.28 0.26 0.26 0.26 0.24 0.24 0.24 0.24 0.24
## [2045] 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.24 0.26 0.26 0.28 0.28 0.30 0.30
## [2059] 0.30 0.30 0.30 0.30 0.30 0.28 0.28 0.28 0.26 0.26 0.26 0.26 0.24 0.24
## [2073] 0.24 0.24 0.24 0.26 0.32 0.32 0.32 0.34 0.36 0.36 0.34 0.34 0.34 0.34
## [2087] 0.34 0.32 0.32 0.30 0.30 0.30 0.26 0.24 0.24 0.24 0.24 0.26 0.26 0.30
## [2101] 0.34 0.36 0.40 0.32 0.34 0.32 0.34 0.38 0.38 0.38 0.36 0.34 0.32 0.32
## [2115] 0.32 0.30 0.30 0.26 0.30 0.28 0.28 0.28 0.32 0.34 0.36 0.40 0.42 0.44
## [2129] 0.44 0.46 0.46 0.46 0.46 0.46 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.38
## [2143] 0.38 0.38 0.40 0.42 0.44 0.46 0.50 0.54 0.60 0.64 0.68 0.74 0.76 0.76
## [2157] 0.74 0.72 0.70 0.70 0.70 0.68 0.64 0.62 0.62 0.54 0.54 0.50 0.46 0.48
## [2171] 0.48 0.38 0.36 0.34 0.32 0.34 0.36 0.36 0.40 0.38 0.42 0.38 0.36 0.34
## [2185] 0.34 0.32 0.30 0.30 0.26 0.24 0.26 0.24 0.24 0.24 0.26 0.32 0.36 0.40
## [2199] 0.42 0.44 0.46 0.50 0.52 0.54 0.52 0.52 0.50 0.46 0.46 0.46 0.46 0.46
## [2213] 0.42 0.42 0.36 0.34 0.34 0.32 0.34 0.36 0.40 0.42 0.46 0.46 0.52 0.56
## [2227] 0.60 0.60 0.52 0.48 0.46 0.44 0.44 0.40 0.38 0.36 0.34 0.34 0.34 0.34
## [2241] 0.32 0.34 0.34 0.34 0.36 0.36 0.40 0.38 0.36 0.34 0.34 0.32 0.32 0.32
## [2255] 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.30 0.30 0.30 0.30 0.30 0.32
## [2269] 0.34 0.34 0.36 0.36 0.36 0.36 0.36 0.38 0.38 0.38 0.38 0.38 0.36 0.36
## [2283] 0.38 0.38 0.38 0.38 0.38 0.36 0.36 0.36 0.36 0.38 0.38 0.40 0.40 0.42
## [2297] 0.46 0.50 0.50 0.52 0.52 0.50 0.50 0.46 0.44 0.44 0.46 0.48 0.46 0.46
## [2311] 0.46 0.46 0.46 0.50 0.52 0.56 0.56 0.60 0.60 0.64 0.72 0.74 0.74 0.74
## [2325] 0.72 0.72 0.68 0.66 0.64 0.58 0.62 0.62 0.60 0.58 0.56 0.54 0.54 0.54
## [2339] 0.48 0.46 0.50 0.52 0.56 0.54 0.50 0.48 0.48 0.44 0.44 0.42 0.42 0.42
## [2353] 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.38 0.38 0.40 0.42
## [2367] 0.42 0.44 0.42 0.44 0.46 0.46 0.44 0.44 0.44 0.42 0.42 0.40 0.38 0.38
## [2381] 0.36 0.34 0.34 0.34 0.34 0.38 0.42 0.46 0.50 0.52 0.54 0.56 0.56 0.60
## [2395] 0.60 0.60 0.56 0.54 0.50 0.46 0.48 0.46 0.44 0.44 0.40 0.40 0.38 0.36
## [2409] 0.36 0.40 0.44 0.50 0.50 0.52 0.52 0.54 0.54 0.54 0.52 0.50 0.46 0.42
## [2423] 0.40 0.40 0.38 0.36 0.36 0.36 0.36 0.36 0.36 0.38 0.40 0.40 0.40 0.40
## [2437] 0.42 0.42 0.46 0.46 0.52 0.52 0.50 0.50 0.50 0.52 0.44 0.44 0.42 0.44
## [2451] 0.44 0.42 0.40 0.40 0.36 0.36 0.36 0.36 0.38 0.40 0.42 0.46 0.46 0.50
## [2465] 0.52 0.54 0.54 0.56 0.56 0.56 0.52 0.50 0.50 0.44 0.46 0.46 0.42 0.42
## [2479] 0.40 0.40 0.40 0.46 0.46 0.50 0.52 0.54 0.56 0.56 0.58 0.60 0.60 0.58
## [2493] 0.64 0.56 0.60 0.56 0.52 0.50 0.50 0.46 0.46 0.48 0.46 0.46 0.48 0.52
## [2507] 0.50 0.52 0.50 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.54 0.50 0.50 0.50
## [2521] 0.48 0.46 0.44 0.42 0.42 0.42 0.40 0.40 0.42 0.44 0.62 0.66 0.62 0.62
## [2535] 0.70 0.70 0.74 0.76 0.76 0.74 0.74 0.70 0.68 0.66 0.62 0.60 0.56 0.52
## [2549] 0.50 0.46 0.44 0.42 0.40 0.42 0.40 0.42 0.42 0.44 0.46 0.48 0.50 0.52
## [2563] 0.52 0.50 0.50 0.46 0.44 0.42 0.42 0.40 0.36 0.36 0.36 0.36 0.34 0.34
## [2577] 0.34 0.34 0.34 0.34 0.36 0.34 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.32
## [2591] 0.32 0.30 0.30 0.32 0.32 0.32 0.32 0.32 0.34 0.34 0.34 0.34 0.36 0.38
## [2605] 0.42 0.46 0.52 0.52 0.58 0.60 0.60 0.60 0.58 0.56 0.54 0.56 0.58 0.54
## [2619] 0.52 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.52 0.56 0.60 0.66 0.68 0.70
## [2633] 0.70 0.66 0.74 0.66 0.64 0.60 0.60 0.54 0.54 0.54 0.52 0.54 0.54 0.50
## [2647] 0.52 0.46 0.50 0.52 0.56 0.60 0.64 0.64 0.66 0.70 0.72 0.74 0.70 0.70
## [2661] 0.68 0.66 0.66 0.62 0.60 0.58 0.62 0.62 0.56 0.54 0.56 0.54 0.56 0.58
## [2675] 0.58 0.64 0.66 0.68 0.70 0.74 0.72 0.70 0.70 0.68 0.68 0.64 0.64 0.62
## [2689] 0.60 0.60 0.60 0.60 0.58 0.58 0.56 0.56 0.56 0.58 0.58 0.60 0.62 0.64
## [2703] 0.66 0.64 0.68 0.70 0.70 0.66 0.66 0.62 0.64 0.62 0.62 0.62 0.64 0.62
## [2717] 0.62 0.64 0.62 0.62 0.64 0.64 0.66 0.62 0.62 0.62 0.62 0.62 0.62 0.66
## [2731] 0.62 0.64 0.64 0.62 0.58 0.56 0.54 0.54 0.52 0.50 0.46 0.46 0.46 0.46
## [2745] 0.50 0.52 0.54 0.54 0.56 0.60 0.56 0.56 0.60 0.56 0.54 0.52 0.52 0.46
## [2759] 0.48 0.46 0.42 0.44 0.44 0.44 0.44 0.42 0.42 0.42 0.40 0.40 0.40 0.44
## [2773] 0.48 0.50 0.52 0.54 0.54 0.56 0.58 0.56 0.54 0.54 0.44 0.44 0.44 0.44
## [2787] 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.42 0.44 0.46 0.48 0.46 0.48 0.50
## [2801] 0.50 0.50 0.50 0.48 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.44 0.44
## [2815] 0.44 0.44 0.44 0.46 0.48 0.50 0.54 0.58 0.62 0.62 0.64 0.66 0.66 0.66
## [2829] 0.64 0.62 0.62 0.60 0.60 0.56 0.56 0.56 0.56 0.54 0.52 0.52 0.52 0.54
## [2843] 0.56 0.60 0.64 0.66 0.68 0.70 0.70 0.70 0.72 0.70 0.70 0.68 0.66 0.64
## [2857] 0.58 0.56 0.52 0.50 0.50 0.42 0.38 0.36 0.34 0.34 0.34 0.34 0.34 0.40
## [2871] 0.44 0.46 0.50 0.48 0.50 0.40 0.42 0.42 0.40 0.40 0.38 0.36 0.36 0.34
## [2885] 0.34 0.34 0.34 0.34 0.34 0.38 0.42 0.46 0.50 0.52 0.52 0.54 0.54 0.56
## [2899] 0.58 0.56 0.56 0.54 0.50 0.50 0.48 0.46 0.44 0.40 0.38 0.36 0.36 0.34
## [2913] 0.36 0.40 0.42 0.46 0.54 0.54 0.56 0.58 0.60 0.60 0.60 0.58 0.54 0.54
## [2927] 0.52 0.48 0.46 0.44 0.42 0.42 0.42 0.42 0.40 0.46 0.42 0.48 0.52 0.54
## [2941] 0.56 0.56 0.60 0.60 0.60 0.60 0.60 0.58 0.58 0.56 0.56 0.54 0.54 0.50
## [2955] 0.50 0.52 0.48 0.46 0.42 0.44 0.44 0.46 0.52 0.56 0.58 0.58 0.60 0.60
## [2969] 0.60 0.60 0.60 0.58 0.58 0.56 0.52 0.52 0.50 0.46 0.46 0.44 0.44 0.46
## [2983] 0.42 0.42 0.44 0.48 0.52 0.54 0.56 0.60 0.60 0.62 0.62 0.62 0.64 0.62
## [2997] 0.62 0.58 0.54 0.52 0.52 0.50 0.48 0.46 0.44 0.44 0.42 0.40 0.42 0.44
## [3011] 0.50 0.52 0.56 0.56 0.60 0.62 0.62 0.64 0.66 0.64 0.64 0.60 0.54 0.54
## [3025] 0.52 0.52 0.52 0.50 0.52 0.50 0.48 0.46 0.46 0.48 0.48 0.52 0.54 0.56
## [3039] 0.60 0.62 0.62 0.64 0.66 0.64 0.62 0.56 0.54 0.54 0.50 0.46 0.46 0.46
## [3053] 0.44 0.44 0.42 0.42 0.44 0.46 0.48 0.50 0.54 0.58 0.58 0.62 0.62 0.64
## [3067] 0.64 0.64 0.62 0.60 0.60 0.56 0.54 0.54 0.52 0.52 0.50 0.50 0.50 0.50
## [3081] 0.50 0.50 0.50 0.50 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52
## [3095] 0.52 0.52 0.52 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.50 0.48 0.50 0.52
## [3109] 0.52 0.52 0.52 0.52 0.54 0.54 0.54 0.56 0.56 0.54 0.54 0.54 0.54 0.52
## [3123] 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.54 0.58 0.58 0.60 0.62 0.62
## [3137] 0.64 0.66 0.64 0.56 0.56 0.56 0.54 0.54 0.56 0.54 0.52 0.52 0.50 0.50
## [3151] 0.50 0.50 0.52 0.52 0.56 0.60 0.62 0.64 0.66 0.68 0.68 0.72 0.60 0.58
## [3165] 0.58 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.56 0.54 0.54 0.52 0.52 0.52
## [3179] 0.52 0.54 0.54 0.56 0.56 0.56 0.62 0.62 0.62 0.62 0.60 0.60 0.58 0.54
## [3193] 0.54 0.54 0.54 0.54 0.52 0.52 0.52 0.52 0.52 0.54 0.56 0.56 0.54 0.54
## [3207] 0.56 0.56 0.58 0.60 0.60 0.60 0.60 0.56 0.56 0.52 0.52 0.52 0.52 0.50
## [3221] 0.50 0.48 0.48 0.48 0.50 0.50 0.52 0.54 0.54 0.54 0.58 0.58 0.54 0.56
## [3235] 0.58 0.56 0.60 0.58 0.54 0.54 0.50 0.48 0.46 0.46 0.44 0.44 0.44 0.44
## [3249] 0.46 0.50 0.54 0.54 0.56 0.58 0.60 0.60 0.62 0.60 0.60 0.62 0.60 0.58
## [3263] 0.58 0.56 0.54 0.52 0.52 0.52 0.52 0.48 0.46 0.46 0.50 0.54 0.56 0.60
## [3277] 0.62 0.64 0.66 0.70 0.72 0.72 0.72 0.72 0.70 0.68 0.62 0.62 0.60 0.58
## [3291] 0.54 0.52 0.52 0.50 0.50 0.50 0.52 0.54 0.60 0.62 0.64 0.70 0.72 0.66
## [3305] 0.62 0.66 0.68 0.70 0.66 0.66 0.64 0.62 0.60 0.58 0.56 0.56 0.56 0.54
## [3319] 0.54 0.54 0.54 0.56 0.60 0.60 0.66 0.68 0.68 0.74 0.72 0.72 0.72 0.72
## [3333] 0.70 0.70 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.58 0.60 0.58 0.58 0.64
## [3347] 0.62 0.64 0.70 0.74 0.76 0.78 0.78 0.74 0.66 0.70 0.70 0.66 0.66 0.64
## [3361] 0.62 0.66 0.60 0.58 0.56 0.54 0.54 0.56 0.56 0.62 0.64 0.68 0.70 0.74
## [3375] 0.74 0.74 0.74 0.76 0.74 0.74 0.72 0.70 0.70 0.66 0.66 0.64 0.64 0.64
## [3389] 0.62 0.60 0.60 0.60 0.60 0.62 0.66 0.72 0.70 0.74 0.78 0.82 0.82 0.82
## [3403] 0.80 0.80 0.80 0.78 0.72 0.72 0.70 0.70 0.68 0.70 0.68 0.66 0.64 0.64
## [3417] 0.64 0.64 0.66 0.70 0.72 0.74 0.76 0.74 0.76 0.78 0.76 0.74 0.72 0.60
## [3431] 0.62 0.60 0.60 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.58 0.60 0.62 0.64
## [3445] 0.70 0.70 0.72 0.72 0.74 0.74 0.76 0.74 0.72 0.70 0.70 0.66 0.66 0.64
## [3459] 0.64 0.62 0.62 0.62 0.60 0.60 0.62 0.62 0.62 0.62 0.66 0.66 0.70 0.72
## [3473] 0.74 0.74 0.74 0.74 0.72 0.72 0.70 0.68 0.66 0.66 0.64 0.64 0.64 0.64
## [3487] 0.62 0.62 0.64 0.64 0.66 0.72 0.80 0.82 0.86 0.86 0.88 0.88 0.88 0.86
## [3501] 0.86 0.52 0.76 0.74 0.72 0.70 0.70 0.68 0.66 0.64 0.66 0.66 0.68 0.74
## [3515] 0.78 0.80 0.82 0.86 0.86 0.90 0.90 0.90 0.90 0.84 0.84 0.78 0.78 0.76
## [3529] 0.74 0.72 0.70 0.70 0.70 0.68 0.68 0.66 0.68 0.70 0.72 0.74 0.76 0.82
## [3543] 0.86 0.90 0.90 0.90 0.86 0.86 0.82 0.74 0.74 0.74 0.74 0.74 0.74 0.72
## [3557] 0.70 0.66 0.66 0.66 0.66 0.70 0.70 0.72 0.72 0.74 0.76 0.80 0.78 0.80
## [3571] 0.80 0.76 0.74 0.72 0.70 0.66 0.64 0.62 0.62 0.60 0.56 0.54 0.52 0.52
## [3585] 0.54 0.54 0.56 0.58 0.62 0.64 0.66 0.68 0.70 0.70 0.72 0.72 0.70 0.68
## [3599] 0.64 0.64 0.62 0.58 0.56 0.54 0.54 0.52 0.52 0.50 0.54 0.56 0.60 0.62
## [3613] 0.64 0.66 0.70 0.74 0.74 0.74 0.72 0.72 0.74 0.70 0.70 0.66 0.64 0.64
## [3627] 0.64 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.64 0.64 0.66
## [3641] 0.70 0.70 0.72 0.74 0.70 0.68 0.66 0.64 0.64 0.62 0.62 0.60 0.58 0.58
## [3655] 0.56 0.56 0.58 0.62 0.64 0.70 0.72 0.74 0.76 0.78 0.78 0.80 0.76 0.78
## [3669] 0.76 0.72 0.68 0.66 0.66 0.64 0.64 0.62 0.60 0.60 0.58 0.58 0.60 0.64
## [3683] 0.68 0.72 0.76 0.76 0.80 0.80 0.80 0.82 0.80 0.80 0.78 0.76 0.74 0.72
## [3697] 0.70 0.68 0.66 0.66 0.64 0.64 0.62 0.62 0.64 0.66 0.76 0.76 0.82 0.84
## [3711] 0.88 0.90 0.92 0.92 0.92 0.92 0.90 0.82 0.80 0.80 0.76 0.76 0.74 0.74
## [3725] 0.72 0.72 0.72 0.70 0.72 0.72 0.76 0.84 0.86 0.90 0.92 0.90 0.92 0.94
## [3739] 0.92 0.90 0.88 0.84 0.80 0.76 0.74 0.74 0.70 0.70 0.68 0.68 0.66 0.66
## [3753] 0.66 0.72 0.74 0.76 0.78 0.82 0.84 0.84 0.86 0.84 0.82 0.82 0.80 0.78
## [3767] 0.76 0.76 0.72 0.72 0.70 0.70 0.70 0.68 0.68 0.68 0.70 0.72 0.74 0.74
## [3781] 0.74 0.76 0.80 0.80 0.82 0.82 0.80 0.74 0.72 0.70 0.68 0.66 0.66 0.66
## [3795] 0.66 0.64 0.64 0.64 0.62 0.62 0.62 0.64 0.70 0.72 0.76 0.78 0.82 0.78
## [3809] 0.82 0.80 0.68 0.68 0.70 0.70 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.60
## [3823] 0.56 0.54 0.54 0.56 0.58 0.62 0.64 0.66 0.66 0.70 0.70 0.70 0.70 0.70
## [3837] 0.70 0.66 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.58 0.54 0.54 0.54 0.56
## [3851] 0.58 0.62 0.62 0.64 0.64 0.64 0.64 0.64 0.68 0.64 0.62 0.64 0.60 0.60
## [3865] 0.58 0.56 0.56 0.54 0.52 0.50 0.50 0.48 0.50 0.54 0.58 0.60 0.64 0.68
## [3879] 0.70 0.74 0.74 0.76 0.76 0.74 0.72 0.70 0.66 0.64 0.62 0.62 0.60 0.58
## [3893] 0.60 0.56 0.56 0.60 0.58 0.56 0.60 0.62 0.66 0.68 0.72 0.72 0.72 0.70
## [3907] 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.56 0.56 0.56 0.56 0.54 0.54
## [3921] 0.56 0.60 0.60 0.70 0.70 0.68 0.70 0.74 0.76 0.78 0.76 0.76 0.76 0.64
## [3935] 0.64 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.62 0.64 0.66 0.70
## [3949] 0.72 0.72 0.74 0.76 0.80 0.82 0.76 0.76 0.74 0.74 0.72 0.72 0.72 0.72
## [3963] 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.66 0.68 0.68 0.70 0.72 0.74 0.74
## [3977] 0.74 0.74 0.76 0.74 0.74 0.72 0.70 0.68 0.66 0.66 0.66 0.64 0.64 0.64
## [3991] 0.62 0.62 0.60 0.56 0.56 0.60 0.60 0.60 0.62 0.64 0.66 0.66 0.70 0.70
## [4005] 0.70 0.66 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.60 0.60 0.60 0.60
## [4019] 0.62 0.62 0.64 0.66 0.70 0.74 0.76 0.78 0.80 0.78 0.76 0.74 0.74 0.72
## [4033] 0.70 0.70 0.66 0.66 0.66 0.66 0.64 0.64 0.66 0.70 0.72 0.72 0.74 0.74
## [4047] 0.80 0.82 0.82 0.82 0.82 0.80 0.80 0.80 0.74 0.74 0.72 0.72 0.72 0.72
## [4061] 0.72 0.72 0.70 0.72 0.72 0.72 0.74 0.74 0.76 0.76 0.76 0.76 0.76 0.76
## [4075] 0.74 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.64
## [4089] 0.66 0.66 0.70 0.74 0.80 0.80 0.80 0.80 0.78 0.82 0.80 0.76 0.76 0.74
## [4103] 0.72 0.70 0.70 0.68 0.68 0.66 0.66 0.64 0.64 0.62 0.64 0.64 0.70 0.72
## [4117] 0.72 0.74 0.74 0.74 0.74 0.74 0.76 0.74 0.72 0.72 0.70 0.68 0.68 0.66
## [4131] 0.64 0.64 0.62 0.60 0.60 0.60 0.60 0.62 0.64 0.66 0.72 0.72 0.74 0.74
## [4145] 0.74 0.74 0.74 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.64 0.62 0.62 0.62
## [4159] 0.60 0.62 0.62 0.64 0.66 0.72 0.72 0.70 0.72 0.72 0.72 0.74 0.74 0.74
## [4173] 0.74 0.72 0.70 0.70 0.68 0.68 0.66 0.66 0.66 0.66 0.66 0.66 0.66 0.68
## [4187] 0.70 0.74 0.80 0.82 0.84 0.84 0.86 0.86 0.86 0.82 0.80 0.74 0.74 0.74
## [4201] 0.70 0.70 0.68 0.68 0.66 0.66 0.66 0.64 0.66 0.70 0.70 0.72 0.74 0.76
## [4215] 0.76 0.80 0.80 0.82 0.82 0.82 0.80 0.76 0.74 0.72 0.70 0.68 0.66 0.64
## [4229] 0.62 0.60 0.58 0.58 0.60 0.62 0.68 0.70 0.72 0.74 0.76 0.76 0.78 0.78
## [4243] 0.80 0.78 0.76 0.74 0.74 0.72 0.70 0.66 0.66 0.66 0.62 0.64 0.62 0.60
## [4257] 0.62 0.66 0.70 0.74 0.76 0.80 0.80 0.80 0.82 0.82 0.82 0.82 0.80 0.78
## [4271] 0.72 0.70 0.70 0.68 0.68 0.66 0.64 0.64 0.62 0.58 0.62 0.64 0.68 0.74
## [4285] 0.80 0.80 0.82 0.82 0.84 0.86 0.88 0.84 0.82 0.80 0.76 0.74 0.72 0.72
## [4299] 0.70 0.70 0.70 0.68 0.68 0.62 0.62 0.64 0.68 0.70 0.74 0.76 0.80 0.80
## [4313] 0.82 0.84 0.84 0.80 0.80 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.64
## [4327] 0.66 0.64 0.64 0.66 0.70 0.72 0.76 0.76 0.78 0.78 0.80 0.82 0.82 0.80
## [4341] 0.80 0.78 0.76 0.74 0.74 0.72 0.70 0.70 0.66 0.66 0.66 0.66 0.66 0.70
## [4355] 0.72 0.74 0.76 0.78 0.80 0.82 0.80 0.82 0.82 0.82 0.80 0.80 0.76 0.78
## [4369] 0.76 0.74 0.72 0.70 0.70 0.70 0.68 0.68 0.70 0.72 0.72 0.70 0.70 0.72
## [4383] 0.74 0.74 0.76 0.76 0.76 0.78 0.76 0.74 0.72 0.70 0.70 0.68 0.66 0.66
## [4397] 0.66 0.64 0.64 0.64 0.64 0.66 0.70 0.74 0.78 0.82 0.84 0.86 0.86 0.86
## [4411] 0.86 0.86 0.84 0.82 0.76 0.74 0.74 0.72 0.74 0.72 0.70 0.68 0.70 0.68
## [4425] 0.70 0.70 0.72 0.76 0.74 0.74 0.76 0.78 0.80 0.80 0.68 0.66 0.66 0.66
## [4439] 0.66 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.64 0.66 0.70 0.74
## [4453] 0.76 0.78 0.80 0.82 0.84 0.84 0.82 0.84 0.82 0.76 0.76 0.74 0.72 0.72
## [4467] 0.70 0.70 0.68 0.66 0.66 0.66 0.64 0.70 0.72 0.74 0.76 0.78 0.82 0.80
## [4481] 0.82 0.84 0.84 0.84 0.82 0.80 0.76 0.74 0.74 0.72 0.70 0.70 0.70 0.68
## [4495] 0.66 0.66 0.68 0.70 0.74 0.78 0.80 0.82 0.84 0.86 0.86 0.86 0.86 0.86
## [4509] 0.86 0.84 0.72 0.70 0.72 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.74 0.76
## [4523] 0.76 0.78 0.82 0.82 0.86 0.86 0.90 0.90 0.86 0.88 0.86 0.84 0.82 0.82
## [4537] 0.80 0.78 0.76 0.76 0.76 0.74 0.74 0.74 0.74 0.76 0.80 0.82 0.82 0.84
## [4551] 0.84 0.82 0.82 0.64 0.66 0.70 0.72 0.70 0.70 0.70 0.68 0.66 0.66 0.66
## [4565] 0.64 0.62 0.62 0.60 0.60 0.62 0.64 0.68 0.70 0.72 0.74 0.76 0.74 0.76
## [4579] 0.76 0.74 0.72 0.72 0.70 0.66 0.64 0.64 0.62 0.62 0.60 0.60 0.60 0.60
## [4593] 0.60 0.64 0.64 0.68 0.66 0.70 0.70 0.70 0.70 0.72 0.72 0.74 0.72 0.70
## [4607] 0.70 0.66 0.66 0.64 0.62 0.60 0.60 0.60 0.60 0.58 0.60 0.62 0.66 0.70
## [4621] 0.72 0.74 0.76 0.76 0.74 0.76 0.76 0.76 0.76 0.74 0.72 0.70 0.70 0.68
## [4635] 0.66 0.64 0.64 0.64 0.62 0.64 0.62 0.64 0.68 0.72 0.74 0.76 0.76 0.80
## [4649] 0.80 0.82 0.80 0.80 0.80 0.76 0.76 0.74 0.72 0.70 0.70 0.70 0.66 0.66
## [4663] 0.64 0.64 0.64 0.68 0.70 0.74 0.76 0.80 0.80 0.82 0.82 0.84 0.84 0.84
## [4677] 0.82 0.80 0.78 0.74 0.76 0.74 0.74 0.74 0.72 0.72 0.72 0.70 0.72 0.74
## [4691] 0.76 0.80 0.82 0.82 0.84 0.86 0.86 0.88 0.90 0.80 0.80 0.76 0.74 0.74
## [4705] 0.74 0.72 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.70 0.74 0.74 0.76 0.80
## [4719] 0.82 0.84 0.86 0.86 0.84 0.84 0.84 0.82 0.80 0.80 0.78 0.76 0.76 0.74
## [4733] 0.74 0.74 0.72 0.72 0.72 0.74 0.76 0.80 0.82 0.86 0.88 0.88 0.90 0.90
## [4747] 0.92 0.92 0.90 0.86 0.84 0.82 0.82 0.80 0.82 0.80 0.78 0.78 0.76 0.74
## [4761] 0.76 0.80 0.84 0.86 0.90 0.90 0.94 0.94 0.96 0.94 0.90 0.88 0.90 0.86
## [4775] 0.84 0.82 0.84 0.80 0.82 0.82 0.82 0.78 0.76 0.76 0.80 0.80 0.84 0.84
## [4789] 0.86 0.90 0.92 0.94 0.92 0.94 0.94 0.94 0.92 0.82 0.82 0.82 0.80 0.80
## [4803] 0.80 0.78 0.80 0.80 0.78 0.78 0.80 0.80 0.82 0.82 0.86 0.84 0.90 0.86
## [4817] 0.86 0.90 0.90 0.90 0.88 0.86 0.84 0.80 0.78 0.76 0.76 0.76 0.74 0.74
## [4831] 0.72 0.72 0.72 0.74 0.76 0.80 0.82 0.84 0.84 0.74 0.72 0.70 0.70 0.72
## [4845] 0.74 0.74 0.72 0.70 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.66 0.68 0.72
## [4859] 0.74 0.76 0.80 0.82 0.84 0.84 0.86 0.88 0.86 0.86 0.84 0.82 0.80 0.78
## [4873] 0.76 0.76 0.78 0.78 0.76 0.72 0.72 0.70 0.70 0.72 0.74 0.76 0.78 0.80
## [4887] 0.82 0.84 0.84 0.86 0.86 0.84 0.82 0.80 0.76 0.74 0.72 0.74 0.70 0.72
## [4901] 0.72 0.72 0.70 0.70 0.70 0.72 0.74 0.78 0.80 0.84 0.84 0.86 0.84 0.86
## [4915] 0.86 0.84 0.84 0.82 0.80 0.78 0.76 0.76 0.74 0.74 0.74 0.74 0.72 0.72
## [4929] 0.72 0.74 0.76 0.86 0.90 0.92 0.96 0.94 0.96 0.96 0.96 0.96 0.92 0.90
## [4943] 0.86 0.82 0.80 0.78 0.76 0.76 0.76 0.74 0.72 0.72 0.72 0.76 0.78 0.82
## [4957] 0.82 0.84 0.84 0.88 0.90 0.90 0.90 0.90 0.88 0.74 0.82 0.80 0.78 0.76
## [4971] 0.76 0.74 0.74 0.74 0.72 0.72 0.74 0.74 0.76 0.80 0.84 0.86 0.90 0.90
## [4985] 0.90 0.92 0.92 0.92 0.86 0.80 0.80 0.78 0.74 0.74 0.72 0.72 0.70 0.70
## [4999] 0.66 0.66 0.66 0.74 0.80 0.82 0.86 0.88 0.90 0.90 0.92 0.90 0.90 0.76
## [5013] 0.78 0.74 0.72 0.70 0.70 0.68 0.66 0.66 0.68 0.66 0.66 0.66 0.68 0.72
## [5027] 0.74 0.78 0.82 0.84 0.86 0.86 0.90 0.90 0.90 0.90 0.86 0.86 0.82 0.80
## [5041] 0.78 0.80 0.80 0.78 0.78 0.76 0.76 0.76 0.72 0.74 0.74 0.74 0.74 0.74
## [5055] 0.76 0.76 0.76 0.70 0.68 0.70 0.70 0.70 0.70 0.68 0.68 0.68 0.68 0.66
## [5069] 0.66 0.66 0.68 0.68 0.68 0.68 0.70 0.72 0.72 0.74 0.76 0.76 0.80 0.80
## [5083] 0.76 0.76 0.70 0.70 0.70 0.70 0.68 0.66 0.66 0.64 0.66 0.64 0.64 0.64
## [5097] 0.64 0.66 0.70 0.72 0.74 0.76 0.76 0.78 0.78 0.76 0.80 0.78 0.76 0.74
## [5111] 0.72 0.70 0.70 0.68 0.66 0.66 0.66 0.66 0.66 0.64 0.64 0.68 0.68 0.74
## [5125] 0.74 0.78 0.80 0.80 0.82 0.84 0.74 0.74 0.72 0.72 0.72 0.70 0.70 0.70
## [5139] 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.72 0.76 0.80 0.82 0.90 0.90
## [5153] 0.86 0.72 0.72 0.74 0.72 0.72 0.72 0.72 0.70 0.70 0.70 0.68 0.66 0.66
## [5167] 0.66 0.70 0.70 0.72 0.74 0.76 0.80 0.82 0.82 0.84 0.82 0.84 0.86 0.86
## [5181] 0.84 0.82 0.80 0.76 0.76 0.74 0.72 0.72 0.72 0.72 0.70 0.70 0.72 0.72
## [5195] 0.74 0.78 0.80 0.80 0.82 0.84 0.86 0.86 0.86 0.80 0.80 0.80 0.80 0.78
## [5209] 0.78 0.76 0.74 0.72 0.72 0.70 0.70 0.68 0.68 0.70 0.74 0.76 0.80 0.80
## [5223] 0.82 0.82 0.84 0.86 0.86 0.84 0.82 0.80 0.76 0.76 0.74 0.74 0.70 0.70
## [5237] 0.66 0.64 0.64 0.64 0.62 0.66 0.70 0.72 0.74 0.76 0.78 0.76 0.80 0.80
## [5251] 0.80 0.80 0.78 0.74 0.72 0.70 0.70 0.66 0.64 0.64 0.62 0.62 0.62 0.60
## [5265] 0.62 0.64 0.68 0.72 0.74 0.76 0.80 0.78 0.80 0.80 0.82 0.82 0.76 0.74
## [5279] 0.72 0.70 0.68 0.68 0.68 0.68 0.68 0.66 0.64 0.64 0.64 0.66 0.70 0.70
## [5293] 0.72 0.74 0.66 0.74 0.74 0.68 0.70 0.72 0.70 0.68 0.68 0.68 0.68 0.66
## [5307] 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.66 0.66 0.70 0.70 0.70 0.72
## [5321] 0.74 0.74 0.74 0.76 0.74 0.72 0.70 0.60 0.60 0.60 0.60 0.60 0.60 0.60
## [5335] 0.60 0.60 0.60 0.60 0.64 0.64 0.68 0.72 0.74 0.78 0.74 0.74 0.74 0.74
## [5349] 0.70 0.70 0.66 0.66 0.66 0.64 0.64 0.64 0.62 0.62 0.64 0.62 0.62 0.64
## [5363] 0.70 0.68 0.70 0.74 0.76 0.76 0.76 0.80 0.80 0.76 0.76 0.74 0.74 0.72
## [5377] 0.70 0.66 0.66 0.64 0.66 0.64 0.64 0.64 0.62 0.66 0.70 0.74 0.76 0.78
## [5391] 0.80 0.80 0.82 0.80 0.82 0.80 0.76 0.76 0.74 0.72 0.70 0.70 0.68 0.66
## [5405] 0.66 0.66 0.64 0.64 0.64 0.64 0.66 0.72 0.74 0.76 0.80 0.80 0.82 0.80
## [5419] 0.80 0.80 0.76 0.66 0.66 0.68 0.70 0.70 0.68 0.64 0.64 0.64 0.64 0.64
## [5433] 0.64 0.66 0.66 0.70 0.72 0.72 0.74 0.76 0.78 0.80 0.80 0.76 0.76 0.62
## [5447] 0.62 0.62 0.60 0.60 0.60 0.62 0.62 0.62 0.60 0.60 0.60 0.62 0.66 0.70
## [5461] 0.72 0.72 0.74 0.76 0.80 0.80 0.80 0.80 0.76 0.74 0.74 0.72 0.70 0.70
## [5475] 0.70 0.68 0.68 0.66 0.68 0.66 0.66 0.68 0.70 0.72 0.74 0.80 0.82 0.80
## [5489] 0.70 0.70 0.72 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.68 0.68 0.70 0.70
## [5503] 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.72 0.74 0.74 0.74 0.74 0.74 0.74
## [5517] 0.72 0.70 0.66 0.64 0.64 0.62 0.60 0.58 0.56 0.56 0.54 0.54 0.54 0.60
## [5531] 0.62 0.66 0.70 0.70 0.70 0.72 0.72 0.72 0.72 0.72 0.72 0.66 0.64 0.62
## [5545] 0.62 0.62 0.62 0.60 0.58 0.58 0.56 0.56 0.56 0.60 0.62 0.64 0.70 0.72
## [5559] 0.74 0.76 0.76 0.76 0.76 0.76 0.74 0.74 0.72 0.70 0.70 0.68 0.68 0.68
## [5573] 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.72 0.74 0.70 0.70 0.70 0.72 0.74
## [5587] 0.72 0.72 0.72 0.66 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.60 0.62 0.62
## [5601] 0.62 0.64 0.66 0.70 0.74 0.76 0.76 0.78 0.80 0.80 0.78 0.74 0.74 0.72
## [5615] 0.72 0.72 0.72 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70 0.70
## [5629] 0.70 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.62 0.62 0.66 0.70 0.70 0.74
## [5643] 0.76 0.78 0.78 0.80 0.76 0.74 0.72 0.72 0.66 0.64 0.62 0.62 0.60 0.60
## [5657] 0.60 0.56 0.56 0.56 0.60 0.62 0.62 0.66 0.66 0.68 0.70 0.70 0.70 0.72
## [5671] 0.70 0.66 0.66 0.64 0.64 0.62 0.60 0.56 0.56 0.54 0.54 0.52 0.52 0.54
## [5685] 0.56 0.62 0.66 0.70 0.72 0.72 0.74 0.74 0.74 0.74 0.72 0.70 0.66 0.66
## [5699] 0.64 0.62 0.62 0.60 0.60 0.56 0.56 0.56 0.54 0.54 0.60 0.62 0.64 0.70
## [5713] 0.72 0.74 0.74 0.74 0.74 0.74 0.72 0.70 0.84 0.66 0.66 0.64 0.60 0.60
## [5727] 0.60 0.58 0.58 0.56 0.56 0.60 0.60 0.62 0.64 0.68 0.72 0.72 0.72 0.72
## [5741] 0.72 0.74 0.72 0.72 0.70 0.66 0.66 0.66 0.64 0.64 0.62 0.62 0.60 0.60
## [5755] 0.60 0.60 0.60 0.62 0.62 0.64 0.66 0.66 0.68 0.70 0.70 0.70 0.68 0.68
## [5769] 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.64
## [5783] 0.66 0.66 0.66 0.70 0.70 0.72 0.72 0.72 0.72 0.72 0.70 0.70 0.68 0.68
## [5797] 0.66 0.66 0.66 0.66 0.64 0.64 0.64 0.64 0.66 0.66 0.66 0.70 0.74 0.76
## [5811] 0.78 0.78 0.78 0.80 0.76 0.76 0.74 0.74 0.72 0.72 0.72 0.70 0.68 0.68
## [5825] 0.68 0.68 0.66 0.66 0.66 0.66 0.68 0.70 0.70 0.72 0.74 0.74 0.68 0.68
## [5839] 0.66 0.66 0.66 0.66 0.66 0.60 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [5853] 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [5867] 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.56 0.60 0.60 0.62 0.60 0.60 0.60
## [5881] 0.56 0.60 0.60 0.62 0.64 0.64 0.64 0.64 0.64 0.64 0.62 0.62 0.60 0.62
## [5895] 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.64 0.64 0.66 0.68 0.70 0.66 0.64
## [5909] 0.64 0.64 0.62 0.62 0.64 0.62 0.62 0.64 0.62 0.62 0.62 0.62 0.62 0.62
## [5923] 0.62 0.62 0.62 0.62 0.62 0.64 0.70 0.72 0.74 0.74 0.70 0.70 0.66 0.64
## [5937] 0.66 0.62 0.62 0.62 0.62 0.60 0.58 0.58 0.58 0.58 0.60 0.62 0.64 0.70
## [5951] 0.72 0.72 0.74 0.74 0.74 0.74 0.74 0.72 0.70 0.66 0.64 0.64 0.62 0.62
## [5965] 0.62 0.62 0.62 0.60 0.60 0.58 0.60 0.64 0.66 0.70 0.70 0.70 0.74 0.72
## [5979] 0.74 0.72 0.72 0.68 0.64 0.62 0.64 0.62 0.58 0.56 0.56 0.56 0.54 0.56
## [5993] 0.56 0.58 0.60 0.64 0.68 0.70 0.72 0.72 0.74 0.74 0.72 0.72 0.70 0.68
## [6007] 0.66 0.64 0.62 0.62 0.60 0.58 0.60 0.58 0.56 0.56 0.56 0.58 0.60 0.64
## [6021] 0.68 0.70 0.72 0.74 0.74 0.74 0.74 0.74 0.70 0.68 0.66 0.64 0.64 0.64
## [6035] 0.62 0.62 0.60 0.60 0.60 0.58 0.58 0.60 0.62 0.64 0.70 0.72 0.74 0.76
## [6049] 0.78 0.78 0.76 0.76 0.72 0.70 0.72 0.66 0.66 0.64 0.64 0.64 0.62 0.62
## [6063] 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.68 0.66 0.64 0.64 0.60 0.54 0.48
## [6077] 0.48 0.46 0.46 0.46 0.44 0.44 0.42 0.42 0.40 0.40 0.40 0.38 0.38 0.40
## [6091] 0.42 0.46 0.50 0.50 0.52 0.54 0.54 0.54 0.52 0.52 0.52 0.50 0.50 0.50
## [6105] 0.48 0.50 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.48 0.50 0.52
## [6119] 0.52 0.52 0.52 0.52 0.54 0.52 0.52 0.52 0.52 0.50 0.50 0.46 0.46 0.46
## [6133] 0.44 0.44 0.44 0.44 0.44 0.46 0.46 0.48 0.50 0.52 0.54 0.56 0.58 0.58
## [6147] 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.52 0.52 0.52 0.50 0.50 0.50
## [6161] 0.50 0.50 0.52 0.54 0.56 0.58 0.58 0.60 0.60 0.60 0.60 0.58 0.56 0.56
## [6175] 0.56 0.56 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [6189] 0.56 0.56 0.56 0.56 0.58 0.62 0.60 0.60 0.60 0.58 0.56 0.56 0.56 0.56
## [6203] 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.56 0.60 0.62 0.64 0.66
## [6217] 0.66 0.66 0.66 0.66 0.64 0.62 0.62 0.60 0.62 0.60 0.60 0.60 0.60 0.60
## [6231] 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.66 0.66 0.68 0.68 0.66
## [6245] 0.64 0.64 0.62 0.64 0.62 0.62 0.62 0.60 0.60 0.60 0.60 0.62 0.62 0.62
## [6259] 0.62 0.62 0.62 0.62 0.62 0.60 0.60 0.62 0.62 0.60 0.60 0.62 0.60 0.60
## [6273] 0.60 0.58 0.58 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.58 0.60 0.60 0.62
## [6287] 0.62 0.64 0.66 0.66 0.64 0.66 0.64 0.62 0.62 0.62 0.62 0.60 0.60 0.60
## [6301] 0.60 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.64 0.66 0.66 0.68 0.66 0.66
## [6315] 0.70 0.68 0.68 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62
## [6329] 0.62 0.62 0.62 0.64 0.64 0.68 0.68 0.70 0.70 0.72 0.70 0.70 0.66 0.66
## [6343] 0.64 0.64 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.62 0.64 0.64
## [6357] 0.64 0.66 0.66 0.70 0.68 0.68 0.66 0.66 0.64 0.64 0.62 0.60 0.60 0.60
## [6371] 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.62 0.62 0.62 0.66 0.68 0.70
## [6385] 0.72 0.70 0.70 0.70 0.66 0.66 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60
## [6399] 0.60 0.60 0.60 0.60 0.60 0.62 0.64 0.62 0.66 0.64 0.68 0.68 0.68 0.66
## [6413] 0.64 0.62 0.60 0.58 0.56 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.52
## [6427] 0.54 0.56 0.60 0.64 0.64 0.66 0.64 0.64 0.62 0.62 0.58 0.54 0.54 0.52
## [6441] 0.52 0.52 0.50 0.48 0.46 0.46 0.44 0.44 0.42 0.42 0.40 0.40 0.40 0.38
## [6455] 0.40 0.40 0.42 0.42 0.40 0.40 0.38 0.38 0.36 0.36 0.36 0.36 0.36 0.34
## [6469] 0.34 0.34 0.34 0.32 0.32 0.34 0.34 0.36 0.36 0.38 0.40 0.40 0.36 0.36
## [6483] 0.36 0.36 0.36 0.38 0.36 0.36 0.36 0.36 0.34 0.36 0.36 0.36 0.34 0.36
## [6497] 0.36 0.36 0.36 0.40 0.40 0.40 0.40 0.42 0.40 0.40 0.40 0.40 0.40 0.40
## [6511] 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.40 0.42 0.44 0.46
## [6525] 0.48 0.54 0.56 0.56 0.58 0.58 0.58 0.56 0.54 0.52 0.52 0.50 0.50 0.48
## [6539] 0.48 0.48 0.46 0.46 0.44 0.44 0.44 0.46 0.52 0.54 0.56 0.60 0.62 0.64
## [6553] 0.64 0.64 0.64 0.64 0.60 0.58 0.52 0.50 0.52 0.50 0.50 0.48 0.46 0.44
## [6567] 0.44 0.42 0.40 0.42 0.44 0.46 0.52 0.54 0.56 0.56 0.58 0.58 0.58 0.56
## [6581] 0.54 0.52 0.50 0.46 0.46 0.44 0.44 0.44 0.42 0.40 0.40 0.42 0.42 0.42
## [6595] 0.46 0.48 0.52 0.56 0.60 0.60 0.62 0.66 0.64 0.60 0.56 0.56 0.54 0.50
## [6609] 0.50 0.50 0.48 0.46 0.46 0.44 0.42 0.42 0.42 0.42 0.46 0.50 0.52 0.58
## [6623] 0.62 0.62 0.62 0.66 0.64 0.62 0.60 0.54 0.52 0.52 0.50 0.48 0.46 0.46
## [6637] 0.46 0.44 0.44 0.44 0.44 0.44 0.46 0.50 0.56 0.62 0.64 0.66 0.68 0.68
## [6651] 0.66 0.62 0.64 0.56 0.56 0.54 0.52 0.50 0.50 0.48 0.48 0.46 0.46 0.46
## [6665] 0.44 0.46 0.52 0.54 0.56 0.62 0.70 0.72 0.74 0.72 0.70 0.66 0.64 0.58
## [6679] 0.60 0.56 0.56 0.54 0.54 0.52 0.52 0.52 0.52 0.52 0.52 0.52 0.56 0.60
## [6693] 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.60 0.58 0.58 0.58 0.56 0.56
## [6707] 0.56 0.56 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [6721] 0.54 0.54 0.56 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54 0.54
## [6735] 0.54 0.54 0.54 0.54 0.54 0.56 0.56 0.62 0.62 0.64 0.66 0.66 0.66 0.62
## [6749] 0.62 0.62 0.62 0.62 0.62 0.58 0.58 0.58 0.56 0.56 0.56 0.56 0.56 0.56
## [6763] 0.56 0.56 0.54 0.52 0.52 0.56 0.62 0.62 0.60 0.58 0.54 0.54 0.52 0.50
## [6777] 0.46 0.46 0.46 0.46 0.46 0.44 0.42 0.42 0.40 0.40 0.46 0.52 0.54 0.56
## [6791] 0.58 0.60 0.60 0.62 0.62 0.58 0.54 0.50 0.50 0.50 0.50 0.48 0.46 0.44
## [6805] 0.42 0.42 0.42 0.42 0.38 0.40 0.44 0.50 0.54 0.56 0.58 0.60 0.60 0.62
## [6819] 0.60 0.58 0.56 0.54 0.54 0.54 0.56 0.56 0.54 0.56 0.56 0.56 0.52 0.50
## [6833] 0.50 0.50 0.50 0.50 0.52 0.56 0.56 0.56 0.58 0.56 0.58 0.56 0.56 0.54
## [6847] 0.52 0.50 0.50 0.48 0.48 0.46 0.44 0.44 0.44 0.46 0.46 0.46 0.50 0.52
## [6861] 0.56 0.60 0.62 0.64 0.62 0.62 0.62 0.60 0.56 0.56 0.54 0.54 0.52 0.52
## [6875] 0.52 0.52 0.52 0.50 0.50 0.52 0.52 0.54 0.52 0.52 0.52 0.52 0.54 0.54
## [6889] 0.54 0.56 0.56 0.56 0.60 0.60 0.58 0.58 0.58 0.56 0.56 0.56 0.50 0.50
## [6903] 0.48 0.44 0.42 0.44 0.44 0.46 0.48 0.48 0.50 0.48 0.48 0.48 0.48 0.46
## [6917] 0.46 0.46 0.44 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.34 0.36 0.36 0.40
## [6931] 0.42 0.46 0.50 0.50 0.48 0.52 0.50 0.50 0.46 0.44 0.44 0.44 0.42 0.42
## [6945] 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.36 0.40 0.42 0.44 0.44 0.46
## [6959] 0.50 0.48 0.48 0.50 0.46 0.44 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.34
## [6973] 0.34 0.34 0.32 0.32 0.34 0.36 0.40 0.42 0.46 0.50 0.52 0.52 0.52 0.52
## [6987] 0.50 0.50 0.46 0.44 0.44 0.42 0.42 0.42 0.40 0.40 0.40 0.40 0.38 0.40
## [7001] 0.38 0.42 0.44 0.46 0.50 0.52 0.54 0.54 0.56 0.54 0.52 0.54 0.48 0.48
## [7015] 0.46 0.48 0.46 0.44 0.44 0.42 0.40 0.38 0.38 0.38 0.40 0.44 0.48 0.50
## [7029] 0.52 0.54 0.56 0.56 0.56 0.56 0.56 0.52 0.48 0.46 0.44 0.46 0.44 0.44
## [7043] 0.44 0.44 0.44 0.42 0.42 0.42 0.42 0.44 0.48 0.52 0.52 0.58 0.56 0.52
## [7057] 0.52 0.52 0.52 0.52 0.50 0.50 0.52 0.48 0.48 0.46 0.46 0.46 0.46 0.48
## [7071] 0.48 0.50 0.46 0.48 0.50 0.50 0.50 0.50 0.50 0.52 0.52 0.56 0.50 0.48
## [7085] 0.44 0.42 0.40 0.36 0.34 0.34 0.32 0.32 0.30 0.30 0.30 0.28 0.28 0.30
## [7099] 0.32 0.34 0.36 0.38 0.38 0.36 0.36 0.36 0.36 0.36 0.36 0.34 0.32 0.30
## [7113] 0.30 0.28 0.30 0.30 0.30 0.30 0.26 0.26 0.26 0.28 0.28 0.26 0.26 0.24
## [7127] 0.24 0.24 0.22 0.22 0.22 0.22 0.24 0.24 0.24 0.22 0.22 0.22 0.22 0.22
## [7141] 0.24 0.22 0.24 0.24 0.24 0.26 0.30 0.32 0.36 0.38 0.40 0.42 0.42 0.42
## [7155] 0.40 0.36 0.56 0.34 0.32 0.30 0.26 0.26 0.26 0.24 0.24 0.24 0.22 0.24
## [7169] 0.24 0.28 0.32 0.36 0.40 0.42 0.44 0.44 0.44 0.42 0.42 0.40 0.40 0.40
## [7183] 0.36 0.36 0.36 0.36 0.36 0.36 0.36 0.34 0.32 0.32 0.34 0.36 0.40 0.44
## [7197] 0.46 0.50 0.48 0.50 0.50 0.48 0.44 0.42 0.42 0.40 0.36 0.36 0.34 0.32
## [7211] 0.30 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.34 0.40 0.42 0.46 0.48 0.50
## [7225] 0.48 0.48 0.44 0.42 0.40 0.40 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32
## [7239] 0.32 0.34 0.32 0.34 0.36 0.40 0.44 0.50 0.52 0.52 0.52 0.52 0.48 0.46
## [7253] 0.44 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.38 0.38 0.40 0.40
## [7267] 0.42 0.42 0.44 0.48 0.44 0.44 0.46 0.46 0.42 0.42 0.40 0.36 0.34 0.34
## [7281] 0.32 0.32 0.32 0.30 0.30 0.28 0.26 0.26 0.26 0.28 0.30 0.32 0.36 0.36
## [7295] 0.40 0.42 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.30 0.28 0.28 0.26 0.26
## [7309] 0.24 0.24 0.24 0.26 0.26 0.28 0.30 0.36 0.42 0.44 0.46 0.46 0.48 0.46
## [7323] 0.44 0.42 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.30 0.28
## [7337] 0.28 0.30 0.34 0.36 0.42 0.46 0.54 0.56 0.56 0.52 0.50 0.46 0.46 0.40
## [7351] 0.38 0.36 0.36 0.34 0.32 0.32 0.32 0.30 0.30 0.30 0.30 0.32 0.36 0.42
## [7365] 0.46 0.52 0.54 0.56 0.58 0.56 0.52 0.48 0.46 0.40 0.40 0.36 0.36 0.36
## [7379] 0.34 0.32 0.32 0.32 0.30 0.30 0.30 0.32 0.34 0.40 0.46 0.50 0.50 0.52
## [7393] 0.52 0.52 0.46 0.44 0.44 0.44 0.40 0.40 0.38 0.40 0.40 0.38 0.38 0.38
## [7407] 0.36 0.36 0.38 0.40 0.42 0.44 0.46 0.42 0.36 0.36 0.36 0.36 0.36 0.36
## [7421] 0.36 0.36 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.30 0.30 0.28 0.28 0.30
## [7435] 0.32 0.32 0.34 0.34 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.32
## [7449] 0.30 0.24 0.24 0.24 0.24 0.20 0.22 0.22 0.22 0.26 0.30 0.34 0.38 0.44
## [7463] 0.48 0.50 0.52 0.52 0.50 0.42 0.42 0.42 0.42 0.42 0.40 0.40 0.36 0.36
## [7477] 0.36 0.36 0.34 0.34 0.34 0.34 0.40 0.44 0.46 0.52 0.52 0.54 0.50 0.54
## [7491] 0.52 0.52 0.50 0.50 0.48 0.48 0.46 0.46 0.46 0.46 0.44 0.44 0.44 0.44
## [7505] 0.44 0.46 0.48 0.50 0.54 0.56 0.60 0.62 0.64 0.62 0.62 0.56 0.60 0.60
## [7519] 0.60 0.58 0.56 0.56 0.56 0.56 0.54 0.56 0.54 0.56 0.54 0.54 0.56 0.56
## [7533] 0.56 0.54 0.54 0.54 0.54 0.52 0.50 0.50 0.50 0.48 0.50 0.46 0.46 0.46
## [7547] 0.46 0.46 0.46 0.44 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46 0.46
## [7561] 0.48 0.48 0.48 0.46 0.46 0.44 0.44 0.42 0.42 0.42 0.42 0.42 0.42 0.40
## [7575] 0.34 0.36 0.34 0.34 0.34 0.34 0.32 0.34 0.34 0.34 0.34 0.32 0.32 0.32
## [7589] 0.30 0.30 0.30 0.26 0.26 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.22
## [7603] 0.26 0.26 0.30 0.32 0.34 0.34 0.34 0.34 0.32 0.30 0.28 0.28 0.28 0.26
## [7617] 0.26 0.26 0.26 0.24 0.26 0.26 0.24 0.24 0.24 0.26 0.28 0.32 0.36 0.40
## [7631] 0.42 0.42 0.42 0.42 0.40 0.38 0.34 0.36 0.36 0.38 0.38 0.38 0.40 0.40
## [7645] 0.40 0.40 0.42 0.42 0.42 0.42 0.44 0.44 0.50 0.50 0.54 0.52 0.52 0.52
## [7659] 0.52 0.54 0.50 0.52 0.48 0.46 0.46 0.46 0.46 0.44 0.44 0.44 0.44 0.42
## [7673] 0.46 0.46 0.48 0.52 0.52 0.50 0.50 0.48 0.44 0.44 0.42 0.42 0.40 0.40
## [7687] 0.40 0.40 0.40 0.38 0.40 0.38 0.38 0.38 0.38 0.38 0.38 0.38 0.40 0.40
## [7701] 0.40 0.40 0.42 0.42 0.44 0.44 0.44 0.44 0.46 0.46 0.46 0.50 0.48 0.48
## [7715] 0.48 0.50 0.50 0.52 0.46 0.44 0.46 0.48 0.52 0.52 0.50 0.48 0.44 0.42
## [7729] 0.42 0.40 0.38 0.40 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.30 0.28 0.30
## [7743] 0.30 0.30 0.26 0.30 0.34 0.36 0.42 0.46 0.48 0.50 0.50 0.50 0.48 0.42
## [7757] 0.40 0.36 0.36 0.36 0.34 0.34 0.34 0.28 0.28 0.30 0.28 0.26 0.26 0.26
## [7771] 0.32 0.36 0.40 0.46 0.50 0.52 0.52 0.50 0.50 0.46 0.42 0.40 0.36 0.34
## [7785] 0.34 0.34 0.32 0.30 0.30 0.30 0.30 0.30 0.26 0.32 0.34 0.36 0.40 0.44
## [7799] 0.48 0.48 0.50 0.46 0.46 0.42 0.40 0.42 0.38 0.38 0.36 0.36 0.36 0.34
## [7813] 0.34 0.34 0.36 0.38 0.38 0.40 0.46 0.46 0.50 0.54 0.54 0.62 0.62 0.56
## [7827] 0.54 0.50 0.48 0.50 0.48 0.48 0.48 0.46 0.46 0.44 0.44 0.40 0.42 0.42
## [7841] 0.42 0.44 0.48 0.52 0.56 0.58 0.60 0.58 0.56 0.58 0.56 0.54 0.54 0.54
## [7855] 0.52 0.52 0.52 0.52 0.50 0.50 0.52 0.50 0.52 0.52 0.54 0.56 0.56 0.50
## [7869] 0.42 0.42 0.40 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.38 0.36 0.36 0.34
## [7883] 0.32 0.32 0.30 0.28 0.26 0.26 0.28 0.30 0.34 0.38 0.36 0.38 0.38 0.36
## [7897] 0.36 0.34 0.34 0.32 0.32 0.32 0.30 0.28 0.28 0.26 0.26 0.26 0.26 0.26
## [7911] 0.24 0.24 0.26 0.30 0.32 0.34 0.36 0.40 0.40 0.40 0.40 0.36 0.36 0.34
## [7925] 0.34 0.30 0.30 0.26 0.26 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.24 0.28
## [7939] 0.30 0.34 0.36 0.40 0.42 0.42 0.42 0.42 0.40 0.34 0.38 0.34 0.32 0.32
## [7953] 0.30 0.26 0.26 0.24 0.22 0.24 0.24 0.22 0.24 0.26 0.32 0.32 0.36 0.36
## [7967] 0.36 0.38 0.38 0.36 0.34 0.30 0.30 0.30 0.32 0.30 0.30 0.30 0.26 0.28
## [7981] 0.26 0.26 0.24 0.26 0.26 0.30 0.32 0.34 0.36 0.40 0.42 0.42 0.42 0.38
## [7995] 0.38 0.38 0.36 0.36 0.34 0.34 0.32 0.32 0.32 0.32 0.30 0.30 0.30 0.32
## [8009] 0.32 0.36 0.36 0.36 0.40 0.42 0.46 0.46 0.50 0.42 0.44 0.46 0.46 0.44
## [8023] 0.44 0.46 0.50 0.46 0.46 0.46 0.46 0.46 0.44 0.46 0.46 0.46 0.46 0.46
## [8037] 0.46 0.46 0.48 0.50 0.46 0.46 0.46 0.46 0.46 0.44 0.46 0.46 0.44 0.46
## [8051] 0.46 0.46 0.46 0.46 0.48 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.44 0.42
## [8065] 0.42 0.40 0.40 0.34 0.34 0.30 0.24 0.24 0.26 0.26 0.28 0.26 0.24 0.22
## [8079] 0.22 0.22 0.22 0.24 0.26 0.28 0.28 0.30 0.32 0.32 0.32 0.30 0.28 0.26
## [8093] 0.26 0.28 0.26 0.24 0.24 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.24 0.26
## [8107] 0.30 0.32 0.36 0.38 0.38 0.38 0.36 0.34 0.34 0.34 0.30 0.30 0.28 0.28
## [8121] 0.26 0.26 0.28 0.28 0.26 0.26 0.24 0.24 0.26 0.28 0.32 0.32 0.32 0.34
## [8135] 0.34 0.34 0.32 0.28 0.26 0.26 0.24 0.22 0.22 0.20 0.20 0.16 0.18 0.16
## [8149] 0.16 0.18 0.16 0.18 0.16 0.20 0.24 0.26 0.26 0.30 0.30 0.30 0.30 0.28
## [8163] 0.24 0.22 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.18 0.18 0.16 0.16 0.14
## [8177] 0.16 0.18 0.22 0.26 0.28 0.30 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.26
## [8191] 0.26 0.24 0.22 0.20 0.20 0.18 0.20 0.18 0.20 0.16 0.20 0.26 0.30 0.34
## [8205] 0.36 0.40 0.40 0.40 0.38 0.36 0.34 0.32 0.32 0.30 0.30 0.26 0.26 0.26
## [8219] 0.26 0.28 0.26 0.26 0.26 0.28 0.26 0.30 0.32 0.34 0.36 0.36 0.38 0.38
## [8233] 0.38 0.36 0.36 0.36 0.34 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.32
## [8247] 0.32 0.32 0.34 0.36 0.40 0.40 0.46 0.50 0.52 0.52 0.46 0.52 0.52 0.52
## [8261] 0.52 0.50 0.52 0.52 0.50 0.50 0.48 0.46 0.50 0.48 0.44 0.38 0.36 0.36
## [8275] 0.34 0.34 0.34 0.34 0.34 0.32 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30
## [8289] 0.30 0.30 0.28 0.26 0.24 0.26 0.26 0.26 0.26 0.26 0.26 0.28 0.28 0.28
## [8303] 0.28 0.28 0.28 0.26 0.26 0.24 0.22 0.20 0.20 0.20 0.20 0.20 0.22 0.22
## [8317] 0.22 0.20 0.20 0.20 0.20 0.22 0.24 0.24 0.26 0.30 0.30 0.32 0.28 0.28
## [8331] 0.28 0.26 0.24 0.22 0.22 0.20 0.20 0.18 0.18 0.16 0.16 0.14 0.16 0.18
## [8345] 0.20 0.22 0.24 0.26 0.30 0.34 0.36 0.38 0.40 0.38 0.36 0.36 0.40 0.36
## [8359] 0.36 0.36 0.36 0.36 0.34 0.34 0.36 0.36 0.36 0.36 0.42 0.36 0.42 0.40
## [8373] 0.44 0.44 0.44 0.44 0.44 0.40 0.38 0.38 0.36 0.36 0.36 0.38 0.34 0.36
## [8387] 0.36 0.36 0.36 0.38 0.36 0.36 0.36 0.40 0.48 0.48 0.46 0.44 0.48 0.48
## [8401] 0.44 0.44 0.44 0.50 0.50 0.50 0.50 0.50 0.50 0.44 0.48 0.44 0.38 0.38
## [8415] 0.36 0.34 0.36 0.38 0.40 0.44 0.48 0.46 0.46 0.48 0.46 0.44 0.44 0.44
## [8429] 0.42 0.42 0.36 0.40 0.40 0.40 0.38 0.38 0.38 0.36 0.38 0.38 0.40 0.40
## [8443] 0.40 0.40 0.40 0.40 0.40 0.38 0.38 0.36 0.36 0.34 0.32 0.32 0.32 0.32
## [8457] 0.32 0.32 0.32 0.32 0.32 0.32 0.30 0.30 0.28 0.30 0.30 0.32 0.34 0.34
## [8471] 0.34 0.32 0.32 0.28 0.30 0.30 0.26 0.26 0.24 0.24 0.24 0.22 0.22 0.22
## [8485] 0.20 0.20 0.22 0.20 0.24 0.26 0.30 0.30 0.32 0.34 0.34 0.36 0.34 0.32
## [8499] 0.32 0.32 0.30 0.28 0.26 0.22 0.28 0.34 0.34 0.34 0.32 0.32 0.32 0.34
## [8513] 0.34 0.34 0.36 0.38 0.38 0.38 0.36 0.34 0.32 0.30 0.30 0.26 0.26 0.26
## [8527] 0.26 0.26 0.26 0.30 0.30 0.30 0.30 0.30 0.30 0.32 0.32 0.32 0.30 0.30
## [8541] 0.42 0.42 0.44 0.40 0.38 0.34 0.32 0.32 0.32 0.30 0.32 0.32 0.32 0.32
## [8555] 0.32 0.32 0.32 0.32 0.32 0.34 0.36 0.34 0.34 0.32 0.32 0.30 0.28 0.26
## [8569] 0.24 0.24 0.22 0.22 0.22 0.22 0.22 0.20 0.20 0.20 0.20 0.20 0.18 0.20
## [8583] 0.20 0.22 0.24 0.26 0.28 0.30 0.30 0.30 0.30 0.30 0.30 0.28 0.28 0.28
## [8597] 0.30 0.28 0.26 0.24 0.24 0.24 0.22 0.24 0.24 0.24 0.26 0.30 0.32 0.32
## [8611] 0.36 0.40 0.42 0.42 0.38 0.36 0.34 0.34 0.36 0.34 0.36 0.38 0.40 0.40
## [8625] 0.40 0.38 0.36 0.40 0.38 0.34 0.38 0.40 0.42 0.52 0.50 0.46 0.46 0.44
## [8639] 0.42 0.42 0.42 0.42 0.40 0.38 0.36</code></pre>
<div id="the-linear-model" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">The Linear Model<a href="#the-linear-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="approaches-to-variable-coding" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Approaches to variable coding<a href="#approaches-to-variable-coding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The purpose of this task is to predict the number of bikers (bikers)
using month (mnth), hour (hr), whether it’s a working day (workingday),
temperature (temp), and weather situation (weathersit).</p>
<p>We begin by fitting a least squares linear regression model to the data.</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="#cb199-1" tabindex="-1"></a>mod.lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(bikers <span class="sc">~</span> mnth <span class="sc">+</span> hr <span class="sc">+</span> workingday <span class="sc">+</span> temp <span class="sc">+</span> weathersit)</span>
<span id="cb199-2"><a href="#cb199-2" tabindex="-1"></a><span class="fu">summary</span>(mod.lm)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = bikers ~ mnth + hr + workingday + temp + weathersit)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -299.00  -45.70   -6.23   41.08  425.29 
## 
## Coefficients:
##                           Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                -68.632      5.307 -12.932  &lt; 2e-16 ***
## mnthFeb                      6.845      4.287   1.597 0.110398    
## mnthMarch                   16.551      4.301   3.848 0.000120 ***
## mnthApril                   41.425      4.972   8.331  &lt; 2e-16 ***
## mnthMay                     72.557      5.641  12.862  &lt; 2e-16 ***
## mnthJune                    67.819      6.544  10.364  &lt; 2e-16 ***
## mnthJuly                    45.324      7.081   6.401 1.63e-10 ***
## mnthAug                     53.243      6.640   8.019 1.21e-15 ***
## mnthSept                    66.678      5.925  11.254  &lt; 2e-16 ***
## mnthOct                     75.834      4.950  15.319  &lt; 2e-16 ***
## mnthNov                     60.310      4.610  13.083  &lt; 2e-16 ***
## mnthDec                     46.458      4.271  10.878  &lt; 2e-16 ***
## hr1                        -14.579      5.699  -2.558 0.010536 *  
## hr2                        -21.579      5.733  -3.764 0.000168 ***
## hr3                        -31.141      5.778  -5.389 7.26e-08 ***
## hr4                        -36.908      5.802  -6.361 2.11e-10 ***
## hr5                        -24.135      5.737  -4.207 2.61e-05 ***
## hr6                         20.600      5.704   3.612 0.000306 ***
## hr7                        120.093      5.693  21.095  &lt; 2e-16 ***
## hr8                        223.662      5.690  39.310  &lt; 2e-16 ***
## hr9                        120.582      5.693  21.182  &lt; 2e-16 ***
## hr10                        83.801      5.705  14.689  &lt; 2e-16 ***
## hr11                       105.423      5.722  18.424  &lt; 2e-16 ***
## hr12                       137.284      5.740  23.916  &lt; 2e-16 ***
## hr13                       136.036      5.760  23.617  &lt; 2e-16 ***
## hr14                       126.636      5.776  21.923  &lt; 2e-16 ***
## hr15                       132.087      5.780  22.852  &lt; 2e-16 ***
## hr16                       178.521      5.772  30.927  &lt; 2e-16 ***
## hr17                       296.267      5.749  51.537  &lt; 2e-16 ***
## hr18                       269.441      5.736  46.976  &lt; 2e-16 ***
## hr19                       186.256      5.714  32.596  &lt; 2e-16 ***
## hr20                       125.549      5.704  22.012  &lt; 2e-16 ***
## hr21                        87.554      5.693  15.378  &lt; 2e-16 ***
## hr22                        59.123      5.689  10.392  &lt; 2e-16 ***
## hr23                        26.838      5.688   4.719 2.41e-06 ***
## workingday                   1.270      1.784   0.711 0.476810    
## temp                       157.209     10.261  15.321  &lt; 2e-16 ***
## weathersitcloudy/misty     -12.890      1.964  -6.562 5.60e-11 ***
## weathersitlight rain/snow  -66.494      2.965 -22.425  &lt; 2e-16 ***
## weathersitheavy rain/snow -109.745     76.667  -1.431 0.152341    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 76.5 on 8605 degrees of freedom
## Multiple R-squared:  0.6745, Adjusted R-squared:  0.6731 
## F-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>In <code>mod.lm</code>, the first level of hr (0) and mnth (January) are treated as
baseline values, meaning no coefficient estimates are provided for them.
Implicitly, their coefficient estimates are zero, and all other levels
are measured relative to these baselines. For instance, the February
coefficient of <span class="math inline">\(6.845\)</span> indicates that, holding all other variables
constant, there are on average about 7 more riders in February than in
January. Similarly, there are about 16.5 more riders in March compared
to January.</p>
<p>However, what if we were to recode the <strong>hr</strong> and <strong>mnth</strong> variables?</p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="#cb201-1" tabindex="-1"></a><span class="fu">contrasts</span>(Bikeshare<span class="sc">$</span>hr) <span class="ot">=</span> <span class="fu">contr.sum</span>(<span class="dv">24</span>)</span>
<span id="cb201-2"><a href="#cb201-2" tabindex="-1"></a><span class="fu">contrasts</span>(Bikeshare<span class="sc">$</span>mnth) <span class="ot">=</span> <span class="fu">contr.sum</span>(<span class="dv">12</span>)</span>
<span id="cb201-3"><a href="#cb201-3" tabindex="-1"></a>mod.lm2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(bikers <span class="sc">~</span> mnth <span class="sc">+</span> hr <span class="sc">+</span> workingday <span class="sc">+</span> temp <span class="sc">+</span> weathersit,</span>
<span id="cb201-4"><a href="#cb201-4" tabindex="-1"></a>              <span class="at">data =</span> Bikeshare)</span>
<span id="cb201-5"><a href="#cb201-5" tabindex="-1"></a><span class="fu">summary</span>(mod.lm2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, 
##     data = Bikeshare)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -299.00  -45.70   -6.23   41.08  425.29 
## 
## Coefficients:
##                            Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                 73.5974     5.1322  14.340  &lt; 2e-16 ***
## mnth1                      -46.0871     4.0855 -11.281  &lt; 2e-16 ***
## mnth2                      -39.2419     3.5391 -11.088  &lt; 2e-16 ***
## mnth3                      -29.5357     3.1552  -9.361  &lt; 2e-16 ***
## mnth4                       -4.6622     2.7406  -1.701  0.08895 .  
## mnth5                       26.4700     2.8508   9.285  &lt; 2e-16 ***
## mnth6                       21.7317     3.4651   6.272 3.75e-10 ***
## mnth7                       -0.7626     3.9084  -0.195  0.84530    
## mnth8                        7.1560     3.5347   2.024  0.04295 *  
## mnth9                       20.5912     3.0456   6.761 1.46e-11 ***
## mnth10                      29.7472     2.6995  11.019  &lt; 2e-16 ***
## mnth11                      14.2229     2.8604   4.972 6.74e-07 ***
## hr1                        -96.1420     3.9554 -24.307  &lt; 2e-16 ***
## hr2                       -110.7213     3.9662 -27.916  &lt; 2e-16 ***
## hr3                       -117.7212     4.0165 -29.310  &lt; 2e-16 ***
## hr4                       -127.2828     4.0808 -31.191  &lt; 2e-16 ***
## hr5                       -133.0495     4.1168 -32.319  &lt; 2e-16 ***
## hr6                       -120.2775     4.0370 -29.794  &lt; 2e-16 ***
## hr7                        -75.5424     3.9916 -18.925  &lt; 2e-16 ***
## hr8                         23.9511     3.9686   6.035 1.65e-09 ***
## hr9                        127.5199     3.9500  32.284  &lt; 2e-16 ***
## hr10                        24.4399     3.9360   6.209 5.57e-10 ***
## hr11                       -12.3407     3.9361  -3.135  0.00172 ** 
## hr12                         9.2814     3.9447   2.353  0.01865 *  
## hr13                        41.1417     3.9571  10.397  &lt; 2e-16 ***
## hr14                        39.8939     3.9750  10.036  &lt; 2e-16 ***
## hr15                        30.4940     3.9910   7.641 2.39e-14 ***
## hr16                        35.9445     3.9949   8.998  &lt; 2e-16 ***
## hr17                        82.3786     3.9883  20.655  &lt; 2e-16 ***
## hr18                       200.1249     3.9638  50.488  &lt; 2e-16 ***
## hr19                       173.2989     3.9561  43.806  &lt; 2e-16 ***
## hr20                        90.1138     3.9400  22.872  &lt; 2e-16 ***
## hr21                        29.4071     3.9362   7.471 8.74e-14 ***
## hr22                        -8.5883     3.9332  -2.184  0.02902 *  
## hr23                       -37.0194     3.9344  -9.409  &lt; 2e-16 ***
## workingday                   1.2696     1.7845   0.711  0.47681    
## temp                       157.2094    10.2612  15.321  &lt; 2e-16 ***
## weathersitcloudy/misty     -12.8903     1.9643  -6.562 5.60e-11 ***
## weathersitlight rain/snow  -66.4944     2.9652 -22.425  &lt; 2e-16 ***
## weathersitheavy rain/snow -109.7446    76.6674  -1.431  0.15234    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 76.5 on 8605 degrees of freedom
## Multiple R-squared:  0.6745, Adjusted R-squared:  0.6731 
## F-statistic: 457.3 on 39 and 8605 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>What is the difference between the two codings? In <code>mod.lm2</code>, a
coefficient estimate is reported for all but the last level of <strong>hr</strong>
and <strong>mnth</strong>. Importantly, in <code>mod.lm2</code>, the coefficient estimate for
the last level of <strong>mnth</strong> is not zero; instead, it equals the negative
of the sum of the coefficient estimates for all the other levels.
Similarly, in <code>mod.lm2</code>, the coefficient estimate for the last level of
<strong>hr</strong> is the negative of the sum of the coefficient estimates for all
the other levels. This means the coefficients of hr and <strong>mnth</strong> in
<code>mod.lm2</code> will always sum to zero and can be interpreted as the
difference from the mean level. For example, the coefficient for January
of <span class="math inline">\(-46.087\)</span> indicates that, holding all other variables constant, there
are typically 46 fewer riders in January compared to the yearly average.</p>
<p>The predictions from either of the two models is identical, as confirmed
below. However, the choice of coding is crucial for correct
interpretation of the model results.</p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="#cb203-1" tabindex="-1"></a><span class="fu">sum</span>((<span class="fu">predict</span>(mod.lm) <span class="sc">-</span> <span class="fu">predict</span>(mod.lm2))<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 1.426274e-18</code></pre>
<p>The sum of squared differences is zero. We can also see this using the
<code>all.equal()</code> function:</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="#cb205-1" tabindex="-1"></a><span class="fu">all.equal</span>(<span class="fu">predict</span>(mod.lm), <span class="fu">predict</span>(mod.lm2))</span></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
</div>
<div id="plotting-coefficient-estimates" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Plotting Coefficient Estimates<a href="#plotting-coefficient-estimates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s plot the coefficient estimates for the variable <strong>mnth</strong>.</p>
<p>First we obtain the coefficients for January through November from the
<code>mod.lm2</code> object. The coefficient for December must be explicitly
computed as the negative sum of all the other months.</p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="#cb207-1" tabindex="-1"></a>coef.months <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">coef</span>(mod.lm2)[<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>], <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">coef</span>(mod.lm2)[<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>]))</span></code></pre></div>
<p>To make the plot, we manually label the <span class="math inline">\(x\)</span>-axis with the names of the
months.</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="#cb208-1" tabindex="-1"></a><span class="fu">plot</span>(coef.months, <span class="at">xlab =</span> <span class="st">&quot;Month&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Coefficient&quot;</span>,</span>
<span id="cb208-2"><a href="#cb208-2" tabindex="-1"></a>    <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">type =</span> <span class="st">&quot;o&quot;</span>)</span>
<span id="cb208-3"><a href="#cb208-3" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">at =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>, <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;J&quot;</span>, <span class="st">&quot;F&quot;</span>, <span class="st">&quot;M&quot;</span>, <span class="st">&quot;A&quot;</span>,</span>
<span id="cb208-4"><a href="#cb208-4" tabindex="-1"></a>    <span class="st">&quot;M&quot;</span>, <span class="st">&quot;J&quot;</span>, <span class="st">&quot;J&quot;</span>, <span class="st">&quot;A&quot;</span>, <span class="st">&quot;S&quot;</span>, <span class="st">&quot;O&quot;</span>, <span class="st">&quot;N&quot;</span>, <span class="st">&quot;D&quot;</span>))</span></code></pre></div>
<p><img src="03-S03-D2_files/figure-html/chunk41-1.png" width="672" /></p>
<p>Now let’s do the same for the variable <strong>mhr</strong>.</p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="#cb209-1" tabindex="-1"></a>coef.hours <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">coef</span>(mod.lm2)[<span class="dv">13</span><span class="sc">:</span><span class="dv">35</span>], <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">coef</span>(mod.lm2)[<span class="dv">13</span><span class="sc">:</span><span class="dv">35</span>]))</span>
<span id="cb209-2"><a href="#cb209-2" tabindex="-1"></a></span>
<span id="cb209-3"><a href="#cb209-3" tabindex="-1"></a><span class="fu">plot</span>(coef.hours, <span class="at">xlab =</span> <span class="st">&quot;Hour&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Coefficient&quot;</span>,</span>
<span id="cb209-4"><a href="#cb209-4" tabindex="-1"></a>     <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">type =</span> <span class="st">&quot;o&quot;</span>)</span></code></pre></div>
<p><img src="03-S03-D2_files/figure-html/chunk42-1.png" width="672" /></p>
<div class="question">
<p>What do these two plots show?</p>
</div>
</div>
</div>
<div id="the-poisson-model" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">The Poisson Model<a href="#the-poisson-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now let’s fit a Poisson model that is more appropriate given that we are
dealing with count data. The approach to fitting the model is very
similar to that of logistic regression except that we use the argument
<code>family = poisson</code>.</p>
<p>As with the linear model, the purpose is to predict the number of bikers
(bikers) using month (mnth), hour (hr), whether it’s a working day
(workingday), temperature (temp), and weather situation (weathersit).</p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="#cb210-1" tabindex="-1"></a>mod.pois <span class="ot">&lt;-</span> <span class="fu">glm</span>(bikers <span class="sc">~</span> mnth <span class="sc">+</span> hr <span class="sc">+</span> workingday <span class="sc">+</span> temp <span class="sc">+</span> weathersit,</span>
<span id="cb210-2"><a href="#cb210-2" tabindex="-1"></a>                <span class="at">data =</span> Bikeshare, <span class="at">family =</span> poisson)</span>
<span id="cb210-3"><a href="#cb210-3" tabindex="-1"></a></span>
<span id="cb210-4"><a href="#cb210-4" tabindex="-1"></a><span class="fu">summary</span>(mod.pois)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = bikers ~ mnth + hr + workingday + temp + weathersit, 
##     family = poisson, data = Bikeshare)
## 
## Deviance Residuals: 
##      Min        1Q    Median        3Q       Max  
## -20.7574   -3.3441   -0.6549    2.6999   21.9628  
## 
## Coefficients:
##                            Estimate Std. Error  z value Pr(&gt;|z|)    
## (Intercept)                4.118245   0.006021  683.964  &lt; 2e-16 ***
## mnth1                     -0.670170   0.005907 -113.445  &lt; 2e-16 ***
## mnth2                     -0.444124   0.004860  -91.379  &lt; 2e-16 ***
## mnth3                     -0.293733   0.004144  -70.886  &lt; 2e-16 ***
## mnth4                      0.021523   0.003125    6.888 5.66e-12 ***
## mnth5                      0.240471   0.002916   82.462  &lt; 2e-16 ***
## mnth6                      0.223235   0.003554   62.818  &lt; 2e-16 ***
## mnth7                      0.103617   0.004125   25.121  &lt; 2e-16 ***
## mnth8                      0.151171   0.003662   41.281  &lt; 2e-16 ***
## mnth9                      0.233493   0.003102   75.281  &lt; 2e-16 ***
## mnth10                     0.267573   0.002785   96.091  &lt; 2e-16 ***
## mnth11                     0.150264   0.003180   47.248  &lt; 2e-16 ***
## hr1                       -0.754386   0.007879  -95.744  &lt; 2e-16 ***
## hr2                       -1.225979   0.009953 -123.173  &lt; 2e-16 ***
## hr3                       -1.563147   0.011869 -131.702  &lt; 2e-16 ***
## hr4                       -2.198304   0.016424 -133.846  &lt; 2e-16 ***
## hr5                       -2.830484   0.022538 -125.586  &lt; 2e-16 ***
## hr6                       -1.814657   0.013464 -134.775  &lt; 2e-16 ***
## hr7                       -0.429888   0.006896  -62.341  &lt; 2e-16 ***
## hr8                        0.575181   0.004406  130.544  &lt; 2e-16 ***
## hr9                        1.076927   0.003563  302.220  &lt; 2e-16 ***
## hr10                       0.581769   0.004286  135.727  &lt; 2e-16 ***
## hr11                       0.336852   0.004720   71.372  &lt; 2e-16 ***
## hr12                       0.494121   0.004392  112.494  &lt; 2e-16 ***
## hr13                       0.679642   0.004069  167.040  &lt; 2e-16 ***
## hr14                       0.673565   0.004089  164.722  &lt; 2e-16 ***
## hr15                       0.624910   0.004178  149.570  &lt; 2e-16 ***
## hr16                       0.653763   0.004132  158.205  &lt; 2e-16 ***
## hr17                       0.874301   0.003784  231.040  &lt; 2e-16 ***
## hr18                       1.294635   0.003254  397.848  &lt; 2e-16 ***
## hr19                       1.212281   0.003321  365.084  &lt; 2e-16 ***
## hr20                       0.914022   0.003700  247.065  &lt; 2e-16 ***
## hr21                       0.616201   0.004191  147.045  &lt; 2e-16 ***
## hr22                       0.364181   0.004659   78.173  &lt; 2e-16 ***
## hr23                       0.117493   0.005225   22.488  &lt; 2e-16 ***
## workingday                 0.014665   0.001955    7.502 6.27e-14 ***
## temp                       0.785292   0.011475   68.434  &lt; 2e-16 ***
## weathersitcloudy/misty    -0.075231   0.002179  -34.528  &lt; 2e-16 ***
## weathersitlight rain/snow -0.575800   0.004058 -141.905  &lt; 2e-16 ***
## weathersitheavy rain/snow -0.926287   0.166782   -5.554 2.79e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 1052921  on 8644  degrees of freedom
## Residual deviance:  228041  on 8605  degrees of freedom
## AIC: 281159
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>Below is a table which summarises what the coefficients tell us:</p>
<table style="width:96%;">
<colgroup>
<col width="22%" />
<col width="73%" />
</colgroup>
<tbody>
<tr class="odd">
<td>intercept</td>
<td>baseline log count of bikers when all predictors
are at their reference levels</td>
</tr>
<tr class="even">
<td>mnth</td>
<td><p>effects of different months (1-11) relative to the
reference month (mnth 12 - December)</p>
<p>For example, the <strong>estimate for January (mnth1)</strong>
is <span class="math inline">\(-0.670170\)</span>. This tells us that the log count
of bikers in January is lower by approximately
0.67 units compared to the reference month
(December).</p>
<p>To facilitate interpretation, we exponentiate the
coefficient: <code>exp(-0.670170) ≈ 0.5116</code>. Therefore,
the expected number of bikers in January is about
<span class="math inline">\(51\%\)</span> that of December. This indicates a decrease
of approximately
<span class="math inline">\(0.511 6-1 = 0.4884(100) = 48.84\%\)</span> compared to
December, holding all other variables constant.</p></td>
</tr>
<tr class="odd">
<td>hr</td>
<td><p>effects of different hours (1-23) relative to the
reference hour (hr24)</p>
<p>For example, the <strong>estimate for 8:00 (hr8)</strong> is
<span class="math inline">\(0.575181\)</span>. This tells us that the log count of
bikers at 8 in the morning is higher by
approximately 0.58 units compared to the reference
hour (midnight). We exponentiate the coefficient:
<code>exp(0.575181) ≈ 1.777452</code>. Therefore, the
expected number of bikers at 8 in the morning is
about <span class="math inline">\(1 .777-1 = 0.77(100) = 77.7\%\)</span> higher than
the number of bikers at midnight, holding all
other variables constant.</p></td>
</tr>
<tr class="even">
<td>workingday</td>
<td><p>since the coefficient is positive, it suggests
that there are more bikers on working days</p>
<p>We exponentiate the coefficient:
<code>exp(0.014665) ≈ 1.014773</code>. Therefore, the
expected number of bikers on working day is about
<span class="math inline">\(1.01 4773-1 = 0.015(100) = 1.5\%\)</span> higher than the
number of bikers during a non-working day, holding
all other variables constant.</p></td>
</tr>
<tr class="odd">
<td>temp</td>
<td><p>since the coefficient is positive, it suggests
that higher temperatures are associated with more
bikers</p>
<p>We exponentiate the coefficient:
<code>exp(0.785292) ≈ 2.193047</code>.</p>
<p>Therefore, for each one-unit increase in the
normalised temperature, the expected number of
bikers is about
<span class="math inline">\(2.193047- 1 = 1.193047(100) = 119.3\%\)</span> higher,
holding all other variables constant</p></td>
</tr>
<tr class="even">
<td>weathersit</td>
<td><p>indicates the effect of different weather
conditions on the number of bikers relative to the
reference (clear weather). Since the coefficients
are negative, this suggests that adverse weather
conditions (e.g., heavy rain/snow) significantly
reduce the number of bikers</p>
<ul>
<li>For cloudy/misty weather, the expected number
of bikers is
<ul>
<li><code>exp(-0.075231) ≈ 0.9275</code> times the number
of bikers in clear weather, indicating a
decrease of about 7.25%.</li>
</ul></li>
<li>For light rain/snow, the expected number of
bikers is
<ul>
<li><code>exp(-0.575800) ≈ 0.5621</code> times the number
of bikers in clear weather, indicating a
decrease of about 43.79%.</li>
</ul></li>
<li>For heavy rain/snow, the expected number of
bikers is
<ul>
<li><code>exp(-0.926287) ≈ 0.3957</code> times the number
of bikers in clear weather, indicating a
decrease of about 60.43%.</li>
</ul></li>
</ul></td>
</tr>
</tbody>
</table>
<p>Let’s plot the coefficients associated with the <strong>mnth</strong> variable.</p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="#cb212-1" tabindex="-1"></a>coef.mnth <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">coef</span>(mod.pois)[<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>], <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">coef</span>(mod.pois)[<span class="dv">2</span><span class="sc">:</span><span class="dv">12</span>]))</span>
<span id="cb212-2"><a href="#cb212-2" tabindex="-1"></a></span>
<span id="cb212-3"><a href="#cb212-3" tabindex="-1"></a><span class="fu">plot</span>(coef.mnth, <span class="at">xlab =</span> <span class="st">&quot;Month&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Coefficient&quot;</span>,</span>
<span id="cb212-4"><a href="#cb212-4" tabindex="-1"></a>     <span class="at">xaxt =</span> <span class="st">&quot;n&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">type =</span> <span class="st">&quot;o&quot;</span>)</span>
<span id="cb212-5"><a href="#cb212-5" tabindex="-1"></a></span>
<span id="cb212-6"><a href="#cb212-6" tabindex="-1"></a><span class="fu">axis</span>(<span class="at">side =</span> <span class="dv">1</span>, <span class="at">at =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>, </span>
<span id="cb212-7"><a href="#cb212-7" tabindex="-1"></a>     <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;J&quot;</span>, <span class="st">&quot;F&quot;</span>, <span class="st">&quot;M&quot;</span>, <span class="st">&quot;A&quot;</span>, <span class="st">&quot;M&quot;</span>, <span class="st">&quot;J&quot;</span>, <span class="st">&quot;J&quot;</span>, <span class="st">&quot;A&quot;</span>, <span class="st">&quot;S&quot;</span>, <span class="st">&quot;O&quot;</span>, <span class="st">&quot;N&quot;</span>, <span class="st">&quot;D&quot;</span>))</span></code></pre></div>
<p><img src="03-S03-D2_files/figure-html/chunk44-1.png" width="672" /></p>
<p>And the coefficients associated with the <strong>hr</strong> variable.</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="#cb213-1" tabindex="-1"></a>coef.hours <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">coef</span>(mod.pois)[<span class="dv">13</span><span class="sc">:</span><span class="dv">35</span>], <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">coef</span>(mod.pois)[<span class="dv">13</span><span class="sc">:</span><span class="dv">35</span>]))</span>
<span id="cb213-2"><a href="#cb213-2" tabindex="-1"></a></span>
<span id="cb213-3"><a href="#cb213-3" tabindex="-1"></a><span class="fu">plot</span>(coef.hours, <span class="at">xlab =</span> <span class="st">&quot;Hour&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Coefficient&quot;</span>,</span>
<span id="cb213-4"><a href="#cb213-4" tabindex="-1"></a>    <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">pch =</span> <span class="dv">19</span>, <span class="at">type =</span> <span class="st">&quot;o&quot;</span>)</span></code></pre></div>
<p><img src="03-S03-D2_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<div class="question">
<p>What do these plots suggest?</p>
</div>
</div>
<div id="linear-versus-poisson-regression" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Linear versus Poisson Regression<a href="#linear-versus-poisson-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s plot the fitted values from the Poisson model and compare them to
those of the linear model. For the Poisson predictions, we must use the
argument <code>type = "response"</code> to specify that we want <code>R</code> to output
<span class="math inline">\(\exp(\hat\beta_0 + \hat\beta_1 X_1 + \ldots +\hat\beta_p X_p)\)</span> rather
than <span class="math inline">\(\hat\beta_0 + \hat\beta_1 X_1 + \ldots + \hat\beta_p X_p\)</span>, which
it will output by default.</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="#cb214-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">predict</span>(mod.lm2), <span class="fu">predict</span>(mod.pois, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>))</span>
<span id="cb214-2"><a href="#cb214-2" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">col =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="03-S03-D2_files/figure-html/chunk45-1.png" width="672" /></p>
<p>As you can see, the predictions from the Poisson regression model are
correlated with those from the linear model; however, the former are
non-negative. As a result the Poisson regression predictions tend to be
larger than those from the linear model for either very low or very high
levels of ridership.</p>
<!--chapter:end:03-S03-D2.Rmd-->
</div>
</div>
<div id="practical-predicting-a-companys-bankruptcy" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Practical: Predicting a company’s bankruptcy<a href="#practical-predicting-a-companys-bankruptcy" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>This practical is adapted from a demonstration created by Dr. Tatjana
Kecojevic, Lecturer in Social Statistics.</em></p>
<div class="file">
<p>For the tasks below, you will require the <strong>four_ratios</strong> dataset.</p>
<p>Click here to download the file:
<a href="data/four_ratios.csv" download="four_ratios.csv"> four_ratios.csv </a>.</p>
<p>Remember to place your data file in a separate subfolder within your R
project working directory.</p>
</div>
<div id="data-and-variables" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Data and Variables<a href="#data-and-variables" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In order to build a model to predict a company’s bankruptcy, an analyst
has collected a sample of data as follows:</p>
<p>Four financial ratios (predictors):</p>
<ul>
<li>x1: Retained Earnings / Total Assets<br />
</li>
<li>x2: Earnings Before Interest and Taxes / Total Assets<br />
</li>
<li>x3: Sales / Total Assets<br />
</li>
<li>x4: Cash Flow / Total Debt</li>
</ul>
<p>And a binary response variable y:</p>
<ul>
<li><p>0 - if the company went bankrupt</p></li>
<li><p>1 - if the company stayed solvent</p></li>
</ul>
</div>
<div id="importing-the-data" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Importing the data<a href="#importing-the-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="#cb215-1" tabindex="-1"></a>four_ratios <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/four_ratios.csv&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
</div>
<div id="loading-required-packages" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Loading required packages<a href="#loading-required-packages" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="#cb216-1" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb216-2"><a href="#cb216-2" tabindex="-1"></a><span class="fu">library</span>(class)</span>
<span id="cb216-3"><a href="#cb216-3" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb216-4"><a href="#cb216-4" tabindex="-1"></a><span class="fu">library</span>(corrplot)</span>
<span id="cb216-5"><a href="#cb216-5" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb216-6"><a href="#cb216-6" tabindex="-1"></a><span class="fu">library</span>(e1071)</span></code></pre></div>
<p>Now let’s explore the dataset. We can see that the predictors are all of
class double. The response is of class integer, despite this being a
binary categorical variable. There are a total of 111 observations, so
quite a small dataset.</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="#cb217-1" tabindex="-1"></a><span class="fu">glimpse</span>(four_ratios)</span></code></pre></div>
<pre><code>## Rows: 111
## Columns: 5
## $ x1 &lt;dbl&gt; 0.3778, 0.2783, 0.1192, 0.6070, 0.5334, 0.3868, 0.3312, 0.0130, 0.6…
## $ x2 &lt;dbl&gt; 0.1316, 0.1166, 0.2035, 0.2040, 0.1650, 0.0681, 0.2157, 0.2366, 0.2…
## $ x3 &lt;dbl&gt; 1.0911, 1.3441, 0.8130, 14.4090, 8.0734, 0.5755, 3.0679, 2.4709, 5.…
## $ x4 &lt;dbl&gt; 1.2784, 0.2216, 1.6702, 0.9844, 1.3474, 1.0579, 2.0899, 1.2230, 1.7…
## $ y  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…</code></pre>
<p>We therefore transform the response to a factor.</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="#cb219-1" tabindex="-1"></a>four_ratios<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="fu">as_factor</span>(four_ratios<span class="sc">$</span>y)</span></code></pre></div>
</div>
<div id="correlation-matrix-and-plot-1" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Correlation Matrix and Plot<a href="#correlation-matrix-and-plot-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s now produce a correlation plot between all pairs of variables in
the dataset.</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="#cb220-1" tabindex="-1"></a>cor_matrix <span class="ot">&lt;-</span> <span class="fu">cor</span>(four_ratios[,<span class="sc">-</span><span class="dv">5</span>])</span>
<span id="cb220-2"><a href="#cb220-2" tabindex="-1"></a></span>
<span id="cb220-3"><a href="#cb220-3" tabindex="-1"></a><span class="fu">corrplot</span>(cor_matrix, <span class="at">type =</span> <span class="st">&quot;lower&quot;</span>, <span class="at">diag =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="03-S03-P1_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>We observe a strong positive correlation between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. The
correlations between other variables are quite weak.</p>
<p>Before we consider the model, we split the data into training and test
sets as we will later on assess model accuracy in correct prediction
using the test data set. We consider a basic fixed split of 80:20. Since
there are a total of 111 observations, we split at row 89</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="#cb221-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb221-2"><a href="#cb221-2" tabindex="-1"></a>split_idx <span class="ot">=</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(four_ratios), <span class="dv">89</span>)</span>
<span id="cb221-3"><a href="#cb221-3" tabindex="-1"></a>train <span class="ot">=</span> four_ratios[split_idx, ]</span>
<span id="cb221-4"><a href="#cb221-4" tabindex="-1"></a>test <span class="ot">=</span> four_ratios[<span class="sc">-</span>split_idx, ]</span></code></pre></div>
</div>
<div id="logistic-regression" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Logistic Regression<a href="#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now let’s first consider logistic regression and build the model. Given
the high correlation between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> (this is to be expected,
given what they measure), we have to reflect on how we want to address
this. Assuming we have knowledge of important factors that can predict
bankruptcy, we can decide to drop one of the two financial ratios. In
this case, we drop <span class="math inline">\(x_2\)</span>.</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="#cb222-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x3 <span class="sc">+</span> x4,</span>
<span id="cb222-2"><a href="#cb222-2" tabindex="-1"></a>           <span class="at">data =</span> train, <span class="at">family =</span> binomial)</span>
<span id="cb222-3"><a href="#cb222-3" tabindex="-1"></a></span>
<span id="cb222-4"><a href="#cb222-4" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x1 + x3 + x4, family = binomial, data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1151  -0.4682   0.0000   0.3720   2.2542  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -2.6970     0.8896  -3.032  0.00243 ** 
## x1           12.6569     3.0942   4.090  4.3e-05 ***
## x3            1.6400     0.5764   2.845  0.00444 ** 
## x4           -0.5247     0.3297  -1.591  0.11151    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 123.369  on 88  degrees of freedom
## Residual deviance:  54.552  on 85  degrees of freedom
## AIC: 62.552
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<p>The key components of R’s <code>summary()</code> function for generalised linear
models for binomial family with the logit link are:</p>
<ul>
<li><p>Call: just like in the case of fitting the lm() model this is R
reminding us what model we ran, what options we specified, etc</p></li>
<li><p>the Deviance Residuals are a measure of model fit. This part of the
output shows the distribution of the deviance residuals for
individual cases used in the model. Below we discuss how to use
summaries of the deviance statistic to assess model fit</p></li>
<li><p>the Coefficients, their standard errors, the z-statistic (sometimes
called a Wald z-statistic), and the associated p-values. The
logistic regression coefficients give the change in the log odds of
the outcome for a one unit increase in the predictor variable</p></li>
</ul>
<p>At the end there are fit indices, including the <em>null</em> and <em>deviance</em>
residuals and the <em>AIC</em>, which are used to assess overall model fit.</p>
<p>We realise that the output above has a resemblance to the standard
regression output. Although they differ in the type of information, they
serve similar functions. Let us make an interpretation of it.</p>
<p>The fitted logarithm of the odds ratio, i.e. logit of the probability p
of the firm remaining solvent is modelled as:</p>
<p><span class="math display">\[\hat{g}(X_1,X_2,...X_q)=−9.8846+0.3238X_1+0.1779X_2+4.9443X_3\]</span>
Remember that here instead of predicting Y we obtain the model to
predict <span class="math inline">\(\log \left( \frac{p}{1-p} \right)\)</span>. Using the transformation of
the logit we get the predicted probabilities of the firm being solvent.</p>
<p>The estimated parameters are expected changes in the logit for unit
change in their corresponding variables when the other, remaining
variables are held fixed. That is, the logistic regression coefficients
give the change in the log odds of the response variable for a unit
increase in the explanatory variable. This is very hard to make sense of. We have predicted log odds and in order to interpret them into a more sensible fashion we need to “anti-log” them as the changes in odds ratio for a unit change in variable
<span class="math inline">\(X_i\)</span> , while the other variables are held fixed.</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="#cb224-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">exp</span>(<span class="fu">coef</span>(fit)), <span class="dv">4</span>)</span></code></pre></div>
<pre><code>## (Intercept)          x1          x3          x4 
##      0.0674 313912.8230      5.1550      0.5917</code></pre>
<p>Now, we can interpret the coefficients in terms of odds. For example, the odds of a firm being solvent (vs being bankrupt) increases by 313913 for a unit change in ratio <span class="math inline">\(X_1\)</span>, all else in the model being being fixed.</p>
</div>
<div id="explaining-the-logit" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Explaining the Logit<a href="#explaining-the-logit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s return to the summary of the fit:</p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="#cb226-1" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x1 + x3 + x4, family = binomial, data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1151  -0.4682   0.0000   0.3720   2.2542  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -2.6970     0.8896  -3.032  0.00243 ** 
## x1           12.6569     3.0942   4.090  4.3e-05 ***
## x3            1.6400     0.5764   2.845  0.00444 ** 
## x4           -0.5247     0.3297  -1.591  0.11151    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 123.369  on 88  degrees of freedom
## Residual deviance:  54.552  on 85  degrees of freedom
## AIC: 62.552
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<p>The column headed as z value is the ratio of the coefficients (Estimate) to their standard errors (Std. Error) known as Wald statistics for testing the hypothesis that the corresponding parameter are zeros. In standard regression this would be the t-test. Next to Wald test statistics we have their corresponding p-values (Pr(&gt;|z|)) which are used to judge the significance of the coefficients. Values smaller than 0.5 would lead to the conclusion that the coefficient is significantly different from 0 at 5% significance level. From the output obtained we see that two p-values are smaller than 0.5.</p>
<p>We now need to make a proper assessment to check if the variables collectively contribute in explaining the logit. That is, we need to examine whether the coefficients <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, <span class="math inline">\(\beta_3\)</span> and <span class="math inline">\(\beta_4\)</span> are all zeros. We do this using the G statistic, which stands for goodness of fit <em>G = likelihood without the predictors − likelihood with the predictors</em>.</p>
<p>G is distributed as a chi-square statistic with 1 degree of freedom, so
a chi-square test is the test of the fit of the model (note, that G is
similar to an <span class="math inline">\(R^2\)</span> type test).</p>
<p><span class="math inline">\(H_0:\beta_i=0\)</span><br />
<span class="math inline">\(H_1:\)</span> at least one is different from <span class="math inline">\(0\)</span></p>
<p>where <span class="math inline">\(i = 1, 2...\)</span>.</p>
<p>We decide on <span class="math inline">\(\alpha = 0.05\)</span>. If its associated p-value is greater than 0.05, we conclude that the variables do not collectively influence the logits, if however p-value is less that 0.05 we conclude that they do collectively influence the logits.</p>
<p>Let’s calculate the G statistic.</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="#cb228-1" tabindex="-1"></a>G_calc <span class="ot">&lt;-</span> fit<span class="sc">$</span>null.deviance <span class="sc">-</span> fit<span class="sc">$</span>deviance</span>
<span id="cb228-2"><a href="#cb228-2" tabindex="-1"></a></span>
<span id="cb228-3"><a href="#cb228-3" tabindex="-1"></a>G_calc</span></code></pre></div>
<pre><code>## [1] 68.81702</code></pre>
<p>Then the degrees of freedom of the predictors.</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="#cb230-1" tabindex="-1"></a>Gdf <span class="ot">&lt;-</span> fit<span class="sc">$</span>df.null <span class="sc">-</span> fit<span class="sc">$</span>df.residual</span>
<span id="cb230-2"><a href="#cb230-2" tabindex="-1"></a></span>
<span id="cb230-3"><a href="#cb230-3" tabindex="-1"></a>Gdf</span></code></pre></div>
<pre><code>## [1] 3</code></pre>
<p>We find the critical value for the G statistic.</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="#cb232-1" tabindex="-1"></a><span class="fu">qchisq</span>(.<span class="dv">95</span>, <span class="at">df =</span> Gdf) </span></code></pre></div>
<pre><code>## [1] 7.814728</code></pre>
<p>And finally, the p-value associated with the G statistic.</p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="#cb234-1" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(G_calc, Gdf)</span></code></pre></div>
<pre><code>## [1] 7.660539e-15</code></pre>
<p>Now, we have to decide whether our model is a statistically valid one.
Since <span class="math inline">\(G_{calc} = 68.82 &gt; G_{crit} = 7.81 ⇒ H1,\)</span> (and the p-value is
much smaller than 0.05), we can conclude that this is a statistically
valid model and that the variables collectively have explanatory power.</p>
<p><strong>However, do we need ALL three variables?</strong></p>
<p>In linear regression we assess the significance of individual regression
coefficients, i.e. the contribution of the individual variables using
the t-test. Here, we use the z scores to conduct the equivalent <em>Wald
statistic (test)</em>. The ratio of the logistic regression has a normal
distribution as opposed to the t-distribution we have seen in linear
regression.</p>
<p>Nonetheless, the set of hypotheses we wish to test is the same:</p>
<p><span class="math inline">\(H_0:\beta_i = 0\)</span> (coefficient is not significant, thus <span class="math inline">\(X_i\)</span> is not
important)<br />
<span class="math inline">\(H_1:\beta_i \ne 0\)</span> (coefficient is significant, thus <span class="math inline">\(X_i\)</span> is
important)</p>
<p><strong>Decision Rule</strong>: If the Wald’s z statistic lies between -2 and +2,
then the financial ratio, <span class="math inline">\(X_i\)</span>, is not needed and can be dropped from
the analysis. Otherwise, we will keep the financial ratio.However, there
is some scope here for subjective judgement depending upon how near to
+/-2 the Wald’s z value is. The p-value decision rule: if <span class="math inline">\(p &gt; 0.05\)</span> ⇒
the variable can be taken out, otherwise if p-value &lt; 0.05 keep the
variable in the model.</p>
<p>Rather than fitting individual models and doing a manual comparison we
can make use of the anova function for comparing the nested model using
the chi-square test.</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="#cb236-1" tabindex="-1"></a><span class="fu">anova</span>(fit, <span class="at">test=</span><span class="st">&quot;Chisq&quot;</span>)</span></code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model: binomial, link: logit
## 
## Response: y
## 
## Terms added sequentially (first to last)
## 
## 
##      Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
## NULL                    88    123.369              
## x1    1   54.441        87     68.928 1.602e-13 ***
## x3    1   12.044        86     56.884 0.0005197 ***
## x4    1    2.332        85     54.552 0.1267323    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Based on the output, we can remove <span class="math inline">\(X_4\)</span> ratio variable from the model
and we update our model (we also know that <span class="math inline">\(X_4\)</span> is not statistically
significant from the summary of the model results earlier).</p>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="#cb238-1" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">update</span>(fit, <span class="sc">~</span>. <span class="sc">-</span>x4, <span class="at">data =</span> train)</span>
<span id="cb238-2"><a href="#cb238-2" tabindex="-1"></a></span>
<span id="cb238-3"><a href="#cb238-3" tabindex="-1"></a><span class="fu">summary</span>(fit2)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x1 + x3, family = binomial, data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2116  -0.4985   0.0000   0.4300   2.3366  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -3.4779     0.8117  -4.285 1.83e-05 ***
## x1           11.6639     2.7793   4.197 2.71e-05 ***
## x3            1.6202     0.5754   2.816  0.00487 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 123.369  on 88  degrees of freedom
## Residual deviance:  56.884  on 86  degrees of freedom
## AIC: 62.884
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<p>To compare the fit of the new model we will use the Akaike Information
Criterion (AIC), which is an index of fit that takes account of
parsimony of the model by penalising for the number of parameters. It is
defined as:</p>
<p><em>AIC = −2× maximum log-likelihood + 2× number of parameters</em></p>
<p>Therefore, smaller values are indicative of a better fit to the data. In
the context of logit fit, the AIC is simply the residual deviance plus
twice the number of regression coefficients.</p>
<p>The AIC can be found at the end of the output.</p>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="#cb240-1" tabindex="-1"></a><span class="fu">summary</span>(fit2)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x1 + x3, family = binomial, data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2116  -0.4985   0.0000   0.4300   2.3366  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -3.4779     0.8117  -4.285 1.83e-05 ***
## x1           11.6639     2.7793   4.197 2.71e-05 ***
## x3            1.6202     0.5754   2.816  0.00487 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 123.369  on 88  degrees of freedom
## Residual deviance:  56.884  on 86  degrees of freedom
## AIC: 62.884
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<p>The AIC of our initial model is 62.552 and of the new model 62.884 (very
little difference). Checking the new model, we can see that it consists
of the variables that all significantly contribute in explaining the
logits. <em>So, in the spirit of parsimony we can choose the second model
to be a better fit.</em> You will learn more about the AIC and other similar
metrics later in the course.</p>
<p>To obtain the overall accuracy rate we need to find the predicted
probabilities of the observations kept aside in the test subset.</p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="#cb242-1" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit2, test, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">&gt;</span> <span class="fl">0.5</span></span></code></pre></div>
<p>Now let’s compute the confusion matrix such that we can compare our
predictions on the test data against the actual values in our dataset.
We can see that our model predicts one instance incorrectly (it predicts
<em>bankrupt</em> when in fact it should predict <em>solvent</em>)</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="#cb243-1" tabindex="-1"></a>(t <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="fu">ifelse</span>(pred, <span class="st">&quot;Solvent (pred)&quot;</span>, <span class="st">&quot;Bankrupt (pred)&quot;</span>), test<span class="sc">$</span>y))</span></code></pre></div>
<pre><code>##                  
##                    0  1
##   Bankrupt (pred) 11  1
##   Solvent (pred)   0 10</code></pre>
<p>If we then compute the overall fraction of correct predictions, we can
see that this is extremely high (0.955) which means that our model is
performing extremely well. <em>In practice</em>: Although the overall accuracy
rate might be easy to compute and to interpret, it makes no distinction
about the type of errors being made. Although we did remove one the
variables due to high correlation, other variables do show some degree
of correlation and so we must be cautious about the strength of the
relationship and therefore, the overall predictive performance.</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="#cb245-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">diag</span>(t)) <span class="sc">/</span> <span class="fu">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.9545455</code></pre>
<p><em>Do note that if you performing any rounding of the probabilities prior
to classification, values near the threshold of 0.5 may be classified
differently than if you were not to round the probabilities.</em></p>
</div>
<div id="tasks-linear-discriminant-analysis" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Tasks: Linear Discriminant Analysis<a href="#tasks-linear-discriminant-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="task-1-8" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Build a LDA classifier for the final logistic model with two predictors
and explain what the output means.</p>
</div>
<div id="task-2-8" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Compute the predictions and explain what the results mean.</p>
</div>
<div id="task-3-8" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 3<a href="#task-3-8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Compute the confusion matrix for the LDA classifier and calculate the
fraction of correct predictions. Explain the results.</p>
</div>
</div>
<div id="tasks-quadratic-discriminant-analysis" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Tasks: Quadratic Discriminant Analysis<a href="#tasks-quadratic-discriminant-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="task-1-9" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-9" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Build a QDA clasifier and describe the output.</p>
</div>
<div id="task-2-9" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-9" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Compute the predictions, the confusion matrix, and the fraction of
correct predictions and explain what the results mean.</p>
</div>
</div>
<div id="tasks-k-nearest-neighbours" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Tasks: <span class="math inline">\(K\)</span>-nearest neighbours<a href="#tasks-k-nearest-neighbours" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="task-1-10" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-10" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Perform KNN regression on the same model as in the previous tasks.</p>
</div>
<div id="task-2-10" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-10" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Produce a confusion matrix and compute the fraction of correct
predictions. Comment on the results.</p>
</div>
<div id="task-3-9" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 3<a href="#task-3-9" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Fit KNN for up to <span class="math inline">\(k = 30\)</span> and then calculate the overall fraction of
correct prediction.</p>
</div>
<div id="task-4-8" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 4<a href="#task-4-8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Create a plot to observe at what value for <code>k</code> the overall fraction of
correct predictions is highest.</p>
</div>
</div>
<div id="tasks-naive-bayes" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Tasks: Naive Bayes<a href="#tasks-naive-bayes" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="task-1-11" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-11" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Build a Naive Bayes classifier for the model used previously and
describe the output.</p>
</div>
<div id="task-2-11" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-11" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Compute the predictions, build a confusion matrix, calculate the
fraction of correct predictions and interpret the results.</p>
<!--chapter:end:03-S03-P1.Rmd-->
</div>
</div>
</div>
<div id="answers-2" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Answers<a href="#answers-2" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="practical-predicting-a-companys-bankruptcy-1" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Practical: Predicting a company’s bankruptcy<a href="#practical-predicting-a-companys-bankruptcy-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>This practical is adapted from a demonstration created by Dr. Tatjana Kecojevic, Lecturer in Social Statistics.</em></p>
<div class="file">
<p>For the tasks below, you will require the <strong>four_ratios</strong> dataset.</p>
<p>Click here to download the file:
<a href="data/four_ratios.csv" download="four_ratios.csv"> four_ratios.csv </a>.</p>
<p>Remember to place your data file in a separate subfolder within your R
project working directory.</p>
</div>
<div id="data-and-variables-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Data and Variables<a href="#data-and-variables-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In order to build a model to predict a company’s bankruptcy, an analyst has collected a sample of data as follows:</p>
<p>Four financial ratios (predictors):</p>
<ul>
<li>x1: Retained Earnings / Total Assets<br />
</li>
<li>x2: Earnings Before Interest and Taxes / Total Assets<br />
</li>
<li>x3: Sales / Total Assets<br />
</li>
<li>x4: Cash Flow / Total Debt</li>
</ul>
<p>And a binary response variable y:</p>
<ul>
<li><p>0 - if the company went bankrupt</p></li>
<li><p>1 - if the company stayed solvent</p></li>
</ul>
</div>
<div id="importing-the-data-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Importing the data<a href="#importing-the-data-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="#cb247-1" tabindex="-1"></a>four_ratios <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/four_ratios.csv&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
</div>
<div id="loading-required-packages-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Loading required packages<a href="#loading-required-packages-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb248-1"><a href="#cb248-1" tabindex="-1"></a><span class="fu">library</span>(MASS)</span>
<span id="cb248-2"><a href="#cb248-2" tabindex="-1"></a><span class="fu">library</span>(class)</span>
<span id="cb248-3"><a href="#cb248-3" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb248-4"><a href="#cb248-4" tabindex="-1"></a><span class="fu">library</span>(corrplot)</span>
<span id="cb248-5"><a href="#cb248-5" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb248-6"><a href="#cb248-6" tabindex="-1"></a><span class="fu">library</span>(e1071)</span></code></pre></div>
<p>We can see that the predictors are all of class double. The response is of class integer, despite this being a binary categorical variable. There are a total of 111 observations, so quite a small dataset.</p>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="#cb249-1" tabindex="-1"></a><span class="fu">glimpse</span>(four_ratios)</span></code></pre></div>
<pre><code>## Rows: 111
## Columns: 5
## $ x1 &lt;dbl&gt; 0.3778, 0.2783, 0.1192, 0.6070, 0.5334, 0.3868, 0.3312, 0.0130, 0.6…
## $ x2 &lt;dbl&gt; 0.1316, 0.1166, 0.2035, 0.2040, 0.1650, 0.0681, 0.2157, 0.2366, 0.2…
## $ x3 &lt;dbl&gt; 1.0911, 1.3441, 0.8130, 14.4090, 8.0734, 0.5755, 3.0679, 2.4709, 5.…
## $ x4 &lt;dbl&gt; 1.2784, 0.2216, 1.6702, 0.9844, 1.3474, 1.0579, 2.0899, 1.2230, 1.7…
## $ y  &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…</code></pre>
<p>We therefore transform the response to a factor.</p>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="#cb251-1" tabindex="-1"></a>four_ratios<span class="sc">$</span>y <span class="ot">&lt;-</span> <span class="fu">as_factor</span>(four_ratios<span class="sc">$</span>y)</span></code></pre></div>
</div>
<div id="correlation-matrix-and-plot-2" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Correlation Matrix and Plot<a href="#correlation-matrix-and-plot-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s now produce a correlation plot between all pairs of variables in the dataset.</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="#cb252-1" tabindex="-1"></a>cor_matrix <span class="ot">&lt;-</span> <span class="fu">cor</span>(four_ratios[,<span class="sc">-</span><span class="dv">5</span>])</span>
<span id="cb252-2"><a href="#cb252-2" tabindex="-1"></a></span>
<span id="cb252-3"><a href="#cb252-3" tabindex="-1"></a><span class="fu">corrplot</span>(cor_matrix, <span class="at">type =</span> <span class="st">&quot;lower&quot;</span>, <span class="at">diag =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><img src="03-S03-ANS_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>We observe a strong positive correlation between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. The correlations between other variables are quite weak.</p>
<p>Before we consider the model, we split the data into training and test sets as we will later on assess model accuracy in correct prediction using the test data set. We consider a basic fixed split of 80:20. Since there are a total of 111 observations, we split at row 89</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="#cb253-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb253-2"><a href="#cb253-2" tabindex="-1"></a>split_idx <span class="ot">=</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(four_ratios), <span class="dv">89</span>)</span>
<span id="cb253-3"><a href="#cb253-3" tabindex="-1"></a>train <span class="ot">=</span> four_ratios[split_idx, ]</span>
<span id="cb253-4"><a href="#cb253-4" tabindex="-1"></a>test <span class="ot">=</span> four_ratios[<span class="sc">-</span>split_idx, ]</span></code></pre></div>
</div>
<div id="logistic-regression-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Logistic Regression<a href="#logistic-regression-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now let’s first consider logistic regression and build the model. Given the high correlation between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> (this is to be expected, given what they measure), we have to reflect on how we want to address this. Assuming we have knowledge of important factors that can predict bankruptcy, we can decide to drop one of the two financial ratios. In this case, we drop <span class="math inline">\(x_2\)</span>.</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="#cb254-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x3 <span class="sc">+</span> x4,</span>
<span id="cb254-2"><a href="#cb254-2" tabindex="-1"></a>           <span class="at">data =</span> train, <span class="at">family =</span> binomial)</span>
<span id="cb254-3"><a href="#cb254-3" tabindex="-1"></a></span>
<span id="cb254-4"><a href="#cb254-4" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x1 + x3 + x4, family = binomial, data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1151  -0.4682   0.0000   0.3720   2.2542  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -2.6970     0.8896  -3.032  0.00243 ** 
## x1           12.6569     3.0942   4.090  4.3e-05 ***
## x3            1.6400     0.5764   2.845  0.00444 ** 
## x4           -0.5247     0.3297  -1.591  0.11151    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 123.369  on 88  degrees of freedom
## Residual deviance:  54.552  on 85  degrees of freedom
## AIC: 62.552
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<p>We now need to make a proper assessment to check if the variables collectively contribute in explaining the logit.</p>
<p>Therefore, we calculate the G statistic.</p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="#cb256-1" tabindex="-1"></a>G_calc <span class="ot">&lt;-</span> fit<span class="sc">$</span>null.deviance <span class="sc">-</span> fit<span class="sc">$</span>deviance</span>
<span id="cb256-2"><a href="#cb256-2" tabindex="-1"></a></span>
<span id="cb256-3"><a href="#cb256-3" tabindex="-1"></a>G_calc</span></code></pre></div>
<pre><code>## [1] 68.81702</code></pre>
<p>Then the degrees of freedom of the predictors.</p>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="#cb258-1" tabindex="-1"></a>Gdf <span class="ot">&lt;-</span> fit<span class="sc">$</span>df.null <span class="sc">-</span> fit<span class="sc">$</span>df.residual</span>
<span id="cb258-2"><a href="#cb258-2" tabindex="-1"></a></span>
<span id="cb258-3"><a href="#cb258-3" tabindex="-1"></a>Gdf</span></code></pre></div>
<pre><code>## [1] 3</code></pre>
<p>We find the critical value for the G statistic.</p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="#cb260-1" tabindex="-1"></a><span class="fu">qchisq</span>(.<span class="dv">95</span>, <span class="at">df =</span> Gdf) </span></code></pre></div>
<pre><code>## [1] 7.814728</code></pre>
<p>And finally, the p-value associated with the G statistic.</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="#cb262-1" tabindex="-1"></a><span class="dv">1</span> <span class="sc">-</span> <span class="fu">pchisq</span>(G_calc, Gdf)</span></code></pre></div>
<pre><code>## [1] 7.660539e-15</code></pre>
<p>Now, we have to decide whether our model is a statistically valid one.
Since <span class="math inline">\(G_{calc} = 68.82 &gt; G_{crit} = 7.81 ⇒ H1,\)</span> (and the p-value is much smaller than 0.05), we can conclude that this is a statistically valid model and that the variables collectively have explanatory power.</p>
<p><strong>However, do we need ALL three variables?</strong></p>
<p>Rather than fitting individual models and doing a manual comparison we can make use of the anova function for comparing the nested model using the chi-square test.</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="#cb264-1" tabindex="-1"></a><span class="fu">anova</span>(fit, <span class="at">test=</span><span class="st">&quot;Chisq&quot;</span>)</span></code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model: binomial, link: logit
## 
## Response: y
## 
## Terms added sequentially (first to last)
## 
## 
##      Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    
## NULL                    88    123.369              
## x1    1   54.441        87     68.928 1.602e-13 ***
## x3    1   12.044        86     56.884 0.0005197 ***
## x4    1    2.332        85     54.552 0.1267323    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Based on the output, we can remove <span class="math inline">\(X_4\)</span> ratio variable from the model and we update our model (we also know that <span class="math inline">\(X_4\)</span> is not statistically significant from the summary of the model results earlier).</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="#cb266-1" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">update</span>(fit, <span class="sc">~</span>. <span class="sc">-</span>x4, <span class="at">data =</span> train)</span>
<span id="cb266-2"><a href="#cb266-2" tabindex="-1"></a></span>
<span id="cb266-3"><a href="#cb266-3" tabindex="-1"></a><span class="fu">summary</span>(fit2)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x1 + x3, family = binomial, data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2116  -0.4985   0.0000   0.4300   2.3366  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -3.4779     0.8117  -4.285 1.83e-05 ***
## x1           11.6639     2.7793   4.197 2.71e-05 ***
## x3            1.6202     0.5754   2.816  0.00487 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 123.369  on 88  degrees of freedom
## Residual deviance:  56.884  on 86  degrees of freedom
## AIC: 62.884
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<p>To compare the fit of the new model we will use the Akaike Information Criterion (AIC), which is an index of fit that takes account of parsimony of the model by penalising for the number of parameters.</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="#cb268-1" tabindex="-1"></a><span class="fu">summary</span>(fit2)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x1 + x3, family = binomial, data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2116  -0.4985   0.0000   0.4300   2.3366  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)  -3.4779     0.8117  -4.285 1.83e-05 ***
## x1           11.6639     2.7793   4.197 2.71e-05 ***
## x3            1.6202     0.5754   2.816  0.00487 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 123.369  on 88  degrees of freedom
## Residual deviance:  56.884  on 86  degrees of freedom
## AIC: 62.884
## 
## Number of Fisher Scoring iterations: 7</code></pre>
<p>The AIC of our initial model is 62.552 and of the new model 62.884 (very little difference). Checking the new model, we can see that it consists of the variables that all significantly contribute in explaining the logits. <em>So, in the spirit of parsimony we can choose the second model to be a better fit.</em> You will learn more about the AIC and other similar metrics later in the course.</p>
<p>To obtain the overall accuracy rate we need to find the predicted probabilities of the observations kept aside in the test subset.</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="#cb270-1" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit2, test, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">&gt;</span> <span class="fl">0.5</span></span></code></pre></div>
<p>Now let’s compute the confusion matrix such that we can compare our predictions on the test data against the actual values in our dataset. We can see that our model predicts one instance incorrectly (it predicts <em>bankrupt</em> when in fact it should predict <em>solvent</em>)</p>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="#cb271-1" tabindex="-1"></a>(t <span class="ot">&lt;-</span> <span class="fu">table</span>(<span class="fu">ifelse</span>(pred, <span class="st">&quot;Solvent (pred)&quot;</span>, <span class="st">&quot;Bankrupt (pred)&quot;</span>), test<span class="sc">$</span>y))</span></code></pre></div>
<pre><code>##                  
##                    0  1
##   Bankrupt (pred) 11  1
##   Solvent (pred)   0 10</code></pre>
<p>If we then compute the overall fraction of correct predictions, we can see that this is extremely high (0.955) which means that our model is performing extremely well. <em>In practice</em>: Although the overall accuracy rate might be easy to compute and to interpret, it makes no distinction about the type of errors being made. Although we did remove one the variables due to high correlation, other variables do show some degree of correlation and so we must be cautious about the strength of the relationship and therefore, the overall predictive performance.</p>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="#cb273-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">diag</span>(t)) <span class="sc">/</span> <span class="fu">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.9545455</code></pre>
<p><em>Do note that if you performing any rounding of the probabilities prior to classification, values near the threshold of 0.5 may be classified differently than if you were not to round the probabilities.</em></p>
</div>
<div id="linear-discriminant-analysis-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Linear Discriminant Analysis<a href="#linear-discriminant-analysis-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="task-1-12" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Task 1<a href="#task-1-12" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Build a LDA classifier for the final logistic model with two predictors and explain what the output means.</p>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb275-1"><a href="#cb275-1" tabindex="-1"></a>fit_lda <span class="ot">&lt;-</span> <span class="fu">lda</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x3, <span class="at">data =</span> train)</span>
<span id="cb275-2"><a href="#cb275-2" tabindex="-1"></a></span>
<span id="cb275-3"><a href="#cb275-3" tabindex="-1"></a>fit_lda</span></code></pre></div>
<pre><code>## Call:
## lda(y ~ x1 + x3, data = train)
## 
## Prior probabilities of groups:
##        0        1 
## 0.494382 0.505618 
## 
## Group means:
##            x1        x3
## 0 0.009738636 0.4628477
## 1 0.341413333 2.6057978
## 
## Coefficients of linear discriminants:
##           LD1
## x1 4.61295546
## x3 0.06590811</code></pre>
<ul>
<li>prior probabilities of groups: these tells us the way in which the two classes are distributed in our <em>training data</em> (i.e. 49.4 % of the observations correspond to bankruptcy whilst 50.6 % to solvency).<br />
</li>
<li>group means: the average of the two predictors within each class which are used by LDA as an estimate of <span class="math inline">\(μ_{k}\)</span>.<br />
</li>
<li>coefficient(s) of linear discriminants: tells us how our predictor(s) influence the score that is used to classify the observations into one of the two categories. Here, the coefficients both predictors are positive and so this indicates that higher values for <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> will make the model more likely classify an observation as belonging to the <strong>solvent</strong> class; also, the larger the absolute value of the coefficient, the stronger the influence on the model.</li>
</ul>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="#cb277-1" tabindex="-1"></a>fit_lda</span></code></pre></div>
<pre><code>## Call:
## lda(y ~ x1 + x3, data = train)
## 
## Prior probabilities of groups:
##        0        1 
## 0.494382 0.505618 
## 
## Group means:
##            x1        x3
## 0 0.009738636 0.4628477
## 1 0.341413333 2.6057978
## 
## Coefficients of linear discriminants:
##           LD1
## x1 4.61295546
## x3 0.06590811</code></pre>
</div>
<div id="task-2-12" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Task 2<a href="#task-2-12" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Compute the predictions and explain what the results mean.</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="#cb279-1" tabindex="-1"></a>result_lda <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_lda, test)</span></code></pre></div>
<p>The <code>class</code> component is a factor that contains the predictions for bankruptcy status (solvent/bankrupt).</p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="#cb280-1" tabindex="-1"></a>result_lda<span class="sc">$</span>class</span></code></pre></div>
<pre><code>##  [1] 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0
## Levels: 0 1</code></pre>
<p>The <code>posterior</code> component is matrix that contains the posterior probability that the corresponding observation belongs to a given class.</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="#cb282-1" tabindex="-1"></a>result_lda<span class="sc">$</span>posterior</span></code></pre></div>
<pre><code>##              0            1
## 1   0.17760365 8.223963e-01
## 2   0.31142722 6.885728e-01
## 10  0.07228194 9.277181e-01
## 11  0.33538335 6.646166e-01
## 19  0.04495309 9.550469e-01
## 20  0.17728895 8.227110e-01
## 24  0.40407718 5.959228e-01
## 28  0.22738932 7.726107e-01
## 29  0.14095907 8.590409e-01
## 37  0.53834405 4.616560e-01
## 40  0.70105814 2.989419e-01
## 44  0.85424906 1.457509e-01
## 45  0.73762416 2.623758e-01
## 49  0.95002382 4.997618e-02
## 56  0.99999946 5.413229e-07
## 58  0.83468414 1.653159e-01
## 62  0.87475890 1.252411e-01
## 64  0.94351212 5.648788e-02
## 66  0.25093559 7.490644e-01
## 85  0.61062622 3.893738e-01
## 105 0.99988023 1.197708e-04
## 109 0.96722224 3.277776e-02</code></pre>
<p>The <code>x</code> component contains the linear discriminants.</p>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="#cb284-1" tabindex="-1"></a>result_lda<span class="sc">$</span>x</span></code></pre></div>
<pre><code>##            LD1
## 1    0.8942494
## 2    0.4519351
## 10   1.5042674
## 11   0.3864033
## 19   1.8058326
## 20   0.8955396
## 24   0.2096297
## 28   0.7090237
## 29   1.0586059
## 37  -0.1147903
## 40  -0.5328419
## 44  -1.0809275
## 45  -0.6413331
## 49  -1.7849665
## 56  -8.6567024
## 58  -0.9916955
## 62  -1.1858701
## 64  -1.7075644
## 66   0.6315464
## 85  -0.2920645
## 105 -5.4259016
## 109 -2.0480872</code></pre>
<p>To obtain our predictions, we can simply extract the <code>class</code> element.</p>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="#cb286-1" tabindex="-1"></a>pred_lda <span class="ot">&lt;-</span> result_lda<span class="sc">$</span>class</span></code></pre></div>
</div>
<div id="task-3-10" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Task 3<a href="#task-3-10" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Compute the confusion matrix for the LDA classifier and calculate the fraction of correct predictions. Explain the results.</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="#cb287-1" tabindex="-1"></a>(t <span class="ot">&lt;-</span> <span class="fu">table</span>(pred_lda, test<span class="sc">$</span>y))</span></code></pre></div>
<pre><code>##         
## pred_lda  0  1
##        0 11  1
##        1  0 10</code></pre>
<p>And now the fraction of correct predictions which, we can see is identical to that obtained for logistic regression.</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="#cb289-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">diag</span>(t)) <span class="sc">/</span> <span class="fu">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.9545455</code></pre>
<p>LDA performs identically to logistic regression.</p>
</div>
</div>
<div id="quadratic-discriminant-analysis-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Quadratic Discriminant Analysis<a href="#quadratic-discriminant-analysis-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="task-1-13" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Task 1<a href="#task-1-13" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Build a QDA clasifier and describe the output.</p>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="#cb291-1" tabindex="-1"></a>fit_qda <span class="ot">&lt;-</span> <span class="fu">qda</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x3, <span class="at">data =</span> train)</span>
<span id="cb291-2"><a href="#cb291-2" tabindex="-1"></a></span>
<span id="cb291-3"><a href="#cb291-3" tabindex="-1"></a>fit_qda</span></code></pre></div>
<pre><code>## Call:
## qda(y ~ x1 + x3, data = train)
## 
## Prior probabilities of groups:
##        0        1 
## 0.494382 0.505618 
## 
## Group means:
##            x1        x3
## 0 0.009738636 0.4628477
## 1 0.341413333 2.6057978</code></pre>
<p>In terms of prior probabilities and group means, the output is identical to that of linear discriminant analysis. However, the output does not include the coefficients of the <em>linear</em> discriminants for obvious reasons.</p>
</div>
<div id="task-2-13" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Task 2<a href="#task-2-13" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Compute the predictions, the confusion matrix, and the fraction of correct predictions and explain what the results mean.</p>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="#cb293-1" tabindex="-1"></a>pred_qda <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_qda, test, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>)<span class="sc">$</span>class</span>
<span id="cb293-2"><a href="#cb293-2" tabindex="-1"></a></span>
<span id="cb293-3"><a href="#cb293-3" tabindex="-1"></a>(t <span class="ot">&lt;-</span> <span class="fu">table</span>(pred_qda, test<span class="sc">$</span>y))</span></code></pre></div>
<pre><code>##         
## pred_qda  0  1
##        0 11  6
##        1  0  5</code></pre>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="#cb295-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">diag</span>(t)) <span class="sc">/</span> <span class="fu">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.7272727</code></pre>
<p>The fraction of correct predictions is lower (0.73) than that for logistic regression and for LDA (0.95). We therefore conclude that in this context, QDA does not perform well in comparison to the previous two approaches.</p>
</div>
</div>
<div id="k-nearest-neighbours-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor"><span class="math inline">\(K\)</span>-nearest neighbours<a href="#k-nearest-neighbours-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="task-1-14" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Task 1<a href="#task-1-14" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Perform KNN regression on the same model as in the previous tasks.</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="#cb297-1" tabindex="-1"></a>fit_knn <span class="ot">&lt;-</span> <span class="fu">knn</span>(train[, <span class="fu">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x3&quot;</span>), <span class="at">drop =</span> <span class="cn">FALSE</span>],</span>
<span id="cb297-2"><a href="#cb297-2" tabindex="-1"></a>               test[, <span class="fu">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x3&quot;</span>), <span class="at">drop =</span> <span class="cn">FALSE</span>],</span>
<span id="cb297-3"><a href="#cb297-3" tabindex="-1"></a>               train<span class="sc">$</span>y, </span>
<span id="cb297-4"><a href="#cb297-4" tabindex="-1"></a>               <span class="at">k =</span> <span class="dv">1</span></span>
<span id="cb297-5"><a href="#cb297-5" tabindex="-1"></a>               )</span></code></pre></div>
</div>
<div id="task-2-14" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Task 2<a href="#task-2-14" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Produce a confusion matrix and compute the fraction of correct predictions. Comment on the results.</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="#cb298-1" tabindex="-1"></a>(t <span class="ot">&lt;-</span> <span class="fu">table</span>(fit_knn, test<span class="sc">$</span>y))</span></code></pre></div>
<pre><code>##        
## fit_knn  0  1
##       0 10  1
##       1  1 10</code></pre>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="#cb300-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">diag</span>(t)) <span class="sc">/</span> <span class="fu">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.9090909</code></pre>
<p>Our overall fraction of correct predictions is 0.91. Therefore, the KNN classifier (<span class="math inline">\(k = 1\)</span>) performs better than QDA (0.73), but worse than logistic regression and and LDA (0.95).</p>
</div>
<div id="task-3-11" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Task 3<a href="#task-3-11" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Fit KNN for up to <span class="math inline">\(k = 30\)</span> and then calculate the overall fraction of correct prediction.</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="#cb302-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb302-2"><a href="#cb302-2" tabindex="-1"></a>knn_k <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">30</span>, <span class="cf">function</span>(k) {</span>
<span id="cb302-3"><a href="#cb302-3" tabindex="-1"></a>  fit_knn <span class="ot">&lt;-</span> <span class="fu">knn</span>(train[, <span class="fu">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x3&quot;</span>), <span class="at">drop =</span> <span class="cn">FALSE</span>],</span>
<span id="cb302-4"><a href="#cb302-4" tabindex="-1"></a>                 test[, <span class="fu">c</span>(<span class="st">&quot;x1&quot;</span>, <span class="st">&quot;x3&quot;</span>), <span class="at">drop =</span> <span class="cn">FALSE</span>],</span>
<span id="cb302-5"><a href="#cb302-5" tabindex="-1"></a>                 train<span class="sc">$</span>y, <span class="at">k =</span> k)</span>
<span id="cb302-6"><a href="#cb302-6" tabindex="-1"></a>  <span class="fu">mean</span>(fit_knn <span class="sc">==</span> test<span class="sc">$</span>y)</span>
<span id="cb302-7"><a href="#cb302-7" tabindex="-1"></a>})</span></code></pre></div>
</div>
<div id="task-4-9" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Task 4<a href="#task-4-9" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Create a plot to observe at what value for <code>k</code> the overall fraction of correct predictions is highest.</p>
<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb303-1"><a href="#cb303-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">30</span>, knn_k, <span class="at">type =</span> <span class="st">&quot;o&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;k&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Fraction correct&quot;</span>)</span></code></pre></div>
<p><img src="03-S03-ANS_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<p>As you can see, the highest fraction of correct predictions is when <span class="math inline">\(k = 1\)</span> (also confirmed by using <code>which.max</code>).</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="#cb304-1" tabindex="-1"></a>(k <span class="ot">&lt;-</span> <span class="fu">which.max</span>(knn_k))</span></code></pre></div>
<pre><code>## [1] 1</code></pre>
<p>The behaviour you observe results from several reasons, one of which is the size of the dataset.</p>
</div>
</div>
<div id="naive-bayes-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Naive Bayes<a href="#naive-bayes-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="task-1-15" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Task 1<a href="#task-1-15" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Build a Naive Bayes classifier for the model used previously and describe the output.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="#cb306-1" tabindex="-1"></a>fit_NBayes <span class="ot">&lt;-</span> <span class="fu">naiveBayes</span>(y<span class="sc">~</span> x1 <span class="sc">+</span> x3, <span class="at">data =</span> train)</span>
<span id="cb306-2"><a href="#cb306-2" tabindex="-1"></a></span>
<span id="cb306-3"><a href="#cb306-3" tabindex="-1"></a>fit_NBayes</span></code></pre></div>
<pre><code>## 
## Naive Bayes Classifier for Discrete Predictors
## 
## Call:
## naiveBayes.default(x = X, y = Y, laplace = laplace)
## 
## A-priori probabilities:
## Y
##        0        1 
## 0.494382 0.505618 
## 
## Conditional probabilities:
##    x1
## Y          [,1]      [,2]
##   0 0.009738636 0.2458544
##   1 0.341413333 0.1529747
## 
##    x3
## Y        [,1]      [,2]
##   0 0.4628477 0.5992219
##   1 2.6057978 5.0402790</code></pre>
<ul>
<li>A-priori probabilities: i.e. prior probabilities (distribution of the classes for the response variable)<br />
</li>
<li>Conditional probabilities: parameters of the model for each predictor by class. Since the predictors are numeric variables, the parameters shown are the mean <code>[,1]</code> and standard deviation <code>[,2]</code> for the predictor values in each class.</li>
</ul>
</div>
<div id="task-2-15" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Task 2<a href="#task-2-15" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Compute the predictions, build a confusion matrix, calculate the fraction of correct predictions and interpret the results.</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="#cb308-1" tabindex="-1"></a>pred_NBayes <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit_NBayes, test, <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span></code></pre></div>
<p>And finally, generate our confusion matrix.</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="#cb309-1" tabindex="-1"></a>(t <span class="ot">&lt;-</span> <span class="fu">table</span>(pred_NBayes, test<span class="sc">$</span>y))</span></code></pre></div>
<pre><code>##            
## pred_NBayes  0  1
##           0 11  8
##           1  0  3</code></pre>
<p>Our overall fraction of correct predictions is <span class="math inline">\(0.64\)</span>.</p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="#cb311-1" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">diag</span>(t)) <span class="sc">/</span> <span class="fu">sum</span>(t)</span></code></pre></div>
<pre><code>## [1] 0.6363636</code></pre>
<p>Naive Bayes performs the worst out of all classifiers we have explored.</p>
<p><em>Based on the approaches we have implemented in this demonstration, logistic regression and LDA perform best.</em></p>
<!--chapter:end:03-S03-ANS.Rmd-->
</div>
</div>
</div>
</div>
<div id="part-section-4" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART*) Section 4<a href="#part-section-4" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="overview-3" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Overview<a href="#overview-3" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div style="color: #333; font-size: 24px; font-style: italic; text-align: justify;">
<p>Section 4: Resampling Methods</p>
</div>
<p>This section is comprised of two demonstrations and a practical adapted from exercises from the core textbook for this course (James et. al 2019) and/or from material developed by Dr. Tatjana Kecojevic, Lecturer in Social Statistics.</p>
<p>James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). <em>An Introduction to Statistical Learning with Applications in R</em>. 2nd ed.New York: Springer. <a href="https://www.statlearning.com/" class="uri">https://www.statlearning.com/</a>.</p>
<div class="ilos">
<p><strong>Learning Outcomes:</strong></p>
<ul>
<li>appreciate the importance of resampling methods in gauging model
accuracy and performance;<br />
</li>
<li>appreciate the differences between cross-validation and
bootstrapping;<br />
</li>
<li>apply cross-validation and bootstrapping in R and interpret the
output.</li>
</ul>
</div>
<p><strong>In this section, you will practice using the functions below. It is highly recommended that you explore these functions further using the Help tab in your RStudio console.</strong></p>
<table>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Function</th>
<th align="center">Description</th>
<th align="center">Package</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><code>sample()</code></td>
<td align="center">obtain a random sample with or without replacement</td>
<td align="center">base R</td>
</tr>
<tr class="even">
<td align="center"><code>lm()</code></td>
<td align="center">fit linear model</td>
<td align="center">base R</td>
</tr>
<tr class="odd">
<td align="center"><code>mean()</code></td>
<td align="center">compute the arithmetic mean</td>
<td align="center">base R</td>
</tr>
<tr class="even">
<td align="center"><code>poly()</code></td>
<td align="center">apply polynomials</td>
<td align="center">base R</td>
</tr>
<tr class="odd">
<td align="center"><code>glm()</code></td>
<td align="center">fit generalised linear model</td>
<td align="center">base R</td>
</tr>
<tr class="even">
<td align="center"><code>cv.glm()</code></td>
<td align="center">calculate estimates for LOOCV (default) or K-fold CV (by specifying <code>K</code>)</td>
<td align="center">boot</td>
</tr>
<tr class="odd">
<td align="center"><code>boot()</code></td>
<td align="center">generate bootstrap estimates</td>
<td align="center">boot</td>
</tr>
</tbody>
</table>
<!--chapter:end:04-S04-overview.Rmd-->
</div>
<div id="demonstration-1-cross-validation" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Demonstration 1: Cross-validation<a href="#demonstration-1-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>So far, we have established the importance of training and test data and you now should have a robust understanding of the differences between training and test error rates in relation to model performance. However, several challenges remain and these include:</p>
<ul>
<li>the size of the training set and whether it is sufficient to adequately train our model;<br />
</li>
<li>the size of the test set and whether it is sufficient to adequately evaluate our model;<br />
</li>
<li>the process of testing our model on just a single test set;<br />
</li>
<li>the nature of the training set.</li>
</ul>
<p>In this demonstration, you will learn more about how cross-validation provides us with ways to address this gap.</p>
<div id="data-and-variables-2" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Data and Variables<a href="#data-and-variables-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this demonstration, we will make use the <strong>Auto</strong> dataset from the core textbook (James et. al 2021). This dataset is part of the <code>ISRL2</code> package. By loading the package, the <strong>Auto</strong> dataset loads automatically.</p>
<p>You should already be familiar with the <strong>Auto</strong> dataset but below is a reminder of the variables it contains:</p>
<ul>
<li>mpg: miles per gallon</li>
<li>cylinders: Number of cylinders between 4 and 8</li>
<li>displacement: Engine displacement (cu. inches)</li>
<li>horsepower: Engine horsepower</li>
<li>weight: Vehicle weight (lbs.)</li>
<li>acceleration: Time to accelerate from 0 to 60 mph (sec.)</li>
<li>year: Model year</li>
<li>origin: Origin of car (1. American, 2. European, 3. Japanese)</li>
<li>name: Vehicle name</li>
</ul>
<p>In addition to <code>ISRL2</code>, you will also require the <code>boot</code> package which first needs to be installed. This package is required for both LOOCV and <span class="math inline">\(k\)</span>-fold CV.</p>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="#cb313-1" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb313-2"><a href="#cb313-2" tabindex="-1"></a><span class="fu">library</span>(boot) <span class="co">#don&#39;t forget to install it first</span></span></code></pre></div>
</div>
<div id="the-validation-set-approach" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">The Validation Set Approach<a href="#the-validation-set-approach" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s first consider the validation set approach.</p>
<p>The <strong>Auto</strong> dataset has a total of 392 observations and the goal is to randomly split these observations using a 50/50 ratio. We can perform this randomised split using the base R <code>sample()</code> function. The first argument within <code>sample()</code> is 392. This is the total number of observations available to us from the <strong>Auto</strong> dataset. The second argument is 196. This represents the number of observations we want to select from the total we have available which is 392. Therefore, the <code>sample()</code> function will return a vector of 196 unique integers which represents a subset of 196 indices from a total of 392.</p>
<p><em>Note: by default, the <code>sample()</code> function conducts the sampling without replacement.</em></p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="#cb314-1" tabindex="-1"></a>training_data <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">392</span>, <span class="dv">196</span>)</span>
<span id="cb314-2"><a href="#cb314-2" tabindex="-1"></a></span>
<span id="cb314-3"><a href="#cb314-3" tabindex="-1"></a>training_data</span></code></pre></div>
<pre><code>##   [1] 108 173   4 125   3 283  67 322 299   1 358  73 121 212 186 120 210  81
##  [19] 380 176 229  16 146 236 195  35 310 233 374 292 315 331 328 345 363  44
##  [37]  93  32 364 189 248  63  96 285 330 320 366 314 106   9  19 326 305 343
##  [55] 308 273 342  62  50 249  77 191 171 235 264 332 370  28 126 166 260 241
##  [73]  45 113 381 290 383 382 202 219  39 100 378 111 178 338 246 376 160  75
##  [91] 142 116 168 220 192  56 232 179 282 344 333 298 153 318  25 355 201 130
## [109] 346 218  78 234  87 145 180 252 107 377 196 371 163 109 193 117 256  52
## [127] 124  22  71 123 257 373  97 263 316 141 239 165 309  30 267 360  68 230
## [145] 158 339 362 208 300 132  36 372  91  21 159 228 324  34 144  86  57 103
## [163] 143  88 255 361 303 272 269 169  12  79 237  20 389  54 183  26 207 352
## [181] 270 149 259 216 217  72 367 348 284  27  94  47 110  69 289 258</code></pre>
<p>Now we need to tell R to fit a linear regression model using only the observations corresponding to the training set. We can do so by using the <code>subset</code> argument which tells R to only select the 196 observations that are indexed at the specific positions as defined by the <strong>training_data</strong> object. In this way, the model will be fitted using only the observations from <strong>Auto</strong> dataset that are defined by this vector of indices.</p>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="#cb316-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data =</span> Auto, <span class="at">subset =</span> training_data)</span></code></pre></div>
<p>Let’s now calculate the Mean Squared Error for the test dataset. We obtain the predictions using the<code>predict()</code> function with which you are already familiar.</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="#cb317-1" tabindex="-1"></a><span class="fu">mean</span>((Auto<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">predict</span>(fit, Auto))[<span class="sc">-</span>training_data]<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 22.50177</code></pre>
<p>Let’s try applying some transformations to our predictor <strong>horsepower</strong> using the <strong>poly()</strong> function and then calculate the test MSE values to observe how the MSE values change.</p>
<p>We first fit a second-degree polynomial regression model:</p>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb319-1"><a href="#cb319-1" tabindex="-1"></a>lm.fit2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(horsepower, <span class="dv">2</span>), <span class="at">data =</span> Auto, <span class="at">subset =</span> training_data)</span></code></pre></div>
<p>Then calculate the test MSE:</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="#cb320-1" tabindex="-1"></a><span class="fu">mean</span>((Auto<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">predict</span>(lm.fit2, Auto))[<span class="sc">-</span>training_data]<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 18.17912</code></pre>
<p>Now we fit a third-degree polynomial regression model</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="#cb322-1" tabindex="-1"></a>lm.fit3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(horsepower, <span class="dv">3</span>), <span class="at">data =</span> Auto, <span class="at">subset =</span> training_data)</span></code></pre></div>
<p>Then calculate the test MSE:</p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb323-1"><a href="#cb323-1" tabindex="-1"></a><span class="fu">mean</span>((Auto<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">predict</span>(lm.fit3, Auto))[<span class="sc">-</span>training_data]<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 18.15096</code></pre>
<p>What would happen if we choose a different training dataset instead? We can try this out by setting the seed to 2, for example.</p>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb325-1"><a href="#cb325-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)  </span>
<span id="cb325-2"><a href="#cb325-2" tabindex="-1"></a>training_data2 <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">392</span>, <span class="dv">196</span>)</span></code></pre></div>
<p>We then run a new set of models: a linear regression model, a second degree polynomial model, and a third degree polynomial model and then calculate the test MSE values for each of the three models.</p>
<p><strong>linear regression model and corresponding test MSE:</strong></p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="#cb326-1" tabindex="-1"></a>lm.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data =</span> Auto, <span class="at">subset =</span> training_data2)</span>
<span id="cb326-2"><a href="#cb326-2" tabindex="-1"></a><span class="fu">mean</span>((Auto<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">predict</span>(lm.fit, Auto))[<span class="sc">-</span>training_data2]<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 25.72651</code></pre>
<p><strong>quadratic regression model and corresponding test MSE:</strong></p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="#cb328-1" tabindex="-1"></a>lm.fit2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(horsepower, <span class="dv">2</span>), <span class="at">data =</span> Auto, <span class="at">subset =</span> training_data2)</span>
<span id="cb328-2"><a href="#cb328-2" tabindex="-1"></a><span class="fu">mean</span>((Auto<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">predict</span>(lm.fit2, Auto))[<span class="sc">-</span>training_data2]<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 20.43036</code></pre>
<p><strong>cubic regression model and corresponding test MSE:</strong></p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="#cb330-1" tabindex="-1"></a>lm.fit3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(horsepower, <span class="dv">3</span>), <span class="at">data =</span> Auto, <span class="at">subset =</span> training_data2)</span>
<span id="cb330-2"><a href="#cb330-2" tabindex="-1"></a><span class="fu">mean</span>((Auto<span class="sc">$</span>mpg <span class="sc">-</span> <span class="fu">predict</span>(lm.fit3, Auto))[<span class="sc">-</span>training_data2]<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 20.38533</code></pre>
<p>The results obtained using the second training dataset are consistent with the findings from the first training dataset whereby a model that predicts <strong>mpg</strong> using a quadratic function of <strong>horsepower</strong> performs better than a model that involves only a linear function of <strong>horsepower</strong>. Also, there is little evidence in favor of a model that uses a cubic function of <strong>horsepower</strong>.</p>
<p>Overall, the main take-away point is that if we were to choose different training sets, we will obtain somewhat different errors on the validation set.</p>
</div>
<div id="leave-one-out-cross-validation" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Leave-One-Out Cross-Validation<a href="#leave-one-out-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now, let’s consider LOOCV. The training set of data is split into two parts, with the difference that here they are not of comparable sizes as the validation set consists of a single observation and the remaining observations make up the training set.</p>
<p><img src="images/loocv.png" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p>Repeating the process <span class="math inline">\(n\)</span> times produces <span class="math inline">\(n\)</span> square errors and the estimate for the test MSE is the average of those <span class="math inline">\(n\)</span> test error estimates:</p>
<p><span class="math display">\[CV_{(n)} = \frac{1}{n} \sum_{i=1}^{n} MSE_i\]</span>
The main advantage of the LOOCV approach over a simple train and test validation is that it produces less bias. The model is fit on <span class="math inline">\(n−1\)</span> training observations, almost as many as there are in the entire data set. Furthermore, since LOOCV is performed multiple times it yields consistent results with less randomness than in a simple train and test approach.</p>
<p>One more down side to the generally high value of <span class="math inline">\(k\)</span> is that the computation side of the procedure becomes more taxing especially if the model is rather a complex one. Come to think about it, the LOOCV requires as many model fits as data points and each model fit uses a subset that is ALMOST the same size as the training.</p>
<p>In R, one way to compute the LOOCV estimate is by using functions from the <code>boot</code> package. This package nevertheless requires that the models are built using the <code>glm()</code> function, including linear models. The <code>glm()</code> function will produce identical results to <code>lm()</code> when fitting a linear model. You do not need to specify the <code>family</code> argument as you did for logistic regression since it is already set by default to <code>gaussian</code>.</p>
<p>Fitting a linear model with <code>glm()</code> is the same as with <code>lm()</code>.</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="#cb332-1" tabindex="-1"></a>glm.fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data =</span> Auto)</span></code></pre></div>
<p>We then perform LOOCV using the <code>cv.glm()</code> function from the <code>boot</code> package.</p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="#cb333-1" tabindex="-1"></a>cv.err <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(Auto, glm.fit)</span></code></pre></div>
<p>The output includes a list of several components. The component of relevance for LOOCV is <code>delta</code>.</p>
<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="#cb334-1" tabindex="-1"></a>cv.err<span class="sc">$</span>delta</span></code></pre></div>
<pre><code>## [1] 24.23151 24.23114</code></pre>
<p>The <code>delta</code> component therefore provides us with the results of the cross-validation. The first value is the raw cross-validation estimate of prediction error whilst the second value is the adjusted cross-validation estimate that compensates for the bias introduced by not using leave-one-out cross-validation. In our case, the two values are identical to two decimal places and so our cross-validation estimate for the test error is approximately <span class="math inline">\(24.23\)</span>.</p>
<p>We can repeat this procedure for increasingly complex polynomial fits. Instead of writing separate code for each model fit, we can automate the process using the <code>for()</code> function that initiates a <em>for loop</em> which iteratively fits polynomial regressions for polynomials of order <span class="math inline">\(i=1\)</span> to <span class="math inline">\(i=10\)</span>, computes the associated cross-validation error, and stores it in the <span class="math inline">\(i\)</span>th element of the vector <code>cv.error</code>.</p>
<div class="sourceCode" id="cb336"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb336-1"><a href="#cb336-1" tabindex="-1"></a>cv.error <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb336-2"><a href="#cb336-2" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) {</span>
<span id="cb336-3"><a href="#cb336-3" tabindex="-1"></a>  glm.fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(horsepower, i), <span class="at">data =</span> Auto)</span>
<span id="cb336-4"><a href="#cb336-4" tabindex="-1"></a>  cv.error[i] <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(Auto, glm.fit)<span class="sc">$</span>delta[<span class="dv">1</span>]</span>
<span id="cb336-5"><a href="#cb336-5" tabindex="-1"></a>}</span></code></pre></div>
<p>The output shows us the estimated test MSE for the linear model and polynomials up to the 10th degree.</p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb337-1"><a href="#cb337-1" tabindex="-1"></a>cv.error</span></code></pre></div>
<pre><code>##  [1] 24.23151 19.24821 19.33498 19.42443 19.03321 18.97864 18.83305 18.96115
##  [9] 19.06863 19.49093</code></pre>
<p>We can see a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials.</p>
</div>
<div id="k-fold-cross-validation" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor"><span class="math inline">\(k\)</span>-Fold Cross-Validation<a href="#k-fold-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Finally, let’s consider <span class="math inline">\(k\)</span>-fold cross-validation. This approach involves randomly dividing the set of observations into <span class="math inline">\(k\)</span> groups (known as folds) of approximately equal size. With cross-validation, the test data is held out (approximately one fifth of data), and the remaining training data is randomly divided into <span class="math inline">\(k\)</span> groups. Several different portions of this training data are used for validation, and the remaining part of the data is used for training as shown in the diagram below. Hence, a fold is treated as a validation set, and the method is fit on to the remaining <span class="math inline">\(k−1\)</span> folds. The <span class="math inline">\(k\)</span> resampled estimates of performance are summarised and used for testing and model building.</p>
<p><img src="images/k-fold.png" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p>In this illustration, the non-holdout data is divided into five portions, and we therefore call this <em>5-fold cross-validation</em>. If there had been ten blocks, it would have been 10-fold cross-validation. The model that has been built using k-fold cross-validation is then tested on the originally held out test data subset. The <span class="math inline">\(k\)</span> resampled estimates of model’s performance are summarised commonly using the mean and standard error to develop a better reasoning of its effectiveness in relation to its tuning parameters. In fact, LOOCV is a special case of k-fold CV in which <span class="math inline">\(k\)</span> is equal to <span class="math inline">\(n\)</span>:</p>
<p><span class="math display">\[CV_{(k)} = \frac{1}{k} \sum_{i=1}^{k} MSE_i\]</span>
A typical choice for <span class="math inline">\(k\)</span> is 5 or 10, but there is no formal rule. One should keep in mind when making a choice that as <span class="math inline">\(k\)</span> increases, the difference in size between the training set and the resampling subset gets smaller, causing the bias of the statistical model to become smaller. However, when compared with k-fold CV, LOOCV results in a poorer estimate as it provides an approximately unbiased estimate for the test error that is highly variable.</p>
<p>Using k-fold cross-validation increases validation sensitivity, allowing better reasoning with the model. One of the key questions is how to choose the number of folds, i.e. how big does <span class="math inline">\(k\)</span> need to be? In general, the choice of the number of folds depends on the size of the dataset. For large datasets, 5-fold cross-validation is considered to be quite accurate and when dealing with very sparse datasets, we may consider using leave-one-out in order to train on as many examples as possible.</p>
<p>There is a bias-variance trade-off associated with the choice of <span class="math inline">\(k\)</span> in k-fold cross-validation. Larger <span class="math inline">\(k\)</span>, for which training folds are closer to the total dataset, results in less bias towards overestimating the true expected error but higher variance and higher running time. We can summarise those findings as follows:</p>
<p>For a large number of folds:</p>
<ul>
<li>positives:small bias of the true error rate estimator (as a result of a very accurate estimator)<br />
</li>
<li>negatives: large variance of the true error rate estimator; computational time is large</li>
</ul>
<p>For a small number of folds:</p>
<ul>
<li>positives: small variance of the estimator; the number of experiments and, therefore, computation time are reduced</li>
<li>negatives: large bias of the estimator</li>
</ul>
<p>In practice, typical choices for k in cross-validation are <span class="math inline">\(k=5\)</span> or <span class="math inline">\(k=10\)</span>, as these values have been shown empirically to yield test error rate estimates that suffer neither from excessively high bias nor from very high variance.</p>
<p>Let’s practice <span class="math inline">\(k\)</span>-fold cross-validation. The same function (<code>cv.glm()</code>) can be used by setting the value of <span class="math inline">\(k\)</span>. By default, the value of K is equal to the number of observations in data which therefore gives us LOOCV.</p>
<p>In this example, we will use <span class="math inline">\(k = 10\)</span>. We will again perform the same approach of increasingly complex polynomial fits as we did for LOOCV. The code is identical to the one we used to LOOCV except that of course, we specified <code>K = 10</code>.</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="#cb339-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">17</span>)</span>
<span id="cb339-2"><a href="#cb339-2" tabindex="-1"></a>cv.error<span class="fl">.10</span> <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb339-3"><a href="#cb339-3" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) {</span>
<span id="cb339-4"><a href="#cb339-4" tabindex="-1"></a>  glm.fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(mpg <span class="sc">~</span> <span class="fu">poly</span>(horsepower, i), <span class="at">data =</span> Auto)</span>
<span id="cb339-5"><a href="#cb339-5" tabindex="-1"></a>  cv.error<span class="fl">.10</span>[i] <span class="ot">&lt;-</span> <span class="fu">cv.glm</span>(Auto, glm.fit, <span class="at">K =</span> <span class="dv">10</span>)<span class="sc">$</span>delta[<span class="dv">1</span>]</span>
<span id="cb339-6"><a href="#cb339-6" tabindex="-1"></a>}</span></code></pre></div>
<p>The output shows us a similar pattern to the estimated test MSE for the linear model and polynomials up to the 10th degree: we see a sharp drop between the linear and quadratic fits but once again no improvement with higher order polynomials. We can therefore conclude that a quadratic fit is suitable.</p>
<div class="sourceCode" id="cb340"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb340-1"><a href="#cb340-1" tabindex="-1"></a>cv.error<span class="fl">.10</span></span></code></pre></div>
<pre><code>##  [1] 24.27207 19.26909 19.34805 19.29496 19.03198 18.89781 19.12061 19.14666
##  [9] 18.87013 20.95520</code></pre>
<p>We saw earlier that when we LOOCV, the two values were essentially the same. In the case of <span class="math inline">\(k\)</span>-fold CV, these values differ slightly but they are still very similar to each other.</p>
<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="#cb342-1" tabindex="-1"></a><span class="fu">cv.glm</span>(Auto, glm.fit, <span class="at">K =</span> <span class="dv">10</span>)<span class="sc">$</span>delta</span></code></pre></div>
<pre><code>## [1] 19.71557 19.60616</code></pre>
<!--chapter:end:04-S04-D1.Rmd-->
</div>
</div>
<div id="demonstration-2-bootstrapping" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Demonstration 2: Bootstrapping<a href="#demonstration-2-bootstrapping" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The bootstrap method was introduced by <a href="https://projecteuclid.org/journals/annals-of-statistics/volume-7/issue-1/Bootstrap-Methods-Another-Look-at-the-Jackknife/10.1214/aos/1176344552.full">Efron in 1979</a>. Since then it has evolved considerably. Due to its intuitive nature, easily grasped by practitioners and available strong computational power necessary for its application, today bootstrapping is regarded as the indispensable tool for data analysis. The method is named after Baron Münchhausen, a fictional character who in one of the stories saved his life by pulling himself out of the bottom of a deep lake by his own hair.</p>
<p>Bootstrapping is a computationally intensive, nonparametric technique that makes probability-based inference about a population characteristic theta, <span class="math inline">\(Θ\)</span>, based on an estimator, <span class="math inline">\(\hat{Θ}\)</span>, using a sample drawn from a population. The data is resampled with replacement many times in order to obtain an empirical estimate of the sampling distribution of the statistic of interest <span class="math inline">\(Θ\)</span>. Thus, bootstrapping enables us to make inference without having to make distributional assumptions (see Efron, 1979). In machine learning, for estimation purposes the idea of bootstrapping datasets has been proposed as an alternative to CV.</p>
<p>A bootstrap sample is a random sample of the data taken with replacement <a href="https://projecteuclid.org/journals/statistical-science/volume-1/issue-1/Bootstrap-Methods-for-Standard-Errors-Confidence-Intervals-and-Other-Measures/10.1214/ss/1177013815.full">Efron and Tibshirani 1986</a>. Consequently, since samples are drawn with replacement, each bootstrap sample is likely to contain duplicate values. Bootstrapping relies on analogy between the sample and the population from which the sample was drawn, by treating the sample as if it is a population. The two key features of bootstrapping a sample with replacement are:</p>
<ul>
<li>a data point is randomly selected for the subset and returned to the original data set, so that it is still available for further selection<br />
</li>
<li>the bootstrap sample is the same size as the original data set from which it was constructed.</li>
</ul>
<p><img src="images/bootstrap.png" style="display: block; margin-left: auto; margin-right: auto;" /></p>
<p>Using uniform re-sampling with replacement, a <span class="math inline">\(B\)</span> number of training sets are produced by bootstrap to produce a performance estimate of a chosen statistical method, ie. model. The model is trained and its performance is estimated on the out-of-sample observations, as depicted in the figure above. The original observations not selected in a particular bootstrap sample are usually referred to as the out-of-bag (OOB). Hence, for a given bootstrap iteration, a model is built on the selected sample and is used to predict the out-of-bag sample. On average around 63.2% of the original sample ends up in any particular bootstrap sample <a href="https://arxiv.org/abs/1602.05822">Mendelson et al. 2016</a>. When allying the bootstrapping procedure for inferential purposes typical chose for B is in the range of a few hundreds to thousands. <a href="https://projecteuclid.org/journals/statistical-science/volume-1/issue-1/Bootstrap-Methods-for-Standard-Errors-Confidence-Intervals-and-Other-Measures/10.1214/ss/1177013815.full">Efron and Tibshirani 1986</a> indicate that <span class="math inline">\(B=50\)</span> and even <span class="math inline">\(B=25\)</span> is usually sufficient for bootstrap standard error estimates and point out that there are rare occasions for which more than <span class="math inline">\(B=200\)</span> replications are needed for estimating a standard error. In the context of using bootstraping for validation purposes the size of <span class="math inline">\(B\)</span> in the range of hundreds may be unacceptably high, and the validation process should be repeated for a specified number of folds <span class="math inline">\(k\)</span>, i.e. set <span class="math inline">\(B=k\)</span>. Hence, the bootstrap resampling with replacement procedure for ML from a data set of size <span class="math inline">\(n\)</span> can be summarised as follows:</p>
<ul>
<li>randomly select with replacement <span class="math inline">\(n\)</span> examples and use this set for training and model building<br />
</li>
<li>the remaining examples that were not selected for training are used for testing<br />
</li>
<li>repeat this process for a specific number of folds <span class="math inline">\(k\)</span><br />
</li>
<li>the true error is estimated as the average error rate on test examples</li>
</ul>
<p>As <a href="https://www.jstor.org/stable/2288636">Efron in his paper on estimating the error rates of prediction rules</a> points out, when performing statistical modelling one might want more than just an estimate of an error rate. Bootstrap methods are helpful in understanding variability all aspects of the prediction problem. In this paper he makes comprehensive comparisons between different resampling methods, drawing the conclusion that in general the bootstrap error rates tend to have less uncertainty than <span class="math inline">\(k\)</span>-fold cross-validation. One should also be aware that for small sample sizes the bias is noticeable and decreases as the sample size becomes larger as shown in <a href="https://www.math.wustl.edu/~kuffner/AlastairYoung/YoungDaniels1990.pdf">Young and Daniels’ paper Bootstrap Bias</a>
.</p>
<div id="data-and-variables-3" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Data and Variables<a href="#data-and-variables-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>How do we implement bootstrapping in R? Follow the steps outlined in this demonstration to build basic coding skills required to perform bootstrapping.</p>
<p>In this demonstration, we will make use the <strong>Portfolio</strong> and <strong>Auto</strong> datasets from the core textbook (James et. al 2021). These datasets are part of the <code>ISRL2</code> package.</p>
<p><strong>Portfolio</strong> is a dataframe contains 100 observations specifically designed to illustrate the concept of bootstrapping and the way in which it is applied in R. The dataframe contains only two variables: <em>X</em>: returns for Asset X, and <em>Y</em>: returns for Asset Y.</p>
<p>In addition to <code>ISRL2</code>, you will also require the <code>boot</code> package.</p>
<div class="sourceCode" id="cb344"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb344-1"><a href="#cb344-1" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb344-2"><a href="#cb344-2" tabindex="-1"></a><span class="fu">library</span>(boot) <span class="co">#you should have already installed this for Demo 1</span></span></code></pre></div>
</div>
<div id="estimating-the-accuracy-of-a-statistic-of-interest" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Estimating the Accuracy of a Statistic of Interest<a href="#estimating-the-accuracy-of-a-statistic-of-interest" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the advantages of bootstrapping as a resampling method is that it can be applied in almost all situations and it is quite simple to perform it with R.</p>
<p>The first step is to create a function that computes our statistic of
interest. This function should take as input the <span class="math inline">\((X,Y)\)</span> data as well as a vector indicating which observations should be used to estimate <span class="math inline">\(\alpha\)</span>. The function then outputs the estimate for <span class="math inline">\(\alpha\)</span> based on the selected observations. We will call our function <code>alpha.fn()</code>,</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="#cb345-1" tabindex="-1"></a>alpha.fn <span class="ot">&lt;-</span> <span class="cf">function</span>(data, index) {</span>
<span id="cb345-2"><a href="#cb345-2" tabindex="-1"></a>  X <span class="ot">&lt;-</span> data<span class="sc">$</span>X[index]</span>
<span id="cb345-3"><a href="#cb345-3" tabindex="-1"></a>  Y <span class="ot">&lt;-</span> data<span class="sc">$</span>Y[index]</span>
<span id="cb345-4"><a href="#cb345-4" tabindex="-1"></a>  (<span class="fu">var</span>(Y) <span class="sc">-</span> <span class="fu">cov</span>(X, Y)) <span class="sc">/</span> (<span class="fu">var</span>(X) <span class="sc">+</span> <span class="fu">var</span>(Y) <span class="sc">-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">cov</span>(X, Y))</span>
<span id="cb345-5"><a href="#cb345-5" tabindex="-1"></a>}</span></code></pre></div>
<p>For example, the following command tells <code>R</code> to estimate <span class="math inline">\(\alpha\)</span> using
all <span class="math inline">\(100\)</span> observations from our dataset.</p>
<div class="sourceCode" id="cb346"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb346-1"><a href="#cb346-1" tabindex="-1"></a><span class="fu">alpha.fn</span>(Portfolio, <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>)</span></code></pre></div>
<pre><code>## [1] 0.5758321</code></pre>
<p>We now use the <code>sample()</code> function to randomly select <span class="math inline">\(100\)</span> observations from the range <span class="math inline">\(1\)</span> to <span class="math inline">\(100\)</span>, with replacement. This is equivalent to constructing one new bootstrap dataset and recomputing <span class="math inline">\(\hat{\alpha}\)</span>
based on the new data set. We can perform this command many, many times, recording all of the corresponding estimates for <span class="math inline">\(\alpha\)</span>, and computing the resulting standard deviation.</p>
<div class="sourceCode" id="cb348"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb348-1"><a href="#cb348-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb348-2"><a href="#cb348-2" tabindex="-1"></a><span class="fu">alpha.fn</span>(Portfolio, <span class="fu">sample</span>(<span class="dv">100</span>, <span class="dv">100</span>, <span class="at">replace =</span> T))</span></code></pre></div>
<pre><code>## [1] 0.5385326</code></pre>
<p>However, the <code>boot()</code> function automates this approach. For example, we can tell R to repeat this command 1000 times and we obtain <span class="math inline">\(R=1,000\)</span> bootstrap estimates for <span class="math inline">\(\alpha\)</span>. The final output shows that using the original data,<span class="math inline">\(\hat{\alpha}=0.5758\)</span>, and that the bootstrap estimate for <span class="math inline">\({\rm SE}(\hat{\alpha})\)</span> is <span class="math inline">\(0.0897\)</span>.</p>
<div class="sourceCode" id="cb350"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb350-1"><a href="#cb350-1" tabindex="-1"></a><span class="fu">boot</span>(Portfolio, alpha.fn, <span class="at">R =</span> <span class="dv">1000</span>)</span></code></pre></div>
<pre><code>## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Portfolio, statistic = alpha.fn, R = 1000)
## 
## 
## Bootstrap Statistics :
##      original       bias    std. error
## t1* 0.5758321 0.0007959475  0.08969074</code></pre>
</div>
<div id="estimating-the-accuracy-of-a-linear-regression-model" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Estimating the Accuracy of a Linear Regression Model<a href="#estimating-the-accuracy-of-a-linear-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The bootstrap approach can be used to assess the variability of the coefficient estimates and predictions from a statistical learning method. In this example, we will assess the variability of the estimates for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, the intercept and slope terms for a simple linear regression model that uses <code>horsepower</code> to predict <code>mpg</code> in the <code>Auto</code> data set.</p>
<p>We will compare the estimates obtained using the bootstrap to those obtained using the formulas for <span class="math inline">\({\rm SE}(\hat{\beta}_0)\)</span> and <span class="math inline">\({\rm SE}(\hat{\beta}_1)\)</span>.</p>
<p>We first create a simple function, <code>boot.fn()</code>, which takes in the
<code>Auto</code> data set as well as a set of indices for the observations, and
returns the intercept and slope estimates for the linear regression model. We then apply this function to the full set of <span class="math inline">\(392\)</span> observations in order to compute the estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> on the entire data set. <em>Note that we do not need the <code>{</code> and <code>}</code> at the beginning and end of the function because it is only one line long.</em></p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="#cb352-1" tabindex="-1"></a>boot.fn <span class="ot">&lt;-</span> <span class="cf">function</span>(data, index)</span>
<span id="cb352-2"><a href="#cb352-2" tabindex="-1"></a>  <span class="fu">coef</span>(<span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data =</span> data, <span class="at">subset =</span> index))</span>
<span id="cb352-3"><a href="#cb352-3" tabindex="-1"></a><span class="fu">boot.fn</span>(Auto, <span class="dv">1</span><span class="sc">:</span><span class="dv">392</span>)</span></code></pre></div>
<pre><code>## (Intercept)  horsepower 
##  39.9358610  -0.1578447</code></pre>
<p>The <code>boot.fn()</code> function can also be used in order to create bootstrap estimates for the intercept and slope terms by randomly sampling from among the observations with replacement (as we did in the previous example with the `<strong>Portfolio</strong> dataset). We can see slight differences for the coefficient estimates each time we repeat this procedure.</p>
<div class="sourceCode" id="cb354"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb354-1"><a href="#cb354-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb354-2"><a href="#cb354-2" tabindex="-1"></a><span class="fu">boot.fn</span>(Auto, <span class="fu">sample</span>(<span class="dv">392</span>, <span class="dv">392</span>, <span class="at">replace =</span> T))</span></code></pre></div>
<pre><code>## (Intercept)  horsepower 
##  40.3404517  -0.1634868</code></pre>
<div class="sourceCode" id="cb356"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb356-1"><a href="#cb356-1" tabindex="-1"></a><span class="fu">boot.fn</span>(Auto, <span class="fu">sample</span>(<span class="dv">392</span>, <span class="dv">392</span>, <span class="at">replace =</span> T))</span></code></pre></div>
<pre><code>## (Intercept)  horsepower 
##  40.1186906  -0.1577063</code></pre>
<p>Now, we use the <code>boot()</code> function to compute the standard errors of 1,000 bootstrap estimates for the intercept and slope terms.</p>
<div class="sourceCode" id="cb358"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb358-1"><a href="#cb358-1" tabindex="-1"></a><span class="fu">boot</span>(Auto, boot.fn, <span class="dv">1000</span>)</span></code></pre></div>
<pre><code>## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Auto, statistic = boot.fn, R = 1000)
## 
## 
## Bootstrap Statistics :
##       original        bias    std. error
## t1* 39.9358610  0.0544513229 0.841289790
## t2* -0.1578447 -0.0006170901 0.007343073</code></pre>
<p>The results show that the bootstrap estimate for <span class="math inline">\({\rm SE}(\hat{\beta}_0)\)</span> is <span class="math inline">\(0.84\)</span>, and that the bootstrap estimate for <span class="math inline">\({\rm SE}(\hat{\beta}_1)\)</span> is <span class="math inline">\(0.0073\)</span>.</p>
<p>How different are those estimates from those provided by fitting the model? Let’s now compute the standard errors for the regression coefficients in a linear model (we use <code>summary</code> and then extract the coefficients using <code>$coef</code>)</p>
<div class="sourceCode" id="cb360"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb360-1"><a href="#cb360-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower, <span class="at">data =</span> Auto))<span class="sc">$</span>coef</span></code></pre></div>
<pre><code>##               Estimate  Std. Error   t value      Pr(&gt;|t|)
## (Intercept) 39.9358610 0.717498656  55.65984 1.220362e-187
## horsepower  -0.1578447 0.006445501 -24.48914  7.031989e-81</code></pre>
<p>The standard error estimates for <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> obtained from fitting the model using <code>lm()</code> are <span class="math inline">\(0.717\)</span> for the intercept and <span class="math inline">\(0.0064\)</span> for the slope.</p>
<p>Interestingly, these are somewhat different from the estimates obtained using the bootstrap.</p>
<p>Does this indicate a problem with the bootstrap? In fact, it suggests the opposite. Reflect on the formulae we covered earlier in the course and the assumptions on which these formulae rely. For example, they depend on the unknown parameter <span class="math inline">\(\sigma^2\)</span> (the noise variance) and so we estimate <span class="math inline">\(\sigma^2\)</span> using the RSS. Now although the formulas for the standard errors do not rely on the linear model being correct, the estimate for <span class="math inline">\(\sigma^2\)</span> does. If we create a scatterplot and examine the relationship between <strong>mpg</strong> and <strong>horsepower</strong>, we can see that there is a non-linear relationship and so the residuals from a linear fit will be inflated, and so will <span class="math inline">\(\hat{\sigma}^2\)</span>.</p>
<div class="sourceCode" id="cb362"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb362-1"><a href="#cb362-1" tabindex="-1"></a><span class="fu">plot</span>(Auto<span class="sc">$</span>mpg, Auto<span class="sc">$</span>horsepower)</span></code></pre></div>
<p><img src="04-S04-D2_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Secondly, the standard formulas assume (somewhat unrealistically) that the <span class="math inline">\(x_i\)</span> are fixed, and all the variability comes from the variation in the errors <span class="math inline">\(\epsilon_i\)</span>.</p>
<p>The bootstrap approach <strong>does not</strong> rely on any of these assumptions, and so it is likely giving a more accurate estimate of the standard errors of
<span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> than is the <code>summary()</code> function.</p>
<p>Given the non-linear association betwen the two variables, let’s now compute the bootstrap standard error estimates and the standard
linear regression estimates that result from fitting a <em>quadratic</em> model to the data.</p>
<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb363-1"><a href="#cb363-1" tabindex="-1"></a>boot.fn <span class="ot">&lt;-</span> <span class="cf">function</span>(data, index)</span>
<span id="cb363-2"><a href="#cb363-2" tabindex="-1"></a>  <span class="fu">coef</span>(</span>
<span id="cb363-3"><a href="#cb363-3" tabindex="-1"></a>      <span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower <span class="sc">+</span> <span class="fu">I</span>(horsepower<span class="sc">^</span><span class="dv">2</span>), </span>
<span id="cb363-4"><a href="#cb363-4" tabindex="-1"></a>        <span class="at">data =</span> data, <span class="at">subset =</span> index)</span>
<span id="cb363-5"><a href="#cb363-5" tabindex="-1"></a>    )</span>
<span id="cb363-6"><a href="#cb363-6" tabindex="-1"></a></span>
<span id="cb363-7"><a href="#cb363-7" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb363-8"><a href="#cb363-8" tabindex="-1"></a><span class="fu">boot</span>(Auto, boot.fn, <span class="dv">1000</span>)</span></code></pre></div>
<pre><code>## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Auto, statistic = boot.fn, R = 1000)
## 
## 
## Bootstrap Statistics :
##         original        bias     std. error
## t1* 56.900099702  3.511640e-02 2.0300222526
## t2* -0.466189630 -7.080834e-04 0.0324241984
## t3*  0.001230536  2.840324e-06 0.0001172164</code></pre>
<p>Since this model provides a good fit to the data, there is now a better correspondence between the bootstrap estimates and the standard estimates of <span class="math inline">\({\rm SE}(\hat{\beta}_0)\)</span>, <span class="math inline">\({\rm SE}(\hat{\beta}_1)\)</span> and <span class="math inline">\({\rm SE}(\hat{\beta}_2)\)</span>.</p>
<div class="sourceCode" id="cb365"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb365-1"><a href="#cb365-1" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(mpg <span class="sc">~</span> horsepower <span class="sc">+</span> <span class="fu">I</span>(horsepower<span class="sc">^</span><span class="dv">2</span>), <span class="at">data =</span> Auto))<span class="sc">$</span>coef</span></code></pre></div>
<pre><code>##                     Estimate   Std. Error   t value      Pr(&gt;|t|)
## (Intercept)     56.900099702 1.8004268063  31.60367 1.740911e-109
## horsepower      -0.466189630 0.0311246171 -14.97816  2.289429e-40
## I(horsepower^2)  0.001230536 0.0001220759  10.08009  2.196340e-21</code></pre>
<!--chapter:end:04-S04-D2.Rmd-->
</div>
</div>
<div id="practical" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Practical<a href="#practical" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Loading the necessary packages:</p>
<div class="sourceCode" id="cb367"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb367-1"><a href="#cb367-1" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb367-2"><a href="#cb367-2" tabindex="-1"></a><span class="fu">library</span>(boot)</span></code></pre></div>
<div id="part-i-the-validation-set-approach" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Part I: The Validation Set Approach<a href="#part-i-the-validation-set-approach" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="file">
<p>For the tasks below, you will require the <strong>Default</strong> dataset. This dataset is part of the <code>ISRL2</code> package from the core textbook (James et. al 2021).</p>
</div>
<p>The <strong>Default</strong> dataset contains 10,000 observations on four variables:</p>
<ul>
<li><p>default: A factor with levels No and Yes indicating whether the customer defaulted on their debt</p></li>
<li><p>student: A factor with levels No and Yes indicating whether the customer is a student</p></li>
<li><p>balance:The average balance that the customer has remaining on their credit card after making their monthly payment</p></li>
<li><p>income: Income of customer</p></li>
</ul>
<p>In this exercise, we will use the validation set approach to estimate the test error of a logistic regression model that predicts the probability of credit card <strong>default</strong> using <strong>income</strong> and <strong>balance</strong>.</p>
<div id="task-1-16" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-16" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the validation set approach, estimate the test error of this model following the guidance below:</p>
<ol style="list-style-type: decimal">
<li>Split the sample set into a training set and a validation set (80/20)<br />
</li>
<li>Fit a multiple logistic regression model using only the training observations.<br />
</li>
<li>Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.<br />
</li>
<li>Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.</li>
</ol>
<p><em>Don’t forget that it is important to set a random seed before splitting the data and running the analysis, to obtain consistent results.</em></p>
</div>
<div id="task-2-16" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-16" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Write a function that repeats the process above three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained. <em>Hint: use the base R function <code>replicate()</code></em>.</p>
</div>
<div id="task-3-12" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 3<a href="#task-3-12" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Write a similar function as in Task 2, but this time include the variable <strong>student</strong> in the model. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate.</p>
</div>
</div>
<div id="part-ii-leave-one-out-cross-validation" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Part II: Leave-one-out Cross-validation<a href="#part-ii-leave-one-out-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="file">
<p>For the tasks below, you will require the <strong>Weekly</strong> dataset. This dataset is part of the <code>ISRL2</code> package from the core textbook (James et. al 2021). By loading the package, the <strong>Weekly</strong> dataset will load automatically.</p>
</div>
<p>The <strong>Weekly</strong> dataset contains 1,089 observations on nine variables. For the tasks in this section, you will require the variables below. For further information on the <strong>Weekly</strong> dataset, type <code>?Weekly</code> in your console after you load the <code>ISLR2</code> package.</p>
<ul>
<li><p>Direction: A factor with levels Down and Up indicating whether the market had a positive or negative return on a given week</p></li>
<li><p>Lag1: Percentage return for previous week</p></li>
<li><p>Lag2: Percentage return for 2 weeks previous</p></li>
</ul>
<p>We will use the validation set approach to estimate the test error of a logistic regression model that predicts market movement (<strong>Direction</strong>) using percentage returns (<strong>Lag1</strong> and <strong>Lag2</strong>)</p>
<div id="task-1-17" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-17" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Fit a logistic regression model that predicts market movement using <strong>Lag1</strong> and <strong>Lag2</strong></p>
</div>
<div id="task-2-17" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-17" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use the model to predict the classification of the first observation. Was this observation correctly classified?</p>
</div>
<div id="task-3-13" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 3<a href="#task-3-13" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Write a for loop that performs each of the steps below:</p>
<ol style="list-style-type: decimal">
<li>Fit a logistic regression model using all but the <span class="math inline">\(i^{th}\)</span> observation to predict <strong>Direction</strong> using <strong>Lag1</strong> and <strong>Lag2</strong>.</li>
<li>Compute the posterior probability of upward market movement for the <span class="math inline">\(i^{th}\)</span> observation.</li>
<li>Use the posterior probability for the <span class="math inline">\(i^{th}\)</span> observation in order to predict whether or not there is upward market movement.</li>
<li>Determine whether or not an error was made in predicting the direction for the <span class="math inline">\(i^{th}\)</span> observation. If an error was made, then indicate this as a 1, and otherwise indicate it as a 0.</li>
</ol>
</div>
<div id="task-4-10" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 4<a href="#task-4-10" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Obtain the LOOCV estimate for the test error. Comment on the results.</p>
</div>
</div>
<div id="part-iii-the-bootstrap" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Part III: The Bootstrap<a href="#part-iii-the-bootstrap" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this exercise, we return to our logistic regression model that predicts the probability of credit card <strong>default</strong> using <strong>income</strong> and <strong>balance</strong> and implement the bootstrap approach to compute estimates for the standard errors of the coefficients.</p>
<div id="task-1-18" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-18" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Write a function called <code>boot.fn()</code>, that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.</p>
</div>
<div id="task-2-18" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-18" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use the <code>boot()</code> function together with the new <code>boot.fn()</code> function to estimate the standard errors of the logistic regression coefficients for income and balance. Comment on the estimated standard errors obtained using the <code>glm()</code> function and using your own bootstrap function.</p>
</div>
</div>
<div id="bonus" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Bonus<a href="#bonus" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="file">
<p>For the tasks below, you will require the <strong>Boston</strong> dataset. This dataset is part of the <code>ISRL2</code> package from the core textbook (James et. al 2021). By loading the package, the <strong>Boston</strong> dataset will load automatically.</p>
</div>
<div id="task-1-19" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-19" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Provide an estimate for the population mean of the <strong>medv</strong> variable which is the median value of owner-occupied homes.</p>
</div>
<div id="task-2-19" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-19" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Provide an estimate of the standard error of <span class="math inline">\(\hat{\mu}\)</span> and interpret the result. <strong>Hint: You can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.</strong></p>
</div>
<div id="task-3-14" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 3<a href="#task-3-14" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now estimate the standard error of <span class="math inline">\(\hat{\mu}\)</span> using the bootstrap. How does this compare to your answer from Task 2?</p>
</div>
<div id="task-4-11" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 4<a href="#task-4-11" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Based on your bootstrap estimate, provide a 95% confidence interval for the mean of medv. Compare it to the results obtained using <code>t.test(Boston$medv)</code>. Hint: You can approximate a 95% confidence interval using the formula <span class="math inline">\([\hat{\mu}−2SE(\hat{\mu}),\hat{\mu} + 2SE(\hat{\mu})]\)</span>.</p>
</div>
<div id="task-5-8" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 5<a href="#task-5-8" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Based on this data set, provide an estimate, <span class="math inline">\(\hat{\mu}_{med}\)</span> for the median value of <strong>medv</strong> in the population.</p>
</div>
<div id="task-6-6" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 6<a href="#task-6-6" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Estimate the standard error of <span class="math inline">\(\hat{\mu}_{med}\)</span> using the bootstrap. Comment on your findings.</p>
</div>
<div id="task-7-4" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 7<a href="#task-7-4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Based on this data set, provide an estimate for the tenth percentile of medv in Boston census tracts. Call this quantity <span class="math inline">\(\hat{\mu}_{0.1}\)</span>. <em>Hint: you can use the <code>quantile()</code> function.</em></p>
</div>
<div id="task-8-4" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 8<a href="#task-8-4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use the bootstrap to estimate the standard error of <span class="math inline">\(\hat{\mu}_{0.1}\)</span> and comment on your findings.</p>
<!--chapter:end:04-S04-P1.Rmd-->
</div>
</div>
</div>
<div id="answers-3" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Answers<a href="#answers-3" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Loading the necessary packages:</p>
<div class="sourceCode" id="cb368"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb368-1"><a href="#cb368-1" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb368-2"><a href="#cb368-2" tabindex="-1"></a><span class="fu">library</span>(boot)</span></code></pre></div>
<div id="part-i-the-validation-set-approach-1" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Part I: The Validation Set Approach<a href="#part-i-the-validation-set-approach-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="file">
<p>For the tasks below, you will require the <strong>Default</strong> dataset. This dataset is part of the <code>ISRL2</code> package from the core textbook (James et. al 2021). By loading the package, the <strong>Default</strong> dataset will load automatically.</p>
</div>
<p>The <strong>Default</strong> dataset contains 10,000 observations on four variables:</p>
<ul>
<li><p>default: A factor with levels No and Yes indicating whether the customer defaulted on their debt</p></li>
<li><p>student: A factor with levels No and Yes indicating whether the customer is a student</p></li>
<li><p>balance:The average balance that the customer has remaining on their credit card after making their monthly payment</p></li>
<li><p>income: Income of customer</p></li>
</ul>
<p>In this exercise, we will use the validation set approach to estimate the test error of a logistic regression model that predicts the probability of credit card <strong>default</strong> using <strong>income</strong> and <strong>balance</strong>.</p>
<div id="task-1-20" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-20" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using the validation set approach, estimate the test error of this model following the guidance below:</p>
<ol style="list-style-type: decimal">
<li>Split the sample set into a training set and a validation set (80/20)<br />
</li>
<li>Fit a multiple logistic regression model using only the training observations.<br />
</li>
<li>Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.<br />
</li>
<li>Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.</li>
</ol>
<p><em>Don’t forget that it is important to set a random seed before splitting the data and running the analysis, to obtain consistent results.</em></p>
<p><strong>1. Split the sample set into a training set and a validation set (80/20)</strong> {-}</p>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb369-1"><a href="#cb369-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb369-2"><a href="#cb369-2" tabindex="-1"></a>split_idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(Default), <span class="dv">8000</span>)</span>
<span id="cb369-3"><a href="#cb369-3" tabindex="-1"></a>default_train <span class="ot">&lt;-</span> Default[split_idx, ]</span>
<span id="cb369-4"><a href="#cb369-4" tabindex="-1"></a>default_test <span class="ot">&lt;-</span> Default[<span class="sc">-</span>split_idx, ]</span></code></pre></div>
<p><strong>2. Fit a multiple logistic regression model using only the training observations.</strong> {-}</p>
<div class="sourceCode" id="cb370"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb370-1"><a href="#cb370-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(default <span class="sc">~</span> income <span class="sc">+</span> balance, <span class="at">data =</span> default_train, </span>
<span id="cb370-2"><a href="#cb370-2" tabindex="-1"></a>           <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span></code></pre></div>
<p><strong>3. Obtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.</strong> {-}</p>
<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb371-1"><a href="#cb371-1" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">predict</span>(fit, default_test, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>)</span>
<span id="cb371-2"><a href="#cb371-2" tabindex="-1"></a></span>
<span id="cb371-3"><a href="#cb371-3" tabindex="-1"></a><span class="fu">table</span>(pred, default_test<span class="sc">$</span>default)</span></code></pre></div>
<pre><code>##      
## pred    No  Yes
##   No  1932   44
##   Yes    7   17</code></pre>
<p><strong>4. Compute the validation set error, which is the fraction of the observations in the validation set that are misclassified.</strong> {-}</p>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb373-1"><a href="#cb373-1" tabindex="-1"></a><span class="fu">mean</span>(pred <span class="sc">!=</span> default_test<span class="sc">$</span>default)</span></code></pre></div>
<pre><code>## [1] 0.0255</code></pre>
</div>
<div id="task-2-20" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-20" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Write a function that repeats the process above three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained. <em>Hint: use the base R function <code>replicate()</code></em>.</p>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb375-1"><a href="#cb375-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb375-2"><a href="#cb375-2" tabindex="-1"></a><span class="fu">replicate</span>(<span class="dv">3</span>, {</span>
<span id="cb375-3"><a href="#cb375-3" tabindex="-1"></a>  split_idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(Default), <span class="dv">8000</span>)</span>
<span id="cb375-4"><a href="#cb375-4" tabindex="-1"></a>  default_train <span class="ot">&lt;-</span> Default[split_idx, ]</span>
<span id="cb375-5"><a href="#cb375-5" tabindex="-1"></a>  default_test <span class="ot">&lt;-</span> Default[<span class="sc">-</span>split_idx, ]</span>
<span id="cb375-6"><a href="#cb375-6" tabindex="-1"></a>  </span>
<span id="cb375-7"><a href="#cb375-7" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(default <span class="sc">~</span> income <span class="sc">+</span> balance, <span class="at">data =</span> default_train, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb375-8"><a href="#cb375-8" tabindex="-1"></a>  </span>
<span id="cb375-9"><a href="#cb375-9" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">predict</span>(fit, default_test, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>)</span>
<span id="cb375-10"><a href="#cb375-10" tabindex="-1"></a>  </span>
<span id="cb375-11"><a href="#cb375-11" tabindex="-1"></a>  <span class="fu">mean</span>(pred <span class="sc">!=</span> default_test<span class="sc">$</span>default)</span>
<span id="cb375-12"><a href="#cb375-12" tabindex="-1"></a>})</span></code></pre></div>
<pre><code>## [1] 0.0255 0.0310 0.0270</code></pre>
<p>The results obtained vary (as expected) and depend on the samples allocated to training and the test dataset.</p>
</div>
<div id="task-3-15" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 3<a href="#task-3-15" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Write a similar function as in Task 2, but this time include the variable <strong>student</strong> in the model. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate.</p>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb377-1"><a href="#cb377-1" tabindex="-1"></a><span class="fu">replicate</span>(<span class="dv">3</span>, {</span>
<span id="cb377-2"><a href="#cb377-2" tabindex="-1"></a>  split_idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">nrow</span>(Default), <span class="dv">8000</span>)</span>
<span id="cb377-3"><a href="#cb377-3" tabindex="-1"></a>  default_train <span class="ot">&lt;-</span> Default[split_idx, ]</span>
<span id="cb377-4"><a href="#cb377-4" tabindex="-1"></a>  default_test <span class="ot">&lt;-</span> Default[<span class="sc">-</span>split_idx, ]</span>
<span id="cb377-5"><a href="#cb377-5" tabindex="-1"></a>  </span>
<span id="cb377-6"><a href="#cb377-6" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(default <span class="sc">~</span> income <span class="sc">+</span> balance <span class="sc">+</span> student, <span class="at">data =</span> default_train, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb377-7"><a href="#cb377-7" tabindex="-1"></a>  </span>
<span id="cb377-8"><a href="#cb377-8" tabindex="-1"></a>  pred <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(<span class="fu">predict</span>(fit, <span class="at">newdata =</span> default_test, <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="st">&quot;Yes&quot;</span>, <span class="st">&quot;No&quot;</span>)</span>
<span id="cb377-9"><a href="#cb377-9" tabindex="-1"></a>  </span>
<span id="cb377-10"><a href="#cb377-10" tabindex="-1"></a>  <span class="fu">mean</span>(pred <span class="sc">!=</span> default_test<span class="sc">$</span>default)</span>
<span id="cb377-11"><a href="#cb377-11" tabindex="-1"></a>})</span></code></pre></div>
<pre><code>## [1] 0.0265 0.0305 0.0275</code></pre>
<p>Including the <strong>student</strong> variable does not seem to make a substantial improvement to the test error.</p>
</div>
</div>
<div id="part-ii-leave-one-out-cross-validation-1" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Part II: Leave-one-out Cross-validation<a href="#part-ii-leave-one-out-cross-validation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="file">
<p>For the tasks below, you will require the <strong>Weekly</strong> dataset. This dataset is part of the <code>ISRL2</code> package from the core textbook (James et. al 2021). By loading the package, the <strong>Default</strong> dataset will load automatically.</p>
</div>
<p>The <strong>Weekly</strong> dataset contains 1,089 observations on nine variables. For the tasks in this section, you will require the variables below. For further information on the <strong>Weekly</strong> dataset, type <code>?Weekly</code> in your console after you load the <code>ISLR2</code> package.</p>
<ul>
<li><p>Direction: A factor with levels Down and Up indicating whether the market had a positive or negative return on a given week</p></li>
<li><p>Lag1: Percentage return for previous week</p></li>
<li><p>Lag2: Percentage return for 2 weeks previous</p></li>
</ul>
<p>We will use the validation set approach to estimate the test error of a logistic regression model that predicts market movement (<strong>Direction</strong>) using percentage returns (<strong>Lag1</strong> and <strong>Lag2</strong>)</p>
<div id="task-1-21" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-21" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Fit a logistic regression model that predicts market movement using <strong>Lag1</strong> and <strong>Lag2</strong></p>
<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb379-1"><a href="#cb379-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2, <span class="at">data =</span> Weekly[<span class="sc">-</span><span class="dv">1</span>, ], <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span></code></pre></div>
</div>
<div id="task-2-21" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-21" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use the model to predict the classification of the first observation. Was this observation correctly classified?</p>
<div class="sourceCode" id="cb380"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb380-1"><a href="#cb380-1" tabindex="-1"></a><span class="fu">predict</span>(fit, <span class="at">newdata =</span> Weekly[<span class="dv">1</span>, , <span class="at">drop =</span> <span class="cn">FALSE</span>], <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">&gt;</span> <span class="fl">0.5</span></span></code></pre></div>
<pre><code>##    1 
## TRUE</code></pre>
<p>No, the observation was incorrectly classified.</p>
</div>
<div id="task-3-16" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 3<a href="#task-3-16" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Write a for loop that performs each of the steps below:</p>
<ol style="list-style-type: decimal">
<li>Fit a logistic regression model using all but the <span class="math inline">\(i^{th}\)</span> observation to predict <strong>Direction</strong> using <strong>Lag1</strong> and <strong>Lag2</strong>.</li>
<li>Compute the posterior probability of upward market movement for the <span class="math inline">\(i^{th}\)</span> observation.</li>
<li>Use the posterior probability for the <span class="math inline">\(i^{th}\)</span> observation in order to predict whether or not there is upward market movement.</li>
<li>Determine whether or not an error was made in predicting the direction for the <span class="math inline">\(i^{th}\)</span> observation. If an error was made, then indicate this as a 1, and otherwise indicate it as a 0.</li>
</ol>
<div class="sourceCode" id="cb382"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb382-1"><a href="#cb382-1" tabindex="-1"></a>error <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">nrow</span>(Weekly))</span>
<span id="cb382-2"><a href="#cb382-2" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(Weekly)) {</span>
<span id="cb382-3"><a href="#cb382-3" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(Direction <span class="sc">~</span> Lag1 <span class="sc">+</span> Lag2, <span class="at">data =</span> Weekly[<span class="sc">-</span>i, ], <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb382-4"><a href="#cb382-4" tabindex="-1"></a>  p <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, <span class="at">newdata =</span> Weekly[i, , <span class="at">drop =</span> <span class="cn">FALSE</span>], <span class="at">type =</span> <span class="st">&quot;response&quot;</span>) <span class="sc">&gt;</span> <span class="fl">0.5</span></span>
<span id="cb382-5"><a href="#cb382-5" tabindex="-1"></a>  error[i] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(p, <span class="st">&quot;Down&quot;</span>, <span class="st">&quot;Up&quot;</span>) <span class="sc">==</span> Weekly<span class="sc">$</span>Direction[i]</span>
<span id="cb382-6"><a href="#cb382-6" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="task-4-12" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 4<a href="#task-4-12" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Obtain the LOOCV estimate for the test error. Comment on the results.</p>
<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb383-1"><a href="#cb383-1" tabindex="-1"></a><span class="fu">mean</span>(error)</span></code></pre></div>
<pre><code>## [1] 0.4499541</code></pre>
<p>The LOOCV test error rate is 45% which implies that our predictions are marginally more often correct than not.</p>
</div>
</div>
<div id="part-iii-the-bootstrap-1" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Part III: The Bootstrap<a href="#part-iii-the-bootstrap-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this exercise, we return to our logistic regression model that predicts the probability of credit card <strong>default</strong> using <strong>income</strong> and <strong>balance</strong> and implement the bootstrap approach to compute estimates for the standard errors of the coefficients.</p>
<div class="sourceCode" id="cb385"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb385-1"><a href="#cb385-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(default <span class="sc">~</span> income <span class="sc">+</span> balance, <span class="at">data =</span> Default, <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb385-2"><a href="#cb385-2" tabindex="-1"></a></span>
<span id="cb385-3"><a href="#cb385-3" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = default ~ income + balance, family = &quot;binomial&quot;, 
##     data = Default)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4725  -0.1444  -0.0574  -0.0211   3.7245  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) -1.154e+01  4.348e-01 -26.545  &lt; 2e-16 ***
## income       2.081e-05  4.985e-06   4.174 2.99e-05 ***
## balance      5.647e-03  2.274e-04  24.836  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2920.6  on 9999  degrees of freedom
## Residual deviance: 1579.0  on 9997  degrees of freedom
## AIC: 1585
## 
## Number of Fisher Scoring iterations: 8</code></pre>
<div id="task-1-22" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-22" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Write a function called <code>boot.fn()</code>, that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.</p>
<div class="sourceCode" id="cb387"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb387-1"><a href="#cb387-1" tabindex="-1"></a>boot.fn <span class="ot">&lt;-</span> <span class="cf">function</span>(x, i) {</span>
<span id="cb387-2"><a href="#cb387-2" tabindex="-1"></a>  fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(default <span class="sc">~</span> income <span class="sc">+</span> balance, <span class="at">data =</span> x[i, ], <span class="at">family =</span> <span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb387-3"><a href="#cb387-3" tabindex="-1"></a>  <span class="fu">coef</span>(fit)[<span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb387-4"><a href="#cb387-4" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="task-2-22" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-22" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use the <code>boot()</code> function together with the new <code>boot.fn()</code> function to estimate the standard errors of the logistic regression coefficients for income and balance. Comment on the estimated standard errors obtained using the <code>glm()</code> function and using your own bootstrap function.</p>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb388-1"><a href="#cb388-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb388-2"><a href="#cb388-2" tabindex="-1"></a><span class="fu">boot</span>(Default, boot.fn, <span class="at">R =</span> <span class="dv">1000</span>)</span></code></pre></div>
<pre><code>## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Default, statistic = boot.fn, R = 1000)
## 
## 
## Bootstrap Statistics :
##         original       bias     std. error
## t1* 2.080898e-05 2.737444e-08 5.073444e-06
## t2* 5.647103e-03 1.176249e-05 2.299133e-04</code></pre>
<p>The standard errors obtained by bootstrapping are similar to those estimated by <code>glm()</code>.</p>
</div>
</div>
<div id="bonus-1" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Bonus<a href="#bonus-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="file">
<p>For the tasks below, you will require the <strong>Boston</strong> dataset. This dataset is part of the <code>ISRL2</code> package from the core textbook (James et. al 2021). By loading the package, the <strong>Boston</strong> dataset will load automatically.</p>
</div>
<div id="task-1-23" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 1<a href="#task-1-23" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Provide an estimate for the population mean of the <strong>medv</strong> variable which is the median value of owner-occupied homes.</p>
<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb390-1"><a href="#cb390-1" tabindex="-1"></a>(mu <span class="ot">&lt;-</span> <span class="fu">mean</span>(Boston<span class="sc">$</span>medv))</span></code></pre></div>
<pre><code>## [1] 22.53281</code></pre>
</div>
<div id="task-2-23" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 2<a href="#task-2-23" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Provide an estimate of the standard error of <span class="math inline">\(\hat{\mu}\)</span> and interpret the result. <strong>Hint: You can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.</strong></p>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb392-1"><a href="#cb392-1" tabindex="-1"></a><span class="fu">sd</span>(Boston<span class="sc">$</span>medv) <span class="sc">/</span> <span class="fu">sqrt</span>(<span class="fu">length</span>(Boston<span class="sc">$</span>medv))</span></code></pre></div>
<pre><code>## [1] 0.4088611</code></pre>
</div>
<div id="task-3-17" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 3<a href="#task-3-17" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now estimate the standard error of <span class="math inline">\(\hat{\mu}\)</span> using the bootstrap. How does this compare to your answer from Task 2?</p>
<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb394-1"><a href="#cb394-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb394-2"><a href="#cb394-2" tabindex="-1"></a>(bs <span class="ot">&lt;-</span> <span class="fu">boot</span>(Boston<span class="sc">$</span>medv, <span class="cf">function</span>(v, i) <span class="fu">mean</span>(v[i]), <span class="dv">10000</span>))</span></code></pre></div>
<pre><code>## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Boston$medv, statistic = function(v, i) mean(v[i]), 
##     R = 10000)
## 
## 
## Bootstrap Statistics :
##     original      bias    std. error
## t1* 22.53281 0.002175751   0.4029139</code></pre>
<p>The standard error using the bootstrap (0.403) is very close to that obtained from the formula above (0.409).</p>
</div>
<div id="task-4-13" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 4<a href="#task-4-13" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Based on your bootstrap estimate, provide a 95% confidence interval for the mean of medv. Compare it to the results obtained using <code>t.test(Boston$medv)</code>. Hint: You can approximate a 95% confidence interval using the formula <span class="math inline">\([\hat{\mu}−2SE(\hat{\mu}),\hat{\mu} + 2SE(\hat{\mu})]\)</span>.</p>
<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb396-1"><a href="#cb396-1" tabindex="-1"></a>se <span class="ot">&lt;-</span> <span class="fu">sd</span>(bs<span class="sc">$</span>t)</span>
<span id="cb396-2"><a href="#cb396-2" tabindex="-1"></a><span class="fu">c</span>(mu <span class="sc">-</span> <span class="dv">2</span> <span class="sc">*</span> se, mu <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> se)</span></code></pre></div>
<pre><code>## [1] 21.72698 23.33863</code></pre>
</div>
<div id="task-5-9" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 5<a href="#task-5-9" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Based on this data set, provide an estimate, <span class="math inline">\(\hat{\mu}_{med}\)</span> for the median value of <strong>medv</strong> in the population.</p>
<div class="sourceCode" id="cb398"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb398-1"><a href="#cb398-1" tabindex="-1"></a><span class="fu">median</span>(Boston<span class="sc">$</span>medv)</span></code></pre></div>
<pre><code>## [1] 21.2</code></pre>
</div>
<div id="task-6-7" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 6<a href="#task-6-7" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Estimate the standard error of <span class="math inline">\(\hat{\mu}_{med}\)</span> using the bootstrap. Comment on your findings.</p>
<div class="sourceCode" id="cb400"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb400-1"><a href="#cb400-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb400-2"><a href="#cb400-2" tabindex="-1"></a><span class="fu">boot</span>(Boston<span class="sc">$</span>medv, <span class="cf">function</span>(v, i) <span class="fu">median</span>(v[i]), <span class="dv">10000</span>)</span></code></pre></div>
<pre><code>## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Boston$medv, statistic = function(v, i) median(v[i]), 
##     R = 10000)
## 
## 
## Bootstrap Statistics :
##     original   bias    std. error
## t1*     21.2 -0.01331   0.3744634</code></pre>
<p>The estimated standard error of the median is 0.374. This is lower than the standard error of the mean.</p>
</div>
<div id="task-7-5" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 7<a href="#task-7-5" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Based on this data set, provide an estimate for the tenth percentile of medv in Boston census tracts. Call this quantity <span class="math inline">\(\hat{\mu}_{0.1}\)</span>. <em>Hint: you can use the <code>quantile()</code> function.</em></p>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb402-1"><a href="#cb402-1" tabindex="-1"></a><span class="fu">quantile</span>(Boston<span class="sc">$</span>medv, <span class="fl">0.1</span>)</span></code></pre></div>
<pre><code>##   10% 
## 12.75</code></pre>
</div>
<div id="task-8-5" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Task 8<a href="#task-8-5" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Use the bootstrap to estimate the standard error of <span class="math inline">\(\hat{\mu}_{0.1}\)</span> and comment on your findings.</p>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb404-1"><a href="#cb404-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb404-2"><a href="#cb404-2" tabindex="-1"></a><span class="fu">boot</span>(Boston<span class="sc">$</span>medv, <span class="cf">function</span>(v, i) <span class="fu">quantile</span>(v[i], <span class="fl">0.1</span>), <span class="dv">10000</span>)</span></code></pre></div>
<pre><code>## 
## ORDINARY NONPARAMETRIC BOOTSTRAP
## 
## 
## Call:
## boot(data = Boston$medv, statistic = function(v, i) quantile(v[i], 
##     0.1), R = 10000)
## 
## 
## Bootstrap Statistics :
##     original   bias    std. error
## t1*    12.75 0.013405    0.497298</code></pre>
<p>We get a standard error of about 0.5. This is higher than the standard error of the median. Nevertheless the standard error is still quite small, thus we can be fairly confidence about the value of the 10th percentile.</p>
<!--chapter:end:04-S04-ANS.Rmd-->
</div>
</div>
</div>
<div id="part-section-5" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART*) Section 5<a href="#part-section-5" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="overview-4" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Overview<a href="#overview-4" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div style="color: #333; font-size: 24px; font-style: italic; text-align: justify;">
<p>Section 5: Data Science in Practice</p>
</div>
<p>This section is comprised of two practicals and two demonstrations.</p>
<p>The two practicals were adapted from demonstrations created by Dr.
Tatjana Kecojevic, Lecturer in Social Statistics.</p>
<p>Demonstration 1 was developed by Dr. George Wood, Lecturer in Social
Statistics whilst Demonstration 2 was adapted from exercises from the
core textbook for this course (James et. al 2019).</p>
<p>James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). <em>An
Introduction to Statistical Learning with Applications in R</em>. 2nd ed.New
York: Springer. <a href="https://www.statlearning.com/" class="uri">https://www.statlearning.com/</a>.</p>
<div class="ilos">
<p><strong>Learning Outcomes:</strong></p>
<ul>
<li><p>create suitable visualisations to describe univariate and bivariate
data;</p></li>
<li><p>build simple and multiple linear regression models with quantitative
and qualitative predictors and interpret the results;</p></li>
<li><p>assess model fit and evaluate the importance of individual
predictors;</p></li>
<li><p>make decisions about the structure of the ‘final’ model;</p></li>
<li><p>build logistic regression models and interpret the results;</p></li>
<li><p>calculate false positive and false negative rates;</p></li>
<li><p>build and interpret regression and classification trees;</p></li>
<li><p>apply ensemble methods (bagging, random forests, and boosting) and
interpret output.</p></li>
</ul>
</div>
<p><strong>In this section, you will practice using the functions below. It is
highly recommended that you explore these functions further using the
Help tab in your RStudio console.</strong></p>
<table>
<colgroup>
<col width="23%" />
<col width="38%" />
<col width="38%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Function</th>
<th align="center">Description</th>
<th align="center">Package</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><code>head()</code></td>
<td align="center">obtain the first parts of a data object</td>
<td align="center">utils</td>
</tr>
<tr class="even">
<td align="center"><code>attach()</code></td>
<td align="center">attach data to R search path</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>glimpse()</code></td>
<td align="center">obtain a glimpse of the data</td>
<td align="center">tidyverse (dplyr)</td>
</tr>
<tr class="even">
<td align="center"><code>summary()</code></td>
<td align="center">produce summary results</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>boxplot()</code></td>
<td align="center">produce box plots</td>
<td align="center">graphics</td>
</tr>
<tr class="even">
<td align="center"><code>plot()</code></td>
<td align="center">plot data objects</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>lm()</code></td>
<td align="center">fit linear model</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>qf()</code></td>
<td align="center">generation for the F distribution</td>
<td align="center">stats</td>
</tr>
<tr class="odd">
<td align="center"><code>qt()</code></td>
<td align="center">generation for the t distribution</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>attributes()</code></td>
<td align="center">acces object attributes</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>unclass()</code></td>
<td align="center">remove clas attribute</td>
<td align="center">base</td>
</tr>
<tr class="even">
<td align="center"><code>options()</code></td>
<td align="center">global options settings</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>contrasts()</code></td>
<td align="center">set/view contrasts of factors</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>select()</code></td>
<td align="center">keep/drop columns</td>
<td align="center">tidyverse (dplyr)</td>
</tr>
<tr class="odd">
<td align="center"><code>group_by()</code></td>
<td align="center">group data according to specific variables</td>
<td align="center">tidyverse (dplyr)</td>
</tr>
<tr class="even">
<td align="center"><code>summarise()</code></td>
<td align="center">summarise each group down to one row</td>
<td align="center">tidyverse (dplyr)</td>
</tr>
<tr class="odd">
<td align="center"><code>ggpairs()</code></td>
<td align="center">ggplot2 generalised pairs plot</td>
<td align="center">GGally</td>
</tr>
<tr class="even">
<td align="center"><code>mutate()</code></td>
<td align="center">create, modify, delete columns`</td>
<td align="center">tidyverse (dplyr)</td>
</tr>
<tr class="odd">
<td align="center"><code>read.csv()</code></td>
<td align="center">read file in table format and create data frame</td>
<td align="center">utils</td>
</tr>
<tr class="even">
<td align="center"><code>read_csv()</code></td>
<td align="center">read delimited file into tibble</td>
<td align="center">tidyverse (readr)</td>
</tr>
<tr class="odd">
<td align="center"><code>ggplot()</code></td>
<td align="center">create new ggplot</td>
<td align="center">ggplot2</td>
</tr>
<tr class="even">
<td align="center"><code>rename()</code></td>
<td align="center">rename columns</td>
<td align="center">tidyverse (dplyr)</td>
</tr>
<tr class="odd">
<td align="center"><code>filter()</code></td>
<td align="center">keep rows that match a condition</td>
<td align="center">tidyverse (dplyr)</td>
</tr>
<tr class="even">
<td align="center"><code>glue()</code></td>
<td align="center">format and interpolate string</td>
<td align="center">glue</td>
</tr>
<tr class="odd">
<td align="center"><code>bind_rows()</code></td>
<td align="center">bind multiple data frames by row</td>
<td align="center">tidyverse (dplyr)</td>
</tr>
<tr class="even">
<td align="center"><code>table()</code></td>
<td align="center">build contingency table</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>glm()</code></td>
<td align="center">fit generalised linear model</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>exp()</code></td>
<td align="center">compute exponential function</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>coef()</code></td>
<td align="center">extract model coefficients</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>count()</code></td>
<td align="center">count observations</td>
<td align="center">tidyverse (dplyr)</td>
</tr>
<tr class="odd">
<td align="center"><code>ifelse()</code></td>
<td align="center">conditional element selection</td>
<td align="center">base</td>
</tr>
<tr class="even">
<td align="center"><code>sample()</code></td>
<td align="center">take a sample of a given size with or without replacement</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>set.seed()</code></td>
<td align="center">(pseudo) random number generation</td>
<td align="center">base</td>
</tr>
<tr class="even">
<td align="center"><code>tree()</code></td>
<td align="center">fit classification or regression tree</td>
<td align="center">tree</td>
</tr>
<tr class="odd">
<td align="center"><code>text()</code></td>
<td align="center">add text to plot</td>
<td align="center">graphics</td>
</tr>
<tr class="even">
<td align="center"><code>predict()</code></td>
<td align="center">obtain predictions from model fitting functions</td>
<td align="center">stats</td>
</tr>
<tr class="odd">
<td align="center"><code>cv.tree()</code></td>
<td align="center">cross-validation for choosing tree complexity</td>
<td align="center">tree</td>
</tr>
<tr class="even">
<td align="center"><code>par()</code></td>
<td align="center">set graphical parameters</td>
<td align="center">graphics</td>
</tr>
<tr class="odd">
<td align="center"><code>prune.misclass()</code></td>
<td align="center">cost-complexity pruning of tree object</td>
<td align="center">tree</td>
</tr>
<tr class="even">
<td align="center"><code>randomForest()</code></td>
<td align="center">classification or regression with random forest</td>
<td align="center">randomForest</td>
</tr>
<tr class="odd">
<td align="center"><code>varImpPlot()</code></td>
<td align="center">variable importance plot</td>
<td align="center">randomForest</td>
</tr>
<tr class="even">
<td align="center"><code>importance()</code></td>
<td align="center">extract variable importance measure</td>
<td align="center">randomForest</td>
</tr>
<tr class="odd">
<td align="center"><code>mean()</code></td>
<td align="center">calculated arithmetic mean</td>
<td align="center">base</td>
</tr>
<tr class="even">
<td align="center"><code>gbm()</code></td>
<td align="center">fit generalised boosted regression models</td>
<td align="center">gbm</td>
</tr>
</tbody>
</table>
<!--chapter:end:05-S05-overview.Rmd-->
</div>
<div id="practical-1-academic-salary" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Practical 1: Academic Salary<a href="#practical-1-academic-salary" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>This practical is based on a demonstration created by Dr. Tatjana Kecojevic, Lecturer in Social Statistics.</em></p>
<div class="file">
<p>For the tasks below, you will require the <strong>Salaries</strong> dataset. This
dataset is part of the <code>carData</code> R package.</p>
<p>To access the dataset, load the <code>carData</code> package (make sure to first
install the package).</p>
<p>You will also require the <code>GGally</code> package; please make sure to install it.</p>
</div>
<p><strong>Salaries</strong> is a data frame with 397 observations. This dataset
consists of nine-month academic salary for Assistant Professors,
Associate Professors and Professors in a college in the U.S to monitor
salary differences between male and female faculty members. The data are
from 2008-09.</p>
<p>There are six variables:</p>
<table>
<colgroup>
<col width="26%" />
<col width="73%" />
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Variable Name</strong></td>
<td><strong>Variable Description</strong></td>
</tr>
<tr class="even">
<td>rank</td>
<td>a factor with levels = AssocProf, AsstProf, Prof</td>
</tr>
<tr class="odd">
<td>discipline</td>
<td>a factor with levels A = theoretical departments) or B = applied departments</td>
</tr>
<tr class="even">
<td>yrs.since.phd</td>
<td>years since PhD</td>
</tr>
<tr class="odd">
<td>yrs.service</td>
<td>years of service</td>
</tr>
<tr class="even">
<td>sex</td>
<td>a factor with levels Female and Male</td>
</tr>
<tr class="odd">
<td>salary</td>
<td>nine-month salary, in dollars.</td>
</tr>
</tbody>
</table>
<p>Let’s first load the packages:</p>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb406-1"><a href="#cb406-1" tabindex="-1"></a><span class="fu">library</span>(carData)</span>
<span id="cb406-2"><a href="#cb406-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb406-3"><a href="#cb406-3" tabindex="-1"></a><span class="fu">library</span>(GGally)</span></code></pre></div>
<p>Once you load the <code>carData</code> package, the <strong>Salaries</strong> dataset will be
‘loaded’ too and can be accessed without needing to assign it to a
separate object.</p>
<div class="sourceCode" id="cb407"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb407-1"><a href="#cb407-1" tabindex="-1"></a><span class="fu">head</span>(Salaries)</span></code></pre></div>
<pre><code>##        rank discipline yrs.since.phd yrs.service  sex salary
## 1      Prof          B            19          18 Male 139750
## 2      Prof          B            20          16 Male 173200
## 3  AsstProf          B             4           3 Male  79750
## 4      Prof          B            45          39 Male 115000
## 5      Prof          B            40          41 Male 141500
## 6 AssocProf          B             6           6 Male  97000</code></pre>
<p>As usual, we can access variables within the dataset by indexing them.</p>
<div class="sourceCode" id="cb409"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb409-1"><a href="#cb409-1" tabindex="-1"></a>Salaries<span class="sc">$</span>salary</span></code></pre></div>
<pre><code>##   [1] 139750 173200  79750 115000 141500  97000 175000 147765 119250 129000
##  [11] 119800  79800  77700  78000 104800 117150 101000 103450 124750 137000
##  [21]  89565 102580  93904 113068  74830 106294 134885  82379  77000 118223
##  [31] 132261  79916 117256  80225  80225  77000 155750  86373 125196 100938
##  [41] 146500  93418 101299 231545  94384 114778  98193 151768 140096  70768
##  [51] 126621 108875  74692 106639 103760  83900 117704  90215 100135  75044
##  [61]  90304  75243 109785 103613  68404 100522 101000  99418 111512  91412
##  [71] 126320 146856 100131  92391 113398  73266 150480 193000  86100  84240
##  [81] 150743 135585 144640  88825 122960 132825 152708  88400 172272 107008
##  [91]  97032 105128 105631 166024 123683  84000  95611 129676 102235 106689
## [101] 133217 126933 153303 127512  83850 113543  82099  82600  81500 131205
## [111] 112429  82100  72500 104279 105000 120806 148500 117515  72500  73500
## [121] 115313 124309  97262  62884  96614  78162 155500  72500 113278  73000
## [131]  83001  76840  77500  72500 168635 136000 108262 105668  73877 152664
## [141] 100102  81500 106608  89942 112696 119015  92000 156938 144651  95079
## [151] 128148  92000 111168 103994  92000 118971 113341  88000  95408 137167
## [161]  89516 176500  98510  89942  88795 105890 167284 130664 101210 181257
## [171]  91227 151575  93164 134185 105000 111751  95436 100944 147349  92000
## [181] 142467 141136 100000 150000 101000 134000 103750 107500 106300 153750
## [191] 180000 133700 122100  86250  90000 113600  92700  92000 189409 114500
## [201]  92700 119700 160400 152500 165000  96545 162200 120000  91300 163200
## [211]  91000 111350 128400 126200 118700 145350 146000 105350 109650 119500
## [221] 170000 145200 107150 129600  87800 122400  63900  70000  88175 133900
## [231]  91000  73300 148750 117555  69700  81700 114000  63100  77202  96200
## [241]  69200 122875 102600 108200  84273  90450  91100 101100 128800 204000
## [251] 109000 102000 132000  77500 116450  83000 140300  74000  73800  92550
## [261]  88600 107550 121200 126000  99000 134800 143940 104350  89650 103700
## [271] 143250 194800  73000  74000  78500  93000 107200 163200 107100 100600
## [281] 136500 103600  57800 155865  88650  81800 115800  85000 150500  74000
## [291] 174500 168500 183800 104800 107300  97150 126300 148800  72300  70700
## [301]  88600 127100 170500 105260 144050 111350  74500 122500  74000 166800
## [311]  92050 108100  94350 100351 146800  84716  71065  67559 134550 135027
## [321] 104428  95642 126431 161101 162221  84500 124714 151650  99247 134778
## [331] 192253 116518 105450 145098 104542 151445  98053 145000 128464 137317
## [341] 106231 124312 114596 162150 150376 107986 142023 128250  80139 144309
## [351] 186960  93519 142500 138000  83600 145028  88709 107309 109954  78785
## [361] 121946 109646 138771  81285 205500 101036 115435 108413 131950 134690
## [371]  78182 110515 109707 136660 103275 103649  74856  77081 150680 104121
## [381]  75996 172505  86895 105000 125192 114330 139219 109305 119450 186023
## [391] 166605 151292 103106 150564 101738  95329  81035</code></pre>
<p>However, if we want to access variables within the dataset without
needing to index them we can use the base R <code>attach()</code> function.</p>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb411-1"><a href="#cb411-1" tabindex="-1"></a><span class="fu">attach</span>(Salaries)</span></code></pre></div>
<p>So now, we can call on the variables from the dataset directly.</p>
<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb412-1"><a href="#cb412-1" tabindex="-1"></a>salary</span></code></pre></div>
<pre><code>##   [1] 139750 173200  79750 115000 141500  97000 175000 147765 119250 129000
##  [11] 119800  79800  77700  78000 104800 117150 101000 103450 124750 137000
##  [21]  89565 102580  93904 113068  74830 106294 134885  82379  77000 118223
##  [31] 132261  79916 117256  80225  80225  77000 155750  86373 125196 100938
##  [41] 146500  93418 101299 231545  94384 114778  98193 151768 140096  70768
##  [51] 126621 108875  74692 106639 103760  83900 117704  90215 100135  75044
##  [61]  90304  75243 109785 103613  68404 100522 101000  99418 111512  91412
##  [71] 126320 146856 100131  92391 113398  73266 150480 193000  86100  84240
##  [81] 150743 135585 144640  88825 122960 132825 152708  88400 172272 107008
##  [91]  97032 105128 105631 166024 123683  84000  95611 129676 102235 106689
## [101] 133217 126933 153303 127512  83850 113543  82099  82600  81500 131205
## [111] 112429  82100  72500 104279 105000 120806 148500 117515  72500  73500
## [121] 115313 124309  97262  62884  96614  78162 155500  72500 113278  73000
## [131]  83001  76840  77500  72500 168635 136000 108262 105668  73877 152664
## [141] 100102  81500 106608  89942 112696 119015  92000 156938 144651  95079
## [151] 128148  92000 111168 103994  92000 118971 113341  88000  95408 137167
## [161]  89516 176500  98510  89942  88795 105890 167284 130664 101210 181257
## [171]  91227 151575  93164 134185 105000 111751  95436 100944 147349  92000
## [181] 142467 141136 100000 150000 101000 134000 103750 107500 106300 153750
## [191] 180000 133700 122100  86250  90000 113600  92700  92000 189409 114500
## [201]  92700 119700 160400 152500 165000  96545 162200 120000  91300 163200
## [211]  91000 111350 128400 126200 118700 145350 146000 105350 109650 119500
## [221] 170000 145200 107150 129600  87800 122400  63900  70000  88175 133900
## [231]  91000  73300 148750 117555  69700  81700 114000  63100  77202  96200
## [241]  69200 122875 102600 108200  84273  90450  91100 101100 128800 204000
## [251] 109000 102000 132000  77500 116450  83000 140300  74000  73800  92550
## [261]  88600 107550 121200 126000  99000 134800 143940 104350  89650 103700
## [271] 143250 194800  73000  74000  78500  93000 107200 163200 107100 100600
## [281] 136500 103600  57800 155865  88650  81800 115800  85000 150500  74000
## [291] 174500 168500 183800 104800 107300  97150 126300 148800  72300  70700
## [301]  88600 127100 170500 105260 144050 111350  74500 122500  74000 166800
## [311]  92050 108100  94350 100351 146800  84716  71065  67559 134550 135027
## [321] 104428  95642 126431 161101 162221  84500 124714 151650  99247 134778
## [331] 192253 116518 105450 145098 104542 151445  98053 145000 128464 137317
## [341] 106231 124312 114596 162150 150376 107986 142023 128250  80139 144309
## [351] 186960  93519 142500 138000  83600 145028  88709 107309 109954  78785
## [361] 121946 109646 138771  81285 205500 101036 115435 108413 131950 134690
## [371]  78182 110515 109707 136660 103275 103649  74856  77081 150680 104121
## [381]  75996 172505  86895 105000 125192 114330 139219 109305 119450 186023
## [391] 166605 151292 103106 150564 101738  95329  81035</code></pre>
<div id="part-i" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Part I<a href="#part-i" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="exploring-the-data" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Exploring the data<a href="#exploring-the-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb414"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb414-1"><a href="#cb414-1" tabindex="-1"></a><span class="fu">glimpse</span>(Salaries)</span></code></pre></div>
<pre><code>## Rows: 397
## Columns: 6
## $ rank          &lt;fct&gt; Prof, Prof, AsstProf, Prof, Prof, AssocProf, Prof, Prof,…
## $ discipline    &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, A, A,…
## $ yrs.since.phd &lt;int&gt; 19, 20, 4, 45, 40, 6, 30, 45, 21, 18, 12, 7, 1, 2, 20, 1…
## $ yrs.service   &lt;int&gt; 18, 16, 3, 39, 41, 6, 23, 45, 20, 18, 8, 2, 1, 0, 18, 3,…
## $ sex           &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Fe…
## $ salary        &lt;int&gt; 139750, 173200, 79750, 115000, 141500, 97000, 175000, 14…</code></pre>
<p>We can see that <strong>rank</strong>, <strong>discipline</strong>, and <strong>sex</strong> are already coded
as factors. The variables <strong>yrs.since.phd</strong> and <strong>yrs.service</strong> are
coded as integers.</p>
<p>Our viewpoint states a belief that more years in service will cause
higher salary. Let us focus on the mechanics of fitting the model. First
we will examine the impact of each individual variable to see if our
view point is correct.</p>
<p>We start off with <strong>salary</strong> vs <strong>yrs.since.phd</strong>.</p>
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb416-1"><a href="#cb416-1" tabindex="-1"></a><span class="fu">summary</span>(Salaries)</span></code></pre></div>
<pre><code>##         rank     discipline yrs.since.phd    yrs.service        sex     
##  AsstProf : 67   A:181      Min.   : 1.00   Min.   : 0.00   Female: 39  
##  AssocProf: 64   B:216      1st Qu.:12.00   1st Qu.: 7.00   Male  :358  
##  Prof     :266              Median :21.00   Median :16.00               
##                             Mean   :22.31   Mean   :17.61               
##                             3rd Qu.:32.00   3rd Qu.:27.00               
##                             Max.   :56.00   Max.   :60.00               
##      salary      
##  Min.   : 57800  
##  1st Qu.: 91000  
##  Median :107300  
##  Mean   :113706  
##  3rd Qu.:134185  
##  Max.   :231545</code></pre>
<p>Both explanatory variables, <strong>yrs.since.phd</strong> and <strong>yrs.service</strong> have
mean and median values that are close to each other. However, the mean
and median for the <strong>salary</strong> variable are quite different.</p>
<p>We can better visualise this using boxplots.</p>
<div class="sourceCode" id="cb418"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb418-1"><a href="#cb418-1" tabindex="-1"></a><span class="fu">boxplot</span>(Salaries[,<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&#39;brown1&#39;</span>, <span class="st">&#39;steelblue&#39;</span>), <span class="at">main =</span> <span class="st">&quot;Distribution&quot;</span>)</span>
<span id="cb418-2"><a href="#cb418-2" tabindex="-1"></a>means <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Salaries[,<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>], mean)</span>
<span id="cb418-3"><a href="#cb418-3" tabindex="-1"></a><span class="fu">points</span>(means, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">pch =</span> <span class="dv">22</span>, <span class="at">lwd =</span> <span class="dv">7</span>)</span></code></pre></div>
<p><img src="05-S05-P1_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb419-1"><a href="#cb419-1" tabindex="-1"></a><span class="fu">boxplot</span>(salary, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&#39;chartreuse4&#39;</span>), <span class="at">main =</span> <span class="st">&quot;Distributions&quot;</span>)</span>
<span id="cb419-2"><a href="#cb419-2" tabindex="-1"></a>means <span class="ot">&lt;-</span> <span class="fu">sapply</span>(salary, mean)</span>
<span id="cb419-3"><a href="#cb419-3" tabindex="-1"></a><span class="fu">points</span>(means, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">pch =</span> <span class="dv">22</span>, <span class="at">lwd =</span> <span class="dv">7</span>)</span></code></pre></div>
<p><img src="05-S05-P1_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<div class="question">
<p>What do the box plots indicate?</p>
</div>
</div>
<div id="salary-and-years-since-phd" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Salary and Years since PhD<a href="#salary-and-years-since-phd" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s consider the relationship between <strong>yrs.since.phd</strong> and <strong>salary</strong>
using a scatterplot onto which we add a line of best fit. Note that
since we ‘attached’ the dataset, we can call on the variables without
need to index or specify the dataset by name.</p>
<div class="sourceCode" id="cb420"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb420-1"><a href="#cb420-1" tabindex="-1"></a><span class="fu">plot</span>(salary <span class="sc">~</span> yrs.since.phd, <span class="at">cex =</span>.<span class="dv">6</span>, <span class="at">main =</span> <span class="st">&quot;The Relationship between Nine-month Salary and Years since PhD&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Years since PhD&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Nine-month Salary (dollars)&quot;</span>)</span>
<span id="cb420-2"><a href="#cb420-2" tabindex="-1"></a></span>
<span id="cb420-3"><a href="#cb420-3" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> yrs.since.phd)</span>
<span id="cb420-4"><a href="#cb420-4" tabindex="-1"></a></span>
<span id="cb420-5"><a href="#cb420-5" tabindex="-1"></a><span class="fu">abline</span>(model1, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="05-S05-P1_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb421-1"><a href="#cb421-1" tabindex="-1"></a><span class="fu">summary</span>(model1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = salary ~ yrs.since.phd)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -84171 -19432  -2858  16086 102383 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    91718.7     2765.8  33.162   &lt;2e-16 ***
## yrs.since.phd    985.3      107.4   9.177   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 27530 on 395 degrees of freedom
## Multiple R-squared:  0.1758, Adjusted R-squared:  0.1737 
## F-statistic: 84.23 on 1 and 395 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="question">
<p>What do the results indicate?</p>
</div>
</div>
<div id="salary-and-years-of-service" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Salary and Years of Service<a href="#salary-and-years-of-service" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s find out more about the relationship between nine-month salary and
years of service.</p>
<div class="sourceCode" id="cb423"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb423-1"><a href="#cb423-1" tabindex="-1"></a><span class="fu">plot</span>(salary <span class="sc">~</span> yrs.service, <span class="at">cex =</span>.<span class="dv">6</span>, <span class="at">main =</span> <span class="st">&quot;The Relationship between Nine-month Salary and Years of Service&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Years of Service&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Nine-month Salary (dollars)&quot;</span>)</span>
<span id="cb423-2"><a href="#cb423-2" tabindex="-1"></a></span>
<span id="cb423-3"><a href="#cb423-3" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> yrs.service)</span>
<span id="cb423-4"><a href="#cb423-4" tabindex="-1"></a></span>
<span id="cb423-5"><a href="#cb423-5" tabindex="-1"></a><span class="fu">abline</span>(model1, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="05-S05-P1_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<div class="sourceCode" id="cb424"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb424-1"><a href="#cb424-1" tabindex="-1"></a><span class="fu">summary</span>(model2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = salary ~ yrs.service)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -81933 -20511  -3776  16417 101947 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  99974.7     2416.6   41.37  &lt; 2e-16 ***
## yrs.service    779.6      110.4    7.06 7.53e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 28580 on 395 degrees of freedom
## Multiple R-squared:  0.1121, Adjusted R-squared:  0.1098 
## F-statistic: 49.85 on 1 and 395 DF,  p-value: 7.529e-12</code></pre>
<div class="question">
<p>What do the plot and model results indicate?</p>
</div>
</div>
<div id="the-model" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">The Model<a href="#the-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s consider both variables (years of service and years since PhD) and whether these help explain salary. We define our multiple linear regression model as:</p>
<p><span class="math display">\[y = b_0 + b_1x_1 + b_2x_2 + e\]</span></p>
<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb426-1"><a href="#cb426-1" tabindex="-1"></a>mr_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> yrs.since.phd <span class="sc">+</span> yrs.service)</span>
<span id="cb426-2"><a href="#cb426-2" tabindex="-1"></a><span class="fu">summary</span>(mr_model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = salary ~ yrs.since.phd + yrs.service)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -79735 -19823  -2617  15149 106149 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    89912.2     2843.6  31.620  &lt; 2e-16 ***
## yrs.since.phd   1562.9      256.8   6.086 2.75e-09 ***
## yrs.service     -629.1      254.5  -2.472   0.0138 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 27360 on 394 degrees of freedom
## Multiple R-squared:  0.1883, Adjusted R-squared:  0.1842 
## F-statistic: 45.71 on 2 and 394 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="test-a-does-the-fitted-model-make-sense" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Test a): Does the fitted model make sense?<a href="#test-a-does-the-fitted-model-make-sense" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><em>Do the estimated coefficients have the correct sign?</em></p>
<p>The estimated model of best fit is:</p>
<p><span class="math inline">\(salary = 89912.2 + 1562.9yrs.since.phd − 629.1yrs.service\)</span></p>
<p>We notice that when put together with the variable <strong>yrs.since.phd</strong>,
the <strong>yrs.service</strong> changes sign, which is not in line with our
previously drawn conclusion and the viewpoint. This is the result of
<em>collinearity</em>, which you already know happens when two predictors are
correlated with one another.</p>
<p>(Multi)collinearity can be identified when:</p>
<ul>
<li>a regression coefficient <span class="math inline">\(x_i\)</span> is not significant even though,
theoretically, it should be highly correlated with the response
variable <span class="math inline">\(y\)</span>;<br />
</li>
<li>by adding or deleting an <span class="math inline">\(x_i\)</span> variable, the regression coefficients
change dramatically;<br />
</li>
<li>we get a negative regression coefficient when the response should
increase along with <span class="math inline">\(x_i\)</span>, or we get a positive regression
coefficient when the response should decrease as <span class="math inline">\(x_i\)</span> increases;<br />
</li>
<li>the explanatory variables have high pairwise correlations.</li>
</ul>
<p>Removing one of the correlated explanatory variables usually doesn’t
drastically reduce the <span class="math inline">\(R^2/R^2adj\)</span>.</p>
<p>With this model, using <strong>yrs.since.phd</strong> and <strong>yrs.service</strong> variables
we have managed to explain just over 18% of variation in the variable
<strong>salary</strong>.</p>
</div>
<div id="test-b-overall-is-the-model-a-good-fit" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Test b): Overall, is the model a good fit?<a href="#test-b-overall-is-the-model-a-good-fit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><span class="math inline">\(R^2adj\)</span> is 18.42%, putting this model on the weaker side. However let
us go through the formal procedure and set the hypothesis below. The
null hypothesis of will be tested using the F-test:</p>
<ul>
<li><span class="math inline">\(H_0:R^2=0\)</span> (that is, the set of explanatory variables are
insignificant, or in other words: useless)<br />
</li>
<li><span class="math inline">\(H_1:R^2&gt;0\)</span> (that is, at least one explanatory variable is
significant, or in other words: important)</li>
</ul>
<p>The decision rule is:</p>
<ul>
<li>if <span class="math inline">\(F_{calc} &lt; F_{crit} =&gt; H_0\)</span><br />
</li>
<li>if <span class="math inline">\(F_{calc} &gt; F_{crit} =&gt; H_1\)</span></li>
</ul>
<p>Examining the sample evidence we get that <span class="math inline">\(F_{calc} = 45.71\)</span>. The value
for <span class="math inline">\(F_{crit}\)</span> can be found in the statistical tables for <span class="math inline">\(df1 = 2\)</span> and
<span class="math inline">\(df2 = 394\)</span>.</p>
<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb428-1"><a href="#cb428-1" tabindex="-1"></a><span class="fu">qf</span>(<span class="fl">0.95</span>, <span class="dv">2</span>, <span class="dv">394</span>)</span></code></pre></div>
<pre><code>## [1] 3.018626</code></pre>
<p>Since <span class="math inline">\(F_{crit} = 3.02 &lt; F_{calc} =&gt; H_1\)</span>, this implies that this is a valid
model.</p>
<p>As pointed out earlier, this formal test involves a rather weak alternative hypothesis, which says only that <span class="math inline">\(R^2\)</span> is significantly bigger than 0. With <span class="math inline">\(R^2\)</span> of around 18% we can conclude that this is a useful model worthy of further investigation.</p>
</div>
<div id="test-c-individually-are-the-explanatory-variables-important" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Test c): Individually, are the explanatory variables important?<a href="#test-c-individually-are-the-explanatory-variables-important" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Stage two of our model validation procedure is to examine the importance of any one single explanatory variable used in the fitted model. We have pointed out that just because a set of variables is important does not necessarily mean that each individual variable is contributing towards explaining the behaviour of <span class="math inline">\(Y\)</span>.</p>
<p>We will conduct a set of t-tests to check the validity of each variable one at a time.</p>
<p><strong><span class="math inline">\(b_1\)</span>: previously we concluded that the relationship between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(y\)</span> is positive (in the fitted model parameter <span class="math inline">\(b_1\)</span> is positive). Consequently, we will use one tail t-test to assess the importance of <span class="math inline">\(x_1\)</span> in the model.</strong></p>
<p><span class="math inline">\(H_0:b_1 = 0\)</span> (explanatory variable <span class="math inline">\(i\)</span> is not important)</p>
<p><span class="math inline">\(H_1:b_1 &gt; 0\)</span> (explanatory variable <span class="math inline">\(i\)</span> has a positive influence)</p>
<p>whereby:</p>
<ul>
<li><p>If <span class="math inline">\(t_{calc} &lt; t_{crit} =&gt; H_0\)</span></p></li>
<li><p>If <span class="math inline">\(t_{calc} &gt; t_{crit} =&gt; H_1\)</span></p></li>
</ul>
<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb430-1"><a href="#cb430-1" tabindex="-1"></a><span class="fu">qt</span>(<span class="fl">0.95</span>, <span class="dv">394</span>)</span></code></pre></div>
<pre><code>## [1] 1.64873</code></pre>
<p><span class="math inline">\(t_{calc} = 6.09 &gt; t_{crit} = 1.65 =&gt; H_1\)</span>, which implies that we need to keep x1 in the model.</p>
<p><strong><span class="math inline">\(b_2\)</span>: previously we concluded that the relationship between <span class="math inline">\(x_2\)</span> and y is a positive relationship, but the model is suggesting that it is negative. We will stick to our belief and test if the coefficient should be positive:</strong></p>
<p><span class="math inline">\(H_0:b_2 = 0\)</span> (explanatory variable <span class="math inline">\(i\)</span> is not important)</p>
<p><span class="math inline">\(H_1:b_2 &gt; 0\)</span> (explanatory variable <span class="math inline">\(i\)</span> has a positive influence)</p>
<p>whereby:</p>
<ul>
<li><p>If <span class="math inline">\(t_{calc} &lt; t_{crit} =&gt; H_0\)</span></p></li>
<li><p>If <span class="math inline">\(t_{calc} &gt; t_{crit} =&gt; H_1\)</span></p></li>
</ul>
<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb432-1"><a href="#cb432-1" tabindex="-1"></a><span class="fu">qt</span>(<span class="fl">0.95</span>, <span class="dv">394</span>)</span></code></pre></div>
<pre><code>## [1] 1.64873</code></pre>
<p><span class="math inline">\(t_{calc} = −2.47 &lt; t_{crit} = 1.65 =&gt; H_0\)</span> therefore, the variable should be removed from the model.</p>
<p>The increase in the explain variation of around 1% is negligible in comparison to the best one factor model <span class="math inline">\(salary = f(yrs.since.phd) + e\)</span>. Hence, we will put forward the model <span class="math inline">\(salary = 91719 + 985yrs.since.phd\)</span> as our best fitted model.</p>
<p>Alternatively you could test for the coefficient not being equal to zero and make a conclusion for yourself if this would be a sensible thing to do.</p>
<p>In this example, we have adopted a ‘standard’ regression approach that assumes modelling a relationship between quantitative response and only quantitative predictors. However, often when building multiple regression models, we do not want to be limited to just quantitative predictors.</p>
</div>
</div>
<div id="part-ii" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Part II<a href="#part-ii" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now let’s expand our multiple linear regression model with two <em>quantitative</em> variables to a model that also includes <em>categorical</em> variables.</p>
<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb434-1"><a href="#cb434-1" tabindex="-1"></a><span class="co"># if you are starting a fresh R session, don&#39;t forget to:</span></span>
<span id="cb434-2"><a href="#cb434-2" tabindex="-1"></a></span>
<span id="cb434-3"><a href="#cb434-3" tabindex="-1"></a><span class="co"># load the package</span></span>
<span id="cb434-4"><a href="#cb434-4" tabindex="-1"></a><span class="fu">library</span>(carData)</span>
<span id="cb434-5"><a href="#cb434-5" tabindex="-1"></a></span>
<span id="cb434-6"><a href="#cb434-6" tabindex="-1"></a><span class="co"># attach the dataset</span></span>
<span id="cb434-7"><a href="#cb434-7" tabindex="-1"></a><span class="fu">attach</span>(Salaries)</span></code></pre></div>
<p>In many datasets, categorical (attribute) variables are usually encoded numerically and are accompanied by information about the levels of the variable saved in the levels attribute.</p>
<p>Let’s consider the <strong>sex</strong> variable.</p>
<div class="sourceCode" id="cb435"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb435-1"><a href="#cb435-1" tabindex="-1"></a><span class="fu">attributes</span>(sex)</span></code></pre></div>
<pre><code>## $levels
## [1] &quot;Female&quot; &quot;Male&quot;  
## 
## $class
## [1] &quot;factor&quot;</code></pre>
<p>This variable is already coded as a factor with two levels, <em>Female</em> and <em>Male</em> (which you should already know from earlier in the demonstration). Now, what if we want to transform a variable of class <code>factor</code> into one of class <code>integer</code>?</p>
<div class="sourceCode" id="cb437"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb437-1"><a href="#cb437-1" tabindex="-1"></a><span class="fu">unclass</span>(sex)</span></code></pre></div>
<pre><code>##   [1] 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 1 2
##  [38] 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 2
##  [75] 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2
## [112] 2 2 2 1 2 2 2 2 1 2 2 2 1 2 2 2 1 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [149] 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2
## [186] 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2
## [223] 2 2 2 2 2 2 2 2 1 1 2 1 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2 2 2
## [260] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [297] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 1
## [334] 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 2 2 2
## [371] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## attr(,&quot;levels&quot;)
## [1] &quot;Female&quot; &quot;Male&quot;</code></pre>
<p>We can easily do so with the <code>unclass()</code> function which removes the attributes of a factor variable and transforms the levels into numeric values.</p>
<p>However, when using factor variable in a linear regression model, it would make no sense to treat it as a <em>quantitative</em> explanatory variable. In the context of linear modelling we need to code each category to represent factor levels. Two-level attribute variables are very easy to code. We simply create an indicator or dummy variable that takes on two possible dummy numerical values. Consider the <strong>sex</strong> variable.</p>
<p>We can code this using a dummy variable <span class="math inline">\(d\)</span>:<br />
</p>
<p><span class="math display">\[
d = \begin{cases}
0, &amp; \text{if female} \\
1, &amp; \text{if male}
\end{cases}
\]</span>
💡 This is the default coding used in R. A zero value is assigned to the level which is first alphabetically, unless it is changed by using the <code>releveld()</code> function for example, or by specifying the levels of the factor variable specifically.</p>
<p>So, for a simple regression model predicting nine-month salary using one categorical variable:</p>
<p><span class="math display">\[salary = b_0 + b_1sex + e\]</span><br />
the model is specified as follows:</p>
<p><span class="math display">\[salary_i = b_0 + b_1 sex_i + e_i =
\begin{cases}
b_0 + b_1 \times 1 + e_i = b_0 + b_1 + e_i, &amp; \text{if the person is male} \\
b_0 + b_1 \times 0 + e_i = b_0 + e_i, &amp; \text{if the person is female}
\end{cases}\]</span></p>
<p>where <span class="math inline">\(b_0\)</span> can be interpreted as the average nine-month salary for females, and <span class="math inline">\(b_0 + b_1\)</span> as the nine-month average salary for males. The value of <span class="math inline">\(b_1\)</span> represents the average difference in nine-month salary between females and males.</p>
<p>We can conclude that dealing with an attribute variable with two levels in a linear model is straightforward. In this case, a dummy variable indicates whether an observation has a particular characteristic: yes/no. We can observe it as a ‘switch’ in a model, as this dummy variable can only assume the values <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, where <span class="math inline">\(0\)</span> indicates the absence of the effect, and <span class="math inline">\(1\)</span> indicates the presence. The values <strong>0/1</strong> can be seen as <strong>off/on</strong>.</p>
<p>The way in which R codes dummy variables is controlled by the <code>contrasts</code> option:</p>
<div class="sourceCode" id="cb439"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb439-1"><a href="#cb439-1" tabindex="-1"></a><span class="fu">options</span>(<span class="st">&quot;contrasts&quot;</span>)</span></code></pre></div>
<pre><code>## $contrasts
##         unordered           ordered 
## &quot;contr.treatment&quot;      &quot;contr.poly&quot;</code></pre>
<p>The output points out the conversion of the factor into an appropriate set of contrasts. In particular, the first one: for unordered factors, and the second one: the ordered factors. The former is applicable in our context. To explicitly identify the coding of the factor, i.e. dummy variable used by R, we can use the <code>contrasts()</code> function.</p>
<div class="sourceCode" id="cb441"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb441-1"><a href="#cb441-1" tabindex="-1"></a><span class="fu">contrasts</span>(sex)</span></code></pre></div>
<pre><code>##        Male
## Female    0
## Male      1</code></pre>
<p>Note that applied <code>contr.treatment</code> conversion takes only the value <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> and that for an attribute variable with <span class="math inline">\(k\)</span> levels it will create <span class="math inline">\(k-1\)</span> dummy variables. There are many different ways of coding attribute variables besides the dummy variable approach explained here. All of these different approaches lead to equivalent model fits. What differs are the coefficients (i.e. model parameters as they require different interpretations, arranged to measure particular contrasts). This 0/1 coding implemented in R’s default <code>contr.treatment</code> contrast offers straightforward interpretation of the associated parameter in the model, which often is not the case when implementing other contrasts.</p>
<div id="interpreting-coefficients-of-attribute-variables" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Interpreting coefficients of attribute variables<a href="#interpreting-coefficients-of-attribute-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the case of measured predictors, we are comfortable with the interpretation of the linear model coefficient as a slope, which tells us what a unit increase in the response variable is (i.e. outcome per unit increase in the explanatory variable). This is not necessarily the right interpretation for attribute predictors.</p>
<p>Let’s consider average nine-month salary values for males and females separately.</p>
<div class="sourceCode" id="cb443"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb443-1"><a href="#cb443-1" tabindex="-1"></a>Salaries <span class="sc">%&gt;%</span> </span>
<span id="cb443-2"><a href="#cb443-2" tabindex="-1"></a>  <span class="fu">select</span>(salary, sex) <span class="sc">%&gt;%</span>   </span>
<span id="cb443-3"><a href="#cb443-3" tabindex="-1"></a>  <span class="fu">group_by</span>(sex) <span class="sc">%&gt;%</span> </span>
<span id="cb443-4"><a href="#cb443-4" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">mean=</span><span class="fu">mean</span>(salary))</span></code></pre></div>
<pre><code>## # A tibble: 2 × 2
##   sex       mean
##   &lt;fct&gt;    &lt;dbl&gt;
## 1 Female 101002.
## 2 Male   115090.</code></pre>
<p>If we obtain the mean salary for each sex group we will find that for female professors the average salary is $ $101,002$ and for male professors the average is $ $115,090$. That is, a difference of <span class="math inline">\(\$14,088\)</span>.</p>
<p>If we now look at the parameters of the regression model for salary vs sex where females are coded as zero and males as one, we get exactly the same information, implying that the coefficient is the estimated difference in average between the two groups.</p>
<div class="sourceCode" id="cb445"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb445-1"><a href="#cb445-1" tabindex="-1"></a><span class="fu">lm</span>(salary <span class="sc">~</span>  sex)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = salary ~ sex)
## 
## Coefficients:
## (Intercept)      sexMale  
##      101002        14088</code></pre>
</div>
<div id="fitting-a-multivariate-regression-model" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Fitting a Multivariate Regression Model<a href="#fitting-a-multivariate-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Part I, we explored the extent to which variation in the response variable <strong>salary</strong> is associated with variation in years since PhD and years in service.
Now, we extend the model to also include <strong>sex</strong>, <strong>discipline</strong> and <strong>rank</strong>.
The overall goals of any model we construct is that it should contain enough to explain relations in the data and at the same time be simple enough to understand, explain to others, and use.</p>
<p>For convenience we will adopt the following notation:</p>
<p><span class="math inline">\(y\)</span>: salary<br />
<span class="math inline">\(x_1\)</span>: yrs.since.phd<br />
<span class="math inline">\(x_2\)</span>: yrs.service<br />
<span class="math inline">\(x_3\)</span>: discipline<br />
<span class="math inline">\(x_4\)</span>: sex<br />
<span class="math inline">\(x_5\)</span>: rank</p>
<p>Next, we need to specify the model that embodies our mechanistic understanding of the factors involved and the way that they are related to the response variable. It would make sense to expect that all of the available x variables may impact the behaviour of y, thus the model we wish to build should reflect our viewpoint, i.e. <span class="math inline">\(y=f(x_1,x_2,x_3,x_4,x_5)\)</span>:</p>
<p><span class="math display">\[y=b_0 + b_1x_1 + b_2x_2 + b_3x_3 + b_4x_4 + b_5x_5 + e\]</span>
Our viewpoint states a belief that all explanatory variables have a positive impact on the response. For example, more years in service will cause a higher salary.</p>
<p>Our objective now is to determine the values of the parameters in the model that lead to the best fit of the model to the data. That is, we are not only trying to estimate the parameters of the model, but we are also seeking the minimal adequate model to describe the data.</p>
<p>The best model is the model that produces the least unexplained variation following the principle of parsimony rather than complexity. That is the model should have as few parameters as possible, subject to the constraint that the parameters in the model should all be statistically significant.</p>
<p>For regression modelling in R we use the lm() function, that fits a linear model assuming normal errors and constant variance. We specify the model by a formula that uses arithmetic operators which enable different functionalities from their ordinary ones. But, before we dive into statistical modelling of the given data, we need to take a first step and conduct the most fundamental task of data analysis procedure: <strong>Get to Know Our Data</strong>.</p>
<div class="sourceCode" id="cb447"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb447-1"><a href="#cb447-1" tabindex="-1"></a><span class="fu">ggpairs</span>(Salaries)</span></code></pre></div>
<p><img src="05-S05-P1_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<div class="question">
<p>What information can you extract from this visualisation?</p>
</div>
</div>
<div id="fitting-the-model" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Fitting the Model<a href="#fitting-the-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are no fixed rules when fitting linear models, but there are adopted standards that have proven to work well in practice. We start off by fitting a maximal model then we carry on simplifying it by removing non-significant explanatory variables. This needs to be done with caution, making sure that the simplifications make good scientific sense, and do not lead to significant reductions in explanatory power. Although this should be the adopted strategy for fitting a model, it is not a guarantee to finding all the important structures in a complex data frame.</p>
<p>We can summarise our model building procedure algorithm as follows:</p>
<ol style="list-style-type: decimal">
<li>Fit the maximal model that includes all the variables. Then, assess the overall significance of the model by checking how big the <span class="math inline">\(R^2/\overline{R}^2\)</span> is. If statistically significant, carry on with the model fitting procedure, otherwise stop (F-test).<br />
</li>
<li>Remove the least significant terms one at a time. Then, check the <span class="math inline">\(t_calculated\)</span> for the variables values and perform a one tail or two tail t-test depending on your prior view. If the deletion causes an insignificant increase in <span class="math inline">\(\overline{R}^2\)</span>, leave that term out of the model.<br />
</li>
<li>Keep removing terms from the model until the model contains nothing but significant terms.</li>
</ol>
<p>Let’s build the model. Now, if we plan to use all variables in a dataset, there is no need to write the names of each individual predictor. Instead, we can use a full stop which tell R to include all other variables in the data object that do not already appear in the formula.</p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb448-1"><a href="#cb448-1" tabindex="-1"></a>model_1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> ., <span class="at">data =</span> Salaries)</span>
<span id="cb448-2"><a href="#cb448-2" tabindex="-1"></a></span>
<span id="cb448-3"><a href="#cb448-3" tabindex="-1"></a><span class="fu">summary</span>(model_1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = salary ~ ., data = Salaries)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -65248 -13211  -1775  10384  99592 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    65955.2     4588.6  14.374  &lt; 2e-16 ***
## rankAssocProf  12907.6     4145.3   3.114  0.00198 ** 
## rankProf       45066.0     4237.5  10.635  &lt; 2e-16 ***
## disciplineB    14417.6     2342.9   6.154 1.88e-09 ***
## yrs.since.phd    535.1      241.0   2.220  0.02698 *  
## yrs.service     -489.5      211.9  -2.310  0.02143 *  
## sexMale         4783.5     3858.7   1.240  0.21584    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 22540 on 390 degrees of freedom
## Multiple R-squared:  0.4547, Adjusted R-squared:  0.4463 
## F-statistic:  54.2 on 6 and 390 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="question">
<p>Overall, is the model a good fit? How big is the <span class="math inline">\(R^2/\overline{R}^2\)</span>?
Individually, are the explanatory variables important? What steps are required given the results of the model? What is the structure of the final fitted model and how should the results be interpreted?</p>
</div>
<!--chapter:end:05-S05-P1.Rmd-->
</div>
</div>
</div>
<div id="practical-2-foreign-direct-investment-fdi-study" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Practical 2: Foreign Direct Investment (FDI) Study<a href="#practical-2-foreign-direct-investment-fdi-study" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>This practical has been developed by Dr. Tatjana Kecojevic, Lecturer in Social Statistics.</em></p>
<div class="file">
<p>For the tasks below, you will require the <strong>FDI</strong> dataset.</p>
<p>Click here to download the file:
<a href="data/FDI.csv" download="FDI.csv"> FDI.csv </a>.</p>
<p>Remember to place your data file in a separate subfolder within your R
project working directory.</p>
</div>
<p>A business consultancy firm is compiling a major report about
globalisation. One aspect of this study concerns the determinants of FDI
undertaken by multi-national enterprises. Relevant information from a
sample of 60 multi-national companies that undertook significant
investment in overseas projects was made available as follows:</p>
<table style="width:97%;">
<colgroup>
<col width="22%" />
<col width="73%" />
<col width="1%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Variable Name</th>
<th align="center">Variable Description</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">FDI</td>
<td colspan="2" align="center">Value of FDI undertaken, in £ millions, by
investing company |</td>
</tr>
<tr class="even">
<td align="center">GDP_Cap</td>
<td align="center">GDP per capita, £000s, in the country receiving
the investment</td>
<td></td>
</tr>
<tr class="odd">
<td align="center">Gr_rate</td>
<td align="center">The economic growth rate, in %-terms, in the
country receiving the investment</td>
<td></td>
</tr>
<tr class="even">
<td align="center">ROC</td>
<td align="center">The average return on capital invested, in
%-terms, in the country receiving the investment</td>
<td></td>
</tr>
<tr class="odd">
<td align="center">Stable</td>
<td align="center">The political stability of the country receiving
the investment as measured by the number of
changes in government over the past 25 years</td>
<td></td>
</tr>
<tr class="even">
<td align="center">Infra</td>
<td align="center"><p>Infrastructure facilities (eg transport,
communications) in the country receiving the
investment</p>
<p>Coded: 1 = basic infrastructure 2 = good
infrastructure</p></td>
<td></td>
</tr>
<tr class="odd">
<td align="center">Trade</td>
<td align="center"><p>The openness to trade of the country receiving the
investment</p>
<p>Coded: 1 = trade tightly controlled 2 = some
restrictions on trade 3 = free trade</p></td>
<td></td>
</tr>
</tbody>
</table>
<p>This is a multiple regression type of the problem; FDI is the key
response variable as this study concerns the determinants of FDI
undertaken by multi-national enterprises.</p>
<p>The model is defined as <span class="math inline">\(Y = b_0 + b_1x_1 + b_2x_2 + ... + b_kx_k + e\)</span>,
for the general <span class="math inline">\(k\)</span> explanatory variable model and where e is also known
as the error term <span class="math inline">\(e ∼ N(0,\sigma^2)\)</span>, with the error term from a normal
distribution with a mean of <span class="math inline">\(0\)</span>, and a variance of <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Based on prior knowledge, we make the assumption that GDP_Cap, Gr_rate,
ROC, Infra, and Trade have positive relationships with FDI, whilst
Stable has a negative relationship with FDI.</p>
<p>We will use our best fit model to predict FDI for the following information: <em>country X receiving the investment has GDP per capita of 11.1 and Gr_rate per capita of 3.05; The average return on capital invested is 20.5%; There were 11 changes of government over the past 25 years and country X has good infrastructure with some restrictions on trade.</em></p>
<p>First, let’s load the required packages:</p>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb450-1"><a href="#cb450-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb450-2"><a href="#cb450-2" tabindex="-1"></a><span class="co"># you should have already installed this package for Practical 2 (Section 5)</span></span>
<span id="cb450-3"><a href="#cb450-3" tabindex="-1"></a><span class="fu">library</span>(GGally)</span></code></pre></div>
<p>Let’s import the data into R.</p>
<div class="sourceCode" id="cb451"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb451-1"><a href="#cb451-1" tabindex="-1"></a>mydataq1 <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/FDI.csv&quot;</span>, <span class="at">header =</span> T) </span></code></pre></div>
<p>Now let’s get a glimpse of the data. As you know, there are many ways to
do that, such as, for example, using the tidyverse <code>glimpse</code> function.
This is quite a handy function because it also tells us more about the
class of each variable. We can see that although <strong>Infra</strong> and <strong>Trade</strong>
categorical, these are coded as integers.</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb452-1"><a href="#cb452-1" tabindex="-1"></a><span class="fu">glimpse</span>(mydataq1)</span></code></pre></div>
<pre><code>## Rows: 60
## Columns: 7
## $ FDI     &lt;dbl&gt; 184.00, 187.00, 186.00, 192.00, 188.00, 190.00, 193.00, 194.00…
## $ GDP_Cap &lt;dbl&gt; 4.4, 6.3, 5.3, 5.9, 9.4, 7.6, 8.7, 6.0, 8.4, 10.1, 8.0, 6.9, 7…
## $ Gr_rate &lt;dbl&gt; 2.54, 4.06, 3.79, 3.38, 1.54, 2.25, 3.01, 2.13, 2.18, 3.33, 2.…
## $ ROC     &lt;dbl&gt; 6.7, 9.3, 7.1, 3.9, 6.3, 9.3, 6.3, 9.7, 5.6, 17.1, 9.1, 15.2, …
## $ Stable  &lt;int&gt; 9, 8, 11, 11, 8, 9, 9, 11, 12, 12, 8, 7, 12, 9, 8, 5, 9, 7, 11…
## $ Infra   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ Trade   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3,…</code></pre>
<p>We therefore need to transform them into factors.</p>
<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb454-1"><a href="#cb454-1" tabindex="-1"></a>mydataq1 <span class="ot">&lt;-</span> mydataq1 <span class="sc">%&gt;%</span></span>
<span id="cb454-2"><a href="#cb454-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Infra =</span> <span class="fu">as_factor</span>(Infra),</span>
<span id="cb454-3"><a href="#cb454-3" tabindex="-1"></a>         <span class="at">Trade =</span> <span class="fu">as_factor</span>(Trade)</span>
<span id="cb454-4"><a href="#cb454-4" tabindex="-1"></a>         )</span></code></pre></div>
<p>We can then explore all variables in the dataset as pairs using a matrix
of plots. Among many interesting features, we can note quite strong
correlations among pairs of variables which suggest the presence of
multicollinearity: GDP_Cap and ROC, Infra and GDP_Cap, Infra and ROC,
and Infra and Gr_rate.</p>
<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb455-1"><a href="#cb455-1" tabindex="-1"></a>GGally<span class="sc">::</span><span class="fu">ggpairs</span>(mydataq1)</span></code></pre></div>
<p><img src="05-S05-P2_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Ok, so our initial model is:</p>
<p><span class="math inline">\(FDI = b_0 + b_1GDP\_Cap + b_2Gr\_rate + b_3ROC – b_4Stable + b_5Infra + b_6Trade + e\)</span></p>
<p>where <strong>Infra</strong> and <strong>Stable</strong> are dummy variables.</p>
<p>We can have a look at how these two dummy variables are used in the
model by using the <code>contrasts()</code> function from base R.</p>
<p>The <strong>Infra</strong> variable is coded as <em>1 = basic infrastructure</em> and <em>2 =
good infrastructure</em>. Since this is a binary variable, there will be one
reference category and a single dummy variable.</p>
<div class="sourceCode" id="cb456"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb456-1"><a href="#cb456-1" tabindex="-1"></a><span class="fu">contrasts</span>(mydataq1<span class="sc">$</span>Infra)</span></code></pre></div>
<pre><code>##   2
## 1 0
## 2 1</code></pre>
<p>The <strong>Trade</strong> variable is coded as <em>1 = trade tightly</em> and <em>2 = some
restrictions on trade</em> and <em>2 = some restrictions on trade</em>. Since this
is variable with three categories, there will be one reference category
and two dummy variables.</p>
<div class="sourceCode" id="cb458"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb458-1"><a href="#cb458-1" tabindex="-1"></a><span class="fu">contrasts</span>(mydataq1<span class="sc">$</span>Trade)</span></code></pre></div>
<pre><code>##   2 3
## 1 0 0
## 2 1 0
## 3 0 1</code></pre>
<p>Let’s now fit our multiple regression model.</p>
<div class="sourceCode" id="cb460"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb460-1"><a href="#cb460-1" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(FDI <span class="sc">~</span> GDP_Cap <span class="sc">+</span> Gr_rate <span class="sc">+</span> ROC <span class="sc">+</span> Stable <span class="sc">+</span> Infra <span class="sc">+</span> Trade, <span class="at">data =</span> mydataq1)</span></code></pre></div>
<p>And explore the results.</p>
<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb461-1"><a href="#cb461-1" tabindex="-1"></a><span class="fu">summary</span>(m1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = FDI ~ GDP_Cap + Gr_rate + ROC + Stable + Infra + 
##     Trade, data = mydataq1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.8594 -1.2595 -0.0808  1.4183  8.7210 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 189.472086   2.690418  70.425  &lt; 2e-16 ***
## GDP_Cap       0.938713   0.236206   3.974 0.000219 ***
## Gr_rate      -0.089122   0.498508  -0.179 0.858807    
## ROC           0.003144   0.088466   0.036 0.971782    
## Stable       -0.539889   0.172155  -3.136 0.002816 ** 
## Infra2       -0.169110   1.493552  -0.113 0.910287    
## Trade2        4.875539   0.891476   5.469 1.31e-06 ***
## Trade3        5.890833   1.179891   4.993 7.07e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.734 on 52 degrees of freedom
## Multiple R-squared:  0.7481, Adjusted R-squared:  0.7142 
## F-statistic: 22.06 on 7 and 52 DF,  p-value: 1.646e-13</code></pre>
<div class="question">
<p>What do the results indicate?<br />
How do they compare with our initial assumptions?<br />
How do you proceed with the analysis?</p>
</div>
<!--chapter:end:05-S05-P2.Rmd-->
</div>
<div id="demonstration-1-propublicas-analysis-of-the-compas-tool" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Demonstration 1: ProPublica’s Analysis of the COMPAS Tool<a href="#demonstration-1-propublicas-analysis-of-the-compas-tool" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this demonstration, you will explore the analysis conducted by ProPublica on the COMPAS Recidivism Algorithm.</p>
<div id="notes-on-the-data" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Notes on the data<a href="#notes-on-the-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Please ensure that your first complete the two readings indicated in the learning materials for this section.</p>
<p>Salient points are highlighted below; see the full description from ProPublica for additional details.</p>
<blockquote>
<p><strong>Goal:</strong> We looked at more than 10,000 criminal defendants in Broward County, Florida, and compared their predicted recidivism rates with the rate that actually occurred over a two-year period.</p>
<p><strong>COMPAS tool input (data subjects):</strong> When most defendants are booked in jail, they respond to a COMPAS questionnaire. Their answers are fed into the COMPAS software to generate several scores including predictions of Risk of Recidivism and Risk of Violent Recidivism.</p>
<p><strong>How COMPAS input was acquired by ProPublica:</strong> Through a public records request, ProPublica obtained two years worth of COMPAS scores from the Broward County Sheriff’s Office in Florida. We received data for all 18,610 people who were scored in 2013 and 2014.</p>
<p><strong>COMPAS tool output:</strong> Each pretrial defendant received at least three COMPAS scores: “Risk of Recidivism,” “Risk of Violence” and “Risk of Failure to Appear. <span class="math display">\[...\]</span> COMPAS scores for each defendant ranged from 1 to 10, with ten being the highest risk. Scores 1 to 4 were labeled by COMPAS as”Low;” 5 to 7 were labeled “Medium;” and 8 to 10 were labeled “High.”</p>
<p><strong>Data integration (record linkage):</strong> Starting with the database of COMPAS scores, we built a profile of each person’s criminal history, both before and after they were scored. We collected public criminal records from the Broward County Clerk’s Office website through April 1, 2016. On average, defendants in our dataset were not incarcerated for 622.87 days (sd: 329.19). We matched the criminal records to the COMPAS records using a person’s first and last names and date of birth. This is the same technique used in the Broward County COMPAS validation study conducted by researchers at Florida State University in 2010. We downloaded around 80,000 criminal records from the Broward County Clerk’s Office website.</p>
<p><strong>What is recidivism?</strong> Northpointe defined recidivism as “a finger-printable arrest involving a charge and a filing for any uniform crime reporting (UCR) code.” We interpreted that to mean a criminal offense that resulted in a jail booking and took place after the crime for which the person was COMPAS scored. <span class="math display">\[...\]</span> For most of our analysis, we defined recidivism as a new arrest within two years.</p>
</blockquote>
</div>
<div id="setup" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Setup<a href="#setup" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before proceeding through the tasks, ensure that you install the <code>glue</code> package.</p>
<div id="load-packages" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Load packages<a href="#load-packages" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb463-1"><a href="#cb463-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb463-2"><a href="#cb463-2" tabindex="-1"></a><span class="fu">library</span>(glue)</span></code></pre></div>
</div>
<div id="load-data" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Load data<a href="#load-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb464-1"><a href="#cb464-1" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="st">&quot;https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv&quot;</span></span>
<span id="cb464-2"><a href="#cb464-2" tabindex="-1"></a></span>
<span id="cb464-3"><a href="#cb464-3" tabindex="-1"></a>compas_raw <span class="ot">&lt;-</span> </span>
<span id="cb464-4"><a href="#cb464-4" tabindex="-1"></a>  <span class="fu">read_csv</span>(</span>
<span id="cb464-5"><a href="#cb464-5" tabindex="-1"></a>    <span class="at">file =</span> url,</span>
<span id="cb464-6"><a href="#cb464-6" tabindex="-1"></a>    <span class="at">show_col_types =</span> <span class="cn">FALSE</span></span>
<span id="cb464-7"><a href="#cb464-7" tabindex="-1"></a>  )</span></code></pre></div>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb465-1"><a href="#cb465-1" tabindex="-1"></a><span class="fu">dim</span>(compas_raw)</span></code></pre></div>
<pre><code>## [1] 7214   53</code></pre>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb467-1"><a href="#cb467-1" tabindex="-1"></a><span class="fu">head</span>(compas_raw)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 53
##      id name    first last  compas_screening_date sex   dob          age age_cat
##   &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt; &lt;date&gt;                &lt;chr&gt; &lt;date&gt;     &lt;dbl&gt; &lt;chr&gt;  
## 1     1 miguel… migu… hern… 2013-08-14            Male  1947-04-18    69 Greate…
## 2     3 kevon … kevon dixon 2013-01-27            Male  1982-01-22    34 25 - 45
## 3     4 ed phi… ed    philo 2013-04-14            Male  1991-05-14    24 Less t…
## 4     5 marcu … marcu brown 2013-01-13            Male  1993-01-21    23 Less t…
## 5     6 bouthy… bout… pier… 2013-03-26            Male  1973-01-22    43 25 - 45
## 6     7 marsha… mars… miles 2013-11-30            Male  1971-08-22    44 25 - 45
## # ℹ 44 more variables: race &lt;chr&gt;, juv_fel_count &lt;dbl&gt;,
## #   decile_score...12 &lt;dbl&gt;, juv_misd_count &lt;dbl&gt;, juv_other_count &lt;dbl&gt;,
## #   priors_count...15 &lt;dbl&gt;, days_b_screening_arrest &lt;dbl&gt;, c_jail_in &lt;dttm&gt;,
## #   c_jail_out &lt;dttm&gt;, c_case_number &lt;chr&gt;, c_offense_date &lt;date&gt;,
## #   c_arrest_date &lt;date&gt;, c_days_from_compas &lt;dbl&gt;, c_charge_degree &lt;chr&gt;,
## #   c_charge_desc &lt;chr&gt;, is_recid &lt;dbl&gt;, r_case_number &lt;chr&gt;,
## #   r_charge_degree &lt;chr&gt;, r_days_from_arrest &lt;dbl&gt;, r_offense_date &lt;date&gt;, …</code></pre>
</div>
<div id="inspect-data" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Inspect data<a href="#inspect-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For convenience, here is a table of variable definitions:</p>
<table>
<colgroup>
<col width="50%" />
<col width="50%" />
</colgroup>
<thead>
<tr class="header">
<th>Variable</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>age</td>
<td>Age of the defendant</td>
</tr>
<tr class="even">
<td>age_cat</td>
<td>Age category. It can be &lt; 25, 25-45, &gt;45</td>
</tr>
<tr class="odd">
<td>sex</td>
<td>Sex of the defendant. It is either “Male” or “Female”</td>
</tr>
<tr class="even">
<td>race</td>
<td>Race of the defendant. It can be “African-American”, “Caucasian”, “Hispanic”, “Asian”, or “Other”</td>
</tr>
<tr class="odd">
<td>c_charge_degree</td>
<td>Charge. Either “M” for misdemeanor, “F” for felony, or “O” (not causing jail time)</td>
</tr>
<tr class="even">
<td>priors_count</td>
<td>Count of prior crimes committed by the defendant</td>
</tr>
<tr class="odd">
<td>days_b_screening_arrest</td>
<td>Days between the arrest and COMPAS screening</td>
</tr>
<tr class="even">
<td>decile_score</td>
<td>The COMPAS score estimated by the system. It is between 0-10</td>
</tr>
<tr class="odd">
<td>score_text</td>
<td>Decile score. It can be “Low” (1-4), “Medium” (5-7), or “High” (8-10)</td>
</tr>
<tr class="even">
<td>is_recid</td>
<td>Indicates if the defendant recidivated. It can be 0, 1, or -1</td>
</tr>
<tr class="odd">
<td>two_year_recid</td>
<td>Indicates if the defendant recidivated within two years of COMPAS assessment</td>
</tr>
<tr class="even">
<td>c_jail_in</td>
<td>Date the defendant was in jail</td>
</tr>
<tr class="odd">
<td>c_jail_out</td>
<td>Date when the defendant was released from jail</td>
</tr>
</tbody>
</table>
<p>Plot the distribution of age, race, and sex in the imported data (<code>compas_raw</code>):</p>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb469-1"><a href="#cb469-1" tabindex="-1"></a>compas_raw <span class="sc">|&gt;</span></span>
<span id="cb469-2"><a href="#cb469-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(age)) <span class="sc">+</span></span>
<span id="cb469-3"><a href="#cb469-3" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">binwidth =</span> <span class="dv">1</span>)</span></code></pre></div>
<p><img src="05-S05-D1_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb470-1"><a href="#cb470-1" tabindex="-1"></a>compas_raw <span class="sc">|&gt;</span></span>
<span id="cb470-2"><a href="#cb470-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(race)) <span class="sc">+</span></span>
<span id="cb470-3"><a href="#cb470-3" tabindex="-1"></a>  <span class="fu">geom_bar</span>()</span></code></pre></div>
<p><img src="05-S05-D1_files/figure-html/unnamed-chunk-4-2.png" width="672" /></p>
<div class="sourceCode" id="cb471"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb471-1"><a href="#cb471-1" tabindex="-1"></a>compas_raw <span class="sc">|&gt;</span></span>
<span id="cb471-2"><a href="#cb471-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(sex)) <span class="sc">+</span></span>
<span id="cb471-3"><a href="#cb471-3" tabindex="-1"></a>  <span class="fu">geom_bar</span>()</span></code></pre></div>
<p><img src="05-S05-D1_files/figure-html/unnamed-chunk-4-3.png" width="672" /></p>
</div>
</div>
<div id="preprocess-data" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Preprocess data<a href="#preprocess-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>ProPublica implemented a few pre-processing steps. First, they generated a subset of the data with a few variables of interest. Here, we select even fewer variables, keeping only those that we will use in this notebook. We also relabel the race column.</p>
<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb472-1"><a href="#cb472-1" tabindex="-1"></a>cols <span class="ot">&lt;-</span> </span>
<span id="cb472-2"><a href="#cb472-2" tabindex="-1"></a>  <span class="fu">c</span>(<span class="st">&quot;id&quot;</span>, <span class="st">&quot;age&quot;</span>, <span class="st">&quot;c_charge_degree&quot;</span>, <span class="st">&quot;race&quot;</span>, <span class="st">&quot;age_cat&quot;</span>,</span>
<span id="cb472-3"><a href="#cb472-3" tabindex="-1"></a>    <span class="st">&quot;score_text&quot;</span>, <span class="st">&quot;sex&quot;</span>, <span class="st">&quot;priors_count...15&quot;</span>,</span>
<span id="cb472-4"><a href="#cb472-4" tabindex="-1"></a>    <span class="st">&quot;days_b_screening_arrest&quot;</span>, <span class="st">&quot;decile_score...12&quot;</span>,</span>
<span id="cb472-5"><a href="#cb472-5" tabindex="-1"></a>    <span class="st">&quot;is_recid&quot;</span>, <span class="st">&quot;two_year_recid&quot;</span>)</span>
<span id="cb472-6"><a href="#cb472-6" tabindex="-1"></a></span>
<span id="cb472-7"><a href="#cb472-7" tabindex="-1"></a>compas_selected <span class="ot">&lt;-</span> </span>
<span id="cb472-8"><a href="#cb472-8" tabindex="-1"></a>  compas_raw <span class="sc">|&gt;</span></span>
<span id="cb472-9"><a href="#cb472-9" tabindex="-1"></a>  <span class="fu">select</span>(</span>
<span id="cb472-10"><a href="#cb472-10" tabindex="-1"></a>    <span class="fu">all_of</span>(cols)</span>
<span id="cb472-11"><a href="#cb472-11" tabindex="-1"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb472-12"><a href="#cb472-12" tabindex="-1"></a>  <span class="fu">rename</span>(</span>
<span id="cb472-13"><a href="#cb472-13" tabindex="-1"></a>    <span class="at">priors_count =</span> priors_count...<span class="dv">15</span>,</span>
<span id="cb472-14"><a href="#cb472-14" tabindex="-1"></a>    <span class="at">decile_score =</span> decile_score...<span class="dv">12</span></span>
<span id="cb472-15"><a href="#cb472-15" tabindex="-1"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb472-16"><a href="#cb472-16" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb472-17"><a href="#cb472-17" tabindex="-1"></a>    <span class="at">race =</span> <span class="fu">case_when</span>(</span>
<span id="cb472-18"><a href="#cb472-18" tabindex="-1"></a>      race <span class="sc">==</span> <span class="st">&quot;African-American&quot;</span> <span class="sc">~</span> <span class="st">&quot;Black&quot;</span>,</span>
<span id="cb472-19"><a href="#cb472-19" tabindex="-1"></a>      race <span class="sc">==</span> <span class="st">&quot;Caucasian&quot;</span> <span class="sc">~</span> <span class="st">&quot;White&quot;</span>,</span>
<span id="cb472-20"><a href="#cb472-20" tabindex="-1"></a>      <span class="cn">TRUE</span> <span class="sc">~</span> race</span>
<span id="cb472-21"><a href="#cb472-21" tabindex="-1"></a>    )</span>
<span id="cb472-22"><a href="#cb472-22" tabindex="-1"></a>  )</span>
<span id="cb472-23"><a href="#cb472-23" tabindex="-1"></a></span>
<span id="cb472-24"><a href="#cb472-24" tabindex="-1"></a><span class="fu">head</span>(compas_selected)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 12
##      id   age c_charge_degree race  age_cat        score_text sex   priors_count
##   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;           &lt;chr&gt; &lt;chr&gt;          &lt;chr&gt;      &lt;chr&gt;        &lt;dbl&gt;
## 1     1    69 F               Other Greater than … Low        Male             0
## 2     3    34 F               Black 25 - 45        Low        Male             0
## 3     4    24 F               Black Less than 25   Low        Male             4
## 4     5    23 F               Black Less than 25   High       Male             1
## 5     6    43 F               Other 25 - 45        Low        Male             2
## 6     7    44 M               Other 25 - 45        Low        Male             0
## # ℹ 4 more variables: days_b_screening_arrest &lt;dbl&gt;, decile_score &lt;dbl&gt;,
## #   is_recid &lt;dbl&gt;, two_year_recid &lt;dbl&gt;</code></pre>
<div class="sourceCode" id="cb474"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb474-1"><a href="#cb474-1" tabindex="-1"></a><span class="fu">glimpse</span>(compas_selected)</span></code></pre></div>
<pre><code>## Rows: 7,214
## Columns: 12
## $ id                      &lt;dbl&gt; 1, 3, 4, 5, 6, 7, 8, 9, 10, 13, 14, 15, 16, 18…
## $ age                     &lt;dbl&gt; 69, 34, 24, 23, 43, 44, 41, 43, 39, 21, 27, 23…
## $ c_charge_degree         &lt;chr&gt; &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;…
## $ race                    &lt;chr&gt; &quot;Other&quot;, &quot;Black&quot;, &quot;Black&quot;, &quot;Black&quot;, &quot;Other&quot;, &quot;…
## $ age_cat                 &lt;chr&gt; &quot;Greater than 45&quot;, &quot;25 - 45&quot;, &quot;Less than 25&quot;, …
## $ score_text              &lt;chr&gt; &quot;Low&quot;, &quot;Low&quot;, &quot;Low&quot;, &quot;High&quot;, &quot;Low&quot;, &quot;Low&quot;, &quot;Me…
## $ sex                     &lt;chr&gt; &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;…
## $ priors_count            &lt;dbl&gt; 0, 0, 4, 1, 2, 0, 14, 3, 0, 1, 0, 3, 0, 0, 1, …
## $ days_b_screening_arrest &lt;dbl&gt; -1, -1, -1, NA, NA, 0, -1, -1, -1, 428, -1, 0,…
## $ decile_score            &lt;dbl&gt; 1, 3, 4, 8, 1, 1, 6, 4, 1, 3, 4, 6, 1, 4, 1, 3…
## $ is_recid                &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1…
## $ two_year_recid          &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1…</code></pre>
<p>Take a moment to get a feel for the variables and structure of the data. ProPublica filtered the above data by removing rows where:</p>
<ol style="list-style-type: decimal">
<li>The COMPAS score is missing.</li>
<li>The charge date of the defendant’s COMPAS-scored crime was not within 30 days from the date of arrest. ProPublica assumed that the offense may not be correct in these cases.</li>
<li>The recividist flag is “-1”. In such cases, ProPublica could not find a COMPAS record at all.</li>
<li>The charge is “O”. These are ordinary traffic offenses and do not result in jail time.</li>
</ol>
<p>We implement these conditions here:</p>
<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb476-1"><a href="#cb476-1" tabindex="-1"></a>compas <span class="ot">&lt;-</span></span>
<span id="cb476-2"><a href="#cb476-2" tabindex="-1"></a>  compas_selected <span class="sc">|&gt;</span></span>
<span id="cb476-3"><a href="#cb476-3" tabindex="-1"></a>  <span class="fu">filter</span>(</span>
<span id="cb476-4"><a href="#cb476-4" tabindex="-1"></a>    score_text <span class="sc">!=</span> <span class="st">&quot;N/A&quot;</span>,</span>
<span id="cb476-5"><a href="#cb476-5" tabindex="-1"></a>    days_b_screening_arrest <span class="sc">&lt;=</span> <span class="dv">30</span>,</span>
<span id="cb476-6"><a href="#cb476-6" tabindex="-1"></a>    days_b_screening_arrest <span class="sc">&gt;=</span> <span class="sc">-</span><span class="dv">30</span>,</span>
<span id="cb476-7"><a href="#cb476-7" tabindex="-1"></a>    is_recid <span class="sc">!=</span> <span class="sc">-</span><span class="dv">1</span>,</span>
<span id="cb476-8"><a href="#cb476-8" tabindex="-1"></a>    c_charge_degree <span class="sc">!=</span> <span class="st">&quot;O&quot;</span></span>
<span id="cb476-9"><a href="#cb476-9" tabindex="-1"></a>  )</span></code></pre></div>
<p>Note that ProPublica only included people who had recidivated within two years or had at least two years outside a correctional facility. This pre-processing step is “baked in” to the data that we imported from GitHub in this notebook.</p>
<p>Check the dimensions (i.e. the number of variables and observations) of the imported (<code>compas_raw</code>) and preprocessed (<code>compas</code>) data:</p>
<div class="sourceCode" id="cb477"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb477-1"><a href="#cb477-1" tabindex="-1"></a><span class="fu">glue</span>(<span class="st">&quot;Imported data: {nrow(compas_raw)}, {ncol(compas_raw)}&quot;</span>)</span></code></pre></div>
<pre><code>## Imported data: 7214, 53</code></pre>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb479-1"><a href="#cb479-1" tabindex="-1"></a><span class="fu">glue</span>(<span class="st">&quot;Data after selecting variables: {nrow(compas_selected)}, {ncol(compas_selected)}&quot;</span>)</span></code></pre></div>
<pre><code>## Data after selecting variables: 7214, 12</code></pre>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb481-1"><a href="#cb481-1" tabindex="-1"></a><span class="fu">glue</span>(<span class="st">&quot;Data after filtering observations: {nrow(compas)}, {ncol(compas)}&quot;</span>)</span></code></pre></div>
<pre><code>## Data after filtering observations: 6172, 12</code></pre>
<p>Take the additional step of making sure that the decile score (discussed below) is numeric:</p>
<div class="sourceCode" id="cb483"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb483-1"><a href="#cb483-1" tabindex="-1"></a>compas <span class="ot">&lt;-</span> compas <span class="sc">|&gt;</span> <span class="fu">mutate</span>(<span class="at">decile_score =</span> <span class="fu">as.numeric</span>(decile_score))</span></code></pre></div>
<div id="inspect-data-again" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Inspect data again<a href="#inspect-data-again" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Re-inspect salient variables in the data after the preprocessing steps. Plot the distribution of age, race, and sex in the preprocessed data (<code>compas</code>) and compare these distributions to the imported data (<code>compas_raw</code>):</p>
<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb484-1"><a href="#cb484-1" tabindex="-1"></a>compas_compare <span class="ot">&lt;-</span> </span>
<span id="cb484-2"><a href="#cb484-2" tabindex="-1"></a>  <span class="fu">bind_rows</span>(</span>
<span id="cb484-3"><a href="#cb484-3" tabindex="-1"></a>    <span class="fu">mutate</span>(compas, <span class="at">source =</span> <span class="st">&quot;raw&quot;</span>),</span>
<span id="cb484-4"><a href="#cb484-4" tabindex="-1"></a>    <span class="fu">mutate</span>(compas_raw, <span class="at">source =</span> <span class="st">&quot;preprocessed&quot;</span>),</span>
<span id="cb484-5"><a href="#cb484-5" tabindex="-1"></a>  )</span>
<span id="cb484-6"><a href="#cb484-6" tabindex="-1"></a></span>
<span id="cb484-7"><a href="#cb484-7" tabindex="-1"></a>compas_compare <span class="sc">|&gt;</span></span>
<span id="cb484-8"><a href="#cb484-8" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(age, <span class="at">fill =</span> source)) <span class="sc">+</span></span>
<span id="cb484-9"><a href="#cb484-9" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">position =</span> <span class="st">&quot;identity&quot;</span>)</span></code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="05-S05-D1_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb486-1"><a href="#cb486-1" tabindex="-1"></a>compas_compare <span class="sc">|&gt;</span></span>
<span id="cb486-2"><a href="#cb486-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(race, <span class="at">fill =</span> source)) <span class="sc">+</span></span>
<span id="cb486-3"><a href="#cb486-3" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">position =</span> <span class="st">&quot;identity&quot;</span>)</span></code></pre></div>
<p><img src="05-S05-D1_files/figure-html/unnamed-chunk-9-2.png" width="672" /></p>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb487-1"><a href="#cb487-1" tabindex="-1"></a>compas_compare <span class="sc">|&gt;</span></span>
<span id="cb487-2"><a href="#cb487-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(sex, <span class="at">fill =</span> source)) <span class="sc">+</span></span>
<span id="cb487-3"><a href="#cb487-3" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">position =</span> <span class="st">&quot;identity&quot;</span>)</span></code></pre></div>
<p><img src="05-S05-D1_files/figure-html/unnamed-chunk-9-3.png" width="672" /></p>
<p>Observe that we are iterating through the data analysis: import, inspect &amp; profile, preprocess, and profile again. Generate a crosstab summarizing the number of observations by race and sex:</p>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb488-1"><a href="#cb488-1" tabindex="-1"></a><span class="fu">table</span>(compas<span class="sc">$</span>race, compas<span class="sc">$</span>sex)</span></code></pre></div>
<pre><code>##                  
##                   Female Male
##   Asian                2   29
##   Black              549 2626
##   Hispanic            82  427
##   Native American      2    9
##   Other               58  285
##   White              482 1621</code></pre>
</div>
</div>
<div id="exploratory-analysis" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Exploratory analysis<a href="#exploratory-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s turn our focus to the primary variable of interest: the COMPAS recidivism score. In this exploratory analysis, we are interested in the variable named <code>decile_score</code>.</p>
<p>The ProPublica analysis notes: “Judges are often presented with two sets of scores from the COMPAS system: one that classifies people into high, medium or low risk, and a corresponding decile score.”</p>
<p>Plot the distribution of <code>decile_score</code> for males and for females. To what extent do these distributions differ?</p>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb490-1"><a href="#cb490-1" tabindex="-1"></a><span class="co"># plot decile score by sex</span></span>
<span id="cb490-2"><a href="#cb490-2" tabindex="-1"></a>compas <span class="sc">|&gt;</span></span>
<span id="cb490-3"><a href="#cb490-3" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> decile_score)) <span class="sc">+</span></span>
<span id="cb490-4"><a href="#cb490-4" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(</span>
<span id="cb490-5"><a href="#cb490-5" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">stat</span>(density <span class="sc">*</span> width)),</span>
<span id="cb490-6"><a href="#cb490-6" tabindex="-1"></a>    <span class="at">binwidth =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">&quot;white&quot;</span></span>
<span id="cb490-7"><a href="#cb490-7" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb490-8"><a href="#cb490-8" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) <span class="sc">+</span></span>
<span id="cb490-9"><a href="#cb490-9" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;proportion&quot;</span>) <span class="sc">+</span></span>
<span id="cb490-10"><a href="#cb490-10" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> sex)</span></code></pre></div>
<p><img src="05-S05-D1_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>What about race?</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb491-1"><a href="#cb491-1" tabindex="-1"></a><span class="co"># plot decile score by race</span></span>
<span id="cb491-2"><a href="#cb491-2" tabindex="-1"></a>compas <span class="sc">|&gt;</span></span>
<span id="cb491-3"><a href="#cb491-3" tabindex="-1"></a>  <span class="fu">filter</span>(race <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;Black&quot;</span>, <span class="st">&quot;White&quot;</span>)) <span class="sc">|&gt;</span></span>
<span id="cb491-4"><a href="#cb491-4" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> decile_score)) <span class="sc">+</span></span>
<span id="cb491-5"><a href="#cb491-5" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(</span>
<span id="cb491-6"><a href="#cb491-6" tabindex="-1"></a>    <span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">after_stat</span>(density <span class="sc">*</span> width)),</span>
<span id="cb491-7"><a href="#cb491-7" tabindex="-1"></a>    <span class="at">binwidth =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">&quot;white&quot;</span></span>
<span id="cb491-8"><a href="#cb491-8" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb491-9"><a href="#cb491-9" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>) <span class="sc">+</span></span>
<span id="cb491-10"><a href="#cb491-10" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;proportion&quot;</span>) <span class="sc">+</span></span>
<span id="cb491-11"><a href="#cb491-11" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> race)</span></code></pre></div>
<p><img src="05-S05-D1_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<div id="exercise" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">👉 Exercise<a href="#exercise" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Summarise the difference between the distribution of decile scores for Black defendants and White defendants in this text cell:</p>
<blockquote>
<p><em>Your answer here</em></p>
</blockquote>
</div>
<div id="risk-labels" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Risk labels<a href="#risk-labels" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Plot the distribution of COMPAS-assigned “risk labels” (the variable is named <code>score_text</code>) for Black defendants and White defendants:</p>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb492-1"><a href="#cb492-1" tabindex="-1"></a><span class="co"># plot risk labels by race</span></span>
<span id="cb492-2"><a href="#cb492-2" tabindex="-1"></a>compas <span class="sc">|&gt;</span></span>
<span id="cb492-3"><a href="#cb492-3" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb492-4"><a href="#cb492-4" tabindex="-1"></a>    <span class="at">score_text =</span> <span class="fu">factor</span>(</span>
<span id="cb492-5"><a href="#cb492-5" tabindex="-1"></a>      score_text,</span>
<span id="cb492-6"><a href="#cb492-6" tabindex="-1"></a>      <span class="at">levels =</span> <span class="fu">c</span>(<span class="st">&quot;Low&quot;</span>, <span class="st">&quot;Medium&quot;</span>, <span class="st">&quot;High&quot;</span>)</span>
<span id="cb492-7"><a href="#cb492-7" tabindex="-1"></a>    )</span>
<span id="cb492-8"><a href="#cb492-8" tabindex="-1"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb492-9"><a href="#cb492-9" tabindex="-1"></a>  <span class="fu">filter</span>(race <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;Black&quot;</span>, <span class="st">&quot;White&quot;</span>)) <span class="sc">|&gt;</span></span>
<span id="cb492-10"><a href="#cb492-10" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> score_text, <span class="at">group =</span> race, <span class="at">fill =</span> race)) <span class="sc">+</span></span>
<span id="cb492-11"><a href="#cb492-11" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">after_stat</span>(prop)), <span class="at">position =</span> <span class="st">&quot;dodge&quot;</span>) <span class="sc">+</span></span>
<span id="cb492-12"><a href="#cb492-12" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;proportion&quot;</span>)</span></code></pre></div>
<p><img src="05-S05-D1_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
</div>
<div id="bias-in-compas" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Bias in COMPAS<a href="#bias-in-compas" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>ProPublica focused on racial bias in the COMPAS algorithm. In general terms, ProPublica analyzed (i) how the <em>risk scores</em> vary by race and (ii) the extent to which the <em>risk labels</em> assigned to defendants matches up with their observed recidivism and how this varies by race. We will (approximately) reproduce this analysis below.</p>
<div id="preprocess-data-for-logistic-regression" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Preprocess data for logistic regression<a href="#preprocess-data-for-logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>ProPublica used a logistic regression model to analyze variation in the risk scores by race. In their analysis, they considered a “medium” and “high” risk score to be “high”, and “low” to be low. We will prepare the data accordingly, with <code>low = 0</code> and <code>high = 1</code>:</p>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb493-1"><a href="#cb493-1" tabindex="-1"></a>compas <span class="ot">&lt;-</span> </span>
<span id="cb493-2"><a href="#cb493-2" tabindex="-1"></a>  compas <span class="sc">|&gt;</span></span>
<span id="cb493-3"><a href="#cb493-3" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">score_binary =</span> <span class="fu">ifelse</span>(score_text <span class="sc">==</span> <span class="st">&quot;Low&quot;</span>, <span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb493-4"><a href="#cb493-4" tabindex="-1"></a></span>
<span id="cb493-5"><a href="#cb493-5" tabindex="-1"></a><span class="fu">table</span>(compas<span class="sc">$</span>score_text, compas<span class="sc">$</span>score_binary)</span></code></pre></div>
<pre><code>##         
##             0    1
##   High      0 1144
##   Low    3421    0
##   Medium    0 1607</code></pre>
</div>
<div id="estimate-the-logistic-regression-model" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Estimate the logistic regression model<a href="#estimate-the-logistic-regression-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb495"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb495-1"><a href="#cb495-1" tabindex="-1"></a>model <span class="ot">&lt;-</span> </span>
<span id="cb495-2"><a href="#cb495-2" tabindex="-1"></a>  <span class="fu">glm</span>(</span>
<span id="cb495-3"><a href="#cb495-3" tabindex="-1"></a>    score_binary <span class="sc">~</span></span>
<span id="cb495-4"><a href="#cb495-4" tabindex="-1"></a>      priors_count <span class="sc">+</span></span>
<span id="cb495-5"><a href="#cb495-5" tabindex="-1"></a>      two_year_recid <span class="sc">+</span></span>
<span id="cb495-6"><a href="#cb495-6" tabindex="-1"></a>      c_charge_degree <span class="sc">+</span></span>
<span id="cb495-7"><a href="#cb495-7" tabindex="-1"></a>      age_cat <span class="sc">+</span></span>
<span id="cb495-8"><a href="#cb495-8" tabindex="-1"></a>      race <span class="sc">+</span> </span>
<span id="cb495-9"><a href="#cb495-9" tabindex="-1"></a>      sex,</span>
<span id="cb495-10"><a href="#cb495-10" tabindex="-1"></a>    <span class="at">data =</span> compas,</span>
<span id="cb495-11"><a href="#cb495-11" tabindex="-1"></a>    <span class="at">family =</span> <span class="fu">binomial</span>(<span class="at">link =</span> <span class="st">&quot;logit&quot;</span>)</span>
<span id="cb495-12"><a href="#cb495-12" tabindex="-1"></a>  )</span>
<span id="cb495-13"><a href="#cb495-13" tabindex="-1"></a></span>
<span id="cb495-14"><a href="#cb495-14" tabindex="-1"></a><span class="fu">summary</span>(model)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = score_binary ~ priors_count + two_year_recid + 
##     c_charge_degree + age_cat + race + sex, family = binomial(link = &quot;logit&quot;), 
##     data = compas)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.9966  -0.7919  -0.3303   0.8121   2.6024  
## 
## Coefficients:
##                        Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)            -1.55869    0.48236  -3.231  0.00123 ** 
## priors_count            0.26895    0.01110  24.221  &lt; 2e-16 ***
## two_year_recid          0.68586    0.06402  10.713  &lt; 2e-16 ***
## c_charge_degreeM       -0.31124    0.06655  -4.677 2.91e-06 ***
## age_catGreater than 45 -1.35563    0.09908 -13.682  &lt; 2e-16 ***
## age_catLess than 25     1.30839    0.07593  17.232  &lt; 2e-16 ***
## raceBlack               0.73162    0.47708   1.534  0.12514    
## raceHispanic           -0.17398    0.48897  -0.356  0.72199    
## raceNative American     1.64862    0.89972   1.832  0.06689 .  
## raceOther              -0.57193    0.49897  -1.146  0.25170    
## raceWhite               0.25441    0.47821   0.532  0.59472    
## sexMale                -0.22127    0.07951  -2.783  0.00539 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 8483.3  on 6171  degrees of freedom
## Residual deviance: 6168.4  on 6160  degrees of freedom
## AIC: 6192.4
## 
## Number of Fisher Scoring iterations: 5</code></pre>
</div>
<div id="interpret-estimates" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Interpret estimates<a href="#interpret-estimates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Take a moment to read through the model summary.</p>
<p>One way to interpret the estimates is by calculating odds ratios. To calculate odds ratios, we take the exponential of the coefficients. For example, taking the exponential of the coefficient for defendants aged less than 25 (<span class="math inline">\(\beta_{age&lt;25}\)</span> = 1.30839) will return the odds of score_text taking the value “high” for those aged under 25 relative to those aged 25-45. Calculate this odds ratio here:</p>
<div class="sourceCode" id="cb497"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb497-1"><a href="#cb497-1" tabindex="-1"></a><span class="fu">exp</span>(<span class="fl">1.30839</span>)</span></code></pre></div>
<pre><code>## [1] 3.700212</code></pre>
<p>In words, the odds that COMPAS labeled a defendant as “high risk” of recidivism is 3.7 times greater for someone aged 25-45.</p>
<p>Next, calculate the odds ratio for all of the coefficients in the model:</p>
<div class="sourceCode" id="cb499"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb499-1"><a href="#cb499-1" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(model))</span></code></pre></div>
<pre><code>##            (Intercept)           priors_count         two_year_recid 
##              0.2104123              1.3085835              1.9854836 
##       c_charge_degreeM age_catGreater than 45    age_catLess than 25 
##              0.7325374              0.2577840              3.7002128 
##              raceBlack           raceHispanic    raceNative American 
##              2.0784485              0.8403136              5.1998120 
##              raceOther              raceWhite                sexMale 
##              0.5644338              1.2897066              0.8015029</code></pre>
<p>Take a moment to read through these coefficients. What is the reference category for each variable? (e.g. For females, the reference category is male.) Think in terms of comparisons, for example:</p>
<blockquote>
<p>A person with a value of <span class="math display">\[     \]</span> on variable <span class="math display">\[     \]</span> is <span class="math display">\[     \]</span> times more likely to be labeled high risk compared to a person with a value of <span class="math display">\[     \]</span> on variable <span class="math display">\[     \]</span></p>
</blockquote>
<p>In the female example above, this could be stated:</p>
<blockquote>
<p>“A person with a value of female on variable sex is 1.25 times more likely to be labeled high risk compared to a person with a value of male on variable sex”</p>
</blockquote>
<p>Of course, we should be more straightforward when writing up results. “A person with a value of male on variable sex” is rather verbose; “males” will suffice. Interpreting model estimates in straightforward terms is an underrated skill.</p>
</div>
<div id="exercise-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">👉 Exercise<a href="#exercise-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Summarise the odds associated with the <code>race</code> variable.</p>
<blockquote>
<p><em>Your answer here</em></p>
</blockquote>
</div>
</div>
<div id="predictive-accuracy" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Predictive Accuracy<a href="#predictive-accuracy" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In terms of fairness, ProPublica focused on the predictive accuracy of the COMPAS algorithm. In this case, predictive accuracy refers to the concordance between a person’s recidivism and the label assigned to that person by the COMPAS algorithm. For instance, how often did COMPAS predict that a person was at “high risk” of recidivism and that person in fact recidivated within two years? We can think of this in terms of a 2x2 table:</p>
<table>
<thead>
<tr class="header">
<th align="left"></th>
<th align="center">Did not recidivate</th>
<th align="right">Recidivated</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Labeled high risk</strong></td>
<td align="center">A</td>
<td align="right">B</td>
</tr>
<tr class="even">
<td align="left"><strong>Labeled low risk</strong></td>
<td align="center">C</td>
<td align="right">D</td>
</tr>
</tbody>
</table>
<p>ProPublica reported A and D for black defendants and white defendants, separately.</p>
<div id="exercise-2" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">👉 Exercise<a href="#exercise-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>What are generic terms for A and D? Why focus on A and D?</strong></p>
<blockquote>
<p><em>Your answer here</em></p>
</blockquote>
<p>ProPublica used a somewhat different data set to calculate the predictive accuracy of COMPAS. In this section we will use the <code>compas</code> data we preprocessed above for brevity. Note therefore that the numbers we calculate below will not match those reported by ProPublica. Let’s generate a crosstab of the variable denoting recidivism within two years (<code>is_recid</code>) and the binary score variable (<code>score_binary</code>):</p>
<div class="sourceCode" id="cb501"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb501-1"><a href="#cb501-1" tabindex="-1"></a><span class="fu">table</span>(compas<span class="sc">$</span>is_recid, compas<span class="sc">$</span>score_binary)</span></code></pre></div>
<pre><code>##    
##        0    1
##   0 2248  934
##   1 1173 1817</code></pre>
<p>Based on this crosstab, input the number of true positives, false positives, true negatives, and false negatives:</p>
<div class="sourceCode" id="cb503"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb503-1"><a href="#cb503-1" tabindex="-1"></a>true_positive  <span class="ot">=</span> <span class="dv">1817</span></span>
<span id="cb503-2"><a href="#cb503-2" tabindex="-1"></a>false_positive <span class="ot">=</span> <span class="dv">934</span></span>
<span id="cb503-3"><a href="#cb503-3" tabindex="-1"></a>true_negative  <span class="ot">=</span> <span class="dv">2248</span></span>
<span id="cb503-4"><a href="#cb503-4" tabindex="-1"></a>false_negative <span class="ot">=</span> <span class="dv">1173</span></span></code></pre></div>
<p>You can calculate the false positive rate by taking FP / (FP + TN), where FP is the number of false positives and TN is the number of true negatives. Calculate the false positive rate:</p>
<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb504-1"><a href="#cb504-1" tabindex="-1"></a><span class="fu">glue</span>(</span>
<span id="cb504-2"><a href="#cb504-2" tabindex="-1"></a>  <span class="st">&quot;All defendants, false positive rate:</span></span>
<span id="cb504-3"><a href="#cb504-3" tabindex="-1"></a><span class="st">  {(false_positive / (false_positive + true_negative) * 100)}&quot;</span></span>
<span id="cb504-4"><a href="#cb504-4" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## All defendants, false positive rate:
## 29.3526084223759</code></pre>
<p>Now calculate the false <em>negative</em> rate: (hint, replace the terms in the false positive rate formula in the previous text cell)</p>
<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb506-1"><a href="#cb506-1" tabindex="-1"></a><span class="fu">glue</span>(</span>
<span id="cb506-2"><a href="#cb506-2" tabindex="-1"></a>  <span class="st">&quot;All defendants, false negative rate:</span></span>
<span id="cb506-3"><a href="#cb506-3" tabindex="-1"></a><span class="st">  {(false_negative / (false_negative + true_positive) * 100)}&quot;</span></span>
<span id="cb506-4"><a href="#cb506-4" tabindex="-1"></a>)</span></code></pre></div>
<pre><code>## All defendants, false negative rate:
## 39.2307692307692</code></pre>
<p>How do the false positive and false negative rates vary by race? Calculate the false positive rate and false negative rate for White defendants:</p>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb508-1"><a href="#cb508-1" tabindex="-1"></a>rates <span class="ot">&lt;-</span> </span>
<span id="cb508-2"><a href="#cb508-2" tabindex="-1"></a>  compas <span class="sc">|&gt;</span></span>
<span id="cb508-3"><a href="#cb508-3" tabindex="-1"></a>  <span class="fu">filter</span>(race <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;Black&quot;</span>, <span class="st">&quot;White&quot;</span>)) <span class="sc">|&gt;</span></span>
<span id="cb508-4"><a href="#cb508-4" tabindex="-1"></a>  <span class="fu">group_by</span>(race) <span class="sc">|&gt;</span></span>
<span id="cb508-5"><a href="#cb508-5" tabindex="-1"></a>  <span class="fu">count</span>(score_binary, is_recid)</span></code></pre></div>
<div class="sourceCode" id="cb509"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb509-1"><a href="#cb509-1" tabindex="-1"></a><span class="co"># white defendants</span></span>
<span id="cb509-2"><a href="#cb509-2" tabindex="-1"></a>w_tp <span class="ot">=</span> <span class="dv">430</span></span>
<span id="cb509-3"><a href="#cb509-3" tabindex="-1"></a>w_fp <span class="ot">=</span> <span class="dv">266</span></span>
<span id="cb509-4"><a href="#cb509-4" tabindex="-1"></a>w_tn <span class="ot">=</span> <span class="dv">963</span></span>
<span id="cb509-5"><a href="#cb509-5" tabindex="-1"></a>w_fn <span class="ot">=</span> <span class="dv">444</span></span>
<span id="cb509-6"><a href="#cb509-6" tabindex="-1"></a></span>
<span id="cb509-7"><a href="#cb509-7" tabindex="-1"></a><span class="fu">glue</span>(<span class="st">&quot;White defendants, false positive rate: {(w_fp / (w_fp + w_tn) * 100)}&quot;</span>)</span></code></pre></div>
<pre><code>## White defendants, false positive rate: 21.6436126932465</code></pre>
<div class="sourceCode" id="cb511"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb511-1"><a href="#cb511-1" tabindex="-1"></a><span class="fu">glue</span>(<span class="st">&quot;White defendants, false negative rate: {(w_fn / (w_fn + w_tp) * 100)}&quot;</span>)</span></code></pre></div>
<pre><code>## White defendants, false negative rate: 50.8009153318078</code></pre>
<p>Lastly, calculate the false positive rate and false negative rate for Black defendants:</p>
<div class="sourceCode" id="cb513"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb513-1"><a href="#cb513-1" tabindex="-1"></a>b_tp <span class="ot">=</span> <span class="dv">1248</span></span>
<span id="cb513-2"><a href="#cb513-2" tabindex="-1"></a>b_fp <span class="ot">=</span> <span class="dv">581</span></span>
<span id="cb513-3"><a href="#cb513-3" tabindex="-1"></a>b_tn <span class="ot">=</span> <span class="dv">821</span></span>
<span id="cb513-4"><a href="#cb513-4" tabindex="-1"></a>b_fn <span class="ot">=</span> <span class="dv">525</span></span>
<span id="cb513-5"><a href="#cb513-5" tabindex="-1"></a></span>
<span id="cb513-6"><a href="#cb513-6" tabindex="-1"></a><span class="fu">glue</span>(<span class="st">&quot;Black defendants, false positive rate: {(b_fp / (b_fp + b_tn) * 100)}&quot;</span>)</span></code></pre></div>
<pre><code>## Black defendants, false positive rate: 41.4407988587732</code></pre>
<div class="sourceCode" id="cb515"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb515-1"><a href="#cb515-1" tabindex="-1"></a><span class="fu">glue</span>(<span class="st">&quot;Black defendants, false negative rate: {(b_fn / (b_fn + b_tp) * 100)}&quot;</span>)</span></code></pre></div>
<pre><code>## Black defendants, false negative rate: 29.6108291032149</code></pre>
<p>Take a moment to review and compare the false positive rates and false negative rates above.</p>
</div>
<div id="exercise-3" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">👉 Exercise<a href="#exercise-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Reflect on the false positive and false negative rates for Black and White defendants. What do these rates suggest about the COMPAS algorithm’s and whether it should be used to assist sentencing decisions?</p>
<blockquote>
<p>Your answer here</p>
</blockquote>
<!--chapter:end:05-S05-D1.Rmd-->
</div>
</div>
</div>
<div id="demonstration-2-the-basics-of-decision-trees-and-related-methods" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Demonstration 2: The Basics of Decision Trees and Related Methods<a href="#demonstration-2-the-basics-of-decision-trees-and-related-methods" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this demonstration, you will learn about <em>decision trees</em>:
<em>regression</em> trees are used when the outcome is quantitative and
<em>classification</em> trees used when the outcome is categorical.</p>
<p>The basics are quite simple (even simpler than linear regression!): we
split the predictor space to a number of regions and the prediction for
every outcome in a region is the <em>mean</em> (for regression) or <em>mode</em> (for
classification) of the observations in that region. Given that the
structure of decision trees resemble human decision-making (to a certain
extent) and that the output can be easily illustrated, this makes them
quite easy to interpret, even by non-experts. Also, there is no need for
dummy variables since trees easy handle categorical predictors.</p>
<p>We will explore each of these in greater detail in the walkthrough
below. Before beginning, you will need to install and load the <code>gbm</code>,
<code>tree</code>, and <code>randomForest</code> packages. The <code>tree</code> library is used to
construct classification and regression trees.</p>
<div class="sourceCode" id="cb517"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb517-1"><a href="#cb517-1" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb517-2"><a href="#cb517-2" tabindex="-1"></a><span class="fu">library</span>(tree)</span>
<span id="cb517-3"><a href="#cb517-3" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb517-4"><a href="#cb517-4" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb517-5"><a href="#cb517-5" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
<div id="decision-trees" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Decision Trees<a href="#decision-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="classification-trees" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Classification Trees<a href="#classification-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="file">
<p>For the tasks below, you will require the <strong>Carseats</strong> dataset. This
dataset is part of the <code>ISRL2</code> package from the core textbook (James et.
al 2021). By loading the package, the <strong>Carseats</strong> dataset will load
automatically.</p>
</div>
<p>The <strong>Carseats</strong> is a simulated dataset on sales of child car seats at
different stores. There are 400 observations on 11 variables.</p>
<table>
<colgroup>
<col width="19%" />
<col width="80%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Variable Name</th>
<th align="center">Variable Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Sales</td>
<td align="center">Unit sales (in thousands) at each location</td>
</tr>
<tr class="even">
<td align="center">CompPrice</td>
<td align="center">Price charged by competitor at each location</td>
</tr>
<tr class="odd">
<td align="center">Income</td>
<td align="center">Community income level (in thousands of dollars)</td>
</tr>
<tr class="even">
<td align="center">Advertising</td>
<td align="center">Local advertising budget for company at each location (in thousands of dollars)</td>
</tr>
<tr class="odd">
<td align="center">Population</td>
<td align="center">Population size in region (in thousands)</td>
</tr>
<tr class="even">
<td align="center">Price</td>
<td align="center">Price company charges for car seats at each site</td>
</tr>
<tr class="odd">
<td align="center">ShelveLoc</td>
<td align="center">A factor with levels Bad, Good and Medium indicating the quality of the shelving location for the car seats at each site</td>
</tr>
<tr class="even">
<td align="center">Age</td>
<td align="center">Average age of the local population</td>
</tr>
<tr class="odd">
<td align="center">Education</td>
<td align="center">Education level at each location</td>
</tr>
<tr class="even">
<td align="center">Urban</td>
<td align="center">A factor with levels No and Yes to indicate whether the store is in an urban or rural location</td>
</tr>
<tr class="odd">
<td align="center">US</td>
<td align="center">A factor with levels No and Yes to indicate whether the store is in the US or not</td>
</tr>
</tbody>
</table>
<p>We will now explore how classification trees can be used to predict
whether sales of car seats are high or low. Here <strong>Sales</strong> is continuous
and so we create a new binary variable such that all unit sales over
8,000 dollars are classed as ‘Yes’ (i.e. so high sales) and everything
else and ‘No’ (i.e. so low sales). We store this as a vector with our
response values for the test set for evaluating our model later and we
also add this as a variable in our dataset.</p>
<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb518-1"><a href="#cb518-1" tabindex="-1"></a><span class="fu">attach</span>(Carseats)</span>
<span id="cb518-2"><a href="#cb518-2" tabindex="-1"></a>High <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">ifelse</span>(Sales <span class="sc">&lt;=</span> <span class="dv">8</span>, <span class="st">&quot;No&quot;</span>, <span class="st">&quot;Yes&quot;</span>))</span>
<span id="cb518-3"><a href="#cb518-3" tabindex="-1"></a>Carseats <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(Carseats, High)</span></code></pre></div>
<p>Let’s now split the data into training and test sets. We randomly select
half of the observations from the dataset for our training set and we
allocate the rest to the test set (<code>Carseats.test</code>).</p>
<div class="sourceCode" id="cb519"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb519-1"><a href="#cb519-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb519-2"><a href="#cb519-2" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(Carseats), <span class="dv">200</span>)</span></code></pre></div>
<p>Now let’s fit the tree using the <code>tree()</code> function from the package
<code>tree</code> that you have just installed. As shown below, the basic syntax is
quite simple. Since we want to use all variables in the data object as
predictors, there is no need to list them all in the formula. Instead,
we can simply use a dot. However, we must drop the original <strong>Sales</strong>
variable for obvious reasons. To fit the tree to the training data only,
we must subset the Carseats data using the <strong>train</strong> object which
contains a vector of the randomly selected indices that tells R which
values to subset.</p>
<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb520-1"><a href="#cb520-1" tabindex="-1"></a>tree.carseats <span class="ot">&lt;-</span> <span class="fu">tree</span>(High <span class="sc">~</span> . <span class="sc">-</span> Sales, Carseats, <span class="at">subset =</span> train)</span></code></pre></div>
<p>To better understand trees, let’s explore the structure of our tree
visually first. We simply specify the name of the model and complement
the <code>plot()</code> function with the function <code>text()</code> within which we set the
<code>pretty</code> argument to <code>0</code> in order to include the category names for
categorical predictors.</p>
<div class="sourceCode" id="cb521"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb521-1"><a href="#cb521-1" tabindex="-1"></a><span class="fu">plot</span>(tree.carseats)</span>
<span id="cb521-2"><a href="#cb521-2" tabindex="-1"></a><span class="fu">text</span>(tree.carseats, <span class="at">pretty =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p>Our first split occurs at <strong>Price</strong>, which indicates that this is the
feature (i.e. variable) that is most important for classifying sales as
high or not. From this point forward, further splits are made and the
features at which these splits are made are referred to as <em>internal
nodes</em>. The tree splits first on <strong>Price</strong>, thus dividing the dataset
into two subsets based on whether Price is less than <span class="math inline">\(96.5\)</span>. This is our
split criterion. The left side represents the subset within which price
is less than <span class="math inline">\(96.5\)</span> whilst the right side represents the subset within
which price is equal to or greater than <span class="math inline">\(96.5\)</span>.</p>
<p>Let’s first consider the left side of the tree. The subsequent internal
node is <strong>Population</strong> and the split criterion is such that population
is less than <span class="math inline">\(414\)</span> which means that within the subset where price is
less than <span class="math inline">\(96.5\)</span> (so the left side), the next most significant predictor
is <strong>Population</strong>. Within population, it is <strong>ShelveLoc</strong>, then <strong>Age</strong>
and so on. Hence, the tree continues to split based on these features
and values, further refining the subsets.</p>
<p>The right side represents the subset within which price is greater than
or equal to <span class="math inline">\(96.5\)</span>. Our subsequent node after <strong>Price</strong> is <strong>ShelveLoc</strong>
which is a factor variable. This means that within the subset where
<strong>Price</strong> is greater than or equal to <span class="math inline">\(96.5\)</span>, the location of the
shelves is the next most significant predictor. From there onward, as on
the left side, the tree continues to split further and further at
subsequent internal nodes according to specific split criteria.
Therefore, since the feature at which the first split occurs represents
the feature with the most importance regarding classification, the
internal nodes that are closest to this node are generally more
important than those closer to the bottom of the tree.</p>
<p>The vertical lines that connect a feature with an outcome are called
<em>branches</em>. Each branch represents a decision based on a feature value,
which then progresses to the next internal node or finally, to a <em>leaf
node</em>. A leaf node (also called a terminal node) are the nodes at the
bottom of the tree that provide the final classification. Each leaf node
represents a final decision regarding the classification (e.g., “Yes” or
“No” for high sales). The path from the root to a leaf node gives the
sequence of decisions made to classify an observation. Essentially, the
splits on either side simply refine the classification until a decision
is reached.</p>
<p>So we start off at the first split where <span class="math inline">\(Price &lt; 96.5\)</span>. If
<span class="math inline">\(Price &lt; 96.5\)</span> is true, we move on to the next node on the left
hand-side which is <span class="math inline">\(Population &lt; 414\)</span>. We follow subsequent nodes and
branches based on the feature values of the observation and we reach a
terminal (leaf) node with the final classification (e.g., “Yes” for high
sales). So a terminal node is one from which no further splits occur. In
summary, our tree make use of features to split the data into subsets to
classify whether sales are high or not.</p>
<p>Now let’s explore the output of the tree in greater detail. If we just
type the name of the tree object, R prints output detailed information
about the tree structure that we illustrated . The first line of the
output:
<code>node), split, n, deviance, yval, (yprob) * denotes terminal node</code>
describes what each component of the output means.</p>
<div class="sourceCode" id="cb522"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb522-1"><a href="#cb522-1" tabindex="-1"></a>tree.carseats</span></code></pre></div>
<pre><code>## node), split, n, deviance, yval, (yprob)
##       * denotes terminal node
## 
##   1) root 200 270.000 No ( 0.59500 0.40500 )  
##     2) Price &lt; 96.5 40  47.050 Yes ( 0.27500 0.72500 )  
##       4) Population &lt; 414 31  40.320 Yes ( 0.35484 0.64516 )  
##         8) ShelveLoc: Bad,Medium 25  34.300 Yes ( 0.44000 0.56000 )  
##          16) Age &lt; 64.5 17  20.600 Yes ( 0.29412 0.70588 )  
##            32) Education &lt; 13.5 7   0.000 Yes ( 0.00000 1.00000 ) *
##            33) Education &gt; 13.5 10  13.860 Yes ( 0.50000 0.50000 )  
##              66) Education &lt; 16.5 5   5.004 No ( 0.80000 0.20000 ) *
##              67) Education &gt; 16.5 5   5.004 Yes ( 0.20000 0.80000 ) *
##          17) Age &gt; 64.5 8   8.997 No ( 0.75000 0.25000 ) *
##         9) ShelveLoc: Good 6   0.000 Yes ( 0.00000 1.00000 ) *
##       5) Population &gt; 414 9   0.000 Yes ( 0.00000 1.00000 ) *
##     3) Price &gt; 96.5 160 201.800 No ( 0.67500 0.32500 )  
##       6) ShelveLoc: Bad,Medium 135 154.500 No ( 0.74074 0.25926 )  
##        12) Price &lt; 124.5 82 107.700 No ( 0.63415 0.36585 )  
##          24) Age &lt; 49.5 34  45.230 Yes ( 0.38235 0.61765 )  
##            48) CompPrice &lt; 130.5 21  28.680 No ( 0.57143 0.42857 )  
##              96) Population &lt; 134.5 6   0.000 No ( 1.00000 0.00000 ) *
##              97) Population &gt; 134.5 15  20.190 Yes ( 0.40000 0.60000 )  
##               194) Population &lt; 343 7   5.742 Yes ( 0.14286 0.85714 ) *
##               195) Population &gt; 343 8  10.590 No ( 0.62500 0.37500 ) *
##            49) CompPrice &gt; 130.5 13   7.051 Yes ( 0.07692 0.92308 ) *
##          25) Age &gt; 49.5 48  46.330 No ( 0.81250 0.18750 )  
##            50) CompPrice &lt; 124.5 28  14.410 No ( 0.92857 0.07143 )  
##             100) Price &lt; 101.5 8   8.997 No ( 0.75000 0.25000 ) *
##             101) Price &gt; 101.5 20   0.000 No ( 1.00000 0.00000 ) *
##            51) CompPrice &gt; 124.5 20  25.900 No ( 0.65000 0.35000 )  
##             102) Price &lt; 119 14  19.410 No ( 0.50000 0.50000 )  
##               204) Advertising &lt; 10.5 9  11.460 No ( 0.66667 0.33333 ) *
##               205) Advertising &gt; 10.5 5   5.004 Yes ( 0.20000 0.80000 ) *
##             103) Price &gt; 119 6   0.000 No ( 1.00000 0.00000 ) *
##        13) Price &gt; 124.5 53  33.120 No ( 0.90566 0.09434 )  
##          26) Population &lt; 393.5 34   0.000 No ( 1.00000 0.00000 ) *
##          27) Population &gt; 393.5 19  21.900 No ( 0.73684 0.26316 )  
##            54) CompPrice &lt; 143.5 13   7.051 No ( 0.92308 0.07692 ) *
##            55) CompPrice &gt; 143.5 6   7.638 Yes ( 0.33333 0.66667 ) *
##       7) ShelveLoc: Good 25  31.340 Yes ( 0.32000 0.68000 )  
##        14) Income &lt; 43 7   8.376 No ( 0.71429 0.28571 ) *
##        15) Income &gt; 43 18  16.220 Yes ( 0.16667 0.83333 )  
##          30) US: No 6   8.318 Yes ( 0.50000 0.50000 ) *
##          31) US: Yes 12   0.000 Yes ( 0.00000 1.00000 ) *</code></pre>
<p>So let’s take <code>2) Price &lt; 96.5 40  47.050 Yes</code>( 0.27500 0.72500 )` as
an example.</p>
<ul>
<li><code>2)</code> denotes the unique identifier of the node (so this is node 2),<br />
</li>
<li><code>Price &lt; 96.5</code> is the condition of the split</li>
<li><code>40</code> is the number of observations that reach the node *in this
case 40)</li>
<li><code>47.050</code> is the deviance (i.e. impurity of the node after the split)</li>
<li><code>Yes</code> is the predicted class for this node</li>
<li><code>( 0.27500 0.72500 )</code> are the class probabilities and so the
proportion of observations belonging to each class at the node (in
other words the fraction of observations in that branch that take on
values of <code>Yes</code> and <code>No</code>)<br />
</li>
<li>the <code>*</code> denotes a terminal node</li>
</ul>
<p>Now you may wonder about the root node (<code>1)</code>). This node (node 1)
represents the entire dataset with 200 observations in total, the most
common class at the root node (in this case <code>No</code>) and the proportion of
classes (i.e. Yes and No) in the dataset. The root node corresponds to
the decision to split at <code>Price &lt; 96.5</code> in the graphic, but essentially,
it represents the entire dataset <em>before</em> any splits are made; as you
can see, there is no information about <code>split</code> for the root node itself
since it doesn’t split. In other words, the root node starts the
solitting process, and each subsequent split aims to further refine the
classification by reducing <em>impurity</em> (i.e. variance), with the deviance
value helping in measuring the effectiveness of each split. Therefore
<code>Price &lt; 96.5</code> is the condition for the split the root node.</p>
<p>To better understand how each feature and split contribute to the final
prediction, we must consider not only explore the structure as a whole,
but also consider how the data are split into branches based on
different features, how probabilities change at each node, and finally,
what final predictions are provided by the terminal nodes given the path
from the root node.</p>
<p>Starting at the root node, the splits are conducted according to the
following conditions: Price &lt; 96.5, Population &lt; 414, ShelveLoc: Bad,
Medium, Age &lt; 64.5, and Education &lt; 13.5.</p>
<p>Therefore:</p>
<ul>
<li>Price &lt; 96.5: Among stores with prices lower than 96.5 (40 stores
in total), 72.5% are likely have high sales (hence High = “Yes”).</li>
<li>Population &lt; 414: Of these, 64.5% in lower population areas tend to
have high sales.<br />
</li>
<li>ShelveLoc: Bad, Medium: Among these, 56.0% stores with poor shelving
location still tend to have high sales<br />
</li>
<li>Age &lt; 64.5: Of these, in regions where the average age of the
population is less than 64.5, 70.6% of stores are still likely have
high sales<br />
</li>
<li>Education &lt; 13.5: Finally, in areas where education is less than
13.5, stores are still likely to have high sales with 100%
probability.</li>
</ul>
<p>As with other models we’ve explored so far, <code>summary()</code> also comes in
handy. The output contains additional information.</p>
<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb524-1"><a href="#cb524-1" tabindex="-1"></a><span class="fu">summary</span>(tree.carseats)</span></code></pre></div>
<pre><code>## 
## Classification tree:
## tree(formula = High ~ . - Sales, data = Carseats, subset = train)
## Variables actually used in tree construction:
## [1] &quot;Price&quot;       &quot;Population&quot;  &quot;ShelveLoc&quot;   &quot;Age&quot;         &quot;Education&quot;  
## [6] &quot;CompPrice&quot;   &quot;Advertising&quot; &quot;Income&quot;      &quot;US&quot;         
## Number of terminal nodes:  21 
## Residual mean deviance:  0.5543 = 99.22 / 179 
## Misclassification error rate: 0.115 = 23 / 200</code></pre>
<ul>
<li>variables actually used in tree construction: here we note 9
variables (<strong>Urban</strong> is missing)<br />
</li>
<li>number of terminal nodes: also referred to as ‘leaves’, the tree has
21 terminal nodes; these represent the final decision or
classification outcome.<br />
</li>
<li>residual mean deviance: For classification trees, the deviance is
given by <span class="math inline">\(-2 \sum_m \sum_k n_{mk} \log \hat{p}_{mk}\)</span>, where <span class="math inline">\(n_{mk}\)</span>
is the number of observations in the <span class="math inline">\(m\)</span>th terminal node that belong
to the <span class="math inline">\(k\)</span>th class. The smaller the deviance, the better the fit to
the training data. The <em>residual mean deviance</em> is the deviance
divided by <span class="math inline">\(n-|{T}_0|\)</span>.<br />
</li>
<li>misclassification error rate: this is the training error rate, which
in this case is <span class="math inline">\(11.5\%\)</span>. This seems quite good.</li>
</ul>
<p>Now that you have a good grasp of how trees are built and interpreted,
let’s evaluate the performance of the algorithm on the test data.The
<code>predict()</code> function can be used for this purpose. In the case of a
classification tree, the argument <code>type = "class"</code> instructs <code>R</code> to
return the actual class prediction. This approach leads to correct
predictions for around <span class="math inline">\(77 \%\)</span> of the locations in the test data set.</p>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb526-1"><a href="#cb526-1" tabindex="-1"></a>High.test <span class="ot">&lt;-</span> High[<span class="sc">-</span>train]</span>
<span id="cb526-2"><a href="#cb526-2" tabindex="-1"></a>Carseats.test <span class="ot">&lt;-</span> Carseats[<span class="sc">-</span>train, ]</span>
<span id="cb526-3"><a href="#cb526-3" tabindex="-1"></a>tree.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree.carseats, Carseats.test, <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb526-4"><a href="#cb526-4" tabindex="-1"></a><span class="fu">table</span>(tree.pred, High.test)</span></code></pre></div>
<pre><code>##          High.test
## tree.pred  No Yes
##       No  104  33
##       Yes  13  50</code></pre>
<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb528-1"><a href="#cb528-1" tabindex="-1"></a>(<span class="dv">104</span> <span class="sc">+</span> <span class="dv">50</span>) <span class="sc">/</span> <span class="dv">200</span></span></code></pre></div>
<pre><code>## [1] 0.77</code></pre>
<p>(If you re-run the <code>predict()</code> function then you might get slightly
different results, due to “ties”: for instance, this can happen when the
training observations corresponding to a terminal node are evenly split
between <code>Yes</code> and <code>No</code> response values.)</p>
<p>Next, we consider whether <em>pruning</em> the tree might lead to improved
results. Pruning is used to reduce size and complexity of the tree and
the main goal is to remove parts of the tree that do not provide
additional power in predicting target variables. Pruning is therefore
very important in preventing overfitting, improving interpretability and
enhancing the performance of the tree. One approach is <em>cost-complexity
pruning</em> which involves pruning the tree such that it balances the
trade-off between the accuracy and size of the tree (by minimising the
cost-complexity criterion).</p>
<p>Ok so let’s see how this works in practice. The function <code>cv.tree()</code>
performs <span class="math inline">\(k\)</span>-fold cross-validation in order to determine the optimal
level of tree complexity (i.e. to find the deviance or number of
misclassifications as a function of the cost-complexity parameter <span class="math inline">\(k\)</span>).
This approach involves growing the tree to its full depth and then
removing nodes that provide little predictive power. We use the argument
<code>FUN = prune.misclass</code> to tell R that we want the classification error
rate to guide the cross-validation and pruning process, rather than the
default for the <code>cv.tree()</code> function, which is deviance. The <code>cv.tree()</code>
function reports the number of terminal nodes of each tree considered
(<code>size</code>) as well as the corresponding error rate and the value of the
cost-complexity parameter used (<code>k</code>).</p>
<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb530-1"><a href="#cb530-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">7</span>)</span>
<span id="cb530-2"><a href="#cb530-2" tabindex="-1"></a>cv.carseats <span class="ot">&lt;-</span> <span class="fu">cv.tree</span>(tree.carseats, <span class="at">FUN =</span> prune.misclass)</span></code></pre></div>
<p>The output contains several pieces of important information such as
size, deviance and the values for <span class="math inline">\(k\)</span>. Each of these is paired with the
other, and so the lowest deviance which is 74 (i.e. 74 cross-validation
errors) corresponds to a tree of size 9 (i.e. a tree with 9 terminal
nodes) and a value of 1.4 for <span class="math inline">\(k\)</span>. The lower the value for <span class="math inline">\(k\)</span>, the less
pruning required.</p>
<div class="sourceCode" id="cb531"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb531-1"><a href="#cb531-1" tabindex="-1"></a><span class="fu">names</span>(cv.carseats)</span></code></pre></div>
<pre><code>## [1] &quot;size&quot;   &quot;dev&quot;    &quot;k&quot;      &quot;method&quot;</code></pre>
<div class="sourceCode" id="cb533"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb533-1"><a href="#cb533-1" tabindex="-1"></a>cv.carseats</span></code></pre></div>
<pre><code>## $size
## [1] 21 19 14  9  8  5  3  2  1
## 
## $dev
## [1] 75 75 75 74 82 83 83 85 82
## 
## $k
## [1] -Inf  0.0  1.0  1.4  2.0  3.0  4.0  9.0 18.0
## 
## $method
## [1] &quot;misclass&quot;
## 
## attr(,&quot;class&quot;)
## [1] &quot;prune&quot;         &quot;tree.sequence&quot;</code></pre>
<p>We can also plot the error rate as a function of both <code>size</code> and <code>k</code>.</p>
<div class="sourceCode" id="cb535"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb535-1"><a href="#cb535-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb535-2"><a href="#cb535-2" tabindex="-1"></a><span class="fu">plot</span>(cv.carseats<span class="sc">$</span>size, cv.carseats<span class="sc">$</span>dev, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span>
<span id="cb535-3"><a href="#cb535-3" tabindex="-1"></a><span class="fu">plot</span>(cv.carseats<span class="sc">$</span>k, cv.carseats<span class="sc">$</span>dev, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>We now apply the <code>prune.misclass()</code> function in order to prune the tree
to obtain the nine-node tree as indicated by the cross-validation
results. We specify the size using the <code>best</code> argument.</p>
<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb536-1"><a href="#cb536-1" tabindex="-1"></a>prune.carseats <span class="ot">&lt;-</span> <span class="fu">prune.misclass</span>(tree.carseats, <span class="at">best =</span> <span class="dv">9</span>)</span>
<span id="cb536-2"><a href="#cb536-2" tabindex="-1"></a><span class="fu">plot</span>(prune.carseats)</span>
<span id="cb536-3"><a href="#cb536-3" tabindex="-1"></a><span class="fu">text</span>(prune.carseats, <span class="at">pretty =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>How well does this pruned tree perform on the test data set? Once again,
we apply the <code>predict()</code> function.</p>
<div class="sourceCode" id="cb537"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb537-1"><a href="#cb537-1" tabindex="-1"></a>tree.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(prune.carseats, Carseats.test,</span>
<span id="cb537-2"><a href="#cb537-2" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb537-3"><a href="#cb537-3" tabindex="-1"></a><span class="fu">table</span>(tree.pred, High.test)</span></code></pre></div>
<pre><code>##          High.test
## tree.pred No Yes
##       No  97  25
##       Yes 20  58</code></pre>
<div class="sourceCode" id="cb539"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb539-1"><a href="#cb539-1" tabindex="-1"></a>(<span class="dv">97</span> <span class="sc">+</span> <span class="dv">58</span>) <span class="sc">/</span> <span class="dv">200</span></span></code></pre></div>
<pre><code>## [1] 0.775</code></pre>
<p>Now <span class="math inline">\(77.5 \%\)</span> of the test observations are correctly classified, so not
only has the pruning process produced a more interpretable tree, but it
has also slightly improved the classification accuracy.</p>
<p>To illustrate what happens if we increase the size of the tree, let’s
set the <code>best</code> argument to 14. As you can see, a tree of size 14
produces a more complex tree that is somewhat harder to interpret and
the fraction of correct predictions is also slightly lower.</p>
<div class="sourceCode" id="cb541"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb541-1"><a href="#cb541-1" tabindex="-1"></a>prune.carseats <span class="ot">&lt;-</span> <span class="fu">prune.misclass</span>(tree.carseats, <span class="at">best =</span> <span class="dv">14</span>)</span>
<span id="cb541-2"><a href="#cb541-2" tabindex="-1"></a><span class="fu">plot</span>(prune.carseats)</span>
<span id="cb541-3"><a href="#cb541-3" tabindex="-1"></a><span class="fu">text</span>(prune.carseats, <span class="at">pretty =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb542-1"><a href="#cb542-1" tabindex="-1"></a>tree.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(prune.carseats, Carseats.test,</span>
<span id="cb542-2"><a href="#cb542-2" tabindex="-1"></a>    <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span>
<span id="cb542-3"><a href="#cb542-3" tabindex="-1"></a><span class="fu">table</span>(tree.pred, High.test)</span></code></pre></div>
<pre><code>##          High.test
## tree.pred  No Yes
##       No  102  31
##       Yes  15  52</code></pre>
<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb544-1"><a href="#cb544-1" tabindex="-1"></a>(<span class="dv">102</span> <span class="sc">+</span> <span class="dv">52</span>) <span class="sc">/</span> <span class="dv">200</span></span></code></pre></div>
<pre><code>## [1] 0.77</code></pre>
</div>
<div id="regression-trees" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Regression Trees<a href="#regression-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="file">
<p>For the tasks below, you will require the <strong>Boston</strong> dataset. This
dataset is part of the <code>ISRL2</code> package from the core textbook (James et.
al 2021).</p>
</div>
<p>The <strong>Boston</strong> is a datast that contains housing values in 506 suburbs
of Boston. There are 13 variables.</p>
<table>
<colgroup>
<col width="22%" />
<col width="77%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Variable Name</th>
<th align="center">Variable Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">crim</td>
<td align="center">per capita crime rate by town</td>
</tr>
<tr class="even">
<td align="center">zn</td>
<td align="center">proportion of residential land zoned for lots over 25,000 sq.ft</td>
</tr>
<tr class="odd">
<td align="center">indus</td>
<td align="center">proportion of non-retail business acres per town</td>
</tr>
<tr class="even">
<td align="center">chas</td>
<td align="center">Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)</td>
</tr>
<tr class="odd">
<td align="center">nox</td>
<td align="center">nitrogen oxides concentration (parts per 10 million)</td>
</tr>
<tr class="even">
<td align="center">rm</td>
<td align="center">average number of rooms per dwelling</td>
</tr>
<tr class="odd">
<td align="center">age</td>
<td align="center">proportion of owner-occupied units built prior to 1940</td>
</tr>
<tr class="even">
<td align="center">dis</td>
<td align="center">weighted mean of distances to five Boston employment centers</td>
</tr>
<tr class="odd">
<td align="center">rad</td>
<td align="center">index of accessibility to radial highways</td>
</tr>
<tr class="even">
<td align="center">tax</td>
<td align="center">full-value property-tax rate per $10,000</td>
</tr>
<tr class="odd">
<td align="center">ptratio:</td>
<td align="center">pupil-teacher ratio by town</td>
</tr>
<tr class="even">
<td align="center">lstat</td>
<td align="center">lower status of the population (percent).</td>
</tr>
<tr class="odd">
<td align="center">medv</td>
<td align="center">median value of owner-occupied homes in $1000s.</td>
</tr>
</tbody>
</table>
<p>We will now explore how a regression decision tree can be used to
predict the median value of owner-occupied homes (<strong>medv</strong>) using all
variables in the dataset.</p>
<p>Let’s split the data into a training and test set.</p>
<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb546-1"><a href="#cb546-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb546-2"><a href="#cb546-2" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(Boston), <span class="fu">nrow</span>(Boston) <span class="sc">/</span> <span class="dv">2</span>)</span></code></pre></div>
<p>We fit the model in the same way as we did for the classification tree.</p>
<div class="sourceCode" id="cb547"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb547-1"><a href="#cb547-1" tabindex="-1"></a>tree.boston <span class="ot">&lt;-</span> <span class="fu">tree</span>(medv <span class="sc">~</span> ., Boston, <span class="at">subset =</span> train)</span>
<span id="cb547-2"><a href="#cb547-2" tabindex="-1"></a><span class="fu">summary</span>(tree.boston)</span></code></pre></div>
<pre><code>## 
## Regression tree:
## tree(formula = medv ~ ., data = Boston, subset = train)
## Variables actually used in tree construction:
## [1] &quot;rm&quot;    &quot;lstat&quot; &quot;crim&quot;  &quot;age&quot;  
## Number of terminal nodes:  7 
## Residual mean deviance:  10.38 = 2555 / 246 
## Distribution of residuals:
##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## -10.1800  -1.7770  -0.1775   0.0000   1.9230  16.5800</code></pre>
<p>The structure of the results are similar to those of the classification
tree. But note that for our model here, only four variables were used to
construct the tree, namely <strong>rm</strong>, <strong>lstat</strong>, <strong>crim</strong>, <strong>age</strong>. We saw
this already in the case of the classification tree where one of the
variables was missing.</p>
<p>Why is this the case? The algorithm selects variables according to their
role in improving predictive accuracy at each split. The reason for this
is that we did not add any further arguments to this function and so we
allowed R to use the default settings for tree growth, which include
thresholds for minimum deviance, minimum node size, and maximum tree
depth. By not specifying the control parameter, the function uses
default settings for tree growth, which include thresholds for minimum
deviance, minimum node size, and maximum tree depth (31), preventing the
tree from growing excessively large. However, it is important to note
that we <em>could have fit</em> a much bigger tree, by passing
<code>control = tree.control(nobs = length(train), mindev = 0)</code> into the
<code>tree()</code> function.</p>
<p>Since in our case only four variables were selected, this suggests that
only these four provide a sufficiently significant improvement in
reducing <em>variance</em> at different splits. There are several reasons for
this, besides the predictive power of the variables and the associated
impurity, such as sample size, correlation among variables, and
complexity. We also note that there are 7 terminal nodes, and that the
residual mean deviance is 10.38. In the context of a regression tree,
the deviance is simply the sum of squared errors for the tree.</p>
<div class="sourceCode" id="cb549"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb549-1"><a href="#cb549-1" tabindex="-1"></a>tree.boston</span></code></pre></div>
<pre><code>## node), split, n, deviance, yval
##       * denotes terminal node
## 
##  1) root 253 19450.0 21.79  
##    2) rm &lt; 6.9595 222  6794.0 19.35  
##      4) lstat &lt; 14.405 135  1816.0 22.51  
##        8) rm &lt; 6.543 111   763.1 21.38 *
##        9) rm &gt; 6.543 24   256.5 27.73 *
##      5) lstat &gt; 14.405 87  1554.0 14.46  
##       10) crim &lt; 11.4863 61   613.8 16.23  
##         20) age &lt; 93.95 30   245.7 18.09 *
##         21) age &gt; 93.95 31   164.1 14.43 *
##       11) crim &gt; 11.4863 26   302.7 10.32 *
##    3) rm &gt; 6.9595 31  1929.0 39.21  
##      6) rm &lt; 7.553 16   505.5 33.42 *
##      7) rm &gt; 7.553 15   317.0 45.38 *</code></pre>
<p>In terms of the details of each node, the results are structured
similarly to a classification tree. So for example, for the root node we
have 253 observations, with a total deviance of 19450.0 and a predicted
value (mean response) of 21.79. Our most important feature appears to be
the average number of rooms per dwelling (<strong>rm</strong>) and our split
criterion is <span class="math inline">\(rm &lt; 6.9595\)</span>.</p>
<p>Let’s visualise the tree.</p>
<div class="sourceCode" id="cb551"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb551-1"><a href="#cb551-1" tabindex="-1"></a><span class="fu">plot</span>(tree.boston)</span>
<span id="cb551-2"><a href="#cb551-2" tabindex="-1"></a><span class="fu">text</span>(tree.boston, <span class="at">pretty =</span> <span class="dv">0</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>On the left hand side we have multiple splits corresponding to less than
7 average number of rooms per dwelling whilst of the right side we have
a single split corresponding to 7 or more rooms per dwelling. We also
note one more split in the same variable before the terminal nodes which
tell us the median house prices of owner-occupied homes. For homes in
census tracts in which <code>rm &gt;= 7.553</code>, the regression tree predicts a
median house price of <span class="math inline">\(45.38\)</span>.</p>
<p>Now we apply <span class="math inline">\(k\)</span> fold cross-validation and plot the results to see
whether pruning the tree will improve performance. The <code>cv.tree()</code> runs
<span class="math inline">\(k\)</span>-fold cross-validation to find the deviance or number of
misclassifications as a function of the cost-complexity parameter <span class="math inline">\(k\)</span>.</p>
<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb552-1"><a href="#cb552-1" tabindex="-1"></a>cv.boston <span class="ot">&lt;-</span> <span class="fu">cv.tree</span>(tree.boston)</span>
<span id="cb552-2"><a href="#cb552-2" tabindex="-1"></a></span>
<span id="cb552-3"><a href="#cb552-3" tabindex="-1"></a><span class="fu">plot</span>(cv.boston<span class="sc">$</span>size, cv.boston<span class="sc">$</span>dev, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>In this case, the most complex tree under consideration is selected by
cross-validation and so we use the unpruned tree to make predictions on
the test set.</p>
<div class="sourceCode" id="cb553"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb553-1"><a href="#cb553-1" tabindex="-1"></a>yhat <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree.boston, <span class="at">newdata =</span> Boston[<span class="sc">-</span>train, ])</span>
<span id="cb553-2"><a href="#cb553-2" tabindex="-1"></a>boston.test <span class="ot">&lt;-</span> Boston[<span class="sc">-</span>train, <span class="st">&quot;medv&quot;</span>]</span>
<span id="cb553-3"><a href="#cb553-3" tabindex="-1"></a><span class="fu">plot</span>(yhat, boston.test)</span>
<span id="cb553-4"><a href="#cb553-4" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb554-1"><a href="#cb554-1" tabindex="-1"></a><span class="fu">mean</span>((yhat <span class="sc">-</span> boston.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 35.28688</code></pre>
<p>The test set MSE associated with the regression tree is <span class="math inline">\(35.29\)</span>. The
square root of the MSE is therefore around <span class="math inline">\(5.941\)</span>, indicating that this
model leads to test predictions that are (on average) within
approximately <span class="math inline">\(5.941\)</span> of the true median home value for the census
tract.</p>
</div>
</div>
<div id="ensemble-methods" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Ensemble Methods<a href="#ensemble-methods" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Despite the simplicity of decision trees, this approach is not
necessarily competitive to other supervised approaches we explored so
far in the course, particularly due to high variance, but remember that
this depends on context. For example, when dealing with complex and
highly non-linear relationships, it is possible for decision trees to
outperform more classic supervised learning approaches; we can assess
performance using either cross-validation or the validation set
approach. Do remember that selecting a statistical learning method by
only relying on the test error may not always be sufficient and we may
prefer a decision tree simply because it is easier to interpret and
illustrate.</p>
<p>Nevertheless, since trees do suffer from high variance and are not very
robust with respect to changes in the data, how can we improve
prediction accuracy? Well, we can implement approaches that, at their
core, rely on regression or classification trees as their main building
blocks. These methods are referred to as <em>ensemble methods</em>. Below, we
explore <em>bagging</em>, <em>random forests</em>, and <em>boosting</em>.</p>
<div id="bagging" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Bagging<a href="#bagging" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Also called <em>bootstrap aggregation</em>, bagging is essentially
bootstrapping as you know it from earlier in the course but this time
used in a completely different context, that is, to reduce the variance
of a statistical learning method. It is particularly useful in the case
of decision trees precisely because these suffer from high variance.
Here we consider bagging for in the context of regression trees but it
can also be extended to classification trees as well. Also, bagging is
not restricted to decision trees but can be used for many regression
methods.<br />
In this exercise, we continue with the <code>Boston</code> data and use the bagging
approach to predict median value of owner-occupied homes using all
variables in the dataset.</p>
<p>Using the <code>randomForest</code> package and the function with the same name, we
fit a bagging tree. In addition to the formula, we also must specify the
<code>mtry</code> and <code>importance</code> arguments. The argument <code>mtry = 12</code> tells R that
all <span class="math inline">\(12\)</span> predictors should be considered for each split of the tree,
whilst setting the <code>importance</code> argument to <code>TRUE</code> tells R to also
assess the importance of the predictors.</p>
<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb556-1"><a href="#cb556-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb556-2"><a href="#cb556-2" tabindex="-1"></a>bag.boston <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston,</span>
<span id="cb556-3"><a href="#cb556-3" tabindex="-1"></a>                           <span class="at">subset =</span> train, <span class="at">mtry =</span> <span class="dv">12</span>, <span class="at">importance =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>Now let’s have a look at the output. <em>Note that the exact results
obtained may differ, depending on our R version and version of your
<code>randomForest</code> package</em>.</p>
<div class="sourceCode" id="cb557"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb557-1"><a href="#cb557-1" tabindex="-1"></a>bag.boston</span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = medv ~ ., data = Boston, mtry = 12, importance = TRUE,      subset = train) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 12
## 
##           Mean of squared residuals: 11.40162
##                     % Var explained: 85.17</code></pre>
<p>Here we have a few pieces of important information. We have 500 trees in
total. This means that the bagging procedure built 500 decision trees on
different bootstrapped samples of the training data. In other words, for
a regression problem, the bagging procedures constructs B regression
trees using B bootstrapped training sets, and averages the resulting
predictions. There are 12 variables tried at each split; since it uses
all predictors (as specified by <code>mtry</code>), the trees are grown deep and
are NOT pruned. As we saw earlier with the decision trees, each
individual tree will have high variance (and low bias) but since the
bagging procedure averages all B trees, then this <em>reduces</em> the
variance.</p>
<p>The mean of squared residuals tells us the average squared difference
between predicted and actual values. The percent variance explained is
<span class="math inline">\(85.17\%\)</span>, which indicates a very good fit.</p>
<p>Right, so how about interpretation? Well, given that with bagging we
grow many trees, it is no longer possible to represent the results as a
single tree, and so the resulting model can be difficult to interpret.
Nevertheless, we can learn more about the importance of each predictor
using the <code>varImpPlot()</code> function from the <code>randomForest</code> package that
produces dotcharts.</p>
<div class="sourceCode" id="cb559"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb559-1"><a href="#cb559-1" tabindex="-1"></a><span class="fu">varImpPlot</span>(bag.boston)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<p>Or we can use the <code>importance()</code> function from the same package.</p>
<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb560-1"><a href="#cb560-1" tabindex="-1"></a><span class="fu">importance</span>(bag.boston)</span></code></pre></div>
<pre><code>##           %IncMSE IncNodePurity
## crim    24.948900     873.41761
## zn       4.824575      55.59650
## indus    3.185194     111.59825
## chas    -1.407962      12.46427
## nox     17.549697     260.21778
## rm      52.852626   12231.53572
## age     18.325696     346.97045
## dis      6.046766     272.36149
## rad      3.677714      66.96846
## tax     10.224698     148.96271
## ptratio  9.254946     145.16605
## lstat   53.389661    4789.60479</code></pre>
<p>The first measure is the percent increase in mean squared error
(%IncMSE) and is based upon the <em>out-of-bag error estimation</em> which
provides an internal measure of model performance without the need for a
separate validation set. This is because the out-of-bag observations are
observations that were not used to fit the model and were instead used
to validate the model (usually only two thirds of the observations are
used to fit the model).</p>
<p>The second measure is the increase in node purity (IncNodePurity) which
reflects the total decrease in node impurity that results from splits
over that variable, averaged over all trees. In the case of regression
trees, the node impurity is measured by the training RSS, and for
classification trees by the deviance. By far, <code>rm</code> and <code>lstat</code> seem to
be the most important predictors of the model.</p>
<p>But how well does this bagged model perform on the test set?</p>
<div class="sourceCode" id="cb562"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb562-1"><a href="#cb562-1" tabindex="-1"></a>yhat.bag <span class="ot">&lt;-</span> <span class="fu">predict</span>(bag.boston, <span class="at">newdata =</span> Boston[<span class="sc">-</span>train, ])</span>
<span id="cb562-2"><a href="#cb562-2" tabindex="-1"></a><span class="fu">plot</span>(yhat.bag, boston.test)</span>
<span id="cb562-3"><a href="#cb562-3" tabindex="-1"></a><span class="fu">abline</span>(<span class="dv">0</span>, <span class="dv">1</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<div class="sourceCode" id="cb563"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb563-1"><a href="#cb563-1" tabindex="-1"></a><span class="fu">mean</span>((yhat.bag <span class="sc">-</span> boston.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 23.41916</code></pre>
<p>The test set MSE associated with the bagged regression tree is lower
than the test MSE for an optimally-pruned single tree, but not by
much…</p>
<p>We saw earlier that the bagging was performed using 500 trees. We can
customise the number of trees by telling R how many trees to ‘grow’
using the <code>ntree()</code> argument.</p>
<div class="sourceCode" id="cb565"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb565-1"><a href="#cb565-1" tabindex="-1"></a>bag.boston <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston,</span>
<span id="cb565-2"><a href="#cb565-2" tabindex="-1"></a>    <span class="at">subset =</span> train, <span class="at">mtry =</span> <span class="dv">12</span>, <span class="at">ntree =</span> <span class="dv">25</span>)</span>
<span id="cb565-3"><a href="#cb565-3" tabindex="-1"></a>yhat.bag <span class="ot">&lt;-</span> <span class="fu">predict</span>(bag.boston, <span class="at">newdata =</span> Boston[<span class="sc">-</span>train, ])</span>
<span id="cb565-4"><a href="#cb565-4" tabindex="-1"></a><span class="fu">mean</span>((yhat.bag <span class="sc">-</span> boston.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 25.75055</code></pre>
<p>With 25 trees, we see a slight increase in the test MSE and if we have a
look at the results, we also see a reduction in the variance explained.</p>
<div class="sourceCode" id="cb567"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb567-1"><a href="#cb567-1" tabindex="-1"></a>bag.boston</span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = medv ~ ., data = Boston, mtry = 12, ntree = 25,      subset = train) 
##                Type of random forest: regression
##                      Number of trees: 25
## No. of variables tried at each split: 12
## 
##           Mean of squared residuals: 13.51568
##                     % Var explained: 82.42</code></pre>
</div>
<div id="random-forests" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Random Forests<a href="#random-forests" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Bagging is a special case of a <em>random forest</em> which can be seen as an
improvement over bagged trees because it decorrelates the trees. In
other words, the random forest algorithm does not consider a majority of
the available predictors at each split but only a subset. In this way,
it address the limitation of bagged trees looking very similar to one
another due to strong influences of particular predictors. As we saw
earlier, bagging did improve the performance of the model but this
improvement was not very dramatic relative to the individual optimally
pruned tree we built at the beginning of the demonstration. Hence,
random forests can make trees more reliable precisely because the
average of the resulting trees have less variance.</p>
<p>Growing a random forest is accomplished in exactly the same way as
bagging, except that we use a smaller value of the <code>mtry</code> argument. By
default, <code>randomForest()</code> uses <span class="math inline">\(p/3\)</span> variables when building a random
forest of regression trees, and <span class="math inline">\(\sqrt{p}\)</span> variables when building a
random forest of classification trees. Here we will use <code>mtry = 6</code>.</p>
<div class="sourceCode" id="cb569"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb569-1"><a href="#cb569-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb569-2"><a href="#cb569-2" tabindex="-1"></a>rf.boston <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston,</span>
<span id="cb569-3"><a href="#cb569-3" tabindex="-1"></a>    <span class="at">subset =</span> train, <span class="at">mtry =</span> <span class="dv">6</span>, <span class="at">importance =</span> <span class="cn">TRUE</span>)</span>
<span id="cb569-4"><a href="#cb569-4" tabindex="-1"></a>yhat.rf <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf.boston, <span class="at">newdata =</span> Boston[<span class="sc">-</span>train, ])</span>
<span id="cb569-5"><a href="#cb569-5" tabindex="-1"></a><span class="fu">mean</span>((yhat.rf <span class="sc">-</span> boston.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 20.06644</code></pre>
<p>The test set MSE is lower than the test MSE for bagging and much lower
than that of the decision tree.</p>
<p>Using the <code>importance()</code> function, we can view the importance of each
variable.</p>
<div class="sourceCode" id="cb571"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb571-1"><a href="#cb571-1" tabindex="-1"></a><span class="fu">importance</span>(rf.boston)</span></code></pre></div>
<pre><code>##           %IncMSE IncNodePurity
## crim    19.435587    1070.42307
## zn       3.091630      82.19257
## indus    6.140529     590.09536
## chas     1.370310      36.70356
## nox     13.263466     859.97091
## rm      35.094741    8270.33906
## age     15.144821     634.31220
## dis      9.163776     684.87953
## rad      4.793720      83.18719
## tax      4.410714     292.20949
## ptratio  8.612780     902.20190
## lstat   28.725343    5813.04833</code></pre>
<p>Or display the resuts as dotcharts.</p>
<div class="sourceCode" id="cb573"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb573-1"><a href="#cb573-1" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf.boston)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/chunk24-1.png" width="672" /></p>
<p>The results indicate that across all of the trees considered in the
random forest, the wealth of the community (<code>lstat</code>) and the house size
(<code>rm</code>) are by far the two most important variables.</p>
</div>
<div id="boosting" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Boosting<a href="#boosting" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Finally, let’s consider boosting that works somewhat similarly to
bagging except that it grows trees sequentially. In other words, each
tree is built to correct the errors of the previous trees by focusing on
the residuals (errors) of the model up to that point. Therefore, many
small, shallow trees incrementally improve the performance of the model
which often requires a large number of trees. There are three tuning
parameters to boosting: the number of trees B (seleced using
cross-validation), the shrinkage parameter <span class="math inline">\(\lambda\)</span> (lambda) which
controls the rate at which boosting learns, and the number of splits
which controls complexity.</p>
<p>In this example, we focus on using boosting for a regression tree but
this approach can also be used with other regression or classification
methods.</p>
<p>Here, we continue with the Boston dataset and fit a boosting regression
tree on the same model as earlier. To implement boosting, we can use the
<code>gbm()</code> function from the <code>gbm</code> package. We run <code>gbm()</code> with the option
<code>distribution = "gaussian"</code> since this is a regression problem. The
argument <code>n.trees = 5000</code> indicates that we want <span class="math inline">\(5000\)</span> trees, and the
option <code>interaction.depth = 4</code> limits the depth of each tree. Hence, we
are going for many, many trees of small size to incrementally improve
the model.</p>
<div class="sourceCode" id="cb574"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb574-1"><a href="#cb574-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb574-2"><a href="#cb574-2" tabindex="-1"></a>boost.boston <span class="ot">&lt;-</span> <span class="fu">gbm</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston[train, ],</span>
<span id="cb574-3"><a href="#cb574-3" tabindex="-1"></a>                    <span class="at">distribution =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="at">n.trees =</span> <span class="dv">5000</span>, </span>
<span id="cb574-4"><a href="#cb574-4" tabindex="-1"></a>                    <span class="at">interaction.depth =</span> <span class="dv">4</span>)</span></code></pre></div>
<p>For boosting, the <code>summary()</code> function works in a slightly different
way. It produces a relative influence plot and also outputs the relative
influence statistics.</p>
<div class="sourceCode" id="cb575"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb575-1"><a href="#cb575-1" tabindex="-1"></a><span class="fu">summary</span>(boost.boston)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<pre><code>##             var     rel.inf
## rm           rm 44.48249588
## lstat     lstat 32.70281223
## crim       crim  4.85109954
## dis         dis  4.48693083
## nox         nox  3.75222394
## age         age  3.19769210
## ptratio ptratio  2.81354826
## tax         tax  1.54417603
## indus     indus  1.03384666
## rad         rad  0.87625748
## zn           zn  0.16220479
## chas       chas  0.09671228</code></pre>
<p>We again see that <code>lstat</code> and <code>rm</code> are by far the most important
variables.</p>
<p>We can also produce <em>partial dependence plots</em> for these two variables.
The plots facilitate our interpretation of complex models results from
ensemble methods (or other complex non-linear methods) by allowing us to
visualise the effect of a single predictor on the response.</p>
<div class="sourceCode" id="cb577"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb577-1"><a href="#cb577-1" tabindex="-1"></a><span class="fu">plot</span>(boost.boston, <span class="at">i =</span> <span class="st">&quot;rm&quot;</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<div class="sourceCode" id="cb578"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb578-1"><a href="#cb578-1" tabindex="-1"></a><span class="fu">plot</span>(boost.boston, <span class="at">i =</span> <span class="st">&quot;lstat&quot;</span>)</span></code></pre></div>
<p><img src="05-S05-D2_files/figure-html/unnamed-chunk-31-2.png" width="672" /></p>
<p>These plots illustrate the marginal effect of the selected variables on
the response after <em>integrating</em> out the other variables. In this case,
as we might expect, median house prices are increasing with <code>rm</code> and
decreasing with <code>lstat</code>.</p>
<p>We now use the boosted model to predict <code>medv</code> on the test set:</p>
<div class="sourceCode" id="cb579"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb579-1"><a href="#cb579-1" tabindex="-1"></a>yhat.boost <span class="ot">&lt;-</span> <span class="fu">predict</span>(boost.boston,</span>
<span id="cb579-2"><a href="#cb579-2" tabindex="-1"></a>    <span class="at">newdata =</span> Boston[<span class="sc">-</span>train, ], <span class="at">n.trees =</span> <span class="dv">5000</span>)</span>
<span id="cb579-3"><a href="#cb579-3" tabindex="-1"></a><span class="fu">mean</span>((yhat.boost <span class="sc">-</span> boston.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 18.39057</code></pre>
<p>The test MSE obtained is the lowest thus far and therefore superior to
the test MSE of random forests and bagging. If we want to, we can
perform boosting with a different value of the shrinkage parameter
<span class="math inline">\(\lambda\)</span>. The default value is <span class="math inline">\(0.001\)</span>, but this is easily modified.
Here we take <span class="math inline">\(\lambda=0.2\)</span>.</p>
<div class="sourceCode" id="cb581"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb581-1"><a href="#cb581-1" tabindex="-1"></a>boost.boston <span class="ot">&lt;-</span> <span class="fu">gbm</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston[train, ],</span>
<span id="cb581-2"><a href="#cb581-2" tabindex="-1"></a>    <span class="at">distribution =</span> <span class="st">&quot;gaussian&quot;</span>, <span class="at">n.trees =</span> <span class="dv">5000</span>,</span>
<span id="cb581-3"><a href="#cb581-3" tabindex="-1"></a>    <span class="at">interaction.depth =</span> <span class="dv">4</span>, <span class="at">shrinkage =</span> <span class="fl">0.2</span>, <span class="at">verbose =</span> F)</span>
<span id="cb581-4"><a href="#cb581-4" tabindex="-1"></a>yhat.boost <span class="ot">&lt;-</span> <span class="fu">predict</span>(boost.boston,</span>
<span id="cb581-5"><a href="#cb581-5" tabindex="-1"></a>    <span class="at">newdata =</span> Boston[<span class="sc">-</span>train, ], <span class="at">n.trees =</span> <span class="dv">5000</span>)</span>
<span id="cb581-6"><a href="#cb581-6" tabindex="-1"></a><span class="fu">mean</span>((yhat.boost <span class="sc">-</span> boston.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 16.54778</code></pre>
<p>In this case, using <span class="math inline">\(\lambda=0.2\)</span> leads to an even lower test MSE than
<span class="math inline">\(\lambda=0.001\)</span>. Although typical values are 0.01 or 0.001, the choice
for lambda will depend on the problem at hand.</p>
<p>In this demonstration, you have learned the basics of decision trees,
bagging, random forests, and boosting. For a more in-depth exploration
of these topics, please see the reading assigned for this section.</p>
<!--chapter:end:05-S05-D2.Rmd-->
</div>
</div>
</div>
<div id="answers-4" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Answers<a href="#answers-4" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="practical-1-4" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Practical 1<a href="#practical-1-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>This practical was developed by Dr. Tatjana Kecojevic, Lecturer in Social Statistics.</em></p>
<div class="file">
<p>For the tasks below, you will require the <strong>Salaries</strong> dataset. This
dataset is part of the <code>carData</code> R package.</p>
<p>To access the dataset, load the <code>carData</code> package (make sure to first
install the package).</p>
<p>You will also require the <code>GGally</code> package; please make sure to install it.</p>
</div>
<p><strong>Salaries</strong> is a data frame with 397 observations. This dataset
consists of nine-month academic salary for Assistant Professors,
Associate Professors and Professors in a college in the U.S to monitor
salary differences between male and female faculty members. The data are
from 2008-09.</p>
<p>There are six variables:</p>
<table>
<colgroup>
<col width="26%" />
<col width="73%" />
</colgroup>
<tbody>
<tr class="odd">
<td><strong>Variable Name</strong></td>
<td><strong>Variable Description</strong></td>
</tr>
<tr class="even">
<td>rank</td>
<td>a factor with levels = AssocProf, AsstProf, Prof</td>
</tr>
<tr class="odd">
<td>discipline</td>
<td>a factor with levels A = theoretical departments) or B = applied departments</td>
</tr>
<tr class="even">
<td>yrs.since.phd</td>
<td>years since PhD</td>
</tr>
<tr class="odd">
<td>yrs.service</td>
<td>years of service</td>
</tr>
<tr class="even">
<td>sex</td>
<td>a factor with levels Female and Male</td>
</tr>
<tr class="odd">
<td>salary</td>
<td>nine-month salary, in dollars.</td>
</tr>
</tbody>
</table>
<p>Let’s first load the packages:</p>
<div class="sourceCode" id="cb583"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb583-1"><a href="#cb583-1" tabindex="-1"></a><span class="fu">library</span>(carData)</span>
<span id="cb583-2"><a href="#cb583-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb583-3"><a href="#cb583-3" tabindex="-1"></a><span class="fu">library</span>(GGally)</span></code></pre></div>
<p>Once you load the <code>carData</code> package, the <strong>Salaries</strong> dataset will be
‘loaded’ too and can be accessed without needing to assign it to a
separate object.</p>
<div class="sourceCode" id="cb584"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb584-1"><a href="#cb584-1" tabindex="-1"></a><span class="fu">head</span>(Salaries)</span></code></pre></div>
<pre><code>##        rank discipline yrs.since.phd yrs.service  sex salary
## 1      Prof          B            19          18 Male 139750
## 2      Prof          B            20          16 Male 173200
## 3  AsstProf          B             4           3 Male  79750
## 4      Prof          B            45          39 Male 115000
## 5      Prof          B            40          41 Male 141500
## 6 AssocProf          B             6           6 Male  97000</code></pre>
<p>As usual, we can access variables within the dataset by indexing them.</p>
<div class="sourceCode" id="cb586"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb586-1"><a href="#cb586-1" tabindex="-1"></a>Salaries<span class="sc">$</span>salary</span></code></pre></div>
<pre><code>##   [1] 139750 173200  79750 115000 141500  97000 175000 147765 119250 129000
##  [11] 119800  79800  77700  78000 104800 117150 101000 103450 124750 137000
##  [21]  89565 102580  93904 113068  74830 106294 134885  82379  77000 118223
##  [31] 132261  79916 117256  80225  80225  77000 155750  86373 125196 100938
##  [41] 146500  93418 101299 231545  94384 114778  98193 151768 140096  70768
##  [51] 126621 108875  74692 106639 103760  83900 117704  90215 100135  75044
##  [61]  90304  75243 109785 103613  68404 100522 101000  99418 111512  91412
##  [71] 126320 146856 100131  92391 113398  73266 150480 193000  86100  84240
##  [81] 150743 135585 144640  88825 122960 132825 152708  88400 172272 107008
##  [91]  97032 105128 105631 166024 123683  84000  95611 129676 102235 106689
## [101] 133217 126933 153303 127512  83850 113543  82099  82600  81500 131205
## [111] 112429  82100  72500 104279 105000 120806 148500 117515  72500  73500
## [121] 115313 124309  97262  62884  96614  78162 155500  72500 113278  73000
## [131]  83001  76840  77500  72500 168635 136000 108262 105668  73877 152664
## [141] 100102  81500 106608  89942 112696 119015  92000 156938 144651  95079
## [151] 128148  92000 111168 103994  92000 118971 113341  88000  95408 137167
## [161]  89516 176500  98510  89942  88795 105890 167284 130664 101210 181257
## [171]  91227 151575  93164 134185 105000 111751  95436 100944 147349  92000
## [181] 142467 141136 100000 150000 101000 134000 103750 107500 106300 153750
## [191] 180000 133700 122100  86250  90000 113600  92700  92000 189409 114500
## [201]  92700 119700 160400 152500 165000  96545 162200 120000  91300 163200
## [211]  91000 111350 128400 126200 118700 145350 146000 105350 109650 119500
## [221] 170000 145200 107150 129600  87800 122400  63900  70000  88175 133900
## [231]  91000  73300 148750 117555  69700  81700 114000  63100  77202  96200
## [241]  69200 122875 102600 108200  84273  90450  91100 101100 128800 204000
## [251] 109000 102000 132000  77500 116450  83000 140300  74000  73800  92550
## [261]  88600 107550 121200 126000  99000 134800 143940 104350  89650 103700
## [271] 143250 194800  73000  74000  78500  93000 107200 163200 107100 100600
## [281] 136500 103600  57800 155865  88650  81800 115800  85000 150500  74000
## [291] 174500 168500 183800 104800 107300  97150 126300 148800  72300  70700
## [301]  88600 127100 170500 105260 144050 111350  74500 122500  74000 166800
## [311]  92050 108100  94350 100351 146800  84716  71065  67559 134550 135027
## [321] 104428  95642 126431 161101 162221  84500 124714 151650  99247 134778
## [331] 192253 116518 105450 145098 104542 151445  98053 145000 128464 137317
## [341] 106231 124312 114596 162150 150376 107986 142023 128250  80139 144309
## [351] 186960  93519 142500 138000  83600 145028  88709 107309 109954  78785
## [361] 121946 109646 138771  81285 205500 101036 115435 108413 131950 134690
## [371]  78182 110515 109707 136660 103275 103649  74856  77081 150680 104121
## [381]  75996 172505  86895 105000 125192 114330 139219 109305 119450 186023
## [391] 166605 151292 103106 150564 101738  95329  81035</code></pre>
<p>However, if we want to access variables within the dataset without
needing to index them we can use the base R <code>attach()</code> function.</p>
<div class="sourceCode" id="cb588"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb588-1"><a href="#cb588-1" tabindex="-1"></a><span class="fu">attach</span>(Salaries)</span></code></pre></div>
<p>So now, we can call on the variables from the dataset directly.</p>
<div class="sourceCode" id="cb589"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb589-1"><a href="#cb589-1" tabindex="-1"></a>salary</span></code></pre></div>
<pre><code>##   [1] 139750 173200  79750 115000 141500  97000 175000 147765 119250 129000
##  [11] 119800  79800  77700  78000 104800 117150 101000 103450 124750 137000
##  [21]  89565 102580  93904 113068  74830 106294 134885  82379  77000 118223
##  [31] 132261  79916 117256  80225  80225  77000 155750  86373 125196 100938
##  [41] 146500  93418 101299 231545  94384 114778  98193 151768 140096  70768
##  [51] 126621 108875  74692 106639 103760  83900 117704  90215 100135  75044
##  [61]  90304  75243 109785 103613  68404 100522 101000  99418 111512  91412
##  [71] 126320 146856 100131  92391 113398  73266 150480 193000  86100  84240
##  [81] 150743 135585 144640  88825 122960 132825 152708  88400 172272 107008
##  [91]  97032 105128 105631 166024 123683  84000  95611 129676 102235 106689
## [101] 133217 126933 153303 127512  83850 113543  82099  82600  81500 131205
## [111] 112429  82100  72500 104279 105000 120806 148500 117515  72500  73500
## [121] 115313 124309  97262  62884  96614  78162 155500  72500 113278  73000
## [131]  83001  76840  77500  72500 168635 136000 108262 105668  73877 152664
## [141] 100102  81500 106608  89942 112696 119015  92000 156938 144651  95079
## [151] 128148  92000 111168 103994  92000 118971 113341  88000  95408 137167
## [161]  89516 176500  98510  89942  88795 105890 167284 130664 101210 181257
## [171]  91227 151575  93164 134185 105000 111751  95436 100944 147349  92000
## [181] 142467 141136 100000 150000 101000 134000 103750 107500 106300 153750
## [191] 180000 133700 122100  86250  90000 113600  92700  92000 189409 114500
## [201]  92700 119700 160400 152500 165000  96545 162200 120000  91300 163200
## [211]  91000 111350 128400 126200 118700 145350 146000 105350 109650 119500
## [221] 170000 145200 107150 129600  87800 122400  63900  70000  88175 133900
## [231]  91000  73300 148750 117555  69700  81700 114000  63100  77202  96200
## [241]  69200 122875 102600 108200  84273  90450  91100 101100 128800 204000
## [251] 109000 102000 132000  77500 116450  83000 140300  74000  73800  92550
## [261]  88600 107550 121200 126000  99000 134800 143940 104350  89650 103700
## [271] 143250 194800  73000  74000  78500  93000 107200 163200 107100 100600
## [281] 136500 103600  57800 155865  88650  81800 115800  85000 150500  74000
## [291] 174500 168500 183800 104800 107300  97150 126300 148800  72300  70700
## [301]  88600 127100 170500 105260 144050 111350  74500 122500  74000 166800
## [311]  92050 108100  94350 100351 146800  84716  71065  67559 134550 135027
## [321] 104428  95642 126431 161101 162221  84500 124714 151650  99247 134778
## [331] 192253 116518 105450 145098 104542 151445  98053 145000 128464 137317
## [341] 106231 124312 114596 162150 150376 107986 142023 128250  80139 144309
## [351] 186960  93519 142500 138000  83600 145028  88709 107309 109954  78785
## [361] 121946 109646 138771  81285 205500 101036 115435 108413 131950 134690
## [371]  78182 110515 109707 136660 103275 103649  74856  77081 150680 104121
## [381]  75996 172505  86895 105000 125192 114330 139219 109305 119450 186023
## [391] 166605 151292 103106 150564 101738  95329  81035</code></pre>
<div id="part-i-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Part I<a href="#part-i-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="exploring-the-data-1" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Exploring the data<a href="#exploring-the-data-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s begin by exploring the dataset.</p>
<div class="sourceCode" id="cb591"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb591-1"><a href="#cb591-1" tabindex="-1"></a><span class="fu">glimpse</span>(Salaries)</span></code></pre></div>
<pre><code>## Rows: 397
## Columns: 6
## $ rank          &lt;fct&gt; Prof, Prof, AsstProf, Prof, Prof, AssocProf, Prof, Prof,…
## $ discipline    &lt;fct&gt; B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, B, A, A,…
## $ yrs.since.phd &lt;int&gt; 19, 20, 4, 45, 40, 6, 30, 45, 21, 18, 12, 7, 1, 2, 20, 1…
## $ yrs.service   &lt;int&gt; 18, 16, 3, 39, 41, 6, 23, 45, 20, 18, 8, 2, 1, 0, 18, 3,…
## $ sex           &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Fe…
## $ salary        &lt;int&gt; 139750, 173200, 79750, 115000, 141500, 97000, 175000, 14…</code></pre>
<p>We can see that <strong>rank</strong>, <strong>discipline</strong>, and <strong>sex</strong> are already coded
as factors. The variables <strong>yrs.since.phd</strong> and <strong>yrs.service</strong> are
coded as integers.</p>
<p>Our viewpoint states a belief that more years in service will cause
higher salary. Let us focus on the mechanics of fitting the model. First
we will examine the impact of each individual variable to see if our
view point is correct.</p>
<p>We start off with <strong>salary</strong> vs <strong>yrs.since.phd</strong>.</p>
<div class="sourceCode" id="cb593"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb593-1"><a href="#cb593-1" tabindex="-1"></a><span class="fu">summary</span>(Salaries)</span></code></pre></div>
<pre><code>##         rank     discipline yrs.since.phd    yrs.service        sex     
##  AsstProf : 67   A:181      Min.   : 1.00   Min.   : 0.00   Female: 39  
##  AssocProf: 64   B:216      1st Qu.:12.00   1st Qu.: 7.00   Male  :358  
##  Prof     :266              Median :21.00   Median :16.00               
##                             Mean   :22.31   Mean   :17.61               
##                             3rd Qu.:32.00   3rd Qu.:27.00               
##                             Max.   :56.00   Max.   :60.00               
##      salary      
##  Min.   : 57800  
##  1st Qu.: 91000  
##  Median :107300  
##  Mean   :113706  
##  3rd Qu.:134185  
##  Max.   :231545</code></pre>
<p>Both explanatory variables, <strong>yrs.since.phd</strong> and <strong>yrs.service</strong> have
mean and median values that are close to each other. However, the mean
and median for the <strong>salary</strong> variable are quite different.</p>
<p>We can better visualise this using boxplots.</p>
<div class="sourceCode" id="cb595"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb595-1"><a href="#cb595-1" tabindex="-1"></a><span class="fu">boxplot</span>(Salaries[,<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>], <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&#39;brown1&#39;</span>, <span class="st">&#39;steelblue&#39;</span>), <span class="at">main =</span> <span class="st">&quot;Distribution&quot;</span>)</span>
<span id="cb595-2"><a href="#cb595-2" tabindex="-1"></a>means <span class="ot">&lt;-</span> <span class="fu">sapply</span>(Salaries[,<span class="dv">3</span><span class="sc">:</span><span class="dv">4</span>], mean)</span>
<span id="cb595-3"><a href="#cb595-3" tabindex="-1"></a><span class="fu">points</span>(means, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">pch =</span> <span class="dv">22</span>, <span class="at">lwd =</span> <span class="dv">7</span>)</span></code></pre></div>
<p><img src="05-S05-ANS_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<div class="sourceCode" id="cb596"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb596-1"><a href="#cb596-1" tabindex="-1"></a><span class="fu">boxplot</span>(salary, <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&#39;chartreuse4&#39;</span>), <span class="at">main =</span> <span class="st">&quot;Distributions&quot;</span>)</span>
<span id="cb596-2"><a href="#cb596-2" tabindex="-1"></a>means <span class="ot">&lt;-</span> <span class="fu">sapply</span>(salary, mean)</span>
<span id="cb596-3"><a href="#cb596-3" tabindex="-1"></a><span class="fu">points</span>(means, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>, <span class="at">pch =</span> <span class="dv">22</span>, <span class="at">lwd =</span> <span class="dv">7</span>)</span></code></pre></div>
<p><img src="05-S05-ANS_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<div class="question">
<p>What do the box plots indicate?</p>
</div>
<div class="answers">
<p>We notice that a number of observations are identified as the outliers
that are pulling the mean away from the median.</p>
</div>
</div>
<div id="salary-and-years-since-phd-1" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Salary and Years since PhD<a href="#salary-and-years-since-phd-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s now consider the relationship between <strong>yrs.since.phd</strong> and <strong>salary</strong>
using a scatterplot onto which we add a line of best fit. Note that
since we ‘attached’ the dataset, we can call on the variables without
need to index or specify the dataset by name.</p>
<div class="sourceCode" id="cb597"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb597-1"><a href="#cb597-1" tabindex="-1"></a><span class="fu">plot</span>(salary <span class="sc">~</span> yrs.since.phd, <span class="at">cex =</span>.<span class="dv">6</span>, <span class="at">main =</span> <span class="st">&quot;The Relationship between Nine-month Salary and Years since PhD&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Years since PhD&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Nine-month Salary (dollars)&quot;</span>)</span>
<span id="cb597-2"><a href="#cb597-2" tabindex="-1"></a></span>
<span id="cb597-3"><a href="#cb597-3" tabindex="-1"></a>model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> yrs.since.phd)</span>
<span id="cb597-4"><a href="#cb597-4" tabindex="-1"></a></span>
<span id="cb597-5"><a href="#cb597-5" tabindex="-1"></a><span class="fu">abline</span>(model1, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="05-S05-ANS_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<div class="sourceCode" id="cb598"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb598-1"><a href="#cb598-1" tabindex="-1"></a><span class="fu">summary</span>(model1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = salary ~ yrs.since.phd)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -84171 -19432  -2858  16086 102383 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    91718.7     2765.8  33.162   &lt;2e-16 ***
## yrs.since.phd    985.3      107.4   9.177   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 27530 on 395 degrees of freedom
## Multiple R-squared:  0.1758, Adjusted R-squared:  0.1737 
## F-statistic: 84.23 on 1 and 395 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="question">
<p>What do the results indicate?</p>
</div>
<div class="answers">
<p>The results show that there is a positive relationship between the
nine-month salary and years since PhD completion. The relationship is on
a weak side, with only 17.60% of variability in the response variable
<strong>salary</strong> being explained by the predictor <strong>yrs.since.phd</strong>.</p>
</div>
</div>
<div id="salary-and-years-of-service-1" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Salary and Years of Service<a href="#salary-and-years-of-service-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s find out more about the relationship between nine-month salary and
years of service.</p>
<div class="sourceCode" id="cb600"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb600-1"><a href="#cb600-1" tabindex="-1"></a><span class="fu">plot</span>(salary <span class="sc">~</span> yrs.service, <span class="at">cex =</span>.<span class="dv">6</span>, <span class="at">main =</span> <span class="st">&quot;The Relationship between Nine-month Salary and Years of Service&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Years of Service&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Nine-month Salary (dollars)&quot;</span>)</span>
<span id="cb600-2"><a href="#cb600-2" tabindex="-1"></a></span>
<span id="cb600-3"><a href="#cb600-3" tabindex="-1"></a>model2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> yrs.service)</span>
<span id="cb600-4"><a href="#cb600-4" tabindex="-1"></a></span>
<span id="cb600-5"><a href="#cb600-5" tabindex="-1"></a><span class="fu">abline</span>(model1, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="05-S05-ANS_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<div class="sourceCode" id="cb601"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb601-1"><a href="#cb601-1" tabindex="-1"></a><span class="fu">summary</span>(model2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = salary ~ yrs.service)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -81933 -20511  -3776  16417 101947 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  99974.7     2416.6   41.37  &lt; 2e-16 ***
## yrs.service    779.6      110.4    7.06 7.53e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 28580 on 395 degrees of freedom
## Multiple R-squared:  0.1121, Adjusted R-squared:  0.1098 
## F-statistic: 49.85 on 1 and 395 DF,  p-value: 7.529e-12</code></pre>
<div class="question">
<p>What do the plot and model results indicate?</p>
</div>
<div class="answers">
<p>The plot confirms our viewpoint and again we have a positive
relationship between salary and years of service. This variable explains
around 11 % of variability in the response variable.</p>
<p>Individually, the two variables do not seem to explain much of the
variability in the response.</p>
</div>
</div>
<div id="the-model-1" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">The Model<a href="#the-model-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Let’s consider both variables (years of service and years since PhD) and whether these help explain salary. We define our multiple linear regression model as:</p>
<p><span class="math display">\[y = b_0 + b_1x_1 + b_2x_2 + e\]</span></p>
<div class="sourceCode" id="cb603"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb603-1"><a href="#cb603-1" tabindex="-1"></a>mr_model <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> yrs.since.phd <span class="sc">+</span> yrs.service)</span>
<span id="cb603-2"><a href="#cb603-2" tabindex="-1"></a><span class="fu">summary</span>(mr_model)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = salary ~ yrs.since.phd + yrs.service)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -79735 -19823  -2617  15149 106149 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    89912.2     2843.6  31.620  &lt; 2e-16 ***
## yrs.since.phd   1562.9      256.8   6.086 2.75e-09 ***
## yrs.service     -629.1      254.5  -2.472   0.0138 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 27360 on 394 degrees of freedom
## Multiple R-squared:  0.1883, Adjusted R-squared:  0.1842 
## F-statistic: 45.71 on 2 and 394 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="test-a-does-the-fitted-model-make-sense-1" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Test a): Does the fitted model make sense?<a href="#test-a-does-the-fitted-model-make-sense-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><em>Do the estimated coefficients have the correct sign?</em></p>
<p>The estimated model of best fit is:</p>
<p><span class="math inline">\(salary = 89912.2 + 1562.9yrs.since.phd − 629.1yrs.service\)</span></p>
<p>We notice that when put together with the variable <strong>yrs.since.phd</strong>,
the <strong>yrs.service</strong> changes sign, which is not in line with our
previously drawn conclusion and the viewpoint. This is the result of
<em>collinearity</em>, which you already know happens when two predictors are
correlated with one another.</p>
<p>(Multi)collinearity can be identified when:</p>
<ul>
<li>a regression coefficient <span class="math inline">\(x_i\)</span> is not significant even though,
theoretically, it should be highly correlated with the response
variable <span class="math inline">\(y\)</span>;<br />
</li>
<li>by adding or deleting an <span class="math inline">\(x_i\)</span> variable, the regression coefficients
change dramatically;<br />
</li>
<li>we get a negative regression coefficient when the response should
increase along with <span class="math inline">\(x_i\)</span>, or we get a positive regression
coefficient when the response should decrease as <span class="math inline">\(x_i\)</span> increases;<br />
</li>
<li>the explanatory variables have high pairwise correlations.</li>
</ul>
<p>Removing one of the correlated explanatory variables usually doesn’t
drastically reduce the <span class="math inline">\(R^2/R^2adj\)</span>.</p>
<p>With this model, using <strong>yrs.since.phd</strong> and <strong>yrs.service</strong> variables
we have managed to explain just over 18% of variation in the variable
<strong>salary</strong>.</p>
</div>
<div id="test-b-overall-is-the-model-a-good-fit-1" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Test b): Overall, is the model a good fit?<a href="#test-b-overall-is-the-model-a-good-fit-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="math inline">\(R^2adj\)</span> is 18.42%, putting this model on the weaker side. However let
us go through the formal procedure and set the hypothesis below. The
null hypothesis of will be tested using the F-test:</p>
<ul>
<li><span class="math inline">\(H_0:R^2=0\)</span> (that is, the set of explanatory variables are
insignificant, or in other words: useless)<br />
</li>
<li><span class="math inline">\(H_1:R^2&gt;0\)</span> (that is, at least one explanatory variable is
significant, or in other words: important)</li>
</ul>
<p>The decision rule is:</p>
<ul>
<li>if <span class="math inline">\(F_{calc} &lt; F_{crit} =&gt; H_0\)</span><br />
</li>
<li>if <span class="math inline">\(F_{calc} &gt; F_{crit} =&gt; H_1\)</span></li>
</ul>
<p>Examining the sample evidence we get that <span class="math inline">\(F_{calc} = 45.71\)</span>. The value
for <span class="math inline">\(F_{crit}\)</span> can be found in the statistical tables for <span class="math inline">\(df1 = 2\)</span> and
<span class="math inline">\(df2 = 394\)</span>.</p>
<div class="sourceCode" id="cb605"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb605-1"><a href="#cb605-1" tabindex="-1"></a><span class="fu">qf</span>(<span class="fl">0.95</span>, <span class="dv">2</span>, <span class="dv">394</span>)</span></code></pre></div>
<pre><code>## [1] 3.018626</code></pre>
<p>Since <span class="math inline">\(F_{crit} = 3.02 &lt; F_{calc} =&gt; H_1\)</span>, this implies that this is a valid
model.</p>
<p>As pointed out earlier, this formal test involves a rather weak alternative hypothesis, which says only that <span class="math inline">\(R^2\)</span> is significantly bigger than 0. With <span class="math inline">\(R^2\)</span> of around 18% we can conclude that this is a useful model worthy of further investigation.</p>
</div>
<div id="test-c-individually-are-the-explanatory-variables-important-1" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Test c): Individually, are the explanatory variables important?<a href="#test-c-individually-are-the-explanatory-variables-important-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Stage two of our model validation procedure is to examine the importance of any one single explanatory variable used in the fitted model. We have pointed out that just because a set of variables is important does not necessarily mean that each individual variable is contributing towards explaining the behaviour of <span class="math inline">\(Y\)</span>.</p>
<p>We will conduct a set of t-tests to check the validity of each variable one at a time.</p>
<p><strong><span class="math inline">\(b_1\)</span>: previously we concluded that the relationship between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(y\)</span> is positive (in the fitted model parameter <span class="math inline">\(b_1\)</span> is positive). Consequently, we will use one tail t-test to assess the importance of <span class="math inline">\(x_1\)</span> in the model.</strong></p>
<p><span class="math inline">\(H_0:b_1 = 0\)</span> (explanatory variable <span class="math inline">\(i\)</span> is not important)</p>
<p><span class="math inline">\(H_1:b_1 &gt; 0\)</span> (explanatory variable <span class="math inline">\(i\)</span> has a positive influence)</p>
<p>whereby:</p>
<ul>
<li><p>If <span class="math inline">\(t_{calc} &lt; t_{crit} =&gt; H_0\)</span></p></li>
<li><p>If <span class="math inline">\(t_{calc} &gt; t_{crit} =&gt; H_1\)</span></p></li>
</ul>
<div class="sourceCode" id="cb607"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb607-1"><a href="#cb607-1" tabindex="-1"></a><span class="fu">qt</span>(<span class="fl">0.95</span>, <span class="dv">394</span>)</span></code></pre></div>
<pre><code>## [1] 1.64873</code></pre>
<p><span class="math inline">\(t_{calc} = 6.09 &gt; t_{crit} = 1.65 =&gt; H_1\)</span>, which implies that we need to keep x1 in the model.</p>
<p><strong><span class="math inline">\(b_2\)</span>: previously we concluded that the relationship between <span class="math inline">\(x_2\)</span> and y is a positive relationship, but the model is suggesting that it is negative. We will stick to our belief and test if the coefficient should be positive:</strong></p>
<p><span class="math inline">\(H_0:b_2 = 0\)</span> (explanatory variable <span class="math inline">\(i\)</span> is not important)</p>
<p><span class="math inline">\(H_1:b_2 &gt; 0\)</span> (explanatory variable <span class="math inline">\(i\)</span> has a positive influence)</p>
<p>whereby:</p>
<ul>
<li><p>If <span class="math inline">\(t_{calc} &lt; t_{crit} =&gt; H_0\)</span></p></li>
<li><p>If <span class="math inline">\(t_{calc} &gt; t_{crit} =&gt; H_1\)</span></p></li>
</ul>
<div class="sourceCode" id="cb609"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb609-1"><a href="#cb609-1" tabindex="-1"></a><span class="fu">qt</span>(<span class="fl">0.95</span>, <span class="dv">394</span>)</span></code></pre></div>
<pre><code>## [1] 1.64873</code></pre>
<p><span class="math inline">\(t_{calc} = −2.47 &lt; t_{crit} = 1.65 =&gt; H_0\)</span> therefore, the variable should be removed from the model.</p>
<p>The increase in the explain variation of around 1% is negligible in comparison to the best one factor model <span class="math inline">\(salary = f(yrs.since.phd) + e\)</span>. Hence, we will put forward the model <span class="math inline">\(salary = 91719 + 985yrs.since.phd\)</span> as our best fitted model.</p>
<p>Alternatively you could test for the coefficient not being equal to zero and make a conclusion for yourself if this would be a sensible thing to do.</p>
<p>In this example, we have adopted a ‘standard’ regression approach that assumes modelling a relationship between quantitative response and only quantitative predictors. However, often when building multiple regression models, we do not want to be limited to just quantitative predictors.</p>
</div>
</div>
<div id="part-ii-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Part II<a href="#part-ii-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Now let’s expand our multiple linear regression model with two <em>quantitative</em> variables to a model that also includes <em>categorical</em> variables.</p>
<div class="sourceCode" id="cb611"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb611-1"><a href="#cb611-1" tabindex="-1"></a><span class="co"># if you are starting a fresh R session, don&#39;t forget to:</span></span>
<span id="cb611-2"><a href="#cb611-2" tabindex="-1"></a></span>
<span id="cb611-3"><a href="#cb611-3" tabindex="-1"></a><span class="co"># load the package</span></span>
<span id="cb611-4"><a href="#cb611-4" tabindex="-1"></a><span class="fu">library</span>(carData)</span>
<span id="cb611-5"><a href="#cb611-5" tabindex="-1"></a></span>
<span id="cb611-6"><a href="#cb611-6" tabindex="-1"></a><span class="co"># attach the dataset</span></span>
<span id="cb611-7"><a href="#cb611-7" tabindex="-1"></a><span class="fu">attach</span>(Salaries)</span></code></pre></div>
<pre><code>## The following objects are masked from Salaries (pos = 3):
## 
##     discipline, rank, salary, sex, yrs.service, yrs.since.phd</code></pre>
<p>In many datasets, categorical (attribute) variables are usually encoded numerically and are accompanied by information about the levels of the variable saved in the levels attribute.</p>
<p>Let’s consider the <strong>sex</strong> variable.</p>
<div class="sourceCode" id="cb613"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb613-1"><a href="#cb613-1" tabindex="-1"></a><span class="fu">attributes</span>(sex)</span></code></pre></div>
<pre><code>## $levels
## [1] &quot;Female&quot; &quot;Male&quot;  
## 
## $class
## [1] &quot;factor&quot;</code></pre>
<p>This variable is already coded as a factor with two levels, <em>Female</em> and <em>Male</em> (which you should already know from earlier in the demonstration). Now, what if we want to transform a variable of class <code>factor</code> into one of class <code>integer</code>?</p>
<div class="sourceCode" id="cb615"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb615-1"><a href="#cb615-1" tabindex="-1"></a><span class="fu">unclass</span>(sex)</span></code></pre></div>
<pre><code>##   [1] 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 1 1 2
##  [38] 2 2 2 2 2 2 2 2 2 2 1 1 2 2 2 1 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 1 2 2 2 2 2
##  [75] 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2
## [112] 2 2 2 1 2 2 2 2 1 2 2 2 1 2 2 2 1 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [149] 1 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2
## [186] 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2
## [223] 2 2 2 2 2 2 2 2 1 1 2 1 2 2 2 1 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 1 1 2 2 2 2
## [260] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## [297] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 1
## [334] 2 1 2 2 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 2 2 1 2 2 2 2 2 2 2 2
## [371] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
## attr(,&quot;levels&quot;)
## [1] &quot;Female&quot; &quot;Male&quot;</code></pre>
<p>We can easily do so with the <code>unclass()</code> function which removes the attributes of a factor variable and transforms the levels into numeric values.</p>
<p>However, when using factor variable in a linear regression model, it would make no sense to treat it as a <em>quantitative</em> explanatory variable. In the context of linear modelling we need to code each category to represent factor levels. Two-level attribute variables are very easy to code. We simply create an indicator or dummy variable that takes on two possible dummy numerical values. Consider the <strong>sex</strong> variable.</p>
<p>We can code this using a dummy variable <span class="math inline">\(d\)</span>:<br />
</p>
<p><span class="math display">\[
d = \begin{cases}
0, &amp; \text{if female} \\
1, &amp; \text{if male}
\end{cases}
\]</span>
💡 This is the default coding used in R. A zero value is assigned to the level which is first alphabetically, unless it is changed by using the <code>releveld()</code> function for example, or by specifying the levels of the factor variable specifically.</p>
<p>So, for a simple regression model predicting nine-month salary using one categorical variable:</p>
<p><span class="math display">\[salary = b_0 + b_1sex + e\]</span><br />
the model is specified as follows:</p>
<p><span class="math display">\[salary_i = b_0 + b_1 sex_i + e_i =
\begin{cases}
b_0 + b_1 \times 1 + e_i = b_0 + b_1 + e_i, &amp; \text{if the person is male} \\
b_0 + b_1 \times 0 + e_i = b_0 + e_i, &amp; \text{if the person is female}
\end{cases}\]</span></p>
<p>where <span class="math inline">\(b_0\)</span> can be interpreted as the average nine-month salary for females, and <span class="math inline">\(b_0 + b_1\)</span> as the nine-month average salary for males. The value of <span class="math inline">\(b_1\)</span> represents the average difference in nine-month salary between females and males.</p>
<p>We can conclude that dealing with an attribute variable with two levels in a linear model is straightforward. In this case, a dummy variable indicates whether an observation has a particular characteristic: yes/no. We can observe it as a ‘switch’ in a model, as this dummy variable can only assume the values <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, where <span class="math inline">\(0\)</span> indicates the absence of the effect, and <span class="math inline">\(1\)</span> indicates the presence. The values <strong>0/1</strong> can be seen as <strong>off/on</strong>.</p>
<p>The way in which R codes dummy variables is controlled by the <code>contrasts</code> option:</p>
<div class="sourceCode" id="cb617"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb617-1"><a href="#cb617-1" tabindex="-1"></a><span class="fu">options</span>(<span class="st">&quot;contrasts&quot;</span>)</span></code></pre></div>
<pre><code>## $contrasts
##         unordered           ordered 
## &quot;contr.treatment&quot;      &quot;contr.poly&quot;</code></pre>
<p>The output points out the conversion of the factor into an appropriate set of contrasts. In particular, the first one: for unordered factors, and the second one: the ordered factors. The former is applicable in our context. To explicitly identify the coding of the factor, i.e. dummy variable used by R, we can use the <code>contrasts()</code> function.</p>
<div class="sourceCode" id="cb619"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb619-1"><a href="#cb619-1" tabindex="-1"></a><span class="fu">contrasts</span>(sex)</span></code></pre></div>
<pre><code>##        Male
## Female    0
## Male      1</code></pre>
<p>Note that applied <code>contr.treatment</code> conversion takes only the value <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> and that for an attribute variable with <span class="math inline">\(k\)</span> levels it will create <span class="math inline">\(k-1\)</span> dummy variables. There are many different ways of coding attribute variables besides the dummy variable approach explained here. All of these different approaches lead to equivalent model fits. What differs are the coefficients (i.e. model parameters as they require different interpretations, arranged to measure particular contrasts). This 0/1 coding implemented in R’s default <code>contr.treatment</code> contrast offers straightforward interpretation of the associated parameter in the model, which often is not the case when implementing other contrasts.</p>
<div id="interpreting-coefficients-of-attribute-variables-1" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Interpreting coefficients of attribute variables<a href="#interpreting-coefficients-of-attribute-variables-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In the case of measured predictors, we are comfortable with the interpretation of the linear model coefficient as a slope, which tells us what a unit increase in the response variable is (i.e. outcome per unit increase in the explanatory variable). This is not necessarily the right interpretation for attribute predictors.</p>
<p>Let’s consider average nine-month salary values for males and females separately.</p>
<div class="sourceCode" id="cb621"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb621-1"><a href="#cb621-1" tabindex="-1"></a>Salaries <span class="sc">%&gt;%</span> </span>
<span id="cb621-2"><a href="#cb621-2" tabindex="-1"></a>  <span class="fu">select</span>(salary, sex) <span class="sc">%&gt;%</span>   </span>
<span id="cb621-3"><a href="#cb621-3" tabindex="-1"></a>  <span class="fu">group_by</span>(sex) <span class="sc">%&gt;%</span> </span>
<span id="cb621-4"><a href="#cb621-4" tabindex="-1"></a>  <span class="fu">summarise</span>(<span class="at">mean=</span><span class="fu">mean</span>(salary))</span></code></pre></div>
<pre><code>## # A tibble: 2 × 2
##   sex       mean
##   &lt;fct&gt;    &lt;dbl&gt;
## 1 Female 101002.
## 2 Male   115090.</code></pre>
<p>If we obtain the mean salary for each sex group we will find that for female professors the average salary is $ $101,002$ and for male professors the average is $ $115,090$. That is, a difference of <span class="math inline">\(\$14,088\)</span>.</p>
<p>If we now look at the parameters of the regression model for salary vs sex where females are coded as zero and males as one, we get exactly the same information, implying that the coefficient is the estimated difference in average between the two groups.</p>
<div class="sourceCode" id="cb623"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb623-1"><a href="#cb623-1" tabindex="-1"></a><span class="fu">lm</span>(salary <span class="sc">~</span>  sex)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = salary ~ sex)
## 
## Coefficients:
## (Intercept)      sexMale  
##      101002        14088</code></pre>
</div>
<div id="fitting-a-multivariate-regression-model-1" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Fitting a Multivariate Regression Model<a href="#fitting-a-multivariate-regression-model-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In Part I, we explored the extent to which variation in the response variable <strong>salary</strong> is associated with variation in years since PhD and years in service.
Now, we extend the model to also include <strong>sex</strong>, <strong>discipline</strong> and <strong>rank</strong>.
The overall goals of any model we construct is that it should contain enough to explain relations in the data and at the same time be simple enough to understand, explain to others, and use.</p>
<p>For convenience we will adopt the following notation:</p>
<p><span class="math inline">\(y\)</span>: salary<br />
<span class="math inline">\(x_1\)</span>: yrs.since.phd<br />
<span class="math inline">\(x_2\)</span>: yrs.service<br />
<span class="math inline">\(x_3\)</span>: discipline<br />
<span class="math inline">\(x_4\)</span>: sex<br />
<span class="math inline">\(x_5\)</span>: rank</p>
<p>Next, we need to specify the model that embodies our mechanistic understanding of the factors involved and the way that they are related to the response variable. It would make sense to expect that all of the available x variables may impact the behaviour of y, thus the model we wish to build should reflect our viewpoint, i.e. <span class="math inline">\(y=f(x_1,x_2,x_3,x_4,x_5)\)</span>:</p>
<p><span class="math display">\[y=b_0 + b_1x_1 + b_2x_2 + b_3x_3 + b_4x_4 + b_5x_5 + e\]</span>
Our viewpoint states a belief that all explanatory variables have a positive impact on the response. For example, more years in service will cause a higher salary.</p>
<p>Our objective now is to determine the values of the parameters in the model that lead to the best fit of the model to the data. That is, we are not only trying to estimate the parameters of the model, but we are also seeking the minimal adequate model to describe the data.</p>
<p>The best model is the model that produces the least unexplained variation following the principle of parsimony rather than complexity. That is the model should have as few parameters as possible, subject to the constraint that the parameters in the model should all be statistically significant.</p>
<p>For regression modelling in R we use the lm() function, that fits a linear model assuming normal errors and constant variance. We specify the model by a formula that uses arithmetic operators which enable different functionalities from their ordinary ones. But, before we dive into statistical modelling of the given data, we need to take a first step and conduct the most fundamental task of data analysis procedure: <strong>Get to Know Our Data</strong>.</p>
<div class="sourceCode" id="cb625"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb625-1"><a href="#cb625-1" tabindex="-1"></a><span class="fu">ggpairs</span>(Salaries)</span></code></pre></div>
<pre><code>## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.</code></pre>
<p><img src="05-S05-ANS_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<div class="question">
<p>What information can you extract from this visualisation?</p>
</div>
<div class="answers">
<p>This is an information rich visualisation that includes pairwise relationships of all the variables we want to consider for our model. By focusing on the last column of the plots, we can notice influence from all explanatory variables onto the response, except maybe for discipline and sex. We also notice unbalanced representation of the groups for the variables rank and sex, but for the purpose of our practice in fitting a multi-factor model this isn’t too problematic. We need to be especially concerned with the extent of correlations between the explanatory variables, and what is of particular interest to us is the high multicollinearity between rank, yrs.since.phd and yrs.service, which happens when the variables are highly linearly related. As a consequence, we will need to keep an eye on the significance of using all of these variables in the model.</p>
</div>
</div>
<div id="fitting-the-model-1" class="section level4 unnumbered hasAnchor">
<h4 class="unnumbered hasAnchor">Fitting the Model<a href="#fitting-the-model-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>There are no fixed rules when fitting linear models, but there are adopted standards that have proven to work well in practice. We start off by fitting a maximal model then we carry on simplifying it by removing non-significant explanatory variables. This needs to be done with caution, making sure that the simplifications make good scientific sense, and do not lead to significant reductions in explanatory power. Although this should be the adopted strategy for fitting a model, it is not a guarantee to finding all the important structures in a complex data frame.</p>
<p>We can summarise our model building procedure algorithm as follows:</p>
<ol style="list-style-type: decimal">
<li>Fit the maximal model that includes all the variables. Then, assess the overall significance of the model by checking how big the <span class="math inline">\(R^2/\overline{R}^2\)</span> is. If statistically significant, carry on with the model fitting procedure, otherwise stop (F-test).<br />
</li>
<li>Remove the least significant terms one at a time. Then, check the <span class="math inline">\(t_calculated\)</span> for the variables values and perform a one tail or two tail t-test depending on your prior view. If the deletion causes an insignificant increase in <span class="math inline">\(\overline{R}^2\)</span>, leave that term out of the model.<br />
</li>
<li>Keep removing terms from the model until the model contains nothing but significant terms.</li>
</ol>
<p>Let’s build the model. Now, if we plan to use all variables in a dataset, there is no need to write the names of each individual predictor. Instead, we can use a full stop which tell R to include all other variables in the data object that do not already appear in the formula.</p>
<div class="sourceCode" id="cb627"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb627-1"><a href="#cb627-1" tabindex="-1"></a>model_1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span> ., <span class="at">data =</span> Salaries)</span>
<span id="cb627-2"><a href="#cb627-2" tabindex="-1"></a></span>
<span id="cb627-3"><a href="#cb627-3" tabindex="-1"></a><span class="fu">summary</span>(model_1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = salary ~ ., data = Salaries)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -65248 -13211  -1775  10384  99592 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    65955.2     4588.6  14.374  &lt; 2e-16 ***
## rankAssocProf  12907.6     4145.3   3.114  0.00198 ** 
## rankProf       45066.0     4237.5  10.635  &lt; 2e-16 ***
## disciplineB    14417.6     2342.9   6.154 1.88e-09 ***
## yrs.since.phd    535.1      241.0   2.220  0.02698 *  
## yrs.service     -489.5      211.9  -2.310  0.02143 *  
## sexMale         4783.5     3858.7   1.240  0.21584    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 22540 on 390 degrees of freedom
## Multiple R-squared:  0.4547, Adjusted R-squared:  0.4463 
## F-statistic:  54.2 on 6 and 390 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="question">
<p><strong>Overall, is the model a good fit? How big is the <span class="math inline">\(R^2/\overline{R}^2\)</span>?</strong></p>
</div>
<div class="answers">
<p>The <span class="math inline">\(R^2 = 45.47%\)</span> and the <span class="math inline">\(\overline{R}^2= 44.63%\)</span> are well above the value of zero allowing us to accept this as a valid model without having to formally test it to assess its statistical significance. It manages to explain almost half of the variability in the response variable <strong>salary</strong>.</p>
</div>
<div class="question">
<p><strong>Individually, are the explanatory variables important? What steps are required given the results of the model?</strong></p>
</div>
<p>We identify the <strong>sex</strong> variable as clearly not significant, which is in line with the conclusion we could draw from the boxplot in the pairwise comparison plot for <strong>salary</strong> vs. <strong>sex</strong>. We will remove it to begin the process of model simplification and remove the least significant term. We therefore re-fit (or ‘update’) the model without the <strong>sex</strong> variable.</p>
<div class="sourceCode" id="cb629"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb629-1"><a href="#cb629-1" tabindex="-1"></a>model_2 <span class="ot">&lt;-</span> <span class="fu">update</span>(model_1,<span class="sc">~</span>. <span class="sc">-</span> sex) </span>
<span id="cb629-2"><a href="#cb629-2" tabindex="-1"></a></span>
<span id="cb629-3"><a href="#cb629-3" tabindex="-1"></a><span class="fu">summary</span>(model_2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = salary ~ rank + discipline + yrs.since.phd + yrs.service, 
##     data = Salaries)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -65244 -13498  -1455   9638  99682 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    69869.0     3332.1  20.968  &lt; 2e-16 ***
## rankAssocProf  12831.5     4147.7   3.094  0.00212 ** 
## rankProf       45287.7     4236.7  10.689  &lt; 2e-16 ***
## disciplineB    14505.2     2343.4   6.190 1.52e-09 ***
## yrs.since.phd    534.6      241.2   2.217  0.02720 *  
## yrs.service     -476.7      211.8  -2.250  0.02497 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 22550 on 391 degrees of freedom
## Multiple R-squared:  0.4525, Adjusted R-squared:  0.4455 
## F-statistic: 64.64 on 5 and 391 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We note a slight reduction in <span class="math inline">\(\overline{R}^2\)</span> from <span class="math inline">\(44.63%\)</span> to <span class="math inline">\(44.55%\)</span> which we can regard as an insignificant decrease. The next step is to check the coefficients and assess for the effect of the remaining variables. We identify <strong>yrs.since.phd</strong> and <strong>yrs.service</strong> as the least influential in explaining the variability of salary. To illustrate how to formally assess their effect, we will conduct the t-test for the <strong>yrs.since.phd</strong> variable:</p>
<p><span class="math inline">\(H_0:b_{ysp} = 0\)</span></p>
<p><span class="math inline">\(H_1:b_{ysp} &gt; 0\)</span></p>
<p>Therefore:</p>
<p>If <span class="math inline">\(t_{calc} &lt; t_{crit} =&gt; H_0\)</span></p>
<p>If <span class="math inline">\(t_{calc} &gt; t_{crit} =&gt; H_1\)</span></p>
<div class="sourceCode" id="cb631"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb631-1"><a href="#cb631-1" tabindex="-1"></a><span class="fu">qt</span>(<span class="fl">0.95</span>, <span class="dv">391</span>)</span></code></pre></div>
<pre><code>## [1] 1.64876</code></pre>
<p>As <span class="math inline">\(t_{calc} = 2.217 &gt; t_{crit} = 1.64876 =&gt; H1\)</span>, we will keep the remaining variable and stop with the model simplification and focus on its interpretation.</p>
<div class="question">
<p>Specify the final fitted model.</p>
</div>
<p>The structure of our final fitted model is:</p>
<p><span class="math display">\[y=b_0 + b_1x_1 + b_2x_2 + b_3x_3 + b_4x_4 + e\]</span><br />
where:
<span class="math inline">\(y\)</span>: salary<br />
<span class="math inline">\(x_1\)</span>: yrs.since.phd<br />
<span class="math inline">\(x_2\)</span>: yrs.service<br />
<span class="math inline">\(x_3\)</span>: discipline<br />
<span class="math inline">\(x_4\)</span>: rank</p>
<p>We can take a closer look at the coefficients of our fitted model:</p>
<div class="sourceCode" id="cb633"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb633-1"><a href="#cb633-1" tabindex="-1"></a><span class="fu">coef</span>(model_2)</span></code></pre></div>
<pre><code>##   (Intercept) rankAssocProf      rankProf   disciplineB yrs.since.phd 
##    69869.0110    12831.5375    45287.6890    14505.1514      534.6313 
##   yrs.service 
##     -476.7179</code></pre>
<p>Examining the output we realise that R has created three dummy variables for the variable <strong>rank</strong>:<br />
<span class="math display">\[
dr_1 = \begin{cases}
1 &amp; \text{rank is AsstProf} \\
0 &amp; \text{for rank is not AsstProf}
\end{cases}
\]</span></p>
<p><span class="math display">\[
dr_2 = \begin{cases}
1 &amp; \text{rank is AssocProf} \\
0 &amp; \text{rank is not AssocProf}
\end{cases}
\]</span></p>
<p><span class="math display">\[
dr_3 = \begin{cases}
1 &amp; \text{rank is Prof} \\
0 &amp; \text{rank is not Prof}
\end{cases}
\]</span>
Therefore, R has chosen to use the model:<br />
<span class="math display">\[y = b_0 + b_1dr_2 + b_2dr_3 + b_3d_1 + b_4x_1 + b_5x_2 + e\]</span><br />
where:
- <span class="math inline">\(y\)</span> is salary<br />
- <span class="math inline">\(x_1\)</span> is yrs.since.phd<br />
- <span class="math inline">\(x_2\)</span> is yrs.service<br />
- <span class="math inline">\(dr_2\)</span> and <span class="math inline">\(dr_3\)</span> are the dummy variables defined above for the purpose of coding variable rank<br />
- <span class="math inline">\(d_1\)</span> is a dummy variable used in the coding of variable discipline as explained earlier</p>
<p>Note that R doesn’t need to use <span class="math inline">\(dr_1\)</span> to create three models; it only needs two dummy variables since it is using <span class="math inline">\(dr_1\)</span> as a reference level, also known as the base line. This subsequently allows R to create three models relating to the <strong>rank</strong> variable:</p>
<ul>
<li>AsstProf: <span class="math inline">\(y = b_0 + b_3d_1 + b_4x_1 + b_5x_2 + e\)</span><br />
</li>
<li>AssocProf: <span class="math inline">\(y = (b_0 + b_1) + b_3d_1 + b_4x_1 + b_5x_2 + e\)</span><br />
</li>
<li>Prof: <span class="math inline">\(y = (b_0 + b_2) + b_3d_1 + b_4x_1 + b_5x_2 + e\)</span></li>
</ul>
<p>telling us that:</p>
<ul>
<li><span class="math inline">\(b_0\)</span> is the average salary for an Assistant Professor who works in a ‘theoretical’ department and <span class="math inline">\(b_0 + b_3\)</span> the average salary for an Assistant Professor who works in an ‘applied’ department.</li>
<li><span class="math inline">\((b_0 + b_1)\)</span> is the average salary for an Associate Professor who works in a ‘theoretical’ department and <span class="math inline">\((b_0 + b_1) + b_3\)</span> the average salary for an Associate Professor who works in an ‘applied’ department.<br />
</li>
<li><span class="math inline">\((b_0 + b_2)\)</span> is the average salary for a Professor who works in a ‘theoretical’ department and <span class="math inline">\((b_0 + b_2) + b_3\)</span> the average salary for a Professor who works in an ‘applied’ department.</li>
</ul>
<div class="question">
<p>Interpret the results</p>
</div>
<p>Learning this we can make an interpretation of our final fitted model as follows:</p>
<p>For every year since PhD (yrs.since.phd) on average salary (salary) will go up by $534.63 assuming the rest of the variables are fixed in the model.</p>
<p>For every year in service (yrs.service) on average salary (salary) will go down by $476.72 assuming the rest of the variables are fixed in the model.</p>
<p>The average salary of an Assistant Professor (rank: AsstProf) who works in a “theoretical” department is <span class="math inline">\(\$69,869.01\)</span> and who works in an “applied” department is <span class="math inline">\(\$84,374.16\)</span>; this can vary for the number of years in service and since PhD.<br />
The average salary of an Associate Professor (rank: AssocProf) who works in a ‘theoretical’ department is <span class="math inline">\(\$82,700.55\)</span>, and one who works in an ‘applied’ department is <span class="math inline">\(\$97,205.70\)</span>; this can vary for the number of years in service and since PhD.</p>
<p>The average salary of a Professor (rank: Prof) who works in a ‘theoretical’ department is <span class="math inline">\(\$115,156.70\)</span>, and who works in an ‘applied’ department is <span class="math inline">\(\$129,661.90\)</span>; this can vary for the number of years in service and since PhD.</p>
<p>This model explains around 45% of the variability in the response variable <strong>salary</strong>.</p>
<p>Adding <code>~ 0</code> to the <code>lm()</code> formula enables R to suppress the intercept. Note that if we remove the intercept, then we can directly obtain all “three intercepts” without a base level to fit the final fitted model:</p>
<div class="sourceCode" id="cb635"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb635-1"><a href="#cb635-1" tabindex="-1"></a>model_2_1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(salary <span class="sc">~</span>  <span class="dv">0</span> <span class="sc">+</span> rank <span class="sc">+</span> discipline <span class="sc">+</span> yrs.since.phd <span class="sc">+</span> yrs.service)</span>
<span id="cb635-2"><a href="#cb635-2" tabindex="-1"></a></span>
<span id="cb635-3"><a href="#cb635-3" tabindex="-1"></a><span class="fu">summary</span>(model_2_1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = salary ~ 0 + rank + discipline + yrs.since.phd + 
##     yrs.service)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -65244 -13498  -1455   9638  99682 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## rankAsstProf   69869.0     3332.1  20.968  &lt; 2e-16 ***
## rankAssocProf  82700.5     3916.7  21.115  &lt; 2e-16 ***
## rankProf      115156.7     4350.9  26.467  &lt; 2e-16 ***
## disciplineB    14505.2     2343.4   6.190 1.52e-09 ***
## yrs.since.phd    534.6      241.2   2.217   0.0272 *  
## yrs.service     -476.7      211.8  -2.250   0.0250 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 22550 on 391 degrees of freedom
## Multiple R-squared:  0.9638, Adjusted R-squared:  0.9633 
## F-statistic:  1736 on 6 and 391 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</div>
<div id="practical-2-4" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Practical 2<a href="#practical-2-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>This practical has been developed by Dr. Tatjana Kecojevic, Lecturer in Social Statistics.</em></p>
<div class="file">
<p>For the tasks below, you will require the <strong>FDI</strong> dataset.</p>
<p>Click here to download the file:
<a href="data/FDI.csv" download="FDI.csv"> FDI.csv </a>.</p>
<p>Remember to place your data file in a separate subfolder within your R
project working directory.</p>
</div>
<p>A business consultancy firm is compiling a major report about
globalisation. One aspect of this study concerns the determinants of FDI
undertaken by multi-national enterprises. Relevant information from a
sample of 60 multi-national companies that undertook significant
investment in overseas projects was made available as follows:</p>
<table style="width:97%;">
<colgroup>
<col width="22%" />
<col width="73%" />
<col width="1%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Variable Name</th>
<th align="center">Variable Description</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">FDI</td>
<td colspan="2" align="center">Value of FDI undertaken, in £ millions, by
investing company |</td>
</tr>
<tr class="even">
<td align="center">GDP_Cap</td>
<td align="center">GDP per capita, £000s, in the country receiving
the investment</td>
<td></td>
</tr>
<tr class="odd">
<td align="center">Gr_rate</td>
<td align="center">The economic growth rate, in %-terms, in the
country receiving the investment</td>
<td></td>
</tr>
<tr class="even">
<td align="center">ROC</td>
<td align="center">The average return on capital invested, in
%-terms, in the country receiving the investment</td>
<td></td>
</tr>
<tr class="odd">
<td align="center">Stable</td>
<td align="center">The political stability of the country receiving
the investment as measured by the number of
changes in government over the past 25 years</td>
<td></td>
</tr>
<tr class="even">
<td align="center">Infra</td>
<td align="center"><p>Infrastructure facilities (eg transport,
communications) in the country receiving the
investment</p>
<p>Coded: 1 = basic infrastructure 2 = good
infrastructure</p></td>
<td></td>
</tr>
<tr class="odd">
<td align="center">Trade</td>
<td align="center"><p>The openness to trade of the country receiving the
investment</p>
<p>Coded: 1 = trade tightly controlled 2 = some
restrictions on trade 3 = free trade</p></td>
<td></td>
</tr>
</tbody>
</table>
<p>This is a multiple regression type of the problem; FDI is the key
response variable as this study concerns the determinants of FDI
undertaken by multi-national enterprises.</p>
<p>The model is defined as <span class="math inline">\(Y = b_0 + b_1x_1 + b_2x_2 + ... + b_kx_k + e\)</span>,
for the general <span class="math inline">\(k\)</span> explanatory variable model and where e is also known
as the error term <span class="math inline">\(e ∼ N(0,\sigma^2)\)</span>, with the error term from a normal
distribution with a mean of <span class="math inline">\(0\)</span>, and a variance of <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Based on prior knowledge, we make the assumption that GDP_Cap, Gr_rate,
ROC, Infra, and Trade have positive relationships with FDI, whilst
Stable has a negative relationship with FDI.</p>
<p>We will use our best fit model to predict FDI for the following information: <em>country X receiving the investment has GDP per capita of 11.1 and Gr_rate per capita of 3.05; The average return on capital invested is 20.5%; There were 11 changes of government over the past 25 years and country X has good infrastructure with some restrictions on trade.</em></p>
<p>First, let’s load the required packages:</p>
<div class="sourceCode" id="cb637"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb637-1"><a href="#cb637-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb637-2"><a href="#cb637-2" tabindex="-1"></a><span class="co"># you should have already installed this package as part of the previous Demonstration</span></span>
<span id="cb637-3"><a href="#cb637-3" tabindex="-1"></a><span class="fu">library</span>(GGally)</span></code></pre></div>
<p>Let’s import the data into R.</p>
<div class="sourceCode" id="cb638"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb638-1"><a href="#cb638-1" tabindex="-1"></a>mydataq1 <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&quot;data/FDI.csv&quot;</span>, <span class="at">header =</span> T) </span></code></pre></div>
<p>Now let’s get a glimpse of the data. As you know, there are many ways to
do that, such as, for example, using the tidyverse <code>glimpse</code> function.
This is quite a handy function because it also tells us more about the
class of each variable. We can see that although <strong>Infra</strong> and <strong>Trade</strong>
categorical, these are coded as integers.</p>
<div class="sourceCode" id="cb639"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb639-1"><a href="#cb639-1" tabindex="-1"></a><span class="fu">glimpse</span>(mydataq1)</span></code></pre></div>
<pre><code>## Rows: 60
## Columns: 7
## $ FDI     &lt;dbl&gt; 184.00, 187.00, 186.00, 192.00, 188.00, 190.00, 193.00, 194.00…
## $ GDP_Cap &lt;dbl&gt; 4.4, 6.3, 5.3, 5.9, 9.4, 7.6, 8.7, 6.0, 8.4, 10.1, 8.0, 6.9, 7…
## $ Gr_rate &lt;dbl&gt; 2.54, 4.06, 3.79, 3.38, 1.54, 2.25, 3.01, 2.13, 2.18, 3.33, 2.…
## $ ROC     &lt;dbl&gt; 6.7, 9.3, 7.1, 3.9, 6.3, 9.3, 6.3, 9.7, 5.6, 17.1, 9.1, 15.2, …
## $ Stable  &lt;int&gt; 9, 8, 11, 11, 8, 9, 9, 11, 12, 12, 8, 7, 12, 9, 8, 5, 9, 7, 11…
## $ Infra   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ Trade   &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3,…</code></pre>
<p>We therefore need to transform them into factors.</p>
<div class="sourceCode" id="cb641"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb641-1"><a href="#cb641-1" tabindex="-1"></a>mydataq1 <span class="ot">&lt;-</span> mydataq1 <span class="sc">%&gt;%</span></span>
<span id="cb641-2"><a href="#cb641-2" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Infra =</span> <span class="fu">as_factor</span>(Infra),</span>
<span id="cb641-3"><a href="#cb641-3" tabindex="-1"></a>         <span class="at">Trade =</span> <span class="fu">as_factor</span>(Trade)</span>
<span id="cb641-4"><a href="#cb641-4" tabindex="-1"></a>         )</span></code></pre></div>
<p>We can then explore all variables in the dataset as pairs using a matrix
of plots. Among many interesting features, we can note quite strong
correlations among pairs of variables which suggest the presence of
multicollinearity: GDP_Cap and ROC, Infra and GDP_Cap, Infra and ROC,
and Infra and Gr_rate.</p>
<div class="sourceCode" id="cb642"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb642-1"><a href="#cb642-1" tabindex="-1"></a>GGally<span class="sc">::</span><span class="fu">ggpairs</span>(mydataq1)</span></code></pre></div>
<p><img src="05-S05-ANS_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
<p>Ok, so our initial model is:</p>
<p><span class="math inline">\(FDI = b_0 + b_1GDP\_Cap + b_2Gr\_rate + b_3ROC – b_4Stable + b_5Infra + b_6Trade + e\)</span></p>
<p>where <strong>Infra</strong> and <strong>Stable</strong> are dummy variables.</p>
<p>We can have a look at how these two dummy variables are used in the
model by using the <code>contrasts()</code> function from base R.</p>
<p>The <strong>Infra</strong> variable is coded as <em>1 = basic infrastructure</em> and <em>2 = good infrastructure</em>. Since this is a binary variable, there will be one
reference category and a single dummy variable.</p>
<div class="sourceCode" id="cb643"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb643-1"><a href="#cb643-1" tabindex="-1"></a><span class="fu">contrasts</span>(mydataq1<span class="sc">$</span>Infra)</span></code></pre></div>
<pre><code>##   2
## 1 0
## 2 1</code></pre>
<p>The <strong>Trade</strong> variable is coded as <em>1 = trade tightly</em> and <em>2 = some
restrictions on trade</em> and <em>2 = some restrictions on trade</em>. Since this
is variable with three categories, there will be one reference category
and two dummy variables.</p>
<div class="sourceCode" id="cb645"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb645-1"><a href="#cb645-1" tabindex="-1"></a><span class="fu">contrasts</span>(mydataq1<span class="sc">$</span>Trade)</span></code></pre></div>
<pre><code>##   2 3
## 1 0 0
## 2 1 0
## 3 0 1</code></pre>
<p>Let’s now fit our multiple regression model.</p>
<div class="sourceCode" id="cb647"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb647-1"><a href="#cb647-1" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(FDI <span class="sc">~</span> GDP_Cap <span class="sc">+</span> Gr_rate <span class="sc">+</span> ROC <span class="sc">+</span> Stable <span class="sc">+</span> Infra <span class="sc">+</span> Trade, <span class="at">data =</span> mydataq1)</span></code></pre></div>
<p>And explore the results.</p>
<div class="sourceCode" id="cb648"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb648-1"><a href="#cb648-1" tabindex="-1"></a><span class="fu">summary</span>(m1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = FDI ~ GDP_Cap + Gr_rate + ROC + Stable + Infra + 
##     Trade, data = mydataq1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.8594 -1.2595 -0.0808  1.4183  8.7210 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 189.472086   2.690418  70.425  &lt; 2e-16 ***
## GDP_Cap       0.938713   0.236206   3.974 0.000219 ***
## Gr_rate      -0.089122   0.498508  -0.179 0.858807    
## ROC           0.003144   0.088466   0.036 0.971782    
## Stable       -0.539889   0.172155  -3.136 0.002816 ** 
## Infra2       -0.169110   1.493552  -0.113 0.910287    
## Trade2        4.875539   0.891476   5.469 1.31e-06 ***
## Trade3        5.890833   1.179891   4.993 7.07e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.734 on 52 degrees of freedom
## Multiple R-squared:  0.7481, Adjusted R-squared:  0.7142 
## F-statistic: 22.06 on 7 and 52 DF,  p-value: 1.646e-13</code></pre>
<div class="question">
<p>What do the results indicate?<br />
How do they compare with our initial assumptions?<br />
How do you proceed with the analysis?</p>
</div>
<p>The results provide us with several pieces of important information. We
initially assumed that the relationship between Gr_rate and FDI and
Infra and FDI are positive. However, we can see that the estimated
coefficients are negative. Also, there are several variables that are
not statistically significant. The <strong>ROC</strong> variable has the highest
p-value. Overall, the model appears to be a good fit given that 74.81% of variability is being explained. Also, make a note of the adjusted r-squared value which is 71.42 % (as the name implies, this measure adjusts the r-squared value according to the number of predictors in the model).</p>
<p>Ok, so given the evidence of multicollinearity from earlier and given
the lack of statistical significance, we can proceed to remove the
variable with the largest p-value (so the <strong>ROC</strong>) variable and refit
the model.</p>
<div class="sourceCode" id="cb650"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb650-1"><a href="#cb650-1" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(FDI <span class="sc">~</span> GDP_Cap <span class="sc">+</span> Gr_rate <span class="sc">+</span> Stable <span class="sc">+</span> Infra <span class="sc">+</span> Trade, <span class="at">data =</span> mydataq1)</span>
<span id="cb650-2"><a href="#cb650-2" tabindex="-1"></a></span>
<span id="cb650-3"><a href="#cb650-3" tabindex="-1"></a><span class="co"># or </span></span>
<span id="cb650-4"><a href="#cb650-4" tabindex="-1"></a></span>
<span id="cb650-5"><a href="#cb650-5" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">update</span>(m1,<span class="sc">~</span>. <span class="sc">-</span> ROC, <span class="at">data =</span> mydataq1) </span></code></pre></div>
<p>Now, we see that the explained variability is the same as in model 1 (74.81%). However, the adjusted R-squared has increased slightly (from 71.4% to about 72%).</p>
<div class="sourceCode" id="cb651"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb651-1"><a href="#cb651-1" tabindex="-1"></a><span class="fu">summary</span>(m2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = FDI ~ GDP_Cap + Gr_rate + Stable + Infra + Trade, 
##     data = mydataq1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.8633 -1.2684 -0.0897  1.4174  8.7346 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 189.4790     2.6579  71.289  &lt; 2e-16 ***
## GDP_Cap       0.9409     0.2258   4.167 0.000114 ***
## Gr_rate      -0.0846     0.4774  -0.177 0.860031    
## Stable       -0.5413     0.1663  -3.255 0.001976 ** 
## Infra2       -0.1358     1.1519  -0.118 0.906598    
## Trade2        4.8805     0.8723   5.595 7.92e-07 ***
## Trade3        5.9116     1.0151   5.824 3.45e-07 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.708 on 53 degrees of freedom
## Multiple R-squared:  0.7481, Adjusted R-squared:  0.7196 
## F-statistic: 26.23 on 6 and 53 DF,  p-value: 3.05e-14</code></pre>
<p>We can observe that the <strong>Infra</strong> variable has the largest p-value and,
as before with the <strong>ROC</strong> variable, we remove it and refit the model by
removing the least significant term.</p>
<div class="sourceCode" id="cb653"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb653-1"><a href="#cb653-1" tabindex="-1"></a>m3 <span class="ot">&lt;-</span> <span class="fu">update</span>(m2,<span class="sc">~</span>. <span class="sc">-</span> Infra, <span class="at">data =</span> mydataq1) </span></code></pre></div>
<p>The r-squared value has decreased slightly from 74.81% to 74.8. But, again, we see that the adjusted R-squared increased from about 72% in model 2 to about 72.5%.</p>
<div class="sourceCode" id="cb654"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb654-1"><a href="#cb654-1" tabindex="-1"></a><span class="fu">summary</span>(m3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = FDI ~ GDP_Cap + Gr_rate + Stable + Trade, data = mydataq1)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.8072 -1.3012 -0.0538  1.3545  8.7185 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 189.63356    2.29103  82.772  &lt; 2e-16 ***
## GDP_Cap       0.92068    0.14544   6.331 5.01e-08 ***
## Gr_rate      -0.09192    0.46906  -0.196  0.84537    
## Stable       -0.54240    0.16445  -3.298  0.00173 ** 
## Trade2        4.89108    0.85969   5.689 5.34e-07 ***
## Trade3        5.95001    0.95263   6.246 6.86e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.683 on 54 degrees of freedom
## Multiple R-squared:  0.748,  Adjusted R-squared:  0.7247 
## F-statistic: 32.06 on 5 and 54 DF,  p-value: 5.104e-15</code></pre>
<p>The <strong>Gr_rate</strong> variable has the largest p-value and, as before, we
remove it and refit the model by removing the least significant term.</p>
<div class="sourceCode" id="cb656"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb656-1"><a href="#cb656-1" tabindex="-1"></a>m4 <span class="ot">&lt;-</span> <span class="fu">update</span>(m3,<span class="sc">~</span>. <span class="sc">-</span> Gr_rate, <span class="at">data =</span> mydataq1) </span></code></pre></div>
<p>Finally, we obtain a model where all coefficients are statistically
significant (although the <strong>Stable</strong> is significant at an <span class="math inline">\(\alpha\)</span> level
of 0.05). We see that the r-squared value has again decreased slightly to 74.79% but the explained variability is highest for this model, about 73% (the adjusted r-squared penalises the addition of predictors that are non-significant; since we removed these, the value increased).</p>
<p>Overall, the explained variability is still about 75% (the decrease across the models was extremely small).</p>
<div class="sourceCode" id="cb657"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb657-1"><a href="#cb657-1" tabindex="-1"></a><span class="fu">summary</span>(m4)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = FDI ~ GDP_Cap + Stable + Trade, data = mydataq1)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -5.661 -1.300 -0.035  1.317  8.626 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 189.4274     2.0173  93.903  &lt; 2e-16 ***
## GDP_Cap       0.9109     0.1355   6.723 1.07e-08 ***
## Stable       -0.5411     0.1629  -3.322  0.00159 ** 
## Trade2        4.8837     0.8513   5.737 4.27e-07 ***
## Trade3        5.9368     0.9419   6.303 5.19e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.66 on 55 degrees of freedom
## Multiple R-squared:  0.7479, Adjusted R-squared:  0.7295 
## F-statistic: 40.78 on 4 and 55 DF,  p-value: 7.573e-16</code></pre>
<p>Now, we can specify the model as:</p>
<p><span class="math display">\[
\begin{align*}
FDI &amp;= 189.4274 + 0.9109 \cdot GDP\_Cap - 0.5411 \cdot stable + 0.0000 \cdot Trade1 \\
    &amp;\phantom{= 189.4274 + 0.9109 \cdot GDP\_Cap - 0.5411 \cdot stable} + 4.8837 \cdot Trade2 \\
    &amp;\phantom{= 189.4274 + 0.9109 \cdot GDP\_Cap - 0.5411 \cdot stable} + 5.9368 \cdot Trade3
\end{align*}
\]</span></p>
<p>Finally, we use our best fit model to predict FDI for the following information: <em>country X receiving the investment has GDP per capita of 11.1 and Gr_rate per capita of 3.05; The average return on capital invested is 20.5%; There were 11 changes of government over the past 25 years and country X has good infrastructure with some restrictions on trade.</em></p>
<p><span class="math inline">\(FDI = 189.4274 + 0.9109*11.1 - 0.5411*11 + 4.8837\)</span></p>
<div class="sourceCode" id="cb659"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb659-1"><a href="#cb659-1" tabindex="-1"></a><span class="fl">189.4274</span> <span class="sc">+</span> <span class="fl">0.9109</span><span class="sc">*</span><span class="fl">11.1</span> <span class="sc">-</span> <span class="fl">0.5411</span><span class="sc">*</span><span class="dv">11</span> <span class="sc">+</span> <span class="fl">4.8837</span></span></code></pre></div>
<pre><code>## [1] 198.47</code></pre>
<p>Given that our final model (model 4) explains about 75% of the variability, we can conclude that our prediction of 198.47 is a fairly good one.</p>
<!--chapter:end:05-S05-ANS.Rmd-->
</div>
</div>
<div id="part-section-6" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART*) Section 6<a href="#part-section-6" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="overview-5" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Overview<a href="#overview-5" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div style="color: #333; font-size: 24px; font-style: italic; text-align: justify;">
<p>Section 6: Linear Model Selection and Regularisation</p>
</div>
<p>This section is comprised of four demonstrations using tasks, exercises,
and examples from the core textbook for this course:</p>
<p>James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). <em>An
Introduction to Statistical Learning with Applications in R</em>. 2nd ed.New
York: Springer. <a href="https://www.statlearning.com/" class="uri">https://www.statlearning.com/</a>.</p>
<div class="ilos">
<p><strong>Learning Outcomes:</strong></p>
<ul>
<li><p>perform subset and stepwise selection and interpret the results;</p></li>
<li><p>perform ridge regression and lasso and interpret the results;</p></li>
<li><p>perform PCR and PLS and interpret the results;</p></li>
<li><p>appreciate the differences between subset selection, regularisation,
and dimension reduction techniques;</p></li>
<li><p>address non-linearity using different approaches (polynomials, step
functions, splines, local regression, and GAMs).</p></li>
</ul>
</div>
<p><strong>In this section, we will cover the following functions:</strong></p>
<table>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Function</th>
<th align="center">Description</th>
<th align="center">Package</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><code>regsubsets()</code></td>
<td align="center">Model selection by exhaustive search, forward or backward stepwise, or sequential replacement</td>
<td align="center">leaps</td>
</tr>
<tr class="even">
<td align="center"><code>summary()</code></td>
<td align="center">produce result summaries</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>plot()</code> and associated plot functions (<code>points(), lines(), matlines(), title(), legend</code> )</td>
<td align="center">create plot and add features</td>
<td align="center">base and graphics</td>
</tr>
<tr class="even">
<td align="center"><code>which.max()</code>, <code>which.min()</code></td>
<td align="center">index of minimum or maximum value</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>coef()</code></td>
<td align="center">extract model coefficients</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>sample()</code></td>
<td align="center">take a sample with or without repalcement</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>rep()</code></td>
<td align="center">replicate elements of vectors, lists</td>
<td align="center">base</td>
</tr>
<tr class="even">
<td align="center"><code>matrix()</code></td>
<td align="center">create matrix</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>na.omit()</code></td>
<td align="center">handle missing values</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>model.matrix()</code></td>
<td align="center">create matrix</td>
<td align="center">stats</td>
</tr>
<tr class="odd">
<td align="center"><code>seq()</code></td>
<td align="center">generate sequence</td>
<td align="center">base</td>
</tr>
<tr class="even">
<td align="center"><code>glmnet()</code></td>
<td align="center">fit glm with lasso or regularisation</td>
<td align="center">glmnet</td>
</tr>
<tr class="odd">
<td align="center"><code>predict()</code></td>
<td align="center">obtain model predictions</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>mean()</code></td>
<td align="center">calculate arithmetic mean</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>cv.glmnet()</code></td>
<td align="center">cross-validation for glmnet</td>
<td align="center">glmnet</td>
</tr>
<tr class="even">
<td align="center"><code>pcr()</code></td>
<td align="center">principal components regression</td>
<td align="center">pls</td>
</tr>
<tr class="odd">
<td align="center"><code>validationplot()</code></td>
<td align="center">create validation plot</td>
<td align="center">pls</td>
</tr>
<tr class="even">
<td align="center"><code>plsr()</code></td>
<td align="center">partial least squares regression</td>
<td align="center">pls</td>
</tr>
<tr class="odd">
<td align="center"><code>poly()</code></td>
<td align="center">compute orthogonal polynomials</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>I()</code></td>
<td align="center">treat object “as is”</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>cbind()</code></td>
<td align="center">combine objects by rows or columns</td>
<td align="center">base</td>
</tr>
<tr class="even">
<td align="center"><code>glm()</code></td>
<td align="center">fit generalised linear models</td>
<td align="center">stats</td>
</tr>
<tr class="odd">
<td align="center"><code>anova()</code></td>
<td align="center">compute analysis or variance or deviance tables</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>exp()</code></td>
<td align="center">compute exponential function</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>table()</code></td>
<td align="center">create contingency table</td>
<td align="center">base</td>
</tr>
<tr class="even">
<td align="center"><code>cut()</code></td>
<td align="center">convert numeric to factor</td>
<td align="center">base</td>
</tr>
<tr class="odd">
<td align="center"><code>bs()</code></td>
<td align="center">generate B-splines matrix</td>
<td align="center">splines</td>
</tr>
<tr class="even">
<td align="center"><code>loess()</code></td>
<td align="center">local polynomial regression fitting</td>
<td align="center">stats</td>
</tr>
<tr class="odd">
<td align="center"><code>gam()</code></td>
<td align="center">fit generalised additive models</td>
<td align="center">gam</td>
</tr>
<tr class="even">
<td align="center"><code>s()</code></td>
<td align="center">specify smoothing spline fit in a GAM formula</td>
<td align="center">gam</td>
</tr>
<tr class="odd">
<td align="center"><code>lo()</code></td>
<td align="center">specify loess fit in a GAM formula</td>
<td align="center">gam</td>
</tr>
</tbody>
</table>
<!--chapter:end:06-S06-overview.Rmd-->
</div>
<div id="demonstration-1-subset-and-stepwise-selection" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Demonstration 1: Subset and Stepwise Selection<a href="#demonstration-1-subset-and-stepwise-selection" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="file">
<p>For the tasks below, you will require the <strong>Boston</strong> dataset. This dataset is part of the <code>ISRL2</code> package from the core textbook (James et. al 2021).</p>
<p>You will also need the <code>leaps</code> package; please make sure to install and load it before you begin the practical.</p>
</div>
<p>The goal of this demonstration is to predict median value of owner-occupied homes (<strong>medv</strong>) using all variables in the dataset. You should already be familiar with this dataset. To remind yourself of the variables it contains, type <code>?Boston</code> in your console after you load the <code>ISLR2</code> package.</p>
<p>Loading the packages:</p>
<div class="sourceCode" id="cb661"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb661-1"><a href="#cb661-1" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb661-2"><a href="#cb661-2" tabindex="-1"></a><span class="fu">library</span>(leaps)</span></code></pre></div>
<div id="best-subset-selection" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Best Subset Selection<a href="#best-subset-selection" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Best subset selection can be performed using the <code>regsubsets()</code> function from the <code>leaps</code> package. This function works by identifying the best model (quantified using the RSS) that contains a given number of predictors. The syntax is similar to other model fitting functions you have encountered so far in the course.</p>
<div class="sourceCode" id="cb662"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb662-1"><a href="#cb662-1" tabindex="-1"></a><span class="fu">attach</span>(Boston)</span>
<span id="cb662-2"><a href="#cb662-2" tabindex="-1"></a>regfit.full <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(medv <span class="sc">~</span> ., Boston)</span></code></pre></div>
<p>Let’s explore the results using <code>summary()</code>.</p>
<div class="sourceCode" id="cb663"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb663-1"><a href="#cb663-1" tabindex="-1"></a><span class="fu">summary</span>(regfit.full)</span></code></pre></div>
<pre><code>## Subset selection object
## Call: regsubsets.formula(medv ~ ., Boston)
## 12 Variables  (and intercept)
##         Forced in Forced out
## crim        FALSE      FALSE
## zn          FALSE      FALSE
## indus       FALSE      FALSE
## chas        FALSE      FALSE
## nox         FALSE      FALSE
## rm          FALSE      FALSE
## age         FALSE      FALSE
## dis         FALSE      FALSE
## rad         FALSE      FALSE
## tax         FALSE      FALSE
## ptratio     FALSE      FALSE
## lstat       FALSE      FALSE
## 1 subsets of each size up to 8
## Selection Algorithm: exhaustive
##          crim zn  indus chas nox rm  age dis rad tax ptratio lstat
## 1  ( 1 ) &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;     &quot;*&quot;  
## 2  ( 1 ) &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;     &quot;*&quot;  
## 3  ( 1 ) &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 4  ( 1 ) &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 5  ( 1 ) &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 6  ( 1 ) &quot; &quot;  &quot; &quot; &quot; &quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 7  ( 1 ) &quot; &quot;  &quot;*&quot; &quot; &quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 8  ( 1 ) &quot;*&quot;  &quot;*&quot; &quot; &quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;</code></pre>
<p>As you can see, the output does not contain any coefficients or related information. This is because the function performs an exhaustive model search to identify the best set of variables for each model size but does not actually fit each model.</p>
<p>Here we see that the function produced models of up to 8 predictors (this is the default option for the function). An asterisk indicates that a given variable is included in the corresponding model; for example, the output indicates that the best two-variable model contains variables <strong>rm</strong> and <strong>lstat</strong>.</p>
<p>The default option can be overridden such that the model search is performed with more than 8 variables by providing a value for the <code>nvmax</code> argument. Excluding the response variable, we have a total of 12 variables available in the dataset and so we set <code>nvmax</code> to 12.</p>
<div class="sourceCode" id="cb665"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb665-1"><a href="#cb665-1" tabindex="-1"></a>regfit.full <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(medv <span class="sc">~</span> ., Boston, <span class="at">nvmax =</span> <span class="dv">12</span>)</span></code></pre></div>
<p>Now, it does not seem that <code>summary</code> provided much information. Actually, to extract relevant information about each model such as the r-squared, adjusted r-squared, Mallow’s <span class="math inline">\(C_p\)</span>, BIC, etc., we can assign the summary results to an object and then use the function on that object.</p>
<div class="sourceCode" id="cb666"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb666-1"><a href="#cb666-1" tabindex="-1"></a>reg.summary <span class="ot">&lt;-</span> <span class="fu">summary</span>(regfit.full)</span>
<span id="cb666-2"><a href="#cb666-2" tabindex="-1"></a><span class="fu">summary</span>(reg.summary)</span></code></pre></div>
<pre><code>##        Length Class      Mode     
## which  156    -none-     logical  
## rsq     12    -none-     numeric  
## rss     12    -none-     numeric  
## adjr2   12    -none-     numeric  
## cp      12    -none-     numeric  
## bic     12    -none-     numeric  
## outmat 144    -none-     character
## obj     28    regsubsets list</code></pre>
<p>Now, this object has additional information which we can access by using the <span class="math inline">\(\$\)</span> sign. For instance, we can obtain the <span class="math inline">\(R^2\)</span> statistic for each one of the 12 models.</p>
<div class="sourceCode" id="cb668"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb668-1"><a href="#cb668-1" tabindex="-1"></a>reg.summary<span class="sc">$</span>rsq</span></code></pre></div>
<pre><code>##  [1] 0.5441463 0.6385616 0.6786242 0.6903077 0.7080893 0.7157742 0.7196230
##  [8] 0.7236239 0.7282911 0.7342423 0.7342817 0.7343070</code></pre>
<p>We can see that the <span class="math inline">\(R^2\)</span> increases from about <span class="math inline">\(54\%\)</span> (for the model with one variable) to about <span class="math inline">\(73 \%\)</span> (when all 12 predictors are included). But note that there is little difference in the <span class="math inline">\(R^2\)</span> from the model with 10 variable onward. As expected, the <span class="math inline">\(R^2\)</span> statistic increases monotonically as more variables are included.</p>
<p>The <code>summary()</code> function also returns <span class="math inline">\(R^2\)</span>, RSS, adjusted <span class="math inline">\(R^2\)</span>, <span class="math inline">\(C_p\)</span>, and BIC and we can examine these to try to select the <em>best</em> overall model.</p>
<p>Plotting RSS, adjusted <span class="math inline">\(R^2\)</span>, <span class="math inline">\(C_p\)</span>, and BIC for all of the models at once will help us decide which model to select.</p>
<div class="sourceCode" id="cb670"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb670-1"><a href="#cb670-1" tabindex="-1"></a><span class="co">#producing a multipanel plot on 2 rows and 2 columns</span></span>
<span id="cb670-2"><a href="#cb670-2" tabindex="-1"></a></span>
<span id="cb670-3"><a href="#cb670-3" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb670-4"><a href="#cb670-4" tabindex="-1"></a></span>
<span id="cb670-5"><a href="#cb670-5" tabindex="-1"></a><span class="co">#plotting the RSS</span></span>
<span id="cb670-6"><a href="#cb670-6" tabindex="-1"></a><span class="fu">plot</span>(reg.summary<span class="sc">$</span>rss, <span class="at">xlab =</span> <span class="st">&quot;Number of Variables&quot;</span>,</span>
<span id="cb670-7"><a href="#cb670-7" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;RSS&quot;</span>, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>)</span>
<span id="cb670-8"><a href="#cb670-8" tabindex="-1"></a></span>
<span id="cb670-9"><a href="#cb670-9" tabindex="-1"></a><span class="co">#plotting the adjusted r-squared</span></span>
<span id="cb670-10"><a href="#cb670-10" tabindex="-1"></a><span class="fu">plot</span>(reg.summary<span class="sc">$</span>adjr2, <span class="at">xlab =</span> <span class="st">&quot;Number of Variables&quot;</span>,</span>
<span id="cb670-11"><a href="#cb670-11" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Adjusted RSq&quot;</span>, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>)</span>
<span id="cb670-12"><a href="#cb670-12" tabindex="-1"></a></span>
<span id="cb670-13"><a href="#cb670-13" tabindex="-1"></a><span class="co">#finding the largest value for the adjusted r-squared</span></span>
<span id="cb670-14"><a href="#cb670-14" tabindex="-1"></a>max_index_adjr2 <span class="ot">&lt;-</span> <span class="fu">which.max</span>(reg.summary<span class="sc">$</span>adjr2)</span>
<span id="cb670-15"><a href="#cb670-15" tabindex="-1"></a></span>
<span id="cb670-16"><a href="#cb670-16" tabindex="-1"></a><span class="co">#highlighting the largest value for the adjusted r-squared</span></span>
<span id="cb670-17"><a href="#cb670-17" tabindex="-1"></a><span class="fu">points</span>(max_index_adjr2 <span class="ot">&lt;-</span> <span class="fu">which.max</span>(reg.summary<span class="sc">$</span>adjr2), </span>
<span id="cb670-18"><a href="#cb670-18" tabindex="-1"></a>       reg.summary<span class="sc">$</span>adjr2[max_index_adjr2], </span>
<span id="cb670-19"><a href="#cb670-19" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">pch =</span> <span class="dv">20</span>)</span>
<span id="cb670-20"><a href="#cb670-20" tabindex="-1"></a></span>
<span id="cb670-21"><a href="#cb670-21" tabindex="-1"></a><span class="co">#plotting Mallow&#39;s Cp</span></span>
<span id="cb670-22"><a href="#cb670-22" tabindex="-1"></a></span>
<span id="cb670-23"><a href="#cb670-23" tabindex="-1"></a><span class="fu">plot</span>(reg.summary<span class="sc">$</span>cp, <span class="at">xlab =</span> <span class="st">&quot;Number of Variables&quot;</span>,</span>
<span id="cb670-24"><a href="#cb670-24" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Cp&quot;</span>, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>)</span>
<span id="cb670-25"><a href="#cb670-25" tabindex="-1"></a></span>
<span id="cb670-26"><a href="#cb670-26" tabindex="-1"></a><span class="co">#finding the lowest values for Mallow&#39;s Cp</span></span>
<span id="cb670-27"><a href="#cb670-27" tabindex="-1"></a>min_index_Cp <span class="ot">&lt;-</span> <span class="fu">which.min</span>(reg.summary<span class="sc">$</span>cp)</span>
<span id="cb670-28"><a href="#cb670-28" tabindex="-1"></a></span>
<span id="cb670-29"><a href="#cb670-29" tabindex="-1"></a><span class="co">#highlighting the lowest value for Mallow&#39;s Cp</span></span>
<span id="cb670-30"><a href="#cb670-30" tabindex="-1"></a></span>
<span id="cb670-31"><a href="#cb670-31" tabindex="-1"></a><span class="fu">points</span>(min_index_Cp, reg.summary<span class="sc">$</span>cp[min_index_Cp], </span>
<span id="cb670-32"><a href="#cb670-32" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>, <span class="at">pch =</span> <span class="dv">20</span>)</span>
<span id="cb670-33"><a href="#cb670-33" tabindex="-1"></a></span>
<span id="cb670-34"><a href="#cb670-34" tabindex="-1"></a><span class="co">#plotting the BIC</span></span>
<span id="cb670-35"><a href="#cb670-35" tabindex="-1"></a><span class="fu">plot</span>(reg.summary<span class="sc">$</span>bic, <span class="at">xlab =</span> <span class="st">&quot;Number of Variables&quot;</span>,</span>
<span id="cb670-36"><a href="#cb670-36" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;BIC&quot;</span>, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>)</span>
<span id="cb670-37"><a href="#cb670-37" tabindex="-1"></a></span>
<span id="cb670-38"><a href="#cb670-38" tabindex="-1"></a><span class="co">#finding the lowest values for the BIC</span></span>
<span id="cb670-39"><a href="#cb670-39" tabindex="-1"></a>min_index_bic <span class="ot">&lt;-</span> <span class="fu">which.min</span>(reg.summary<span class="sc">$</span>bic)</span>
<span id="cb670-40"><a href="#cb670-40" tabindex="-1"></a></span>
<span id="cb670-41"><a href="#cb670-41" tabindex="-1"></a><span class="co">#highlighting the lowest value for the BIC</span></span>
<span id="cb670-42"><a href="#cb670-42" tabindex="-1"></a><span class="fu">points</span>(min_index_bic, reg.summary<span class="sc">$</span>bic[min_index_bic], </span>
<span id="cb670-43"><a href="#cb670-43" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">cex =</span> <span class="dv">2</span>,<span class="at">pch =</span> <span class="dv">20</span>)</span></code></pre></div>
<p><img src="06-S06-D1_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>The <code>points()</code> command works like the <code>plot()</code> command, except that it adds points on a plot that has already been created. The <code>which.max()</code> or <code>which.min()</code> functions can be used to identify the location of the maximum and minimum point of a vector, respectively. To emphasise the maximum and minimum points, we plot a red dot.</p>
<p>The <code>regsubsets()</code> function has a built-in <code>plot()</code> command which can be used to display the selected variables for the best model with a given number of predictors, ranked according to different measures. This type of plot is handy particularly when there are large number of variables. To find out more about this function, type <code>?plot.regsubsets</code>.</p>
<div class="sourceCode" id="cb671"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb671-1"><a href="#cb671-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb671-2"><a href="#cb671-2" tabindex="-1"></a></span>
<span id="cb671-3"><a href="#cb671-3" tabindex="-1"></a><span class="fu">plot</span>(regfit.full, <span class="at">scale =</span> <span class="st">&quot;r2&quot;</span>)</span>
<span id="cb671-4"><a href="#cb671-4" tabindex="-1"></a><span class="fu">plot</span>(regfit.full, <span class="at">scale =</span> <span class="st">&quot;adjr2&quot;</span>)</span>
<span id="cb671-5"><a href="#cb671-5" tabindex="-1"></a><span class="fu">plot</span>(regfit.full, <span class="at">scale =</span> <span class="st">&quot;Cp&quot;</span>)</span>
<span id="cb671-6"><a href="#cb671-6" tabindex="-1"></a><span class="fu">plot</span>(regfit.full, <span class="at">scale =</span> <span class="st">&quot;bic&quot;</span>)</span></code></pre></div>
<p><img src="06-S06-D1_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>The top row of each plot contains a black square for each variable selected according to the optimal model associated with that statistic; lower values indicate better models. The model with the lowest BIC is the ten-variable model with the following variables: crim, zn, chas, nox, rm, dis, rad, tax, ptratio and lstat.</p>
<p>Now that we have selected our best model, we can extract coefficient estimates using the <code>coef()</code> function and proceed with our analysis.</p>
<div class="sourceCode" id="cb672"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb672-1"><a href="#cb672-1" tabindex="-1"></a><span class="fu">coef</span>(regfit.full, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##  (Intercept)         crim           zn         chas          nox           rm 
##  41.45174748  -0.12166488   0.04619119   2.87187265 -18.26242664   3.67295747 
##          dis          rad          tax      ptratio        lstat 
##  -1.51595105   0.28393226  -0.01229150  -0.93096144  -0.54650916</code></pre>
</div>
<div id="stepwise-selection" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Stepwise Selection<a href="#stepwise-selection" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We can also use the <code>regsubsets()</code> function to perform forward stepwise or backward stepwise selection, using the argument <code>method = "forward"</code> or <code>method = "backward"</code>.</p>
<p><strong>Forward Stepwise Selection</strong>:</p>
<div class="sourceCode" id="cb674"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb674-1"><a href="#cb674-1" tabindex="-1"></a>regfit.fwd <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston,</span>
<span id="cb674-2"><a href="#cb674-2" tabindex="-1"></a>                         <span class="at">nvmax =</span> <span class="dv">12</span>, <span class="at">method =</span> <span class="st">&quot;forward&quot;</span>)</span>
<span id="cb674-3"><a href="#cb674-3" tabindex="-1"></a><span class="fu">summary</span>(regfit.fwd)</span></code></pre></div>
<pre><code>## Subset selection object
## Call: regsubsets.formula(medv ~ ., data = Boston, nvmax = 12, method = &quot;forward&quot;)
## 12 Variables  (and intercept)
##         Forced in Forced out
## crim        FALSE      FALSE
## zn          FALSE      FALSE
## indus       FALSE      FALSE
## chas        FALSE      FALSE
## nox         FALSE      FALSE
## rm          FALSE      FALSE
## age         FALSE      FALSE
## dis         FALSE      FALSE
## rad         FALSE      FALSE
## tax         FALSE      FALSE
## ptratio     FALSE      FALSE
## lstat       FALSE      FALSE
## 1 subsets of each size up to 12
## Selection Algorithm: forward
##           crim zn  indus chas nox rm  age dis rad tax ptratio lstat
## 1  ( 1 )  &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;     &quot;*&quot;  
## 2  ( 1 )  &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;     &quot;*&quot;  
## 3  ( 1 )  &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 4  ( 1 )  &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 5  ( 1 )  &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 6  ( 1 )  &quot; &quot;  &quot; &quot; &quot; &quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 7  ( 1 )  &quot; &quot;  &quot;*&quot; &quot; &quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 8  ( 1 )  &quot;*&quot;  &quot;*&quot; &quot; &quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 9  ( 1 )  &quot;*&quot;  &quot;*&quot; &quot; &quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 10  ( 1 ) &quot;*&quot;  &quot;*&quot; &quot; &quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;     &quot;*&quot;  
## 11  ( 1 ) &quot;*&quot;  &quot;*&quot; &quot; &quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;     &quot;*&quot;  
## 12  ( 1 ) &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;     &quot;*&quot;</code></pre>
<p>For instance, we see that using forward stepwise selection, the best one-variable model contains only <strong>lstat</strong>, and the best two-variable model additionally includes <strong>rm</strong>.</p>
<p><strong>Backward Stepwise Selection</strong>:</p>
<div class="sourceCode" id="cb676"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb676-1"><a href="#cb676-1" tabindex="-1"></a>regfit.bwd <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston,</span>
<span id="cb676-2"><a href="#cb676-2" tabindex="-1"></a>                         <span class="at">nvmax =</span> <span class="dv">12</span>, <span class="at">method =</span> <span class="st">&quot;backward&quot;</span>)</span>
<span id="cb676-3"><a href="#cb676-3" tabindex="-1"></a><span class="fu">summary</span>(regfit.bwd)</span></code></pre></div>
<pre><code>## Subset selection object
## Call: regsubsets.formula(medv ~ ., data = Boston, nvmax = 12, method = &quot;backward&quot;)
## 12 Variables  (and intercept)
##         Forced in Forced out
## crim        FALSE      FALSE
## zn          FALSE      FALSE
## indus       FALSE      FALSE
## chas        FALSE      FALSE
## nox         FALSE      FALSE
## rm          FALSE      FALSE
## age         FALSE      FALSE
## dis         FALSE      FALSE
## rad         FALSE      FALSE
## tax         FALSE      FALSE
## ptratio     FALSE      FALSE
## lstat       FALSE      FALSE
## 1 subsets of each size up to 12
## Selection Algorithm: backward
##           crim zn  indus chas nox rm  age dis rad tax ptratio lstat
## 1  ( 1 )  &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;     &quot;*&quot;  
## 2  ( 1 )  &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;     &quot;*&quot;  
## 3  ( 1 )  &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 4  ( 1 )  &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 5  ( 1 )  &quot; &quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 6  ( 1 )  &quot;*&quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 7  ( 1 )  &quot;*&quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot;     &quot;*&quot;  
## 8  ( 1 )  &quot;*&quot;  &quot; &quot; &quot; &quot;   &quot; &quot;  &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;     &quot;*&quot;  
## 9  ( 1 )  &quot;*&quot;  &quot;*&quot; &quot; &quot;   &quot; &quot;  &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;     &quot;*&quot;  
## 10  ( 1 ) &quot;*&quot;  &quot;*&quot; &quot; &quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;     &quot;*&quot;  
## 11  ( 1 ) &quot;*&quot;  &quot;*&quot; &quot; &quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;     &quot;*&quot;  
## 12  ( 1 ) &quot;*&quot;  &quot;*&quot; &quot;*&quot;   &quot;*&quot;  &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot;     &quot;*&quot;</code></pre>
<p>If we compare the output from the forward and backward stepwise selection based on the asterisks, we can see that forward and backward stepwise selection yield identical models for those with 1 to five and 10, 11 and (obviously) 12 variables. However, the models with 6 to 9 variables are different.</p>
<p>For example, if we have a look at the 6-model variables, we can see that not only do the variables differ but evidently, that the coefficients differ. If we compare the stepwise selection results with the best subset selection model also, we can see that the results from best subset and forward stepwise selection are the same for these data.</p>
<div class="sourceCode" id="cb678"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb678-1"><a href="#cb678-1" tabindex="-1"></a><span class="fu">coef</span>(regfit.fwd, <span class="dv">6</span>)</span></code></pre></div>
<pre><code>## (Intercept)        chas         nox          rm         dis     ptratio 
##  36.9226340   3.2443048 -18.7404327   4.1118117  -1.1445857  -1.0027463 
##       lstat 
##  -0.5698442</code></pre>
<div class="sourceCode" id="cb680"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb680-1"><a href="#cb680-1" tabindex="-1"></a><span class="fu">coef</span>(regfit.bwd, <span class="dv">6</span>)</span></code></pre></div>
<pre><code>##  (Intercept)         crim          nox           rm          dis      ptratio 
##  35.54553027  -0.07202363 -17.00665420   4.25196130  -1.20334533  -1.00080347 
##        lstat 
##  -0.55353166</code></pre>
<div class="sourceCode" id="cb682"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb682-1"><a href="#cb682-1" tabindex="-1"></a><span class="fu">coef</span>(regfit.full, <span class="dv">6</span>)</span></code></pre></div>
<pre><code>## (Intercept)        chas         nox          rm         dis     ptratio 
##  36.9226340   3.2443048 -18.7404327   4.1118117  -1.1445857  -1.0027463 
##       lstat 
##  -0.5698442</code></pre>
</div>
<div id="choosing-among-models-using-the-validation-set-approach-and-cross-validation" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Choosing Among Models Using the Validation-Set Approach and Cross-Validation<a href="#choosing-among-models-using-the-validation-set-approach-and-cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As discussed in the learning materials, we can use different measures such as <span class="math inline">\(C_p\)</span> and BIC to select one model from among a set of models of different sizes.</p>
<p>However, to obtain accurate estimates of the test error through resampling methods, it is crucial that we perform all steps related to model fitting (including variable selection) only on training observations. In other words, to determine which model of a given size is best, we must consider only the training observations.</p>
<div id="the-validation-set-approach-1" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">The Validation Set Approach<a href="#the-validation-set-approach-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let’s first consider the validation set approach. We split the data into training and test sets.</p>
<div class="sourceCode" id="cb684"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb684-1"><a href="#cb684-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb684-2"><a href="#cb684-2" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">c</span>(<span class="cn">TRUE</span>, <span class="cn">FALSE</span>), <span class="fu">nrow</span>(Boston), <span class="at">replace =</span> <span class="cn">TRUE</span>)</span>
<span id="cb684-3"><a href="#cb684-3" tabindex="-1"></a>test <span class="ot">&lt;-</span> (<span class="sc">!</span>train)</span></code></pre></div>
<p>And only once we have done that, we apply <code>regsubsets()</code> to the training set to perform best subset selection.</p>
<div class="sourceCode" id="cb685"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb685-1"><a href="#cb685-1" tabindex="-1"></a>regfit.best <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston[train, ], <span class="at">nvmax =</span> <span class="dv">12</span>)</span></code></pre></div>
<p>Let’s now compute the validation set error for the best model of each model size. We first make a model matrix from the test data.</p>
<div class="sourceCode" id="cb686"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb686-1"><a href="#cb686-1" tabindex="-1"></a>test.mat <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston[test, ])</span></code></pre></div>
<p>The<code>model.matrix()</code> function from the built-in <code>stats</code> package can be used for building an “X” matrix from data. Now we run a loop, and for each size <code>i</code>, we extract the coefficients from <code>regfit.best</code> for the best model of that size, multiply them into
the appropriate columns of the test model matrix to form the predictions, and compute the test MSE.</p>
<div class="sourceCode" id="cb687"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb687-1"><a href="#cb687-1" tabindex="-1"></a>val.errors <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, <span class="dv">12</span>)</span>
<span id="cb687-2"><a href="#cb687-2" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>) {</span>
<span id="cb687-3"><a href="#cb687-3" tabindex="-1"></a> coefi <span class="ot">&lt;-</span> <span class="fu">coef</span>(regfit.best, <span class="at">id =</span> i)</span>
<span id="cb687-4"><a href="#cb687-4" tabindex="-1"></a> pred <span class="ot">&lt;-</span> test.mat[, <span class="fu">names</span>(coefi)] <span class="sc">%*%</span> coefi</span>
<span id="cb687-5"><a href="#cb687-5" tabindex="-1"></a> val.errors[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>((Boston<span class="sc">$</span>medv[test] <span class="sc">-</span> pred)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb687-6"><a href="#cb687-6" tabindex="-1"></a>}</span></code></pre></div>
<p>We find that the best model is the one that contains ten variables.</p>
<div class="sourceCode" id="cb688"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb688-1"><a href="#cb688-1" tabindex="-1"></a>val.errors</span></code></pre></div>
<pre><code>##  [1] 34.80258 27.60874 24.56211 23.50839 21.52928 21.08942 20.83503 20.45645
##  [9] 20.21655 19.66630 19.77416 19.79268</code></pre>
<div class="sourceCode" id="cb690"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb690-1"><a href="#cb690-1" tabindex="-1"></a><span class="fu">which.min</span>(val.errors)</span></code></pre></div>
<pre><code>## [1] 10</code></pre>
<div class="sourceCode" id="cb692"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb692-1"><a href="#cb692-1" tabindex="-1"></a><span class="fu">coef</span>(regfit.best, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##  (Intercept)         crim           zn         chas          nox           rm 
##  40.32858935  -0.13566265   0.05194674   3.52712484 -16.35131034   3.68720921 
##          dis          rad          tax      ptratio        lstat 
##  -1.42553389   0.27680029  -0.01097970  -0.93449308  -0.57986120</code></pre>
<p>Since we will be using this function again, we can capture our steps above and write our own predict method (the reason is that there is no <code>predict()</code> method for <code>regsubsets()</code>)</p>
<div class="sourceCode" id="cb694"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb694-1"><a href="#cb694-1" tabindex="-1"></a> predict.regsubsets <span class="ot">&lt;-</span> <span class="cf">function</span>(object, newdata, id, ...) {</span>
<span id="cb694-2"><a href="#cb694-2" tabindex="-1"></a>  form <span class="ot">&lt;-</span> <span class="fu">as.formula</span>(object<span class="sc">$</span>call[[<span class="dv">2</span>]])</span>
<span id="cb694-3"><a href="#cb694-3" tabindex="-1"></a>  mat <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(form, newdata)</span>
<span id="cb694-4"><a href="#cb694-4" tabindex="-1"></a>  coefi <span class="ot">&lt;-</span> <span class="fu">coef</span>(object, <span class="at">id =</span> id)</span>
<span id="cb694-5"><a href="#cb694-5" tabindex="-1"></a>  xvars <span class="ot">&lt;-</span> <span class="fu">names</span>(coefi)</span>
<span id="cb694-6"><a href="#cb694-6" tabindex="-1"></a>  mat[, xvars] <span class="sc">%*%</span> coefi</span>
<span id="cb694-7"><a href="#cb694-7" tabindex="-1"></a> }</span></code></pre></div>
<p>This function pretty much mimics what we did in the previous steps. The only complex part is how we extracted the formula used in the call to <code>regsubsets()</code>.</p>
<p>Finally, we perform best subset selection on the full data set, and select the best 10-variable model. It is important that we make use of the full data set in order to obtain more accurate coefficient estimates. Note that we perform best subset selection on the full data set and select the best 10-variable model, rather than simply using the variables that were obtained from the training set, because the best 10-variable model on the full data set may differ from the corresponding model on the training set.</p>
<div class="sourceCode" id="cb695"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb695-1"><a href="#cb695-1" tabindex="-1"></a>regfit.best <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston,</span>
<span id="cb695-2"><a href="#cb695-2" tabindex="-1"></a>    <span class="at">nvmax =</span> <span class="dv">12</span>)</span>
<span id="cb695-3"><a href="#cb695-3" tabindex="-1"></a><span class="fu">coef</span>(regfit.best, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##  (Intercept)         crim           zn         chas          nox           rm 
##  41.45174748  -0.12166488   0.04619119   2.87187265 -18.26242664   3.67295747 
##          dis          rad          tax      ptratio        lstat 
##  -1.51595105   0.28393226  -0.01229150  -0.93096144  -0.54650916</code></pre>
<p>In fact, we see that for these data, the best 10-variable model on the full data set has the same set of variables as the best 10-variable model on the training set.</p>
</div>
<div id="cross-validation" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">Cross-validation<a href="#cross-validation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now try to choose among the models of different sizes using cross-validation which involves performing best subset selection <em>within each of the <span class="math inline">\(k\)</span> training sets</em>.
Despite this, we see that with its clever subsetting syntax, <code>R</code> makes this job quite easy. First, we create a vector that allocates each observation to one of <span class="math inline">\(k=10\)</span> folds, and we create a matrix in which we will store the results.</p>
<div class="sourceCode" id="cb697"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb697-1"><a href="#cb697-1" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb697-2"><a href="#cb697-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(Boston)</span>
<span id="cb697-3"><a href="#cb697-3" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb697-4"><a href="#cb697-4" tabindex="-1"></a>folds <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>k, <span class="at">length =</span> n))</span>
<span id="cb697-5"><a href="#cb697-5" tabindex="-1"></a>cv.errors <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, k, <span class="dv">12</span>,</span>
<span id="cb697-6"><a href="#cb697-6" tabindex="-1"></a>                    <span class="at">dimnames =</span> <span class="fu">list</span>(<span class="cn">NULL</span>, <span class="fu">paste</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>)))</span></code></pre></div>
<p>Now we write a for loop that performs cross-validation. In the <span class="math inline">\(j\)</span>th fold, the elements of <code>folds</code> that equal <code>j</code> are in the test set, and the remainder are in the training set. We make our predictions for each model size (using our new <code>predict()</code> method), compute the test errors on the appropriate subset, and store them in the appropriate slot in the matrix <code>cv.errors</code>. Note that in the following code <code>R</code> will automatically use our <code>predict.regsubsets()</code> function when we call <code>predict()</code> because the <code>best.fit</code> object has class <code>regsubsets</code>.</p>
<div class="sourceCode" id="cb698"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb698-1"><a href="#cb698-1" tabindex="-1"></a><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>k) {</span>
<span id="cb698-2"><a href="#cb698-2" tabindex="-1"></a>  best.fit <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(medv <span class="sc">~</span> .,</span>
<span id="cb698-3"><a href="#cb698-3" tabindex="-1"></a>       <span class="at">data =</span> Boston[folds <span class="sc">!=</span> j, ],</span>
<span id="cb698-4"><a href="#cb698-4" tabindex="-1"></a>       <span class="at">nvmax =</span> <span class="dv">12</span>)</span>
<span id="cb698-5"><a href="#cb698-5" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">12</span>) {</span>
<span id="cb698-6"><a href="#cb698-6" tabindex="-1"></a>    pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(best.fit, Boston[folds <span class="sc">==</span> j, ], <span class="at">id =</span> i)</span>
<span id="cb698-7"><a href="#cb698-7" tabindex="-1"></a>    cv.errors[j, i] <span class="ot">&lt;-</span></span>
<span id="cb698-8"><a href="#cb698-8" tabindex="-1"></a>         <span class="fu">mean</span>((Boston<span class="sc">$</span>medv[folds <span class="sc">==</span> j] <span class="sc">-</span> pred)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb698-9"><a href="#cb698-9" tabindex="-1"></a>   }</span>
<span id="cb698-10"><a href="#cb698-10" tabindex="-1"></a> }</span></code></pre></div>
<p>This has given us a <span class="math inline">\(10 \times 12\)</span> matrix, of which the <span class="math inline">\((j,i)\)</span>th element corresponds to the test MSE for the <span class="math inline">\(j\)</span>th cross-validation fold for the best <span class="math inline">\(i\)</span>-variable model. We use the <code>apply()</code> function to average over the columns of this matrix in order to obtain a vector for which the <span class="math inline">\(i\)</span>th element is the cross-validation error for the <span class="math inline">\(i\)</span>-variable model.</p>
<div class="sourceCode" id="cb699"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb699-1"><a href="#cb699-1" tabindex="-1"></a>mean.cv.errors <span class="ot">&lt;-</span> <span class="fu">apply</span>(cv.errors, <span class="dv">2</span>, mean)</span>
<span id="cb699-2"><a href="#cb699-2" tabindex="-1"></a>mean.cv.errors</span></code></pre></div>
<pre><code>##        1        2        3        4        5        6        7        8 
## 38.78478 31.10456 27.77372 27.50234 25.49326 24.89269 25.17946 25.35254 
##        9       10       11       12 
## 24.99038 23.75908 23.89258 23.91715</code></pre>
<div class="sourceCode" id="cb701"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb701-1"><a href="#cb701-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb701-2"><a href="#cb701-2" tabindex="-1"></a><span class="fu">plot</span>(mean.cv.errors, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>)</span></code></pre></div>
<p><img src="06-S06-D1_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p>We see that cross-validation selects a 10-variable model. We now perform best subset selection on the full data set in order to obtain the 10-variable model and proceed with our analysis.</p>
<div class="sourceCode" id="cb702"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb702-1"><a href="#cb702-1" tabindex="-1"></a>reg.best <span class="ot">&lt;-</span> <span class="fu">regsubsets</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> Boston, <span class="at">nvmax =</span> <span class="dv">12</span>)</span>
<span id="cb702-2"><a href="#cb702-2" tabindex="-1"></a><span class="fu">coef</span>(reg.best, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##  (Intercept)         crim           zn         chas          nox           rm 
##  41.45174748  -0.12166488   0.04619119   2.87187265 -18.26242664   3.67295747 
##          dis          rad          tax      ptratio        lstat 
##  -1.51595105   0.28393226  -0.01229150  -0.93096144  -0.54650916</code></pre>
<!--chapter:end:06-S06-D1.Rmd-->
</div>
</div>
</div>
<div id="demonstration-2-regularisation" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Demonstration 2: Regularisation<a href="#demonstration-2-regularisation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="file">
<p>For the tasks below, you require the <strong>Hitters</strong> dataset from the <code>ISRL2</code> package.</p>
<p>You will also need the <code>glmnet</code> package; please make sure to install and load it before you begin the practical.</p>
</div>
<p>The <strong>Hitters</strong> dataset contains Major League Baseball Data from the 1986 and 1987 season. It is a dataframe with 322 observations and 20 variables. To learn more about the variables, type <code>?Hitters</code> in your console.</p>
<p>The goal of this demonstration is to predict salary of major league players using ridge regression and lasso.</p>
<p>Loading the required packages:</p>
<div class="sourceCode" id="cb704"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb704-1"><a href="#cb704-1" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb704-2"><a href="#cb704-2" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span></code></pre></div>
<div id="ridge-regression" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Ridge Regression<a href="#ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before we proceed, we must check whether the dataset has any missing values.</p>
<div class="sourceCode" id="cb705"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb705-1"><a href="#cb705-1" tabindex="-1"></a><span class="fu">attach</span>(Hitters)</span>
<span id="cb705-2"><a href="#cb705-2" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">is.na</span>(Hitters<span class="sc">$</span>Salary))</span></code></pre></div>
<pre><code>## [1] 59</code></pre>
<p>We see that <code>Salary</code> is missing for <span class="math inline">\(59\)</span> players and so we must remove it before the analysis.</p>
<div class="sourceCode" id="cb707"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb707-1"><a href="#cb707-1" tabindex="-1"></a>Hitters <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(Hitters)</span>
<span id="cb707-2"><a href="#cb707-2" tabindex="-1"></a><span class="fu">sum</span>(<span class="fu">is.na</span>(Hitters))</span></code></pre></div>
<pre><code>## [1] 0</code></pre>
<p>In R, we can use the <code>glmnet</code> package to perform ridge regression (amongst many other types of models, including lasso). This function works slightly differently to the other model fitting functions we have covered so far in the course. This is because the <code>glmnet()</code> function does not accept a formula-based specification (i.e. <code>y ~ x</code>) since it only takes numerical, quantitative inputs. Therefore, instead of a formula, it requires the predictors to be specified as a matrix <code>x</code> and the response as a separate object <code>y</code>.</p>
<p>Therefore, we need to prepare our data adequately before performing regularisation. The <code>model.matrix()</code> function from the built-in <code>stats</code> package is useful for creating our matrix <code>x</code>. This function produces a matrix of all predictors (in this case, 12) and automatically transforms any factor variables into dummy variables.</p>
<div class="sourceCode" id="cb709"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb709-1"><a href="#cb709-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary <span class="sc">~</span> ., Hitters)</span>
<span id="cb709-2"><a href="#cb709-2" tabindex="-1"></a>x</span></code></pre></div>
<pre><code>##                    (Intercept) AtBat Hits HmRun Runs RBI Walks Years CAtBat
## -Alan Ashby                  1   315   81     7   24  38    39    14   3449
## -Alvin Davis                 1   479  130    18   66  72    76     3   1624
## -Andre Dawson                1   496  141    20   65  78    37    11   5628
## -Andres Galarraga            1   321   87    10   39  42    30     2    396
## -Alfredo Griffin             1   594  169     4   74  51    35    11   4408
## -Al Newman                   1   185   37     1   23   8    21     2    214
## -Argenis Salazar             1   298   73     0   24  24     7     3    509
## -Andres Thomas               1   323   81     6   26  32     8     2    341
## -Andre Thornton              1   401   92    17   49  66    65    13   5206
## -Alan Trammell               1   574  159    21  107  75    59    10   4631
## -Alex Trevino                1   202   53     4   31  26    27     9   1876
## -Andy VanSlyke               1   418  113    13   48  61    47     4   1512
## -Alan Wiggins                1   239   60     0   30  11    22     6   1941
## -Bill Almon                  1   196   43     7   29  27    30    13   3231
## -Buddy Bell                  1   568  158    20   89  75    73    15   8068
## -Buddy Biancalana            1   190   46     2   24   8    15     5    479
## -Bruce Bochy                 1   127   32     8   16  22    14     8    727
## -Barry Bonds                 1   413   92    16   72  48    65     1    413
## -Bobby Bonilla               1   426  109     3   55  43    62     1    426
## -Bob Brenly                  1   472  116    16   60  62    74     6   1924
## -Bill Buckner                1   629  168    18   73 102    40    18   8424
## -Brett Butler                1   587  163     4   92  51    70     6   2695
## -Bob Dernier                 1   324   73     4   32  18    22     7   1931
## -Bo Diaz                     1   474  129    10   50  56    40    10   2331
## -Bill Doran                  1   550  152     6   92  37    81     5   2308
## -Brian Downing               1   513  137    20   90  95    90    14   5201
## -Billy Hatcher               1   419  108     6   55  36    22     3    591
## -Brook Jacoby                1   583  168    17   83  80    56     5   1646
## -Bob Kearney                 1   204   49     6   23  25    12     7   1309
## -Bill Madlock                1   379  106    10   38  60    30    14   6207
## -Bob Melvin                  1   268   60     5   24  25    15     2    350
## -BillyJo Robidoux            1   181   41     1   15  21    33     2    232
## -Bill Schroeder              1   217   46     7   32  19     9     4    694
## -Chris Bando                 1   254   68     2   28  26    22     6    999
## -Chris Brown                 1   416  132     7   57  49    33     3    932
## -Carmen Castillo             1   205   57     8   34  32     9     5    756
## -Chili Davis                 1   526  146    13   71  70    84     6   2648
## -Carlton Fisk                1   457  101    14   42  63    22    17   6521
## -Curt Ford                   1   214   53     2   30  29    23     2    226
## -Carney Lansford             1   591  168    19   80  72    39     9   4478
## -Chet Lemon                  1   403  101    12   45  53    39    12   5150
## -Candy Maldonado             1   405  102    18   49  85    20     6    950
## -Carmelo Martinez            1   244   58     9   28  25    35     4   1335
## -Craig Reynolds              1   313   78     6   32  41    12    12   3742
## -Cal Ripken                  1   627  177    25   98  81    70     6   3210
## -Cory Snyder                 1   416  113    24   58  69    16     1    416
## -Chris Speier                1   155   44     6   21  23    15    16   6631
## -Curt Wilkerson              1   236   56     0   27  15    11     4   1115
## -Dave Anderson               1   216   53     1   31  15    22     4    926
## -Don Baylor                  1   585  139    31   93  94    62    17   7546
## -Daryl Boston                1   199   53     5   29  22    21     3    514
## -Darnell Coles               1   521  142    20   67  86    45     4    815
## -Dave Concepcion             1   311   81     3   42  30    26    17   8247
## -Doug DeCinces               1   512  131    26   69  96    52    14   5347
## -Darrell Evans               1   507  122    29   78  85    91    18   7761
## -Dwight Evans                1   529  137    26   86  97    97    15   6661
## -Damaso Garcia               1   424  119     6   57  46    13     9   3651
## -Dan Gladden                 1   351   97     4   55  29    39     4   1258
## -Dave Henderson              1   388  103    15   59  47    39     6   2174
## -Donnie Hill                 1   339   96     4   37  29    23     4   1064
## -Davey Lopes                 1   255   70     7   49  35    43    15   6311
## -Don Mattingly               1   677  238    31  117 113    53     5   2223
## -Dale Murphy                 1   614  163    29   89  83    75    11   5017
## -Dwayne Murphy               1   329   83     9   50  39    56     9   3828
## -Dave Parker                 1   637  174    31   89 116    56    14   6727
## -Dan Pasqua                  1   280   82    16   44  45    47     2    428
## -Darrell Porter              1   155   41    12   21  29    22    16   5409
## -Dick Schofield              1   458  114    13   67  57    48     4   1350
## -Don Slaught                 1   314   83    13   39  46    16     5   1457
## -Darryl Strawberry           1   475  123    27   76  93    72     4   1810
## -Dale Sveum                  1   317   78     7   35  35    32     1    317
## -Danny Tartabull             1   511  138    25   76  96    61     3    592
## -Denny Walling               1   382  119    13   54  58    36    12   2133
## -Dave Winfield               1   565  148    24   90 104    77    14   7287
## -Eric Davis                  1   415  115    27   97  71    68     3    711
## -Eddie Milner                1   424  110    15   70  47    36     7   2130
## -Eddie Murray                1   495  151    17   61  84    78    10   5624
## -Ed Romero                   1   233   49     2   41  23    18     8   1350
## -Frank White                 1   566  154    22   76  84    43    14   6100
## -George Bell                 1   641  198    31  101 108    41     5   2129
## -Glenn Braggs                1   215   51     4   19  18    11     1    215
## -George Brett                1   441  128    16   70  73    80    14   6675
## -Greg Brock                  1   325   76    16   33  52    37     5   1506
## -Gary Carter                 1   490  125    24   81 105    62    13   6063
## -Glenn Davis                 1   574  152    31   91 101    64     3    985
## -Gary Gaetti                 1   596  171    34   91 108    52     6   2862
## -Greg Gagne                  1   472  118    12   63  54    30     4    793
## -George Hendrick             1   283   77    14   45  47    26    16   6840
## -Glenn Hubbard               1   408   94     4   42  36    66     9   3573
## -Garth Iorg                  1   327   85     3   30  44    20     8   2140
## -Gary Matthews               1   370   96    21   49  46    60    15   6986
## -Graig Nettles               1   354   77    16   36  55    41    20   8716
## -Gary Pettis                 1   539  139     5   93  58    69     5   1469
## -Gary Redus                  1   340   84    11   62  33    47     5   1516
## -Garry Templeton             1   510  126     2   42  44    35    11   5562
## -Greg Walker                 1   282   78    13   37  51    29     5   1649
## -Gary Ward                   1   380  120     5   54  51    31     8   3118
## -Glenn Wilson                1   584  158    15   70  84    42     5   2358
## -Harold Baines               1   570  169    21   72  88    38     7   3754
## -Hubie Brooks                1   306  104    14   50  58    25     7   2954
## -Howard Johnson              1   220   54    10   30  39    31     5   1185
## -Hal McRae                   1   278   70     7   22  37    18    18   7186
## -Harold Reynolds             1   445   99     1   46  24    29     4    618
## -Harry Spilman               1   143   39     5   18  30    15     9    639
## -Herm Winningham             1   185   40     4   23  11    18     3    524
## -Jesse Barfield              1   589  170    40  107 108    69     6   2325
## -Juan Beniquez               1   343  103     6   48  36    40    15   4338
## -John Cangelosi              1   438  103     2   65  32    71     2    440
## -Jose Canseco                1   600  144    33   85 117    65     2    696
## -Joe Carter                  1   663  200    29  108 121    32     4   1447
## -Jack Clark                  1   232   55     9   34  23    45    12   4405
## -Jose Cruz                   1   479  133    10   48  72    55    17   7472
## -Jody Davis                  1   528  132    21   61  74    41     6   2641
## -Jim Dwyer                   1   160   39     8   18  31    22    14   2128
## -Julio Franco                1   599  183    10   80  74    32     5   2482
## -Jim Gantner                 1   497  136     7   58  38    26    11   3871
## -Johnny Grubb                1   210   70    13   32  51    28    15   4040
## -Jack Howell                 1   151   41     4   26  21    19     2    288
## -John Kruk                   1   278   86     4   33  38    45     1    278
## -Jeffrey Leonard             1   341   95     6   48  42    20    10   2964
## -Jim Morrison                1   537  147    23   58  88    47    10   2744
## -John Moses                  1   399  102     3   56  34    34     5    670
## -Jerry Mumphrey              1   309   94     5   37  32    26    13   4618
## -Jim Presley                 1   616  163    27   83 107    32     3   1437
## -Johnny Ray                  1   579  174     7   67  78    58     6   3053
## -Jeff Reed                   1   165   39     2   13   9    16     3    196
## -Jim Rice                    1   618  200    20   98 110    62    13   7127
## -Jerry Royster               1   257   66     5   31  26    32    14   3910
## -John Russell                1   315   76    13   35  60    25     3    630
## -Juan Samuel                 1   591  157    16   90  78    26     4   2020
## -John Shelby                 1   404   92    11   54  49    18     6   1354
## -Joel Skinner                1   315   73     5   23  37    16     4    450
## -Jim Sundberg                1   429   91    12   41  42    57    13   5590
## -Jose Uribe                  1   453  101     3   46  43    61     3    948
## -Joel Youngblood             1   184   47     5   20  28    18    11   3327
## -Kevin Bass                  1   591  184    20   83  79    38     5   1689
## -Kal Daniels                 1   181   58     6   34  23    22     1    181
## -Kirk Gibson                 1   441  118    28   84  86    68     8   2723
## -Ken Griffey                 1   490  150    21   69  58    35    14   6126
## -Keith Hernandez             1   551  171    13   94  83    94    13   6090
## -Kent Hrbek                  1   550  147    29   85  91    71     6   2816
## -Ken Landreaux               1   283   74     4   34  29    22    10   3919
## -Kevin McReynolds            1   560  161    26   89  96    66     4   1789
## -Kevin Mitchell              1   328   91    12   51  43    33     2    342
## -Keith Moreland              1   586  159    12   72  79    53     9   3082
## -Ken Oberkfell               1   503  136     5   62  48    83    10   3423
## -Ken Phelps                  1   344   85    24   69  64    88     7    911
## -Kirby Puckett               1   680  223    31  119  96    34     3   1928
## -Kurt Stillwell              1   279   64     0   31  26    30     1    279
## -Leon Durham                 1   484  127    20   66  65    67     7   3006
## -Len Dykstra                 1   431  127     8   77  45    58     2    667
## -Larry Herndon               1   283   70     8   33  37    27    12   4479
## -Lee Lacy                    1   491  141    11   77  47    37    15   4291
## -Len Matuszek                1   199   52     9   26  28    21     6    805
## -Lloyd Moseby                1   589  149    21   89  86    64     7   3558
## -Lance Parrish               1   327   84    22   53  62    38    10   4273
## -Larry Parrish               1   464  128    28   67  94    52    13   5829
## -Larry Sheets                1   338   92    18   42  60    21     3    682
## -Lou Whitaker                1   584  157    20   95  73    63    10   4704
## -Mike Aldrete                1   216   54     2   27  25    33     1    216
## -Marty Barrett               1   625  179     4   94  60    65     5   1696
## -Mike Davis                  1   489  131    19   77  55    34     7   2051
## -Mike Diaz                   1   209   56    12   22  36    19     2    216
## -Mariano Duncan              1   407   93     8   47  30    30     2    969
## -Mike Easler                 1   490  148    14   64  78    49    13   3400
## -Mel Hall                    1   442  131    18   68  77    33     6   1416
## -Mike Heath                  1   288   65     8   30  36    27     9   2815
## -Mike Kingery                1   209   54     3   25  14    12     1    209
## -Mike LaValliere             1   303   71     3   18  30    36     3    344
## -Mike Marshall               1   330   77    19   47  53    27     6   1928
## -Mike Pagliarulo             1   504  120    28   71  71    54     3   1085
## -Mark Salas                  1   258   60     8   28  33    18     3    638
## -Mike Schmidt                1    20    1     0    0   0     0     2     41
## -Mike Scioscia               1   374   94     5   36  26    62     7   1968
## -Mickey Tettleton            1   211   43    10   26  35    39     3    498
## -Milt Thompson               1   299   75     6   38  23    26     3    580
## -Mitch Webster               1   576  167     8   89  49    57     4    822
## -Mookie Wilson               1   381  110     9   61  45    32     7   3015
## -Marvell Wynne               1   288   76     7   34  37    15     4   1644
## -Mike Young                  1   369   93     9   43  42    49     5   1258
## -Ozzie Guillen               1   547  137     2   58  47    12     2   1038
## -Oddibe McDowell             1   572  152    18  105  49    65     2    978
## -Ozzie Smith                 1   514  144     0   67  54    79     9   4739
## -Ozzie Virgil                1   359   80    15   45  48    63     7   1493
## -Phil Bradley                1   526  163    12   88  50    77     4   1556
## -Phil Garner                 1   313   83     9   43  41    30    14   5885
## -Pete Incaviglia             1   540  135    30   82  88    55     1    540
## -Paul Molitor                1   437  123     9   62  55    40     9   4139
## -Pete Rose                   1   237   52     0   15  25    30    24  14053
## -Pat Sheridan                1   236   56     6   41  19    21     5   1257
## -Pat Tabler                  1   473  154     6   61  48    29     6   1966
## -Rafael Belliard             1   309   72     0   33  31    26     5    354
## -Rick Burleson               1   271   77     5   35  29    33    12   4933
## -Randy Bush                  1   357   96     7   50  45    39     5   1394
## -Rick Cerone                 1   216   56     4   22  18    15    12   2796
## -Ron Cey                     1   256   70    13   42  36    44    16   7058
## -Rob Deer                    1   466  108    33   75  86    72     3    652
## -Rick Dempsey                1   327   68    13   42  29    45    18   3949
## -Ron Hassey                  1   341  110     9   45  49    46     9   2331
## -Rickey Henderson            1   608  160    28  130  74    89     8   4071
## -Reggie Jackson              1   419  101    18   65  58    92    20   9528
## -Ron Kittle                  1   376   82    21   42  60    35     5   1770
## -Ray Knight                  1   486  145    11   51  76    40    11   3967
## -Rick Leach                  1   246   76     5   35  39    13     6    912
## -Rick Manning                1   205   52     8   31  27    17    12   5134
## -Rance Mulliniks             1   348   90    11   50  45    43    10   2288
## -Ron Oester                  1   523  135     8   52  44    52     9   3368
## -Rey Quinones                1   312   68     2   32  22    24     1    312
## -Rafael Ramirez              1   496  119     8   57  33    21     7   3358
## -Ronn Reynolds               1   126   27     3    8  10     5     4    239
## -Ron Roenicke                1   275   68     5   42  42    61     6    961
## -Ryne Sandberg               1   627  178    14   68  76    46     6   3146
## -Rafael Santana              1   394   86     1   38  28    36     4   1089
## -Rick Schu                   1   208   57     8   32  25    18     3    653
## -Ruben Sierra                1   382  101    16   50  55    22     1    382
## -Roy Smalley                 1   459  113    20   59  57    68    12   5348
## -Robby Thompson              1   549  149     7   73  47    42     1    549
## -Rob Wilfong                 1   288   63     3   25  33    16    10   2682
## -Robin Yount                 1   522  163     9   82  46    62    13   7037
## -Steve Balboni               1   512  117    29   54  88    43     6   1750
## -Scott Bradley               1   220   66     5   20  28    13     3    290
## -Sid Bream                   1   522  140    16   73  77    60     4    730
## -Steve Buechele              1   461  112    18   54  54    35     2    680
## -Shawon Dunston              1   581  145    17   66  68    21     2    831
## -Scott Fletcher              1   530  159     3   82  50    47     6   1619
## -Steve Garvey                1   557  142    21   58  81    23    18   8759
## -Steve Jeltz                 1   439   96     0   44  36    65     4    711
## -Steve Lombardozzi           1   453  103     8   53  33    52     2    507
## -Spike Owen                  1   528  122     1   67  45    51     4   1716
## -Steve Sax                   1   633  210     6   91  56    59     6   3070
## -Tony Bernazard              1   562  169    17   88  73    53     8   3181
## -Tom Brookens                1   281   76     3   42  25    20     8   2658
## -Tom Brunansky               1   593  152    23   69  75    53     6   2765
## -Tony Fernandez              1   687  213    10   91  65    27     4   1518
## -Tim Flannery                1   368  103     3   48  28    54     8   1897
## -Tom Foley                   1   263   70     1   26  23    30     4    888
## -Tony Gwynn                  1   642  211    14  107  59    52     5   2364
## -Terry Harper                1   265   68     8   26  30    29     7   1337
## -Tommy Herr                  1   559  141     2   48  61    73     8   3162
## -Tim Hulett                  1   520  120    17   53  44    21     4    927
## -Terry Kennedy               1    19    4     1    2   3     1     1     19
## -Tito Landrum                1   205   43     2   24  17    20     7    854
## -Tim Laudner                 1   193   47    10   21  29    24     6   1136
## -Tom Paciorek                1   213   61     4   17  22     3    17   4061
## -Tony Pena                   1   510  147    10   56  52    53     7   2872
## -Terry Pendleton             1   578  138     1   56  59    34     3   1399
## -Tony Phillips               1   441  113     5   76  52    76     5   1546
## -Terry Puhl                  1   172   42     3   17  14    15    10   4086
## -Ted Simmons                 1   127   32     4   14  25    12    19   8396
## -Tim Teufel                  1   279   69     4   35  31    32     4   1359
## -Tim Wallach                 1   480  112    18   50  71    44     7   3031
## -Vince Coleman               1   600  139     0   94  29    60     2   1236
## -Von Hayes                   1   610  186    19  107  98    74     6   2728
## -Vance Law                   1   360   81     5   37  44    37     7   2268
## -Wally Backman               1   387  124     1   67  27    36     7   1775
## -Wade Boggs                  1   580  207     8  107  71   105     5   2778
## -Will Clark                  1   408  117    11   66  41    34     1    408
## -Wally Joyner                1   593  172    22   82 100    57     1    593
## -Willie McGee                1   497  127     7   65  48    37     5   2703
## -Willie Randolph             1   492  136     5   76  50    94    12   5511
## -Wayne Tolleson              1   475  126     3   61  43    52     6   1700
## -Willie Upshaw               1   573  144     9   85  60    78     8   3198
## -Willie Wilson               1   631  170     9   77  44    31    11   4908
##                    CHits CHmRun CRuns CRBI CWalks LeagueN DivisionW PutOuts
## -Alan Ashby          835     69   321  414    375       1         1     632
## -Alvin Davis         457     63   224  266    263       0         1     880
## -Andre Dawson       1575    225   828  838    354       1         0     200
## -Andres Galarraga    101     12    48   46     33       1         0     805
## -Alfredo Griffin    1133     19   501  336    194       0         1     282
## -Al Newman            42      1    30    9     24       1         0      76
## -Argenis Salazar     108      0    41   37     12       0         1     121
## -Andres Thomas        86      6    32   34      8       1         1     143
## -Andre Thornton     1332    253   784  890    866       0         0       0
## -Alan Trammell      1300     90   702  504    488       0         0     238
## -Alex Trevino        467     15   192  186    161       1         1     304
## -Andy VanSlyke       392     41   205  204    203       1         0     211
## -Alan Wiggins        510      4   309  103    207       0         0     121
## -Bill Almon          825     36   376  290    238       1         0      80
## -Buddy Bell         2273    177  1045  993    732       1         1     105
## -Buddy Biancalana    102      5    65   23     39       0         1     102
## -Bruce Bochy         180     24    67   82     56       1         1     202
## -Barry Bonds          92     16    72   48     65       1         0     280
## -Bobby Bonilla       109      3    55   43     62       0         1     361
## -Bob Brenly          489     67   242  251    240       1         1     518
## -Bill Buckner       2464    164  1008 1072    402       0         0    1067
## -Brett Butler        747     17   442  198    317       0         0     434
## -Bob Dernier         491     13   291  108    180       1         0     222
## -Bo Diaz             604     61   246  327    166       1         1     732
## -Bill Doran          633     32   349  182    308       1         1     262
## -Brian Downing      1382    166   763  734    784       0         1     267
## -Billy Hatcher       149      8    80   46     31       1         1     226
## -Brook Jacoby        452     44   219  208    136       0         0     109
## -Bob Kearney         308     27   126  132     66       0         1     419
## -Bill Madlock       1906    146   859  803    571       1         1      72
## -Bob Melvin           78      5    34   29     18       1         1     442
## -BillyJo Robidoux     50      4    20   29     45       0         0     326
## -Bill Schroeder      160     32    86   76     32       0         0     307
## -Chris Bando         236     21   108  117    118       0         0     359
## -Chris Brown         273     24   113  121     80       1         1      73
## -Carmen Castillo     192     32   117  107     51       0         0      58
## -Chili Davis         715     77   352  342    289       1         1     303
## -Carlton Fisk       1767    281  1003  977    619       0         1     389
## -Curt Ford            59      2    32   32     27       1         0     109
## -Carney Lansford    1307    113   634  563    319       0         1      67
## -Chet Lemon         1429    166   747  666    526       0         0     316
## -Candy Maldonado     231     29    99  138     64       1         1     161
## -Carmelo Martinez    333     49   164  179    194       1         1     142
## -Craig Reynolds      968     35   409  321    170       1         1     106
## -Cal Ripken          927    133   529  472    313       0         0     240
## -Cory Snyder         113     24    58   69     16       0         0     203
## -Chris Speier       1634     98   698  661    777       1         0      53
## -Curt Wilkerson      270      1   116   64     57       0         1     125
## -Dave Anderson       210      9   118   69    114       1         1      73
## -Don Baylor         1982    315  1141 1179    727       0         0       0
## -Daryl Boston        120      8    57   40     39       0         1     152
## -Darnell Coles       205     22    99  103     78       0         0     107
## -Dave Concepcion    2198    100   950  909    690       1         1     153
## -Doug DeCinces      1397    221   712  815    548       0         1     119
## -Darrell Evans      1947    347  1175 1152   1380       0         0     808
## -Dwight Evans       1785    291  1082  949    989       0         0     280
## -Damaso Garcia      1046     32   461  301    112       0         0     224
## -Dan Gladden         353     16   196  110    117       1         1     226
## -Dave Henderson      555     80   285  274    186       0         1     182
## -Donnie Hill         290     11   123  108     55       0         1     104
## -Davey Lopes        1661    154  1019  608    820       1         0      51
## -Don Mattingly       737     93   349  401    171       0         0    1377
## -Dale Murphy        1388    266   813  822    617       1         1     303
## -Dwayne Murphy       948    145   575  528    635       0         1     276
## -Dave Parker        2024    247   978 1093    495       1         1     278
## -Dan Pasqua          113     25    61   70     63       0         0     148
## -Darrell Porter     1338    181   746  805    875       0         1     165
## -Dick Schofield      298     28   160  123    122       0         1     246
## -Don Slaught         405     28   156  159     76       0         1     533
## -Darryl Strawberry   471    108   292  343    267       1         0     226
## -Dale Sveum           78      7    35   35     32       0         0      45
## -Danny Tartabull     164     28    87  110     71       0         1     157
## -Denny Walling       594     41   287  294    227       1         1      59
## -Dave Winfield      2083    305  1135 1234    791       0         0     292
## -Eric Davis          184     45   156  119     99       1         1     274
## -Eddie Milner        544     38   335  174    258       1         1     292
## -Eddie Murray       1679    275   884 1015    709       0         0    1045
## -Ed Romero           336      7   166  122    106       0         0     102
## -Frank White        1583    131   743  693    300       0         1     316
## -George Bell         610     92   297  319    117       0         0     269
## -Glenn Braggs         51      4    19   18     11       0         0     116
## -George Brett       2095    209  1072 1050    695       0         1      97
## -Greg Brock          351     71   195  219    214       1         1     726
## -Gary Carter        1646    271   847  999    680       1         0     869
## -Glenn Davis         260     53   148  173     95       1         1    1253
## -Gary Gaetti         728    107   361  401    224       0         1     118
## -Greg Gagne          187     14   102   80     50       0         1     228
## -George Hendrick    1910    259   915 1067    546       0         1     144
## -Glenn Hubbard       866     59   429  365    410       1         1     282
## -Garth Iorg          568     16   216  208     93       0         0      91
## -Gary Matthews      1972    231  1070  955    921       1         0     137
## -Graig Nettles      2172    384  1172 1267   1057       1         1      83
## -Gary Pettis         369     12   247  126    198       0         1     462
## -Gary Redus          376     42   284  141    219       1         0     185
## -Garry Templeton    1578     44   703  519    256       1         1     207
## -Greg Walker         453     73   211  280    138       0         1     670
## -Gary Ward           900     92   444  419    240       0         1     237
## -Glenn Wilson        636     58   265  316    134       1         0     331
## -Harold Baines      1077    140   492  589    263       0         1     295
## -Hubie Brooks        822     55   313  377    187       1         0     116
## -Howard Johnson      299     40   145  154    128       1         0      50
## -Hal McRae          2081    190   935 1088    643       0         1       0
## -Harold Reynolds     129      1    72   31     48       0         1     278
## -Harry Spilman       151     16    80   97     61       1         1     138
## -Herm Winningham     125      7    58   37     47       1         0      97
## -Jesse Barfield      634    128   371  376    238       0         0     368
## -Juan Beniquez      1193     70   581  421    325       0         0     211
## -John Cangelosi      103      2    67   32     71       0         1     276
## -Jose Canseco        173     38   101  130     69       0         1     319
## -Joe Carter          404     57   210  222     68       0         0     241
## -Jack Clark         1213    194   702  705    625       1         0     623
## -Jose Cruz          2147    153   980 1032    854       1         1     237
## -Jody Davis          671     97   273  383    226       1         0     885
## -Jim Dwyer           543     56   304  268    298       0         0      33
## -Julio Franco        715     27   330  326    158       0         0     231
## -Jim Gantner        1066     40   450  367    241       0         0     304
## -Johnny Grubb       1130     97   544  462    551       0         0       0
## -Jack Howell          68      9    45   39     35       0         1      28
## -John Kruk            86      4    33   38     45       1         1     102
## -Jeffrey Leonard     808     81   379  428    221       1         1     158
## -Jim Morrison        730     97   302  351    174       1         0      92
## -John Moses          167      4    89   48     54       0         1     211
## -Jerry Mumphrey     1330     57   616  522    436       1         0     161
## -Jim Presley         377     65   181  227     82       0         1     110
## -Johnny Ray          880     32   366  337    218       1         0     280
## -Jeff Reed            44      2    18   10     18       0         1     332
## -Jim Rice           2163    351  1104 1289    564       0         0     330
## -Jerry Royster       979     33   518  324    382       1         1      87
## -John Russell        151     24    68   94     55       1         0     498
## -Juan Samuel         541     52   310  226     91       1         0     290
## -John Shelby         325     30   188  135     63       0         0     222
## -Joel Skinner        108      6    38   46     28       0         1     227
## -Jim Sundberg       1397     83   578  579    644       0         1     686
## -Jose Uribe          218      6    96   72     91       1         1     249
## -Joel Youngblood     890     74   419  382    304       1         1      49
## -Kevin Bass          462     40   219  195     82       1         1     303
## -Kal Daniels          58      6    34   23     22       1         1      88
## -Kirk Gibson         750    126   433  420    309       0         0     190
## -Ken Griffey        1839    121   983  707    600       0         0      96
## -Keith Hernandez    1840    128   969  900    917       1         0    1199
## -Kent Hrbek          815    117   405  474    319       0         1    1218
## -Ken Landreaux      1062     85   505  456    283       1         1     145
## -Kevin McReynolds    470     65   233  260    155       1         1     332
## -Kevin Mitchell       94     12    51   44     33       1         0     145
## -Keith Moreland      880     83   363  477    295       1         0     181
## -Ken Oberkfell       970     20   408  303    414       1         1      65
## -Ken Phelps          214     64   150  156    187       0         1       0
## -Kirby Puckett       587     35   262  201     91       0         1     429
## -Kurt Stillwell       64      0    31   26     30       1         1     107
## -Leon Durham         844    116   436  458    377       1         0    1231
## -Len Dykstra         187      9   117   64     88       1         0     283
## -Larry Herndon      1222     94   557  483    307       0         0     156
## -Lee Lacy           1240     84   615  430    340       0         0     239
## -Len Matuszek        191     30   113  119     87       1         1     235
## -Lloyd Moseby        928    102   513  471    351       0         0     371
## -Lance Parrish      1123    212   577  700    334       0         0     483
## -Larry Parrish      1552    210   740  840    452       0         1       0
## -Larry Sheets        185     36    88  112     50       0         0       0
## -Lou Whitaker       1320     93   724  522    576       0         0     276
## -Mike Aldrete         54      2    27   25     33       1         1     317
## -Marty Barrett       476     12   216  163    166       0         0     303
## -Mike Davis          549     62   300  263    153       0         1     310
## -Mike Diaz            58     12    24   37     19       1         0     201
## -Mariano Duncan      230     14   121   69     68       1         1     172
## -Mike Easler        1000    113   445  491    301       0         0       0
## -Mel Hall            398     47   210  203    136       0         0     233
## -Mike Heath          698     55   315  325    189       1         0     259
## -Mike Kingery         54      3    25   14     12       0         1     102
## -Mike LaValliere      76      3    20   36     45       1         0     468
## -Mike Marshall       516     90   247  288    161       1         1     149
## -Mike Pagliarulo     259     54   150  167    114       0         0     103
## -Mark Salas          170     17    80   75     36       0         1     358
## -Mike Schmidt          9      2     6    7      4       1         0      78
## -Mike Scioscia       519     26   181  199    288       1         1     756
## -Mickey Tettleton    116     14    59   55     78       0         1     463
## -Milt Thompson       160      8    71   33     44       1         0     212
## -Mitch Webster       232     19   132   83     79       1         0     325
## -Mookie Wilson       834     40   451  249    168       1         0     228
## -Marvell Wynne       408     16   198  120    113       1         1     203
## -Mike Young          323     54   181  177    157       0         0     149
## -Ozzie Guillen       271      3   129   80     24       0         1     261
## -Oddibe McDowell     249     36   168   91    101       0         1     325
## -Ozzie Smith        1169     13   583  374    528       1         0     229
## -Ozzie Virgil        359     61   176  202    175       1         1     682
## -Phil Bradley        470     38   245  167    174       0         1     250
## -Phil Garner        1543    104   751  714    535       1         1      58
## -Pete Incaviglia     135     30    82   88     55       0         1     157
## -Paul Molitor       1203     79   676  390    364       0         0      82
## -Pete Rose          4256    160  2165 1314   1566       1         1     523
## -Pat Sheridan        329     24   166  125    105       0         0     172
## -Pat Tabler          566     29   250  252    178       0         0     846
## -Rafael Belliard      82      0    41   32     26       1         0     117
## -Rick Burleson      1358     48   630  435    403       0         1      62
## -Randy Bush          344     43   178  192    136       0         1     167
## -Rick Cerone         665     43   266  304    198       0         0     391
## -Ron Cey            1845    312   965 1128    990       1         0      41
## -Rob Deer            142     44   102  109    102       0         0     286
## -Rick Dempsey        939     78   438  380    466       0         0     659
## -Ron Hassey          658     50   249  322    274       0         0     251
## -Rickey Henderson   1182    103   862  417    708       0         0     426
## -Reggie Jackson     2510    548  1509 1659   1342       0         1       0
## -Ron Kittle          408    115   238  299    157       0         1       0
## -Ray Knight         1102     67   410  497    284       1         0      88
## -Rick Leach          234     12   102   96     80       0         0      44
## -Rick Manning       1323     56   643  445    459       0         0     155
## -Rance Mulliniks     614     43   295  273    269       0         0      60
## -Ron Oester          895     39   377  284    296       1         1     367
## -Rey Quinones         68      2    32   22     24       0         0      86
## -Rafael Ramirez      882     36   365  280    165       1         1     155
## -Ronn Reynolds        49      3    16   13     14       1         0     190
## -Ron Roenicke        238     16   128  104    172       1         0     181
## -Ryne Sandberg       902     74   494  345    242       1         0     309
## -Rafael Santana      267      3    94   71     76       1         0     203
## -Rick Schu           170     17    98   54     62       1         0      42
## -Ruben Sierra        101     16    50   55     22       0         1     200
## -Roy Smalley        1369    155   713  660    735       0         1       0
## -Robby Thompson      149      7    73   47     42       1         1     255
## -Rob Wilfong         667     38   315  259    204       0         1     135
## -Robin Yount        2019    153  1043  827    535       0         0     352
## -Steve Balboni       412    100   204  276    155       0         1    1236
## -Scott Bradley        80      5    27   31     15       0         1     281
## -Sid Bream           185     22    93  106     86       1         0    1320
## -Steve Buechele      160     24    76   75     49       0         1     111
## -Shawon Dunston      210     21   106   86     40       1         0     320
## -Scott Fletcher      426     11   218  149    163       0         1     196
## -Steve Garvey       2583    271  1138 1299    478       1         1    1160
## -Steve Jeltz         148      1    68   56     99       1         0     229
## -Steve Lombardozzi   123      8    63   39     58       0         1     289
## -Spike Owen          403     12   211  146    155       0         1     209
## -Steve Sax           872     19   420  230    274       1         1     367
## -Tony Bernazard      841     61   450  342    373       0         0     351
## -Tom Brookens        657     48   324  300    179       0         0     106
## -Tom Brunansky       686    133   369  384    321       0         1     315
## -Tony Fernandez      448     15   196  137     89       0         0     294
## -Tim Flannery        493      9   207  162    198       1         1     209
## -Tom Foley           220      9    83   82     86       1         0      81
## -Tony Gwynn          770     27   352  230    193       1         1     337
## -Terry Harper        339     32   135  163    128       1         1      92
## -Tommy Herr          874     16   421  349    359       1         0     352
## -Tim Hulett          227     22   106   80     52       0         1      70
## -Terry Kennedy         4      1     2    3      1       1         1     692
## -Tito Landrum        219     12   105   99     71       1         0     131
## -Tim Laudner         256     42   129  139    106       0         1     299
## -Tom Paciorek       1145     83   488  491    244       0         1     178
## -Tony Pena           821     63   307  340    174       1         0     810
## -Terry Pendleton     357      7   149  161     87       1         0     133
## -Tony Phillips       397     17   226  149    191       0         1     160
## -Terry Puhl         1150     57   579  363    406       1         1      65
## -Ted Simmons        2402    242  1048 1348    819       1         1     167
## -Tim Teufel          355     31   180  148    158       1         0     133
## -Tim Wallach         771    110   338  406    239       1         0      94
## -Vince Coleman       309      1   201   69    110       1         0     300
## -Von Hayes           753     69   399  366    286       1         0    1182
## -Vance Law           566     41   279  257    246       1         0     170
## -Wally Backman       506      6   272  125    194       1         0     186
## -Wade Boggs          978     32   474  322    417       0         0     121
## -Will Clark          117     11    66   41     34       1         1     942
## -Wally Joyner        172     22    82  100     57       0         1    1222
## -Willie McGee        806     32   379  311    138       1         0     325
## -Willie Randolph    1511     39   897  451    875       0         0     313
## -Wayne Tolleson      433      7   217   93    146       0         1      37
## -Willie Upshaw       857     97   470  420    332       0         0    1314
## -Willie Wilson      1457     30   775  357    249       0         1     408
##                    Assists Errors NewLeagueN
## -Alan Ashby             43     10          1
## -Alvin Davis            82     14          0
## -Andre Dawson           11      3          1
## -Andres Galarraga       40      4          1
## -Alfredo Griffin       421     25          0
## -Al Newman             127      7          0
## -Argenis Salazar       283      9          0
## -Andres Thomas         290     19          1
## -Andre Thornton          0      0          0
## -Alan Trammell         445     22          0
## -Alex Trevino           45     11          1
## -Andy VanSlyke          11      7          1
## -Alan Wiggins          151      6          0
## -Bill Almon             45      8          1
## -Buddy Bell            290     10          1
## -Buddy Biancalana      177     16          0
## -Bruce Bochy            22      2          1
## -Barry Bonds             9      5          1
## -Bobby Bonilla          22      2          1
## -Bob Brenly             55      3          1
## -Bill Buckner          157     14          0
## -Brett Butler            9      3          0
## -Bob Dernier             3      3          1
## -Bo Diaz                83     13          1
## -Bill Doran            329     16          1
## -Brian Downing           5      3          0
## -Billy Hatcher           7      4          1
## -Brook Jacoby          292     25          0
## -Bob Kearney            46      5          0
## -Bill Madlock          170     24          1
## -Bob Melvin             59      6          1
## -BillyJo Robidoux       29      5          0
## -Bill Schroeder         25      1          0
## -Chris Bando            30      4          0
## -Chris Brown           177     18          1
## -Carmen Castillo         4      4          0
## -Chili Davis             9      9          1
## -Carlton Fisk           39      4          0
## -Curt Ford               7      3          1
## -Carney Lansford       147      4          0
## -Chet Lemon              6      5          0
## -Candy Maldonado        10      3          1
## -Carmelo Martinez       14      2          1
## -Craig Reynolds        206      7          1
## -Cal Ripken            482     13          0
## -Cory Snyder            70     10          0
## -Chris Speier           88      3          1
## -Curt Wilkerson        199     13          0
## -Dave Anderson         152     11          1
## -Don Baylor              0      0          0
## -Daryl Boston            3      5          0
## -Darnell Coles         242     23          0
## -Dave Concepcion       223     10          1
## -Doug DeCinces         216     12          0
## -Darrell Evans         108      2          0
## -Dwight Evans           10      5          0
## -Damaso Garcia         286      8          1
## -Dan Gladden             7      3          0
## -Dave Henderson          9      4          0
## -Donnie Hill           213      9          0
## -Davey Lopes            54      8          1
## -Don Mattingly         100      6          0
## -Dale Murphy             6      6          1
## -Dwayne Murphy           6      2          0
## -Dave Parker             9      9          1
## -Dan Pasqua              4      2          0
## -Darrell Porter          9      1          0
## -Dick Schofield        389     18          0
## -Don Slaught            40      4          0
## -Darryl Strawberry      10      6          1
## -Dale Sveum            122     26          0
## -Danny Tartabull         7      8          0
## -Denny Walling         156      9          1
## -Dave Winfield           9      5          0
## -Eric Davis              2      7          1
## -Eddie Milner            6      3          1
## -Eddie Murray           88     13          0
## -Ed Romero             132     10          0
## -Frank White           439     10          0
## -George Bell            17     10          0
## -Glenn Braggs            5     12          0
## -George Brett          218     16          0
## -Greg Brock             87      3          0
## -Gary Carter            62      8          1
## -Glenn Davis           111     11          1
## -Gary Gaetti           334     21          0
## -Greg Gagne            377     26          0
## -George Hendrick         6      5          0
## -Glenn Hubbard         487     19          1
## -Garth Iorg            185     12          0
## -Gary Matthews           5      9          1
## -Graig Nettles         174     16          1
## -Gary Pettis             9      7          0
## -Gary Redus              8      4          0
## -Garry Templeton       358     20          1
## -Greg Walker            57      5          0
## -Gary Ward               8      1          0
## -Glenn Wilson           20      4          1
## -Harold Baines          15      5          0
## -Hubie Brooks          222     15          1
## -Howard Johnson        136     20          1
## -Hal McRae               0      0          0
## -Harold Reynolds       415     16          0
## -Harry Spilman          15      1          1
## -Herm Winningham         2      2          1
## -Jesse Barfield         20      3          0
## -Juan Beniquez          56     13          0
## -John Cangelosi          7      9          1
## -Jose Canseco            4     14          0
## -Joe Carter              8      6          0
## -Jack Clark             35      3          1
## -Jose Cruz               5      4          1
## -Jody Davis            105      8          1
## -Jim Dwyer               3      0          0
## -Julio Franco          374     18          0
## -Jim Gantner           347     10          0
## -Johnny Grubb            0      0          0
## -Jack Howell            56      2          0
## -John Kruk               4      2          1
## -Jeffrey Leonard         4      5          1
## -Jim Morrison          257     20          1
## -John Moses              9      3          0
## -Jerry Mumphrey          3      3          1
## -Jim Presley           308     15          0
## -Johnny Ray            479      5          1
## -Jeff Reed              19      2          1
## -Jim Rice               16      8          0
## -Jerry Royster         166     14          0
## -John Russell           39     13          1
## -Juan Samuel           440     25          1
## -John Shelby             5      5          0
## -Joel Skinner           15      3          0
## -Jim Sundberg           46      4          1
## -Jose Uribe            444     16          1
## -Joel Youngblood         2      0          1
## -Kevin Bass             12      5          1
## -Kal Daniels             0      3          1
## -Kirk Gibson             2      2          0
## -Ken Griffey             5      3          1
## -Keith Hernandez       149      5          1
## -Kent Hrbek            104     10          0
## -Ken Landreaux           5      7          1
## -Kevin McReynolds        9      8          1
## -Kevin Mitchell         59      8          1
## -Keith Moreland         13      4          1
## -Ken Oberkfell         258      8          1
## -Ken Phelps              0      0          0
## -Kirby Puckett           8      6          0
## -Kurt Stillwell        205     16          1
## -Leon Durham            80      7          1
## -Len Dykstra             8      3          1
## -Larry Herndon           2      2          0
## -Lee Lacy                8      2          0
## -Len Matuszek           22      5          1
## -Lloyd Moseby            6      6          0
## -Lance Parrish          48      6          1
## -Larry Parrish           0      0          0
## -Larry Sheets            0      0          0
## -Lou Whitaker          421     11          0
## -Mike Aldrete           36      1          1
## -Marty Barrett         450     14          0
## -Mike Davis              9      9          0
## -Mike Diaz               6      3          1
## -Mariano Duncan        317     25          1
## -Mike Easler             0      0          1
## -Mel Hall                7      7          0
## -Mike Heath             30     10          0
## -Mike Kingery            6      3          0
## -Mike LaValliere        47      6          1
## -Mike Marshall           8      6          1
## -Mike Pagliarulo       283     19          0
## -Mark Salas             32      8          0
## -Mike Schmidt          220      6          1
## -Mike Scioscia          64     15          1
## -Mickey Tettleton       32      8          0
## -Milt Thompson           1      2          1
## -Mitch Webster          12      8          1
## -Mookie Wilson           7      5          1
## -Marvell Wynne           3      3          1
## -Mike Young              1      6          0
## -Ozzie Guillen         459     22          0
## -Oddibe McDowell        13      3          0
## -Ozzie Smith           453     15          1
## -Ozzie Virgil           93     13          1
## -Phil Bradley           11      1          0
## -Phil Garner           141     23          1
## -Pete Incaviglia         6     14          0
## -Paul Molitor          170     15          0
## -Pete Rose              43      6          1
## -Pat Sheridan            1      4          0
## -Pat Tabler             84      9          0
## -Rafael Belliard       269     12          1
## -Rick Burleson          90      3          0
## -Randy Bush              2      4          0
## -Rick Cerone            44      4          0
## -Ron Cey               118      8          0
## -Rob Deer                8      8          0
## -Rick Dempsey           53      7          0
## -Ron Hassey              9      4          0
## -Rickey Henderson        4      6          0
## -Reggie Jackson          0      0          0
## -Ron Kittle              0      0          0
## -Ray Knight            204     16          0
## -Rick Leach              0      1          0
## -Rick Manning            3      2          0
## -Rance Mulliniks       176      6          0
## -Ron Oester            475     19          1
## -Rey Quinones          150     15          0
## -Rafael Ramirez        371     29          1
## -Ronn Reynolds           2      9          1
## -Ron Roenicke            3      2          1
## -Ryne Sandberg         492      5          1
## -Rafael Santana        369     16          1
## -Rick Schu              94     13          1
## -Ruben Sierra            7      6          0
## -Roy Smalley             0      0          0
## -Robby Thompson        450     17          1
## -Rob Wilfong           257      7          0
## -Robin Yount             9      1          0
## -Steve Balboni          98     18          0
## -Scott Bradley          21      3          0
## -Sid Bream             166     17          1
## -Steve Buechele        226     11          0
## -Shawon Dunston        465     32          1
## -Scott Fletcher        354     15          0
## -Steve Garvey           53      7          1
## -Steve Jeltz           406     22          1
## -Steve Lombardozzi     407      6          0
## -Spike Owen            372     17          0
## -Steve Sax             432     16          1
## -Tony Bernazard        442     17          0
## -Tom Brookens          144      7          0
## -Tom Brunansky          10      6          0
## -Tony Fernandez        445     13          0
## -Tim Flannery          246      3          1
## -Tom Foley             147      4          1
## -Tony Gwynn             19      4          1
## -Terry Harper            5      3          0
## -Tommy Herr            414      9          1
## -Tim Hulett            144     11          0
## -Terry Kennedy          70      8          0
## -Tito Landrum            6      1          1
## -Tim Laudner            13      5          0
## -Tom Paciorek           45      4          0
## -Tony Pena              99     18          1
## -Terry Pendleton       371     20          1
## -Tony Phillips         290     11          0
## -Terry Puhl              0      0          1
## -Ted Simmons            18      6          1
## -Tim Teufel            173      9          1
## -Tim Wallach           270     16          1
## -Vince Coleman          12      9          1
## -Von Hayes              96     13          1
## -Vance Law             284      3          1
## -Wally Backman         290     17          1
## -Wade Boggs            267     19          0
## -Will Clark             72     11          1
## -Wally Joyner          139     15          0
## -Willie McGee            9      3          1
## -Willie Randolph       381     20          0
## -Wayne Tolleson        113      7          0
## -Willie Upshaw         131     12          0
## -Willie Wilson           4      3          0
## attr(,&quot;assign&quot;)
##  [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19
## attr(,&quot;contrasts&quot;)
## attr(,&quot;contrasts&quot;)$League
## [1] &quot;contr.treatment&quot;
## 
## attr(,&quot;contrasts&quot;)$Division
## [1] &quot;contr.treatment&quot;
## 
## attr(,&quot;contrasts&quot;)$NewLeague
## [1] &quot;contr.treatment&quot;</code></pre>
<p>As you can see from the output, the function also adds an extra column called <code>intercept</code> to the <strong>x</strong> object. Since our data frame does not contain any values for the intercept, R automatically adds a 1 across all rows and so we need to drop the column before proceeding (hence the reason for <code>[, -1]</code>). For our <strong>y</strong> object, we only need the values of the response variable <strong>Salary</strong>.</p>
<div class="sourceCode" id="cb711"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb711-1"><a href="#cb711-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary <span class="sc">~</span> ., Hitters)[, <span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb711-2"><a href="#cb711-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> Hitters<span class="sc">$</span>Salary</span></code></pre></div>
<p>Also, by default the <code>glmnet()</code> function performs ridge regression for an automatically selected range of <span class="math inline">\(\lambda\)</span> values. However, here we have chosen to implement the function over a grid of values ranging from <span class="math inline">\(\lambda=10^{10}\)</span> to <span class="math inline">\(\lambda=10^{-2}\)</span>, essentially covering the full range of scenarios from the null model containing only the intercept, to the least squares fit. We call this object <strong>grid</strong> and feed this range of values to the <code>glmnet()</code> argument for <code>lambda</code>. The <code>glmnet()</code> function also has an <code>alpha</code> argument that determines what type of model is fit. So, for a ridge regression model, we need to set <code>alpha</code> to <code>0</code>.</p>
<p>Also, tote that by default, the <code>glmnet()</code> function standardises the variables so that they are on the same scale (to override this, set the argument <code>standardize = FALSE</code>).</p>
<div class="sourceCode" id="cb712"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb712-1"><a href="#cb712-1" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="dv">10</span><span class="sc">^</span><span class="fu">seq</span>(<span class="dv">10</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="at">length =</span> <span class="dv">100</span>)</span>
<span id="cb712-2"><a href="#cb712-2" tabindex="-1"></a></span>
<span id="cb712-3"><a href="#cb712-3" tabindex="-1"></a>ridge.mod <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>, <span class="at">lambda =</span> grid)</span></code></pre></div>
<p>Associated with each value of <span class="math inline">\(\lambda\)</span> is a vector of ridge regression coefficients, stored in a matrix that can be accessed using <code>coef()</code>. In this case, it is a <span class="math inline">\(20 \times 100\)</span> matrix, with <span class="math inline">\(20\)</span> rows (one for each predictor, plus an intercept) and <span class="math inline">\(100\)</span> columns (one for each value of <span class="math inline">\(\lambda\)</span>).</p>
<div class="sourceCode" id="cb713"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb713-1"><a href="#cb713-1" tabindex="-1"></a><span class="fu">dim</span>(<span class="fu">coef</span>(ridge.mod))</span></code></pre></div>
<pre><code>## [1]  20 100</code></pre>
<p>The resulting <code>ridge.mod</code> object has various values associated with it such as lambda which can be accessed using the <span class="math inline">\(\$\)</span> sign. So for example, the 50th value for lambda (midway) is <span class="math inline">\(\lambda=11{,}498\)</span></p>
<div class="sourceCode" id="cb715"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb715-1"><a href="#cb715-1" tabindex="-1"></a>ridge.mod<span class="sc">$</span>lambda[<span class="dv">50</span>]</span></code></pre></div>
<pre><code>## [1] 11497.57</code></pre>
<p>We expect the coefficient estimates to be much smaller, in terms of <span class="math inline">\(\ell_2\)</span> norm, when a large value of <span class="math inline">\(\lambda\)</span> is used, as compared to when a small value of <span class="math inline">\(\lambda\)</span> is used. These are the coefficients when <span class="math inline">\(\lambda=11{,}498\)</span>, along with their <span class="math inline">\(\ell_2\)</span> norm:</p>
<div class="sourceCode" id="cb717"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb717-1"><a href="#cb717-1" tabindex="-1"></a><span class="fu">coef</span>(ridge.mod)[, <span class="dv">50</span>]</span></code></pre></div>
<pre><code>##   (Intercept)         AtBat          Hits         HmRun          Runs 
## 407.356050200   0.036957182   0.138180344   0.524629976   0.230701523 
##           RBI         Walks         Years        CAtBat         CHits 
##   0.239841459   0.289618741   1.107702929   0.003131815   0.011653637 
##        CHmRun         CRuns          CRBI        CWalks       LeagueN 
##   0.087545670   0.023379882   0.024138320   0.025015421   0.085028114 
##     DivisionW       PutOuts       Assists        Errors    NewLeagueN 
##  -6.215440973   0.016482577   0.002612988  -0.020502690   0.301433531</code></pre>
<div class="sourceCode" id="cb719"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb719-1"><a href="#cb719-1" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>(<span class="fu">coef</span>(ridge.mod)[<span class="sc">-</span><span class="dv">1</span>, <span class="dv">50</span>]<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 6.360612</code></pre>
<p>In contrast, for smaller values of <span class="math inline">\(\lambda\)</span>, the associated <span class="math inline">\(\ell_2\)</span> norm of the coefficients are much larger.</p>
<div class="sourceCode" id="cb721"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb721-1"><a href="#cb721-1" tabindex="-1"></a>ridge.mod<span class="sc">$</span>lambda[<span class="dv">60</span>]</span></code></pre></div>
<pre><code>## [1] 705.4802</code></pre>
<div class="sourceCode" id="cb723"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb723-1"><a href="#cb723-1" tabindex="-1"></a><span class="fu">coef</span>(ridge.mod)[, <span class="dv">60</span>]</span></code></pre></div>
<pre><code>##  (Intercept)        AtBat         Hits        HmRun         Runs          RBI 
##  54.32519950   0.11211115   0.65622409   1.17980910   0.93769713   0.84718546 
##        Walks        Years       CAtBat        CHits       CHmRun        CRuns 
##   1.31987948   2.59640425   0.01083413   0.04674557   0.33777318   0.09355528 
##         CRBI       CWalks      LeagueN    DivisionW      PutOuts      Assists 
##   0.09780402   0.07189612  13.68370191 -54.65877750   0.11852289   0.01606037 
##       Errors   NewLeagueN 
##  -0.70358655   8.61181213</code></pre>
<div class="sourceCode" id="cb725"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb725-1"><a href="#cb725-1" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">sum</span>(<span class="fu">coef</span>(ridge.mod)[<span class="sc">-</span><span class="dv">1</span>, <span class="dv">60</span>]<span class="sc">^</span><span class="dv">2</span>))</span></code></pre></div>
<pre><code>## [1] 57.11001</code></pre>
<p>We can also use the <code>predict()</code> function for a number of purposes. For instance, we can obtain the ridge regression coefficients for a new value of <span class="math inline">\(\lambda\)</span>, for example, <span class="math inline">\(50\)</span>.</p>
<div class="sourceCode" id="cb727"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb727-1"><a href="#cb727-1" tabindex="-1"></a><span class="fu">predict</span>(ridge.mod, <span class="at">s =</span> <span class="dv">50</span>, <span class="at">type =</span> <span class="st">&quot;coefficients&quot;</span>)</span></code></pre></div>
<pre><code>## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                        s1
## (Intercept)  4.876610e+01
## AtBat       -3.580999e-01
## Hits         1.969359e+00
## HmRun       -1.278248e+00
## Runs         1.145892e+00
## RBI          8.038292e-01
## Walks        2.716186e+00
## Years       -6.218319e+00
## CAtBat       5.447837e-03
## CHits        1.064895e-01
## CHmRun       6.244860e-01
## CRuns        2.214985e-01
## CRBI         2.186914e-01
## CWalks      -1.500245e-01
## LeagueN      4.592589e+01
## DivisionW   -1.182011e+02
## PutOuts      2.502322e-01
## Assists      1.215665e-01
## Errors      -3.278600e+00
## NewLeagueN  -9.496680e+00</code></pre>
<p>We now split the samples into a training set and a test set in order to estimate the test error of ridge regression. As you already know, there many ways to split datasets, one of which is to randomly choose a subset of numbers between <span class="math inline">\(1\)</span> and <span class="math inline">\(n\)</span>; these can then be used as the indices for the training observations.</p>
<p>We first set a random seed so that the results obtained will be reproducible.</p>
<div class="sourceCode" id="cb729"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb729-1"><a href="#cb729-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb729-2"><a href="#cb729-2" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(x), <span class="fu">nrow</span>(x) <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb729-3"><a href="#cb729-3" tabindex="-1"></a>test <span class="ot">&lt;-</span> (<span class="sc">-</span>train)</span>
<span id="cb729-4"><a href="#cb729-4" tabindex="-1"></a>y.test <span class="ot">&lt;-</span> y[test]</span></code></pre></div>
<p>Next we fit a ridge regression model on the training set.</p>
<div class="sourceCode" id="cb730"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb730-1"><a href="#cb730-1" tabindex="-1"></a>ridge.mod <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x[train, ], y[train], <span class="at">alpha =</span> <span class="dv">0</span>,</span>
<span id="cb730-2"><a href="#cb730-2" tabindex="-1"></a>                    <span class="at">lambda =</span> grid, <span class="at">thresh =</span> <span class="fl">1e-12</span>)</span></code></pre></div>
<p>And then evaluate its MSE on the test set, using <span class="math inline">\(\lambda = 4\)</span>. This time we get predictions for a test set, by replacing <code>type="coefficients"</code> with the <code>newx</code> argument. The test MSE is <span class="math inline">\(142{,}199\)</span></p>
<div class="sourceCode" id="cb731"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb731-1"><a href="#cb731-1" tabindex="-1"></a>ridge.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(ridge.mod, <span class="at">s =</span> <span class="dv">4</span>, <span class="at">newx =</span> x[test, ])</span>
<span id="cb731-2"><a href="#cb731-2" tabindex="-1"></a><span class="fu">mean</span>((ridge.pred <span class="sc">-</span> y.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 142199.2</code></pre>
<p>We now check whether there is any benefit to performing ridge regression with <span class="math inline">\(\lambda=4\)</span> instead of just performing least squares regression.</p>
<p>Recall that least squares is simply ridge regression with <span class="math inline">\(\lambda=0\)</span>. (In order for <code>glmnet()</code> to yield the exact least squares coefficients when <span class="math inline">\(\lambda=0\)</span>, we use the argument <code>exact = T</code> when calling the <code>predict()</code> function. Otherwise, the <code>predict()</code> function will interpolate over the grid of <span class="math inline">\(\lambda\)</span> values used in fitting the <code>glmnet()</code> model, yielding approximate results. When we use <code>exact = T</code>, there remains a slight discrepancy in the third decimal place between the output of <code>glmnet()</code> when <span class="math inline">\(\lambda = 0\)</span> and the output of <code>lm()</code>; this is due to numerical approximation on the part of <code>glmnet()</code>.</p>
<div class="sourceCode" id="cb733"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb733-1"><a href="#cb733-1" tabindex="-1"></a>ridge.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(ridge.mod, <span class="at">s =</span> <span class="dv">0</span>, <span class="at">newx =</span> x[test, ],</span>
<span id="cb733-2"><a href="#cb733-2" tabindex="-1"></a>                      <span class="at">exact =</span> T, <span class="at">x =</span> x[train, ], <span class="at">y =</span> y[train])</span>
<span id="cb733-3"><a href="#cb733-3" tabindex="-1"></a></span>
<span id="cb733-4"><a href="#cb733-4" tabindex="-1"></a><span class="fu">mean</span>((ridge.pred <span class="sc">-</span> y.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 168588.6</code></pre>
<p>We can see that the test MSE for the linear model is higher than for the ridge regression model.</p>
<p>Note that in general, if we want to fit a (unpenalized) least squares model, then we should use the <code>lm()</code> function, since that function provides more useful outputs, such as standard errors and p-values for the coefficients.</p>
<div class="sourceCode" id="cb735"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb735-1"><a href="#cb735-1" tabindex="-1"></a><span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">subset =</span> train)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x, subset = train)
## 
## Coefficients:
## (Intercept)       xAtBat        xHits       xHmRun        xRuns         xRBI  
##    274.0145      -0.3521      -1.6377       5.8145       1.5424       1.1243  
##      xWalks       xYears      xCAtBat       xCHits      xCHmRun       xCRuns  
##      3.7287     -16.3773      -0.6412       3.1632       3.4008      -0.9739  
##       xCRBI      xCWalks     xLeagueN   xDivisionW     xPutOuts     xAssists  
##     -0.6005       0.3379     119.1486    -144.0831       0.1976       0.6804  
##     xErrors  xNewLeagueN  
##     -4.7128     -71.0951</code></pre>
<div class="sourceCode" id="cb737"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb737-1"><a href="#cb737-1" tabindex="-1"></a><span class="fu">predict</span>(ridge.mod, <span class="at">s =</span> <span class="dv">0</span>, <span class="at">exact =</span> T, <span class="at">type =</span> <span class="st">&quot;coefficients&quot;</span>,</span>
<span id="cb737-2"><a href="#cb737-2" tabindex="-1"></a>    <span class="at">x =</span> x[train, ], <span class="at">y =</span> y[train])</span></code></pre></div>
<pre><code>## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                       s1
## (Intercept)  274.0200994
## AtBat         -0.3521900
## Hits          -1.6371383
## HmRun          5.8146692
## Runs           1.5423361
## RBI            1.1241837
## Walks          3.7288406
## Years        -16.3795195
## CAtBat        -0.6411235
## CHits          3.1629444
## CHmRun         3.4005281
## CRuns         -0.9739405
## CRBI          -0.6003976
## CWalks         0.3378422
## LeagueN      119.1434637
## DivisionW   -144.0853061
## PutOuts        0.1976300
## Assists        0.6804200
## Errors        -4.7127879
## NewLeagueN   -71.0898914</code></pre>
<p>Also, instead of arbitrarily choosing <span class="math inline">\(\lambda=4\)</span>, it would be better to use cross-validation to choose the tuning parameter <span class="math inline">\(\lambda\)</span>. We can do this using the built-in cross-validation function, <code>cv.glmnet()</code>. By default, the function performs ten-fold cross-validation, though this can be changed using the argument <code>nfolds</code>.</p>
<div class="sourceCode" id="cb739"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb739-1"><a href="#cb739-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb739-2"><a href="#cb739-2" tabindex="-1"></a>cv.out <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x[train, ], y[train], <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb739-3"><a href="#cb739-3" tabindex="-1"></a><span class="fu">plot</span>(cv.out)</span></code></pre></div>
<p><img src="06-S06-D2_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<div class="sourceCode" id="cb740"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb740-1"><a href="#cb740-1" tabindex="-1"></a>bestlam <span class="ot">&lt;-</span> cv.out<span class="sc">$</span>lambda.min</span>
<span id="cb740-2"><a href="#cb740-2" tabindex="-1"></a>bestlam</span></code></pre></div>
<pre><code>## [1] 326.0828</code></pre>
<p>Therefore, we see that the value of <span class="math inline">\(\lambda\)</span> that results in the smallest cross-validation error is <span class="math inline">\(326\)</span>.</p>
<p>Now let’s find out the test MSE associated with this value of <span class="math inline">\(\lambda\)</span>.</p>
<div class="sourceCode" id="cb742"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb742-1"><a href="#cb742-1" tabindex="-1"></a>ridge.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(ridge.mod, <span class="at">s =</span> bestlam,</span>
<span id="cb742-2"><a href="#cb742-2" tabindex="-1"></a>    <span class="at">newx =</span> x[test, ])</span>
<span id="cb742-3"><a href="#cb742-3" tabindex="-1"></a><span class="fu">mean</span>((ridge.pred <span class="sc">-</span> y.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 139856.6</code></pre>
<p>This represents a further improvement over the test MSE that we got using <span class="math inline">\(\lambda=4\)</span>.</p>
<p>Now we refit our ridge regression model on the full data set, using the value of <span class="math inline">\(\lambda\)</span> chosen by cross-validation, and examine the coefficient estimates, we can see that none of the coefficients are zero since ridge regression does not perform variable selection.</p>
<div class="sourceCode" id="cb744"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb744-1"><a href="#cb744-1" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb744-2"><a href="#cb744-2" tabindex="-1"></a><span class="fu">predict</span>(out, <span class="at">type =</span> <span class="st">&quot;coefficients&quot;</span>, <span class="at">s =</span> bestlam)</span></code></pre></div>
<pre><code>## 20 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                       s1
## (Intercept)  15.44383120
## AtBat         0.07715547
## Hits          0.85911582
## HmRun         0.60103106
## Runs          1.06369007
## RBI           0.87936105
## Walks         1.62444617
## Years         1.35254778
## CAtBat        0.01134999
## CHits         0.05746654
## CHmRun        0.40680157
## CRuns         0.11456224
## CRBI          0.12116504
## CWalks        0.05299202
## LeagueN      22.09143197
## DivisionW   -79.04032656
## PutOuts       0.16619903
## Assists       0.02941950
## Errors       -1.36092945
## NewLeagueN    9.12487765</code></pre>
</div>
<div id="the-lasso" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">The Lasso<a href="#the-lasso" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We saw that ridge regression with a suitable choice of <span class="math inline">\(\lambda\)</span> can outperform least squares.</p>
<p>Let’s now consider whether the lasso can yield either a more accurate or a more interpretable model than ridge regression. In order to fit a lasso model, we once again use the <code>glmnet()</code> function; however, this time we use the argument
<code>alpha = 1</code>.</p>
<div class="sourceCode" id="cb746"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb746-1"><a href="#cb746-1" tabindex="-1"></a>lasso.mod <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x[train, ], y[train], <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> grid)</span>
<span id="cb746-2"><a href="#cb746-2" tabindex="-1"></a><span class="fu">plot</span>(lasso.mod)</span></code></pre></div>
<pre><code>## Warning in regularize.values(x, y, ties, missing(ties), na.rm = na.rm):
## collapsing to unique &#39;x&#39; values</code></pre>
<p><img src="06-S06-D2_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>We can see from the coefficient plot that depending on the choice of tuning parameter, some of the coefficients will be exactly equal to zero. We now perform cross-validation and compute the associated test error.</p>
<div class="sourceCode" id="cb748"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb748-1"><a href="#cb748-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb748-2"><a href="#cb748-2" tabindex="-1"></a>cv.out <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x[train, ], y[train], <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb748-3"><a href="#cb748-3" tabindex="-1"></a><span class="fu">plot</span>(cv.out)</span></code></pre></div>
<p><img src="06-S06-D2_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<div class="sourceCode" id="cb749"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb749-1"><a href="#cb749-1" tabindex="-1"></a>bestlam <span class="ot">&lt;-</span> cv.out<span class="sc">$</span>lambda.min</span>
<span id="cb749-2"><a href="#cb749-2" tabindex="-1"></a>lasso.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(lasso.mod, <span class="at">s =</span> bestlam,</span>
<span id="cb749-3"><a href="#cb749-3" tabindex="-1"></a>    <span class="at">newx =</span> x[test, ])</span>
<span id="cb749-4"><a href="#cb749-4" tabindex="-1"></a><span class="fu">mean</span>((lasso.pred <span class="sc">-</span> y.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 143673.6</code></pre>
<p>This is lower than the test set MSE of the least squares model, but slightly larger than the test MSE for ridge regression with <span class="math inline">\(\lambda\)</span> chosen by cross-validation.</p>
<p>However, the lasso has a substantial advantage over ridge regression in that the resulting coefficient estimates are sparse. Here we see that 8 of the 19 coefficient estimates are exactly zero. So the lasso model with <span class="math inline">\(\lambda\)</span> chosen by cross-validation contains only eleven variables.</p>
<div class="sourceCode" id="cb751"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb751-1"><a href="#cb751-1" tabindex="-1"></a>out <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> grid)</span>
<span id="cb751-2"><a href="#cb751-2" tabindex="-1"></a>lasso.coef <span class="ot">&lt;-</span> <span class="fu">predict</span>(out, <span class="at">type =</span> <span class="st">&quot;coefficients&quot;</span>, <span class="at">s =</span> bestlam)[<span class="dv">1</span><span class="sc">:</span><span class="dv">20</span>, ]</span>
<span id="cb751-3"><a href="#cb751-3" tabindex="-1"></a></span>
<span id="cb751-4"><a href="#cb751-4" tabindex="-1"></a>lasso.coef</span></code></pre></div>
<pre><code>##   (Intercept)         AtBat          Hits         HmRun          Runs 
##    1.27479059   -0.05497143    2.18034583    0.00000000    0.00000000 
##           RBI         Walks         Years        CAtBat         CHits 
##    0.00000000    2.29192406   -0.33806109    0.00000000    0.00000000 
##        CHmRun         CRuns          CRBI        CWalks       LeagueN 
##    0.02825013    0.21628385    0.41712537    0.00000000   20.28615023 
##     DivisionW       PutOuts       Assists        Errors    NewLeagueN 
## -116.16755870    0.23752385    0.00000000   -0.85629148    0.00000000</code></pre>
<div class="sourceCode" id="cb753"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb753-1"><a href="#cb753-1" tabindex="-1"></a>lasso.coef[lasso.coef <span class="sc">!=</span> <span class="dv">0</span>]</span></code></pre></div>
<pre><code>##   (Intercept)         AtBat          Hits         Walks         Years 
##    1.27479059   -0.05497143    2.18034583    2.29192406   -0.33806109 
##        CHmRun         CRuns          CRBI       LeagueN     DivisionW 
##    0.02825013    0.21628385    0.41712537   20.28615023 -116.16755870 
##       PutOuts        Errors 
##    0.23752385   -0.85629148</code></pre>
<!--chapter:end:06-S06-D2.Rmd-->
</div>
</div>
<div id="demonstration-3-pcr-and-pls-regression" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Demonstration 3: PCR and PLS Regression<a href="#demonstration-3-pcr-and-pls-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="file">
<p>For the tasks below, you require the <strong>Hitters</strong> dataset from the <code>ISRL2</code> package.</p>
<p>You will also need the <code>pls</code> package; please make sure to install and load it before you begin the practical.</p>
</div>
<p>The <strong>Hitters</strong> dataset contains Major League Baseball Data from the 1986 and 1987 season. It is a dataframe with 322 observations and 20 variables. To learn more about the variables type <code>?Hitters</code> in your console.</p>
<p>The goal of this demonstration is to predict salary of major league players using ridge regression and lasso.</p>
<p>Loading the required packages:</p>
<div class="sourceCode" id="cb755"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb755-1"><a href="#cb755-1" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb755-2"><a href="#cb755-2" tabindex="-1"></a><span class="fu">library</span>(pls)</span></code></pre></div>
<p>Removing missing values:</p>
<div class="sourceCode" id="cb756"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb756-1"><a href="#cb756-1" tabindex="-1"></a><span class="fu">attach</span>(Hitters)</span>
<span id="cb756-2"><a href="#cb756-2" tabindex="-1"></a>Hitters <span class="ot">&lt;-</span> <span class="fu">na.omit</span>(Hitters)</span></code></pre></div>
<p>Preparing the data:</p>
<div class="sourceCode" id="cb757"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb757-1"><a href="#cb757-1" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(Salary <span class="sc">~</span> ., Hitters)[, <span class="sc">-</span><span class="dv">1</span>]</span>
<span id="cb757-2"><a href="#cb757-2" tabindex="-1"></a>y <span class="ot">&lt;-</span> Hitters<span class="sc">$</span>Salary</span></code></pre></div>
<div id="principal-components-regression" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Principal Components Regression<a href="#principal-components-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Principal components regression (PCR) can be performed using the <code>pcr()</code> function, which is part of the <code>pls</code> package.</p>
<p>In this demonstration, we continue using the <strong>Hitters</strong> dataset to predict salary of major league players. We first set a random seed so that the results obtained will be reproducible and then split the data into training and test sets.</p>
<div class="sourceCode" id="cb758"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb758-1"><a href="#cb758-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb758-2"><a href="#cb758-2" tabindex="-1"></a>train <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(x), <span class="fu">nrow</span>(x) <span class="sc">/</span> <span class="dv">2</span>)</span>
<span id="cb758-3"><a href="#cb758-3" tabindex="-1"></a>test <span class="ot">&lt;-</span> (<span class="sc">-</span>train)</span>
<span id="cb758-4"><a href="#cb758-4" tabindex="-1"></a>y.test <span class="ot">&lt;-</span> y[test]</span></code></pre></div>
<p>The syntax for the <code>pcr()</code> function is similar to that for <code>lm()</code>, with a few additional options. Setting <code>scale = TRUE</code> has the effect of <em>standardising</em> each predictor, prior to generating the principal components, so that the scale on which each variable is measured will not have an effect.
Setting <code>validation = "CV"</code> causes <code>pcr()</code> to compute the ten-fold cross-validation error for each possible value of <span class="math inline">\(M\)</span>, the number of principal components used.</p>
<div class="sourceCode" id="cb759"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb759-1"><a href="#cb759-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2</span>)</span>
<span id="cb759-2"><a href="#cb759-2" tabindex="-1"></a>pcr.fit <span class="ot">&lt;-</span> <span class="fu">pcr</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> Hitters, <span class="at">scale =</span> <span class="cn">TRUE</span>,</span>
<span id="cb759-3"><a href="#cb759-3" tabindex="-1"></a>               <span class="at">validation =</span> <span class="st">&quot;CV&quot;</span>)</span></code></pre></div>
<p>As usual, the results can be explored using <code>summary()</code>.</p>
<div class="sourceCode" id="cb760"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb760-1"><a href="#cb760-1" tabindex="-1"></a><span class="fu">summary</span>(pcr.fit)</span></code></pre></div>
<pre><code>## Data:    X dimension: 263 19 
##  Y dimension: 263 1
## Fit method: svdpc
## Number of components considered: 19
## 
## VALIDATION: RMSEP
## Cross-validated using 10 random segments.
##        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
## CV             452    351.9    353.2    355.0    352.8    348.4    343.6
## adjCV          452    351.6    352.7    354.4    352.1    347.6    342.7
##        7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
## CV       345.5    347.7    349.6     351.4     352.1     353.5     358.2
## adjCV    344.7    346.7    348.5     350.1     350.7     352.0     356.5
##        14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
## CV        349.7     349.4     339.9     341.6     339.2     339.6
## adjCV     348.0     347.7     338.2     339.7     337.2     337.6
## 
## TRAINING: % variance explained
##         1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps
## X         38.31    60.16    70.84    79.03    84.29    88.63    92.26    94.96
## Salary    40.63    41.58    42.17    43.22    44.90    46.48    46.69    46.75
##         9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps
## X         96.28     97.26     97.98     98.65     99.15     99.47     99.75
## Salary    46.86     47.76     47.82     47.85     48.10     50.40     50.55
##         16 comps  17 comps  18 comps  19 comps
## X          99.89     99.97     99.99    100.00
## Salary     53.01     53.85     54.61     54.61</code></pre>
<p>The CV score is provided for each possible number of components. Note that <code>pcr()</code> reports the <em>root mean squared error</em>; in order to obtain the usual MSE, we must square this quantity. For instance, a root mean squared error of <span class="math inline">\(352.8\)</span> corresponds to an MSE of <span class="math inline">\(352.8^2=124{,}468\)</span>.</p>
<p>We could also plot the cross-validation scores using the <code>validationplot()</code> function. Using <code>val.type = "MSEP"</code> will cause the cross-validation MSE to be plotted.</p>
<div class="sourceCode" id="cb762"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb762-1"><a href="#cb762-1" tabindex="-1"></a><span class="fu">validationplot</span>(pcr.fit, <span class="at">val.type =</span> <span class="st">&quot;MSEP&quot;</span>)</span></code></pre></div>
<p><img src="06-S06-D3_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>We see that the smallest cross-validation error occurs when <span class="math inline">\(M=18\)</span> components are used. This is barely fewer than <span class="math inline">\(M=19\)</span>, which amounts to simply performing least squares, because when all of the components are used in PCR no dimension reduction occurs. However, from the plot we also see that the cross-validation error is roughly the same when only one component is included in the model. This suggests that a model that uses just a small number of components might suffice.</p>
<p>The <code>summary()</code> function also provides the <em>percentage of variance explained</em> in the predictors and in the response using different numbers of components. We can think of this as the amount of information about the predictors or the response that is captured using <span class="math inline">\(M\)</span> principal components. For example, setting <span class="math inline">\(M=1\)</span> only captures <span class="math inline">\(38.31 \%\)</span> of all the variance, or information, in the predictors. In contrast, using <span class="math inline">\(M=5\)</span> increases the value to <span class="math inline">\(84.29 \%\)</span>. If we were to use all <span class="math inline">\(M=p=19\)</span> components, this would increase to <span class="math inline">\(100 \%\)</span>.</p>
<p>We now perform PCR on the training data and evaluate its test set performance.</p>
<div class="sourceCode" id="cb763"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb763-1"><a href="#cb763-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb763-2"><a href="#cb763-2" tabindex="-1"></a>pcr.fit <span class="ot">&lt;-</span> <span class="fu">pcr</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> Hitters, <span class="at">subset =</span> train,</span>
<span id="cb763-3"><a href="#cb763-3" tabindex="-1"></a>               <span class="at">scale =</span> <span class="cn">TRUE</span>, <span class="at">validation =</span> <span class="st">&quot;CV&quot;</span>)</span>
<span id="cb763-4"><a href="#cb763-4" tabindex="-1"></a><span class="fu">validationplot</span>(pcr.fit, <span class="at">val.type =</span> <span class="st">&quot;MSEP&quot;</span>)</span></code></pre></div>
<p><img src="06-S06-D3_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>Now we find that the lowest cross-validation error occurs when <span class="math inline">\(M=5\)</span> components are used. We compute the test MSE as follows.</p>
<div class="sourceCode" id="cb764"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb764-1"><a href="#cb764-1" tabindex="-1"></a>pcr.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(pcr.fit, x[test, ], <span class="at">ncomp =</span> <span class="dv">5</span>)</span>
<span id="cb764-2"><a href="#cb764-2" tabindex="-1"></a><span class="fu">mean</span>((pcr.pred <span class="sc">-</span> y.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 142811.8</code></pre>
<p>Finally, we fit PCR on the full data set, using <span class="math inline">\(M=5\)</span>, the number of components identified by cross-validation.</p>
<div class="sourceCode" id="cb766"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb766-1"><a href="#cb766-1" tabindex="-1"></a>pcr.fit <span class="ot">&lt;-</span> <span class="fu">pcr</span>(y <span class="sc">~</span> x, <span class="at">scale =</span> <span class="cn">TRUE</span>, <span class="at">ncomp =</span> <span class="dv">5</span>)</span>
<span id="cb766-2"><a href="#cb766-2" tabindex="-1"></a><span class="fu">summary</span>(pcr.fit)</span></code></pre></div>
<pre><code>## Data:    X dimension: 263 19 
##  Y dimension: 263 1
## Fit method: svdpc
## Number of components considered: 5
## TRAINING: % variance explained
##    1 comps  2 comps  3 comps  4 comps  5 comps
## X    38.31    60.16    70.84    79.03    84.29
## y    40.63    41.58    42.17    43.22    44.90</code></pre>
<p>The test set MSE is competitive with the results obtained using ridge regression and the lasso. However, as a result of the way PCR is implemented, the final model is more difficult to interpret because it does not perform any kind of variable selection or even directly produce coefficient estimates.</p>
</div>
<div id="partial-least-squares" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Partial Least Squares<a href="#partial-least-squares" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We implement partial least squares (PLS) using the <code>plsr()</code> function, also in the <code>pls</code> library. The syntax is identical to that of the <code>pcr()</code> function.</p>
<div class="sourceCode" id="cb768"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb768-1"><a href="#cb768-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb768-2"><a href="#cb768-2" tabindex="-1"></a>pls.fit <span class="ot">&lt;-</span> <span class="fu">plsr</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> Hitters, <span class="at">subset =</span> train, </span>
<span id="cb768-3"><a href="#cb768-3" tabindex="-1"></a>                <span class="at">scale =</span> <span class="cn">TRUE</span>, <span class="at">validation =</span> <span class="st">&quot;CV&quot;</span>)</span>
<span id="cb768-4"><a href="#cb768-4" tabindex="-1"></a><span class="fu">summary</span>(pls.fit)</span></code></pre></div>
<pre><code>## Data:    X dimension: 131 19 
##  Y dimension: 131 1
## Fit method: kernelpls
## Number of components considered: 19
## 
## VALIDATION: RMSEP
## Cross-validated using 10 random segments.
##        (Intercept)  1 comps  2 comps  3 comps  4 comps  5 comps  6 comps
## CV           428.3    325.5    329.9    328.8    339.0    338.9    340.1
## adjCV        428.3    325.0    328.2    327.2    336.6    336.1    336.6
##        7 comps  8 comps  9 comps  10 comps  11 comps  12 comps  13 comps
## CV       339.0    347.1    346.4     343.4     341.5     345.4     356.4
## adjCV    336.2    343.4    342.8     340.2     338.3     341.8     351.1
##        14 comps  15 comps  16 comps  17 comps  18 comps  19 comps
## CV        348.4     349.1     350.0     344.2     344.5     345.0
## adjCV     344.2     345.0     345.9     340.4     340.6     341.1
## 
## TRAINING: % variance explained
##         1 comps  2 comps  3 comps  4 comps  5 comps  6 comps  7 comps  8 comps
## X         39.13    48.80    60.09    75.07    78.58    81.12    88.21    90.71
## Salary    46.36    50.72    52.23    53.03    54.07    54.77    55.05    55.66
##         9 comps  10 comps  11 comps  12 comps  13 comps  14 comps  15 comps
## X         93.17     96.05     97.08     97.61     97.97     98.70     99.12
## Salary    55.95     56.12     56.47     56.68     57.37     57.76     58.08
##         16 comps  17 comps  18 comps  19 comps
## X          99.61     99.70     99.95    100.00
## Salary     58.17     58.49     58.56     58.62</code></pre>
<div class="sourceCode" id="cb770"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb770-1"><a href="#cb770-1" tabindex="-1"></a><span class="fu">validationplot</span>(pls.fit, <span class="at">val.type =</span> <span class="st">&quot;MSEP&quot;</span>)</span></code></pre></div>
<p><img src="06-S06-D3_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>The lowest cross-validation error occurs when only <span class="math inline">\(M=1\)</span> partial least squares directions are used. We now evaluate the corresponding test set MSE.</p>
<div class="sourceCode" id="cb771"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb771-1"><a href="#cb771-1" tabindex="-1"></a>pls.pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(pls.fit, x[test, ], <span class="at">ncomp =</span> <span class="dv">1</span>)</span>
<span id="cb771-2"><a href="#cb771-2" tabindex="-1"></a><span class="fu">mean</span>((pls.pred <span class="sc">-</span> y.test)<span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 151995.3</code></pre>
<p>The test MSE is comparable to, but slightly higher than, the test MSE obtained using ridge regression, the lasso, and PCR.</p>
<p>Finally, we perform PLS using the full data set, using <span class="math inline">\(M=1\)</span>, the number of components identified by cross-validation.</p>
<div class="sourceCode" id="cb773"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb773-1"><a href="#cb773-1" tabindex="-1"></a>pls.fit <span class="ot">&lt;-</span> <span class="fu">plsr</span>(Salary <span class="sc">~</span> ., <span class="at">data =</span> Hitters, <span class="at">scale =</span> <span class="cn">TRUE</span>,</span>
<span id="cb773-2"><a href="#cb773-2" tabindex="-1"></a>    <span class="at">ncomp =</span> <span class="dv">1</span>)</span>
<span id="cb773-3"><a href="#cb773-3" tabindex="-1"></a><span class="fu">summary</span>(pls.fit)</span></code></pre></div>
<pre><code>## Data:    X dimension: 263 19 
##  Y dimension: 263 1
## Fit method: kernelpls
## Number of components considered: 1
## TRAINING: % variance explained
##         1 comps
## X         38.08
## Salary    43.05</code></pre>
<p>Notice that the percentage of variance in <strong>Salary</strong> that the one-component PLS fit explains, <span class="math inline">\(43.05 \%\)</span>, is almost as much as that explained using the final five-component model PCR fit, <span class="math inline">\(44.90 \%\)</span>. This is because PCR only attempts to maximise the amount of variance explained in the predictors, while PLS searches for directions that explain variance in both the predictors and the response.</p>
<!--chapter:end:06-S06-D3.Rmd-->
</div>
</div>
<div id="demonstration-4-beyond-linearity" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Demonstration 4: Beyond Linearity<a href="#demonstration-4-beyond-linearity" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="file">
<p>For the tasks below, you require the <strong>Wage</strong> dataset from the <code>ISRL2</code> package.</p>
<p>You will also need the <code>akima</code> and <code>gam</code> packages; please make sure to install and load it before you begin the practical.</p>
</div>
<p>The <strong>Wage</strong> dataset contains wage and other data for a group of 3000 male workers in the Mid-Atlantic region. It is a data frame with 3,000 observations on 11 variables. To learn more about the variables, type <code>?Wage</code> in your console.</p>
<p>The goal of this demonstration is to fit models that predict wage to exemplify some of the ways in which non-linearity can be addressed.</p>
<p>Loading the required packages:</p>
<div class="sourceCode" id="cb775"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb775-1"><a href="#cb775-1" tabindex="-1"></a><span class="fu">library</span>(ISLR2)</span>
<span id="cb775-2"><a href="#cb775-2" tabindex="-1"></a><span class="fu">library</span>(akima)</span>
<span id="cb775-3"><a href="#cb775-3" tabindex="-1"></a><span class="fu">library</span>(gam)</span>
<span id="cb775-4"><a href="#cb775-4" tabindex="-1"></a><span class="fu">library</span>(splines)</span></code></pre></div>
<div id="polynomial-regression-and-step-functions" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Polynomial Regression and Step Functions<a href="#polynomial-regression-and-step-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let’s first consider polynomials by fitting a linear model predict wage using age up to the fourth degree polynomial. By using the <code>poly()</code> command, we can avoid having to type a formula with powers of <code>age</code> up to the fourth. The function returns a matrix whose columns are a basis of <em>orthogonal polynomials</em>, which essentially means that each column is a linear combination of the variables <code>age</code>, <code>age^2</code>, <code>age^3</code> and <code>age^4</code>.</p>
<div class="sourceCode" id="cb776"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb776-1"><a href="#cb776-1" tabindex="-1"></a><span class="fu">attach</span>(Wage)</span>
<span id="cb776-2"><a href="#cb776-2" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">poly</span>(age, <span class="dv">4</span>), <span class="at">data =</span> Wage)</span>
<span id="cb776-3"><a href="#cb776-3" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(fit))</span></code></pre></div>
<pre><code>##                 Estimate Std. Error    t value     Pr(&gt;|t|)
## (Intercept)    111.70361  0.7287409 153.283015 0.000000e+00
## poly(age, 4)1  447.06785 39.9147851  11.200558 1.484604e-28
## poly(age, 4)2 -478.31581 39.9147851 -11.983424 2.355831e-32
## poly(age, 4)3  125.52169 39.9147851   3.144742 1.678622e-03
## poly(age, 4)4  -77.91118 39.9147851  -1.951938 5.103865e-02</code></pre>
<p>Alternatively, we can set the <code>raw</code> argument to <code>TRUE</code> if we want to obtain <code>age</code>, <code>age^2</code>, <code>age^3</code> and <code>age^4</code> directly.</p>
<div class="sourceCode" id="cb778"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb778-1"><a href="#cb778-1" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">poly</span>(age, <span class="dv">4</span>, <span class="at">raw =</span> T), <span class="at">data =</span> Wage)</span>
<span id="cb778-2"><a href="#cb778-2" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(fit2))</span></code></pre></div>
<pre><code>##                             Estimate   Std. Error   t value     Pr(&gt;|t|)
## (Intercept)            -1.841542e+02 6.004038e+01 -3.067172 0.0021802539
## poly(age, 4, raw = T)1  2.124552e+01 5.886748e+00  3.609042 0.0003123618
## poly(age, 4, raw = T)2 -5.638593e-01 2.061083e-01 -2.735743 0.0062606446
## poly(age, 4, raw = T)3  6.810688e-03 3.065931e-03  2.221409 0.0263977518
## poly(age, 4, raw = T)4 -3.203830e-05 1.641359e-05 -1.951938 0.0510386498</code></pre>
<p>Either approach is acceptable since the choice does not affect the fitted values (although of course it will affect the coefficient estimates).</p>
<p>There are several other equivalent ways of fitting this model such as using the wrapper function <code>I()</code>:</p>
<div class="sourceCode" id="cb780"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb780-1"><a href="#cb780-1" tabindex="-1"></a>fit2a <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> age <span class="sc">+</span> <span class="fu">I</span>(age<span class="sc">^</span><span class="dv">2</span>) <span class="sc">+</span> <span class="fu">I</span>(age<span class="sc">^</span><span class="dv">3</span>) <span class="sc">+</span> <span class="fu">I</span>(age<span class="sc">^</span><span class="dv">4</span>),</span>
<span id="cb780-2"><a href="#cb780-2" tabindex="-1"></a>            <span class="at">data =</span> Wage)</span>
<span id="cb780-3"><a href="#cb780-3" tabindex="-1"></a><span class="fu">coef</span>(fit2a)</span></code></pre></div>
<pre><code>##   (Intercept)           age      I(age^2)      I(age^3)      I(age^4) 
## -1.841542e+02  2.124552e+01 -5.638593e-01  6.810688e-03 -3.203830e-05</code></pre>
<p>Or <code>cbind()</code> , to build a matrix from a collection of vectors:</p>
<div class="sourceCode" id="cb782"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb782-1"><a href="#cb782-1" tabindex="-1"></a>fit2b <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">cbind</span>(age, age<span class="sc">^</span><span class="dv">2</span>, age<span class="sc">^</span><span class="dv">3</span>, age<span class="sc">^</span><span class="dv">4</span>), </span>
<span id="cb782-2"><a href="#cb782-2" tabindex="-1"></a>            <span class="at">data =</span> Wage)</span></code></pre></div>
<p>We now create a grid of values for <code>age</code> at which we want predictions (all ages in the dataset), and then call the generic <code>predict()</code> function, specifying that we want standard errors as well by setting <code>se = TRUE</code>.</p>
<div class="sourceCode" id="cb783"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb783-1"><a href="#cb783-1" tabindex="-1"></a><span class="co">#obtain range of values for the age variable</span></span>
<span id="cb783-2"><a href="#cb783-2" tabindex="-1"></a>agelims <span class="ot">&lt;-</span> <span class="fu">range</span>(age)</span>
<span id="cb783-3"><a href="#cb783-3" tabindex="-1"></a></span>
<span id="cb783-4"><a href="#cb783-4" tabindex="-1"></a><span class="co">#generate sequence of values for age</span></span>
<span id="cb783-5"><a href="#cb783-5" tabindex="-1"></a>age.grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> agelims[<span class="dv">1</span>], <span class="at">to =</span> agelims[<span class="dv">2</span>])</span>
<span id="cb783-6"><a href="#cb783-6" tabindex="-1"></a></span>
<span id="cb783-7"><a href="#cb783-7" tabindex="-1"></a><span class="co"># obtaining predictions with SE</span></span>
<span id="cb783-8"><a href="#cb783-8" tabindex="-1"></a>preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, <span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">age =</span> age.grid),</span>
<span id="cb783-9"><a href="#cb783-9" tabindex="-1"></a>                 <span class="at">se =</span> <span class="cn">TRUE</span>)</span>
<span id="cb783-10"><a href="#cb783-10" tabindex="-1"></a></span>
<span id="cb783-11"><a href="#cb783-11" tabindex="-1"></a><span class="co"># upper and lower bounds</span></span>
<span id="cb783-12"><a href="#cb783-12" tabindex="-1"></a>se.bands <span class="ot">&lt;-</span> <span class="fu">cbind</span>(preds<span class="sc">$</span>fit <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> preds<span class="sc">$</span>se.fit,</span>
<span id="cb783-13"><a href="#cb783-13" tabindex="-1"></a>                  preds<span class="sc">$</span>fit <span class="sc">-</span> <span class="dv">2</span> <span class="sc">*</span> preds<span class="sc">$</span>se.fit)</span></code></pre></div>
<p>Finally, we plot the data and add the fit from the degree-4 polynomial. By plotting the data, we can visualise the model fit together with the associated uncertainty.</p>
<div class="sourceCode" id="cb784"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb784-1"><a href="#cb784-1" tabindex="-1"></a><span class="co">#creating the plot</span></span>
<span id="cb784-2"><a href="#cb784-2" tabindex="-1"></a><span class="fu">plot</span>(age, wage, <span class="at">xlim =</span> agelims, <span class="at">cex =</span> .<span class="dv">5</span>, <span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>)</span>
<span id="cb784-3"><a href="#cb784-3" tabindex="-1"></a></span>
<span id="cb784-4"><a href="#cb784-4" tabindex="-1"></a><span class="co">#adding a title</span></span>
<span id="cb784-5"><a href="#cb784-5" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;Degree-4 Polynomial&quot;</span>, <span class="at">outer =</span> T)</span>
<span id="cb784-6"><a href="#cb784-6" tabindex="-1"></a></span>
<span id="cb784-7"><a href="#cb784-7" tabindex="-1"></a><span class="co">#adding fitted line</span></span>
<span id="cb784-8"><a href="#cb784-8" tabindex="-1"></a><span class="fu">lines</span>(age.grid, preds<span class="sc">$</span>fit, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb784-9"><a href="#cb784-9" tabindex="-1"></a></span>
<span id="cb784-10"><a href="#cb784-10" tabindex="-1"></a><span class="co">#adding CIs</span></span>
<span id="cb784-11"><a href="#cb784-11" tabindex="-1"></a><span class="fu">matlines</span>(age.grid, se.bands, <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lty =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="06-S06-D4_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Ok, so how exactly do we decide which polynomial degree is suitable to fit a model that explains the relationship between <strong>wage</strong> and <strong>age</strong>?</p>
<p>One way is to use ANOVA (F-test). So for example, we can fit models ranging from linear (1st degree polynomial) to a fifth degree polynomial and use ANOVA to determine the simplest model which is sufficient to explain the relationship between the two variables.</p>
<p>Our null hypothesis is that a model <span class="math inline">\(M_1\)</span> is sufficient to explain the data against the alternative hypothesis that a more complex model <span class="math inline">\(M_2\)</span> is required. In order to use the <code>anova()</code> function, remember that <span class="math inline">\(M_1\)</span> and <span class="math inline">\(M_2\)</span> must be <em>nested</em> models. In other words, the predictors in <span class="math inline">\(M_1\)</span> must be a subset of the predictors in <span class="math inline">\(M_2\)</span>.</p>
<p>Therefore, we fit five different sequentially (simplest to most complex)</p>
<div class="sourceCode" id="cb785"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb785-1"><a href="#cb785-1" tabindex="-1"></a>fit<span class="fl">.1</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> age, <span class="at">data =</span> Wage)</span>
<span id="cb785-2"><a href="#cb785-2" tabindex="-1"></a>fit<span class="fl">.2</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">poly</span>(age, <span class="dv">2</span>), <span class="at">data =</span> Wage)</span>
<span id="cb785-3"><a href="#cb785-3" tabindex="-1"></a>fit<span class="fl">.3</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">poly</span>(age, <span class="dv">3</span>), <span class="at">data =</span> Wage)</span>
<span id="cb785-4"><a href="#cb785-4" tabindex="-1"></a>fit<span class="fl">.4</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">poly</span>(age, <span class="dv">4</span>), <span class="at">data =</span> Wage)</span>
<span id="cb785-5"><a href="#cb785-5" tabindex="-1"></a>fit<span class="fl">.5</span> <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">poly</span>(age, <span class="dv">5</span>), <span class="at">data =</span> Wage)</span>
<span id="cb785-6"><a href="#cb785-6" tabindex="-1"></a></span>
<span id="cb785-7"><a href="#cb785-7" tabindex="-1"></a><span class="fu">anova</span>(fit<span class="fl">.1</span>, fit<span class="fl">.2</span>, fit<span class="fl">.3</span>, fit<span class="fl">.4</span>, fit<span class="fl">.5</span>)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: wage ~ age
## Model 2: wage ~ poly(age, 2)
## Model 3: wage ~ poly(age, 3)
## Model 4: wage ~ poly(age, 4)
## Model 5: wage ~ poly(age, 5)
##   Res.Df     RSS Df Sum of Sq        F    Pr(&gt;F)    
## 1   2998 5022216                                    
## 2   2997 4793430  1    228786 143.5931 &lt; 2.2e-16 ***
## 3   2996 4777674  1     15756   9.8888  0.001679 ** 
## 4   2995 4771604  1      6070   3.8098  0.051046 .  
## 5   2994 4770322  1      1283   0.8050  0.369682    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The results show that Model 2 (2nd degree polynomial) and model 3 (3rd degree polynomial) are statistically significant which suggest that a linear model is insufficient. Since the p-values for Models 4 and 5 are large, there is evidence to suggest that higher order models are not justified to explain the relationship between age and wage.</p>
<p><em>As an alternative to using hypothesis tests and ANOVA, we could choose the polynomial degree using cross-validation.</em></p>
<p>Now let’s consider a classification problem and predict whether an individual earns more than <span class="math inline">\(\$250{,}000\)</span> per year or not using a logistic regression model.</p>
<p>We proceed much as before, except that first we create the appropriate response vector, and then apply the <code>glm()</code> function using <code>family = "binomial"</code> in order to fit a polynomial logistic regression model. The expression <code>wage &gt; 250</code> evaluates to a logical variable containing <code>TRUE</code>s and <code>FALSE</code>s, which <code>glm()</code> coerces to binary by setting the <code>TRUE</code>s to 1 and the <code>FALSE</code>s to 0.</p>
<div class="sourceCode" id="cb787"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb787-1"><a href="#cb787-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(<span class="fu">I</span>(wage <span class="sc">&gt;</span> <span class="dv">250</span>) <span class="sc">~</span> <span class="fu">poly</span>(age, <span class="dv">4</span>), <span class="at">data =</span> Wage,</span>
<span id="cb787-2"><a href="#cb787-2" tabindex="-1"></a>           <span class="at">family =</span> binomial)</span></code></pre></div>
<p>As before, we obtain the predictions using the <code>predict()</code> function and make use of the <strong>age.grid</strong> object we created earlier. Since we also want the standard errors, we set <code>se</code> to <code>TRUE</code>.</p>
<div class="sourceCode" id="cb788"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb788-1"><a href="#cb788-1" tabindex="-1"></a>preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, <span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">age =</span> age.grid), <span class="at">se =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>However, calculating the confidence intervals is slightly more complex than in the linear regression case. The default prediction type for a <code>glm()</code> model is <code>type = "link"</code> and so we obtain predictions for the <em>logit</em>: that is, we have fit a model of the form:
<span class="math display">\[
\log\left(\frac{\Pr(Y=1|X)}{1-\Pr(Y=1|X)}\right)=X\beta,
\]</span>
and the predictions given are of the form <span class="math inline">\(X\hat\beta\)</span>. The standard errors given are also for <span class="math inline">\(X \hat\beta\)</span>. In order to obtain confidence intervals for <span class="math inline">\(\Pr(Y=1|X)\)</span>, we use the transformation:
<span class="math display">\[
\Pr(Y=1|X)=\frac{\exp(X\beta)}{1+\exp(X\beta)}.
\]</span></p>
<div class="sourceCode" id="cb789"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb789-1"><a href="#cb789-1" tabindex="-1"></a>pfit <span class="ot">&lt;-</span> <span class="fu">exp</span>(preds<span class="sc">$</span>fit) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(preds<span class="sc">$</span>fit))</span>
<span id="cb789-2"><a href="#cb789-2" tabindex="-1"></a>se.bands.logit <span class="ot">&lt;-</span> <span class="fu">cbind</span>(preds<span class="sc">$</span>fit <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> preds<span class="sc">$</span>se.fit,</span>
<span id="cb789-3"><a href="#cb789-3" tabindex="-1"></a>                        preds<span class="sc">$</span>fit <span class="sc">-</span> <span class="dv">2</span> <span class="sc">*</span> preds<span class="sc">$</span>se.fit)</span>
<span id="cb789-4"><a href="#cb789-4" tabindex="-1"></a>se.bands <span class="ot">&lt;-</span> <span class="fu">exp</span>(se.bands.logit) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fu">exp</span>(se.bands.logit))</span></code></pre></div>
<p>Note that we could have directly computed the probabilities by selecting the <code>type = "response"</code> option in the <code>predict()</code> function.</p>
<div class="sourceCode" id="cb790"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb790-1"><a href="#cb790-1" tabindex="-1"></a>preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, <span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">age =</span> age.grid),</span>
<span id="cb790-2"><a href="#cb790-2" tabindex="-1"></a>                 <span class="at">type =</span> <span class="st">&quot;response&quot;</span>, <span class="at">se =</span> T)</span></code></pre></div>
<p>However, the corresponding confidence intervals would not have been sensible because we would end up with negative probabilities.</p>
<p>Now let’s create a <em>rug plot</em>. The <code>age</code> values corresponding to the observations with <code>wage</code> values above <span class="math inline">\(250\)</span> are denoted as gray marks on the top of the plot, and those with <code>wage</code> values below <span class="math inline">\(250\)</span> are shown as gray marks on the bottom of the plot. We also use the <code>jitter()</code> function to jitter the <code>age</code> values slightly to avoid overplotting.</p>
<div class="sourceCode" id="cb791"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb791-1"><a href="#cb791-1" tabindex="-1"></a><span class="fu">plot</span>(age, <span class="fu">I</span>(wage <span class="sc">&gt;</span> <span class="dv">250</span>), <span class="at">xlim =</span> agelims, <span class="at">type =</span> <span class="st">&quot;n&quot;</span>,</span>
<span id="cb791-2"><a href="#cb791-2" tabindex="-1"></a>     <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, .<span class="dv">2</span>))</span>
<span id="cb791-3"><a href="#cb791-3" tabindex="-1"></a><span class="fu">points</span>(<span class="fu">jitter</span>(age), <span class="fu">I</span>((wage <span class="sc">&gt;</span> <span class="dv">250</span>) <span class="sc">/</span> <span class="dv">5</span>), <span class="at">cex =</span> .<span class="dv">5</span>, <span class="at">pch =</span> <span class="st">&quot;|&quot;</span>, <span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>)</span>
<span id="cb791-4"><a href="#cb791-4" tabindex="-1"></a><span class="fu">lines</span>(age.grid, pfit, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span>
<span id="cb791-5"><a href="#cb791-5" tabindex="-1"></a><span class="fu">matlines</span>(age.grid, se.bands, <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lty =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="06-S06-D4_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
</div>
<div id="step-functions" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Step Functions<a href="#step-functions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In order to fit a step function, we use the <code>cut()</code> function.</p>
<div class="sourceCode" id="cb792"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb792-1"><a href="#cb792-1" tabindex="-1"></a><span class="fu">table</span>(<span class="fu">cut</span>(age, <span class="dv">4</span>))</span></code></pre></div>
<pre><code>## 
## (17.9,33.5]   (33.5,49]   (49,64.5] (64.5,80.1] 
##         750        1399         779          72</code></pre>
<p>Here <code>cut()</code> automatically picked the cutpoints at <span class="math inline">\(33.5\)</span>, <span class="math inline">\(49\)</span>, and <span class="math inline">\(64.5\)</span> years of age. We could also have specified our own cutpoints directly using the <code>breaks</code> option. The function <code>cut()</code> returns an ordered categorical variable.</p>
<p>Therefore, if we use the function directly within <code>lm()</code> then it creates a set of dummy variables. Since <code>age &lt; 33.5</code> category is left out, the intercept coefficient of <span class="math inline">\(\$94{,}160\)</span> can be interpreted as the average salary for those under <span class="math inline">\(33.5\)</span> years of age, and the other coefficients can be interpreted as the average additional salary for those in the other age groups.</p>
<div class="sourceCode" id="cb794"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb794-1"><a href="#cb794-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">cut</span>(age, <span class="dv">4</span>), <span class="at">data =</span> Wage)</span>
<span id="cb794-2"><a href="#cb794-2" tabindex="-1"></a></span>
<span id="cb794-3"><a href="#cb794-3" tabindex="-1"></a>fit</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = wage ~ cut(age, 4), data = Wage)
## 
## Coefficients:
##            (Intercept)    cut(age, 4)(33.5,49]    cut(age, 4)(49,64.5]  
##                 94.158                  24.053                  23.665  
## cut(age, 4)(64.5,80.1]  
##                  7.641</code></pre>
<p>We can also produce predictions and plots just as we did in the case of the polynomial fit.</p>
<div class="sourceCode" id="cb796"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb796-1"><a href="#cb796-1" tabindex="-1"></a><span class="fu">coef</span>(<span class="fu">summary</span>(fit))</span></code></pre></div>
<pre><code>##                         Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept)            94.158392   1.476069 63.789970 0.000000e+00
## cut(age, 4)(33.5,49]   24.053491   1.829431 13.148074 1.982315e-38
## cut(age, 4)(49,64.5]   23.664559   2.067958 11.443444 1.040750e-29
## cut(age, 4)(64.5,80.1]  7.640592   4.987424  1.531972 1.256350e-01</code></pre>
</div>
<div id="splines" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Splines<a href="#splines" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now let’s consider splines. Splines are easily fitted using the <code>bs()</code> function which generates the entire matrix of basis functions for splines with the specified set of knots (cubic splines are produced by default (degree of 3)). Here we knots at ages <span class="math inline">\(25\)</span>, <span class="math inline">\(40\)</span>, and <span class="math inline">\(60\)</span>. This produces a spline with six basis functions</p>
<div class="sourceCode" id="cb798"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb798-1"><a href="#cb798-1" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">bs</span>(age, <span class="at">knots =</span> <span class="fu">c</span>(<span class="dv">25</span>, <span class="dv">40</span>, <span class="dv">60</span>)), <span class="at">data =</span> Wage)</span>
<span id="cb798-2"><a href="#cb798-2" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit, <span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">age =</span> age.grid), <span class="at">se =</span> T)</span>
<span id="cb798-3"><a href="#cb798-3" tabindex="-1"></a><span class="fu">plot</span>(age, wage, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>)</span>
<span id="cb798-4"><a href="#cb798-4" tabindex="-1"></a><span class="fu">lines</span>(age.grid, pred<span class="sc">$</span>fit, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb798-5"><a href="#cb798-5" tabindex="-1"></a><span class="fu">lines</span>(age.grid, pred<span class="sc">$</span>fit <span class="sc">+</span> <span class="dv">2</span> <span class="sc">*</span> pred<span class="sc">$</span>se, <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span>
<span id="cb798-6"><a href="#cb798-6" tabindex="-1"></a><span class="fu">lines</span>(age.grid, pred<span class="sc">$</span>fit <span class="sc">-</span> <span class="dv">2</span> <span class="sc">*</span> pred<span class="sc">$</span>se, <span class="at">lty =</span> <span class="st">&quot;dashed&quot;</span>)</span></code></pre></div>
<p><img src="06-S06-D4_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
<p>Recall that a cubic spline with three knots has seven degrees of freedom; these degrees of freedom are used up by an intercept, plus six basis functions. We could also use the <code>df</code> option to produce a spline with knots at uniform quantiles of the data.</p>
<div class="sourceCode" id="cb799"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb799-1"><a href="#cb799-1" tabindex="-1"></a><span class="fu">dim</span>(<span class="fu">bs</span>(age, <span class="at">knots =</span> <span class="fu">c</span>(<span class="dv">25</span>, <span class="dv">40</span>, <span class="dv">60</span>)))</span></code></pre></div>
<pre><code>## [1] 3000    6</code></pre>
<div class="sourceCode" id="cb801"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb801-1"><a href="#cb801-1" tabindex="-1"></a><span class="fu">dim</span>(<span class="fu">bs</span>(age, <span class="at">df =</span> <span class="dv">6</span>))</span></code></pre></div>
<pre><code>## [1] 3000    6</code></pre>
<div class="sourceCode" id="cb803"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb803-1"><a href="#cb803-1" tabindex="-1"></a><span class="fu">attr</span>(<span class="fu">bs</span>(age, <span class="at">df =</span> <span class="dv">6</span>), <span class="st">&quot;knots&quot;</span>)</span></code></pre></div>
<pre><code>##   25%   50%   75% 
## 33.75 42.00 51.00</code></pre>
<p>As you can see from the output, R has automatically chosen knots at ages <span class="math inline">\(33.8, 42.0\)</span>, and <span class="math inline">\(51.0\)</span>, which correspond to the 25th, 50th, and 75th percentiles of age. The function <code>bs()</code> also has a <code>degree</code> argument, so we can fit splines of any degree, rather than the default.</p>
<p>In order to instead fit a natural spline, we use the <code>ns()</code> function. Here we fit a natural spline with four degrees of freedom. As with the <code>bs()</code> function, we could instead specify the knots directly using the <code>knots</code> option.</p>
<div class="sourceCode" id="cb805"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb805-1"><a href="#cb805-1" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">ns</span>(age, <span class="at">df =</span> <span class="dv">4</span>), <span class="at">data =</span> Wage)</span>
<span id="cb805-2"><a href="#cb805-2" tabindex="-1"></a>pred2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(fit2, <span class="at">newdata =</span> <span class="fu">list</span>(<span class="at">age =</span> age.grid), <span class="at">se =</span> T)</span>
<span id="cb805-3"><a href="#cb805-3" tabindex="-1"></a><span class="fu">plot</span>(age, wage, <span class="at">col =</span> <span class="st">&quot;gray&quot;</span>)</span>
<span id="cb805-4"><a href="#cb805-4" tabindex="-1"></a><span class="fu">lines</span>(age.grid, pred2<span class="sc">$</span>fit, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p><img src="06-S06-D4_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>To fit a smoothing spline, we use the <code>smooth.spline()</code> function. Notice that in the first call to <code>smooth.spline()</code>, we specified <code>df = 16</code>. The function then determines which value of <span class="math inline">\(\lambda\)</span> leads to <span class="math inline">\(16\)</span> degrees of freedom. In the second call to <code>smooth.spline()</code>, we select the smoothness level by cross-validation; this results in a value of <span class="math inline">\(\lambda\)</span> that yields 6.8 degrees of freedom. We plot the results of the two fits for comparison purposes.</p>
<div class="sourceCode" id="cb806"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb806-1"><a href="#cb806-1" tabindex="-1"></a><span class="fu">plot</span>(age, wage, <span class="at">xlim =</span> agelims, <span class="at">cex =</span> .<span class="dv">5</span>, <span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>)</span>
<span id="cb806-2"><a href="#cb806-2" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;Smoothing Spline&quot;</span>)</span>
<span id="cb806-3"><a href="#cb806-3" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">smooth.spline</span>(age, wage, <span class="at">df =</span> <span class="dv">16</span>)</span>
<span id="cb806-4"><a href="#cb806-4" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">smooth.spline</span>(age, wage, <span class="at">cv =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## Warning in smooth.spline(age, wage, cv = TRUE): cross-validation with
## non-unique &#39;x&#39; values seems doubtful</code></pre>
<div class="sourceCode" id="cb808"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb808-1"><a href="#cb808-1" tabindex="-1"></a>fit2<span class="sc">$</span>df</span></code></pre></div>
<pre><code>## [1] 6.794596</code></pre>
<div class="sourceCode" id="cb810"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb810-1"><a href="#cb810-1" tabindex="-1"></a><span class="fu">lines</span>(fit, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb810-2"><a href="#cb810-2" tabindex="-1"></a><span class="fu">lines</span>(fit2, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb810-3"><a href="#cb810-3" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;16 DF&quot;</span>, <span class="st">&quot;6.8 DF&quot;</span>),</span>
<span id="cb810-4"><a href="#cb810-4" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">cex =</span> .<span class="dv">8</span>)</span></code></pre></div>
<p><img src="06-S06-D4_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
</div>
<div id="local-regression" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Local Regression<a href="#local-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In order to perform local regression, we use the <code>loess()</code> function. Here we have performed local linear regression using spans of <span class="math inline">\(0.2\)</span> and <span class="math inline">\(0.5\)</span>: that is, each neighborhood consists of 20 % or 50 % of the observations. The larger the span, the smoother the fit.</p>
<div class="sourceCode" id="cb811"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb811-1"><a href="#cb811-1" tabindex="-1"></a><span class="fu">plot</span>(age, wage, <span class="at">xlim =</span> agelims, <span class="at">cex =</span> .<span class="dv">5</span>, <span class="at">col =</span> <span class="st">&quot;darkgrey&quot;</span>)</span>
<span id="cb811-2"><a href="#cb811-2" tabindex="-1"></a><span class="fu">title</span>(<span class="st">&quot;Local Regression&quot;</span>)</span>
<span id="cb811-3"><a href="#cb811-3" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">loess</span>(wage <span class="sc">~</span> age, <span class="at">span =</span> .<span class="dv">2</span>, <span class="at">data =</span> Wage)</span>
<span id="cb811-4"><a href="#cb811-4" tabindex="-1"></a>fit2 <span class="ot">&lt;-</span> <span class="fu">loess</span>(wage <span class="sc">~</span> age, <span class="at">span =</span> .<span class="dv">5</span>, <span class="at">data =</span> Wage)</span>
<span id="cb811-5"><a href="#cb811-5" tabindex="-1"></a><span class="fu">lines</span>(age.grid, <span class="fu">predict</span>(fit, <span class="fu">data.frame</span>(<span class="at">age =</span> age.grid)),</span>
<span id="cb811-6"><a href="#cb811-6" tabindex="-1"></a>    <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb811-7"><a href="#cb811-7" tabindex="-1"></a><span class="fu">lines</span>(age.grid, <span class="fu">predict</span>(fit2, <span class="fu">data.frame</span>(<span class="at">age =</span> age.grid)),</span>
<span id="cb811-8"><a href="#cb811-8" tabindex="-1"></a>    <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb811-9"><a href="#cb811-9" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Span = 0.2&quot;</span>, <span class="st">&quot;Span = 0.5&quot;</span>),</span>
<span id="cb811-10"><a href="#cb811-10" tabindex="-1"></a>    <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>), <span class="at">lty =</span> <span class="dv">1</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">cex =</span> .<span class="dv">8</span>)</span></code></pre></div>
<p><img src="06-S06-D4_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
</div>
<div id="generalised-additive-models" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Generalised Additive Models<a href="#generalised-additive-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Finally, let’s consider generalised additive models. Below, we fit a GAM to predict <strong>wage</strong> using natural spline functions of <strong>lyear</strong> and <strong>age</strong>, treating <strong>education</strong> as a categorical predictor. Since this is just a big linear regression model using an appropriate choice of basis functions, we can simply do this using the <code>lm()</code> function.</p>
<div class="sourceCode" id="cb812"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb812-1"><a href="#cb812-1" tabindex="-1"></a>gam1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(wage <span class="sc">~</span> <span class="fu">ns</span>(year, <span class="dv">4</span>) <span class="sc">+</span> <span class="fu">ns</span>(age, <span class="dv">5</span>) <span class="sc">+</span> education, </span>
<span id="cb812-2"><a href="#cb812-2" tabindex="-1"></a>           <span class="at">data =</span> Wage)</span></code></pre></div>
<p>We now fit the model using smoothing splines rather than natural splines. In cases of non-linear approaches that cannot be expressed in terms of basis functions and fit using <code>lm()</code>, we will need to use the <code>gam()</code> function from the <code>gam</code> package.</p>
<p>The <code>s()</code> function is used to indicate that we would like to use a smoothing spline. We specify that the function of <strong>lyear</strong> should have <span class="math inline">\(4\)</span> degrees of freedom, and that the function of <strong>age</strong> should have <span class="math inline">\(5\)</span> degrees of freedom. Since <strong>education</strong> is qualitative, we leave it as is (since it will be converted into four dummy variables).</p>
<div class="sourceCode" id="cb813"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb813-1"><a href="#cb813-1" tabindex="-1"></a>gam.m3 <span class="ot">&lt;-</span> <span class="fu">gam</span>(wage <span class="sc">~</span> <span class="fu">s</span>(year, <span class="dv">4</span>) <span class="sc">+</span> <span class="fu">s</span>(age, <span class="dv">5</span>) <span class="sc">+</span> education,</span>
<span id="cb813-2"><a href="#cb813-2" tabindex="-1"></a>              <span class="at">data =</span> Wage)</span></code></pre></div>
<p>The generic <code>plot()</code> function recognises that <code>gam.m3</code> is an object of class <code>Gam</code>, and invokes the appropriate <code>plot.Gam()</code> method.</p>
<div class="sourceCode" id="cb814"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb814-1"><a href="#cb814-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb814-2"><a href="#cb814-2" tabindex="-1"></a><span class="fu">plot</span>(gam.m3, <span class="at">se =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>)</span></code></pre></div>
<p><img src="06-S06-D4_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<p>Conveniently, even though <code>gam1</code> is not of class <code>Gam</code> but rather of class <code>lm</code>, we can still use <code>plot.Gam()</code> on it.</p>
<div class="sourceCode" id="cb815"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb815-1"><a href="#cb815-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb815-2"><a href="#cb815-2" tabindex="-1"></a><span class="fu">plot.Gam</span>(gam1, <span class="at">se =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="06-S06-D4_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<p>Notice here we had to use <code>plot.Gam()</code> rather than the <code>plot()</code> function.</p>
<p>In these plots, the function of <strong>lyear</strong> looks rather linear. We can perform a series of ANOVA tests in order to determine which of these three models is best.</p>
<div class="sourceCode" id="cb816"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb816-1"><a href="#cb816-1" tabindex="-1"></a>gam.m1 <span class="ot">&lt;-</span> <span class="fu">gam</span>(wage <span class="sc">~</span> <span class="fu">s</span>(age, <span class="dv">5</span>) <span class="sc">+</span> education, <span class="at">data =</span> Wage)</span>
<span id="cb816-2"><a href="#cb816-2" tabindex="-1"></a>gam.m2 <span class="ot">&lt;-</span> <span class="fu">gam</span>(wage <span class="sc">~</span> year <span class="sc">+</span> <span class="fu">s</span>(age, <span class="dv">5</span>) <span class="sc">+</span> education,</span>
<span id="cb816-3"><a href="#cb816-3" tabindex="-1"></a>              <span class="at">data =</span> Wage)</span>
<span id="cb816-4"><a href="#cb816-4" tabindex="-1"></a><span class="fu">anova</span>(gam.m1, gam.m2, gam.m3, <span class="at">test =</span> <span class="st">&quot;F&quot;</span>)</span></code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: wage ~ s(age, 5) + education
## Model 2: wage ~ year + s(age, 5) + education
## Model 3: wage ~ s(year, 4) + s(age, 5) + education
##   Resid. Df Resid. Dev Df Deviance       F    Pr(&gt;F)    
## 1      2990    3711731                                  
## 2      2989    3693842  1  17889.2 14.4771 0.0001447 ***
## 3      2986    3689770  3   4071.1  1.0982 0.3485661    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We find that there is compelling evidence that a GAM with a linear function of <strong>lyear</strong> is better than a GAM that does not include <strong>lyear</strong> at all. However, there is no evidence that a non-linear function of <strong>lyear</strong> is needed and so <span class="math inline">\(M_2\)</span> is preferred.</p>
<p>The <code>summary()</code> function can also be used with GAMs.</p>
<div class="sourceCode" id="cb818"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb818-1"><a href="#cb818-1" tabindex="-1"></a><span class="fu">summary</span>(gam.m3)</span></code></pre></div>
<pre><code>## 
## Call: gam(formula = wage ~ s(year, 4) + s(age, 5) + education, data = Wage)
## Deviance Residuals:
##     Min      1Q  Median      3Q     Max 
## -119.43  -19.70   -3.33   14.17  213.48 
## 
## (Dispersion Parameter for gaussian family taken to be 1235.69)
## 
##     Null Deviance: 5222086 on 2999 degrees of freedom
## Residual Deviance: 3689770 on 2986 degrees of freedom
## AIC: 29887.75 
## 
## Number of Local Scoring Iterations: NA 
## 
## Anova for Parametric Effects
##              Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## s(year, 4)    1   27162   27162  21.981 2.877e-06 ***
## s(age, 5)     1  195338  195338 158.081 &lt; 2.2e-16 ***
## education     4 1069726  267432 216.423 &lt; 2.2e-16 ***
## Residuals  2986 3689770    1236                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Anova for Nonparametric Effects
##             Npar Df Npar F  Pr(F)    
## (Intercept)                          
## s(year, 4)        3  1.086 0.3537    
## s(age, 5)         4 32.380 &lt;2e-16 ***
## education                            
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The “Anova for Parametric Effects” p-values clearly demonstrate that <strong>year</strong>, <strong>age</strong>, and <strong>education</strong> are all highly statistically significant, even when only assuming a linear relationship. Alternatively, the “Anova for Nonparametric Effects” p-values for <strong>year</strong> and <strong>age</strong> correspond to a null hypothesis of a linear relationship versus the alternative of a non-linear relationship. The large p-value for <strong>year</strong> reinforces our conclusion from the ANOVA test that a linear function is adequate for this term. However, there is very clear evidence that a non-linear term is required for <strong>age</strong>.</p>
<p>We can make predictions using the <code>predict()</code> method for the class <code>Gam</code>. Here we make predictions on the training set.</p>
<div class="sourceCode" id="cb820"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb820-1"><a href="#cb820-1" tabindex="-1"></a>preds <span class="ot">&lt;-</span> <span class="fu">predict</span>(gam.m2, <span class="at">newdata =</span> Wage)</span></code></pre></div>
<p>We can also use local regression fits as building blocks in a GAM, using the <code>lo()</code> function.</p>
<div class="sourceCode" id="cb821"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb821-1"><a href="#cb821-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb821-2"><a href="#cb821-2" tabindex="-1"></a></span>
<span id="cb821-3"><a href="#cb821-3" tabindex="-1"></a>gam.lo <span class="ot">&lt;-</span> <span class="fu">gam</span>(</span>
<span id="cb821-4"><a href="#cb821-4" tabindex="-1"></a>    wage <span class="sc">~</span> <span class="fu">s</span>(year, <span class="at">df =</span> <span class="dv">4</span>) <span class="sc">+</span> <span class="fu">lo</span>(age, <span class="at">span =</span> <span class="fl">0.7</span>) <span class="sc">+</span> education,</span>
<span id="cb821-5"><a href="#cb821-5" tabindex="-1"></a>    <span class="at">data =</span> Wage</span>
<span id="cb821-6"><a href="#cb821-6" tabindex="-1"></a>    )</span>
<span id="cb821-7"><a href="#cb821-7" tabindex="-1"></a><span class="fu">plot</span>(gam.lo, <span class="at">se =</span> <span class="cn">TRUE</span>, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>)</span></code></pre></div>
<p><img src="06-S06-D4_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>Here we have used local regression for the <code>age</code> term, with a span of <span class="math inline">\(0.7\)</span>. We can also use the <code>lo()</code> function to create interactions before calling the <code>gam()</code> function. For example, the below fits a two-term model, in which the first term is an interaction between <code>lyear</code> and <code>age</code>, fit by a local regression surface.</p>
<div class="sourceCode" id="cb822"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb822-1"><a href="#cb822-1" tabindex="-1"></a>gam.lo.i <span class="ot">&lt;-</span> <span class="fu">gam</span>(wage <span class="sc">~</span> <span class="fu">lo</span>(year, age, <span class="at">span =</span> <span class="fl">0.5</span>) <span class="sc">+</span> education,</span>
<span id="cb822-2"><a href="#cb822-2" tabindex="-1"></a>    <span class="at">data =</span> Wage)</span></code></pre></div>
<p>We can plot the resulting two-dimensional surface if we first install the <code>akima</code> package.</p>
<div class="sourceCode" id="cb823"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb823-1"><a href="#cb823-1" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb823-2"><a href="#cb823-2" tabindex="-1"></a></span>
<span id="cb823-3"><a href="#cb823-3" tabindex="-1"></a><span class="fu">plot</span>(gam.lo.i)</span></code></pre></div>
<p><img src="06-S06-D4_files/figure-html/unnamed-chunk-31-1.png" width="672" /></p>
<p>In order to fit a logistic regression GAM, we once again use the <code>I()</code> function in constructing the binary response variable, and set <code>family=binomial</code>.</p>
<div class="sourceCode" id="cb824"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb824-1"><a href="#cb824-1" tabindex="-1"></a>gam.lr <span class="ot">&lt;-</span> <span class="fu">gam</span>(<span class="fu">I</span>(wage <span class="sc">&gt;</span> <span class="dv">250</span>) <span class="sc">~</span> year <span class="sc">+</span> <span class="fu">s</span>(age, <span class="at">df =</span> <span class="dv">5</span>) <span class="sc">+</span> education,</span>
<span id="cb824-2"><a href="#cb824-2" tabindex="-1"></a>              <span class="at">family =</span> binomial, <span class="at">data =</span> Wage)</span>
<span id="cb824-3"><a href="#cb824-3" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb824-4"><a href="#cb824-4" tabindex="-1"></a><span class="fu">plot</span>(gam.lr, <span class="at">se =</span> T, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>)</span></code></pre></div>
<p><img src="06-S06-D4_files/figure-html/unnamed-chunk-32-1.png" width="672" /></p>
<p>It is easy to see that there are no high earners in the <code>&lt; HS</code> category:</p>
<div class="sourceCode" id="cb825"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb825-1"><a href="#cb825-1" tabindex="-1"></a><span class="fu">table</span>(education, <span class="fu">I</span>(wage <span class="sc">&gt;</span> <span class="dv">250</span>))</span></code></pre></div>
<pre><code>##                     
## education            FALSE TRUE
##   1. &lt; HS Grad         268    0
##   2. HS Grad           966    5
##   3. Some College      643    7
##   4. College Grad      663   22
##   5. Advanced Degree   381   45</code></pre>
<p>Hence, we fit a logistic regression GAM using all but this category. This provides more sensible results.</p>
<div class="sourceCode" id="cb827"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb827-1"><a href="#cb827-1" tabindex="-1"></a>gam.lr.s <span class="ot">&lt;-</span> <span class="fu">gam</span>(<span class="fu">I</span>(wage <span class="sc">&gt;</span> <span class="dv">250</span>) <span class="sc">~</span> year <span class="sc">+</span> <span class="fu">s</span>(age, <span class="at">df =</span> <span class="dv">5</span>) <span class="sc">+</span> education,</span>
<span id="cb827-2"><a href="#cb827-2" tabindex="-1"></a>                <span class="at">family =</span> binomial, <span class="at">data =</span> Wage,</span>
<span id="cb827-3"><a href="#cb827-3" tabindex="-1"></a>                <span class="at">subset =</span> (education <span class="sc">!=</span> <span class="st">&quot;1. &lt; HS Grad&quot;</span>)</span>
<span id="cb827-4"><a href="#cb827-4" tabindex="-1"></a>                )</span>
<span id="cb827-5"><a href="#cb827-5" tabindex="-1"></a></span>
<span id="cb827-6"><a href="#cb827-6" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">3</span>))</span>
<span id="cb827-7"><a href="#cb827-7" tabindex="-1"></a></span>
<span id="cb827-8"><a href="#cb827-8" tabindex="-1"></a><span class="fu">plot</span>(gam.lr.s, <span class="at">se =</span> T, <span class="at">col =</span> <span class="st">&quot;green&quot;</span>)</span></code></pre></div>
<p><img src="06-S06-D4_files/figure-html/unnamed-chunk-34-1.png" width="672" /></p>
<!--chapter:end:06-S06-D4.Rmd-->
</div>
</div>
<div id="part-section-7" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART*) Section 7<a href="#part-section-7" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="overview-6" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Overview<a href="#overview-6" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div style="color: #333; font-size: 24px; font-style: italic; text-align: justify;">
<p>Section 7: Unsupervised Learning</p>
</div>
<p>This section is comprised two demonstrations (accompanied by tasks at
the end of each) developed by Dr. George Wood, Lecturer in Social
Statistics.</p>
<div class="ilos">
<p><strong>Learning Outcomes:</strong></p>
<ul>
<li>preprocess the data for k-means clustering;</li>
<li>perform k-means clustering in R and interpret the results;</li>
<li>evaluate k-means clustering performance;</li>
<li>visualise clusters;</li>
<li>decide number of clusters;</li>
<li>perform PCA analysis and interpret the results;</li>
<li>plot principal components;</li>
<li>compute correlation between observed y and fitted y;</li>
<li>compute cumulative variance explained</li>
<li>compute percentage of variance explained by each principal
component;</li>
</ul>
</div>
<p><strong>In this section, you will practice using the functions below. It is
highly recommended that you explore these functions further using the
Help tab in your RStudio console.</strong></p>
<table style="width:100%;">
<colgroup>
<col width="28%" />
<col width="61%" />
<col width="9%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Function</th>
<th align="center">Description</th>
<th align="center">Package</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><code>kmeans()</code></td>
<td align="center">perform k-means clustering on a data matrix</td>
<td align="center">base R (stats)</td>
</tr>
<tr class="even">
<td align="center"><code>fviz_cluster()</code></td>
<td align="center">visualise clustering results</td>
<td align="center">factoextra</td>
</tr>
<tr class="odd">
<td align="center"><code>map_dbl()</code></td>
<td align="center">apply a function to each element of a vector</td>
<td align="center">purrr</td>
</tr>
<tr class="even">
<td align="center"><code>fviz_nbclust()</code></td>
<td align="center">determining and visualising the optimal number of clusters</td>
<td align="center">factoextra</td>
</tr>
<tr class="odd">
<td align="center"><code>head()</code></td>
<td align="center">returns the first part of a vector, matrix, table, data frame or function</td>
<td align="center">base R (utils)</td>
</tr>
<tr class="even">
<td align="center"><code>glimpse()</code></td>
<td align="center">obtain a glimpse of your data</td>
<td align="center">dplyr</td>
</tr>
<tr class="odd">
<td align="center"><code>gglot()</code></td>
<td align="center">create a new ggplot</td>
<td align="center">ggplot2</td>
</tr>
<tr class="even">
<td align="center"><code>geom_point()</code></td>
<td align="center">create scatterplot</td>
<td align="center">ggplot2</td>
</tr>
<tr class="odd">
<td align="center"><code>scale_color_discrete()</code></td>
<td align="center">use a discrete colour scale</td>
<td align="center">ggplot2</td>
</tr>
<tr class="even">
<td align="center"><code>scale_x_continuous()</code>, <code>scale_y_continuous()</code></td>
<td align="center">position scales for continuous data</td>
<td align="center">ggplot2</td>
</tr>
<tr class="odd">
<td align="center"><code>geom_hline()</code>, <code>geom_vline()</code></td>
<td align="center">add horizontal or vertical lines, respectively</td>
<td align="center">ggplot2</td>
</tr>
<tr class="even">
<td align="center"><code>geom_text()</code></td>
<td align="center">adding text geoms (labelling plots)</td>
<td align="center">ggplot2</td>
</tr>
<tr class="odd">
<td align="center"><code>facet_wrap()</code></td>
<td align="center">wrap a 1D sequence of panels into 2D</td>
<td align="center">ggplot2</td>
</tr>
<tr class="even">
<td align="center"><code>coord_equal()</code></td>
<td align="center">fixed scale coordinate system</td>
<td align="center">ggplot2</td>
</tr>
<tr class="odd">
<td align="center"><code>geom_smooth()</code></td>
<td align="center">smoothed conditional means (addressed overplotting)</td>
<td align="center">ggplot2</td>
</tr>
<tr class="even">
<td align="center"><code>theme()</code></td>
<td align="center">modify components of themes</td>
<td align="center">ggplot2</td>
</tr>
<tr class="odd">
<td align="center"><code>drop_na()</code></td>
<td align="center">drop rows containing missing values</td>
<td align="center">tidyr</td>
</tr>
<tr class="even">
<td align="center"><code>select()</code></td>
<td align="center">keep or drop columns by name and type</td>
<td align="center">dplyr</td>
</tr>
<tr class="odd">
<td align="center"><code>mutate()</code></td>
<td align="center">create, modify, delete columns</td>
<td align="center">dplyr</td>
</tr>
<tr class="even">
<td align="center"><code>table()</code></td>
<td align="center">build a contingency table </td>
<td align="center">base R</td>
</tr>
<tr class="odd">
<td align="center"><code>rownames(), colnames()</code></td>
<td align="center">retrieve or set row or column names of a matrix-like object.</td>
<td align="center">base R</td>
</tr>
<tr class="even">
<td align="center"><code>clean_names()</code></td>
<td align="center">clean names of object (usually data.frame)</td>
<td align="center">janitor</td>
</tr>
<tr class="odd">
<td align="center"><code>scale()</code></td>
<td align="center">scaling and centering matrix-like objects</td>
<td align="center">base R</td>
</tr>
<tr class="even">
<td align="center"><code>pivot_longer()</code></td>
<td align="center">pivot data from wide to long</td>
<td align="center">tidyr</td>
</tr>
<tr class="odd">
<td align="center"><code>PCA()</code></td>
<td align="center">computation of weighted or unweighted principal component analysis of a matrix of interval-scale data</td>
<td align="center">easyCODA</td>
</tr>
<tr class="even">
<td align="center"><code>summary()</code></td>
<td align="center">produce result summaries</td>
<td align="center">base R</td>
</tr>
<tr class="odd">
<td align="center"><code>lm()</code></td>
<td align="center">fit linear models</td>
<td align="center">stats</td>
</tr>
<tr class="even">
<td align="center"><code>cor()</code></td>
<td align="center">compute correlation matrix</td>
<td align="center">stats</td>
</tr>
<tr class="odd">
<td align="center"><code>t()</code></td>
<td align="center">matrix transpose</td>
<td align="center">base</td>
</tr>
<tr class="even">
<td align="center"><code>round()</code></td>
<td align="center">round numbers</td>
<td align="center">base</td>
</tr>
</tbody>
</table>
<!--chapter:end:07-S07-overview.Rmd-->
</div>
<div id="demonstration-1-k-means-clustering-in-r" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Demonstration 1: K-means Clustering in R<a href="#demonstration-1-k-means-clustering-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>In this practical, you will learn how to implement k-means clustering using the Palmer Penguins dataset. You can read more about this data here:</p>
<p><a href="https://allisonhorst.github.io/palmerpenguins/articles/intro.html">Palmer Penguins Dataset</a></p>
<p>The task in this practical is to cluster penguins based on their bill length and flipper length. We will then assess whether the predicted clusters map onto the actual species of penguin in the data. You can view this as an exercise in whether bill length and flipper length is sufficient to differentiate between species of penguin.</p>
<p><img src="images/penguins.png" fig-align="center" /></p>
<p>(Artwork by @allison_horst)</p>
<p>You should modify and experiment with the below code. There is also a brief exercise for you complete at the end of the walk-through.</p>
<p>You will be required to install two new packages first: <code>palmerpenguins</code> and <code>factoextra</code>.</p>
<div id="loading-the-necessary-packages" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Loading the necessary packages<a href="#loading-the-necessary-packages" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb828"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb828-1"><a href="#cb828-1" tabindex="-1"></a><span class="fu">library</span>(palmerpenguins)</span>
<span id="cb828-2"><a href="#cb828-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb828-3"><a href="#cb828-3" tabindex="-1"></a><span class="fu">library</span>(factoextra)</span></code></pre></div>
</div>
<div id="profile-the-palmer-penguins-dataset" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Profile the Palmer Penguins dataset<a href="#profile-the-palmer-penguins-dataset" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb829"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb829-1"><a href="#cb829-1" tabindex="-1"></a><span class="fu">head</span>(penguins)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 8
##   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g
##   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;
## 1 Adelie  Torgersen           39.1          18.7               181        3750
## 2 Adelie  Torgersen           39.5          17.4               186        3800
## 3 Adelie  Torgersen           40.3          18                 195        3250
## 4 Adelie  Torgersen           NA            NA                  NA          NA
## 5 Adelie  Torgersen           36.7          19.3               193        3450
## 6 Adelie  Torgersen           39.3          20.6               190        3650
## # ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;</code></pre>
<div class="sourceCode" id="cb831"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb831-1"><a href="#cb831-1" tabindex="-1"></a><span class="fu">glimpse</span>(penguins)</span></code></pre></div>
<pre><code>## Rows: 344
## Columns: 8
## $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…
## $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…
## $ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …
## $ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …
## $ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…
## $ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …
## $ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…
## $ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…</code></pre>
</div>
<div id="plot" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Plot<a href="#plot" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb833"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb833-1"><a href="#cb833-1" tabindex="-1"></a>penguins <span class="sc">|&gt;</span></span>
<span id="cb833-2"><a href="#cb833-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(bill_length_mm, flipper_length_mm, <span class="at">color =</span> species)) <span class="sc">+</span></span>
<span id="cb833-3"><a href="#cb833-3" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<pre><code>## Warning: Removed 2 rows containing missing values or values outside the scale range
## (`geom_point()`).</code></pre>
<p><img src="07-S07-D1_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
</div>
<div id="preprocess-the-data-for-clustering" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Preprocess the data for clustering<a href="#preprocess-the-data-for-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb835"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb835-1"><a href="#cb835-1" tabindex="-1"></a>pengs <span class="ot">&lt;-</span> </span>
<span id="cb835-2"><a href="#cb835-2" tabindex="-1"></a>  penguins <span class="sc">|&gt;</span></span>
<span id="cb835-3"><a href="#cb835-3" tabindex="-1"></a>  <span class="fu">drop_na</span>()</span></code></pre></div>
</div>
<div id="perform-k-means-clustering" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Perform k-means clustering<a href="#perform-k-means-clustering" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb836"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb836-1"><a href="#cb836-1" tabindex="-1"></a>k2 <span class="ot">&lt;-</span> </span>
<span id="cb836-2"><a href="#cb836-2" tabindex="-1"></a>  pengs <span class="sc">|&gt;</span></span>
<span id="cb836-3"><a href="#cb836-3" tabindex="-1"></a>  <span class="fu">select</span>(bill_length_mm, flipper_length_mm) <span class="sc">|&gt;</span></span>
<span id="cb836-4"><a href="#cb836-4" tabindex="-1"></a>  <span class="fu">kmeans</span>(<span class="at">centers =</span> <span class="dv">3</span>, <span class="at">nstart =</span> <span class="dv">25</span>)</span></code></pre></div>
<p>Let’s add the predicted cluster to the <code>pengs</code> data frame:</p>
<div class="sourceCode" id="cb837"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb837-1"><a href="#cb837-1" tabindex="-1"></a>pengs <span class="ot">&lt;-</span> pengs <span class="sc">|&gt;</span> <span class="fu">mutate</span>(<span class="at">cluster =</span> k2<span class="sc">$</span>cluster)</span></code></pre></div>
</div>
<div id="visualise-the-clusters" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Visualise the clusters<a href="#visualise-the-clusters" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb838"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb838-1"><a href="#cb838-1" tabindex="-1"></a><span class="fu">fviz_cluster</span>(</span>
<span id="cb838-2"><a href="#cb838-2" tabindex="-1"></a>  k2,</span>
<span id="cb838-3"><a href="#cb838-3" tabindex="-1"></a>  <span class="at">data =</span> pengs <span class="sc">|&gt;</span> <span class="fu">select</span>(bill_length_mm, flipper_length_mm)</span>
<span id="cb838-4"><a href="#cb838-4" tabindex="-1"></a>)</span></code></pre></div>
<p><img src="07-S07-D1_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<div class="sourceCode" id="cb839"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb839-1"><a href="#cb839-1" tabindex="-1"></a>pengs <span class="sc">|&gt;</span></span>
<span id="cb839-2"><a href="#cb839-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> bill_length_mm,</span>
<span id="cb839-3"><a href="#cb839-3" tabindex="-1"></a>             <span class="at">y =</span> flipper_length_mm,</span>
<span id="cb839-4"><a href="#cb839-4" tabindex="-1"></a>             <span class="at">color =</span> <span class="fu">factor</span>(cluster),</span>
<span id="cb839-5"><a href="#cb839-5" tabindex="-1"></a>             <span class="at">shape =</span> species)) <span class="sc">+</span></span>
<span id="cb839-6"><a href="#cb839-6" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb839-7"><a href="#cb839-7" tabindex="-1"></a>  <span class="fu">scale_color_discrete</span>(<span class="st">&quot;cluster&quot;</span>)</span></code></pre></div>
<p><img src="07-S07-D1_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<div class="question">
<p>To what extent do the clusters overlap with species?</p>
</div>
<div class="sourceCode" id="cb840"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb840-1"><a href="#cb840-1" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">table</span>(pengs<span class="sc">$</span>species, k2<span class="sc">$</span>cluster)</span>
<span id="cb840-2"><a href="#cb840-2" tabindex="-1"></a>pred</span></code></pre></div>
<pre><code>##            
##               1   2   3
##   Adelie    106   2  38
##   Chinstrap   9   5  54
##   Gentoo      0 117   2</code></pre>
<div class="sourceCode" id="cb842"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb842-1"><a href="#cb842-1" tabindex="-1"></a>overlap <span class="ot">&lt;-</span> <span class="fu">sum</span>(<span class="fu">diag</span>(pred))</span>
<span id="cb842-2"><a href="#cb842-2" tabindex="-1"></a>differs <span class="ot">&lt;-</span> <span class="fu">sum</span>(pred[<span class="fu">upper.tri</span>(pred)], pred[<span class="fu">lower.tri</span>(pred)])</span>
<span id="cb842-3"><a href="#cb842-3" tabindex="-1"></a></span>
<span id="cb842-4"><a href="#cb842-4" tabindex="-1"></a><span class="co"># proportion of penguins &quot;correctly&quot; classified according to species:</span></span>
<span id="cb842-5"><a href="#cb842-5" tabindex="-1"></a>overlap <span class="sc">/</span> <span class="fu">sum</span>(overlap, differs)</span></code></pre></div>
<pre><code>## [1] 0.3393393</code></pre>
<div class="question">
<p>How can we visualise the cluster-species overlap?</p>
</div>
<div class="sourceCode" id="cb844"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb844-1"><a href="#cb844-1" tabindex="-1"></a>pengs <span class="sc">|&gt;</span></span>
<span id="cb844-2"><a href="#cb844-2" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb844-3"><a href="#cb844-3" tabindex="-1"></a>    <span class="at">overlap =</span> <span class="fu">case_when</span>(</span>
<span id="cb844-4"><a href="#cb844-4" tabindex="-1"></a>      species <span class="sc">==</span> <span class="st">&quot;Adelie&quot;</span>    <span class="sc">&amp;</span> cluster <span class="sc">==</span> <span class="dv">1</span> <span class="sc">~</span> <span class="st">&quot;overlap&quot;</span>,</span>
<span id="cb844-5"><a href="#cb844-5" tabindex="-1"></a>      species <span class="sc">==</span> <span class="st">&quot;Chinstrap&quot;</span> <span class="sc">&amp;</span> cluster <span class="sc">==</span> <span class="dv">2</span> <span class="sc">~</span> <span class="st">&quot;overlap&quot;</span>,</span>
<span id="cb844-6"><a href="#cb844-6" tabindex="-1"></a>      species <span class="sc">==</span> <span class="st">&quot;Gentoo&quot;</span>    <span class="sc">&amp;</span> cluster <span class="sc">==</span> <span class="dv">3</span> <span class="sc">~</span> <span class="st">&quot;overlap&quot;</span>,</span>
<span id="cb844-7"><a href="#cb844-7" tabindex="-1"></a>      <span class="cn">TRUE</span> <span class="sc">~</span> <span class="st">&quot;differs&quot;</span></span>
<span id="cb844-8"><a href="#cb844-8" tabindex="-1"></a>    )</span>
<span id="cb844-9"><a href="#cb844-9" tabindex="-1"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb844-10"><a href="#cb844-10" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> bill_length_mm,</span>
<span id="cb844-11"><a href="#cb844-11" tabindex="-1"></a>             <span class="at">y =</span> flipper_length_mm,</span>
<span id="cb844-12"><a href="#cb844-12" tabindex="-1"></a>             <span class="at">color =</span> overlap,</span>
<span id="cb844-13"><a href="#cb844-13" tabindex="-1"></a>             <span class="at">shape =</span> species)) <span class="sc">+</span></span>
<span id="cb844-14"><a href="#cb844-14" tabindex="-1"></a>  <span class="fu">geom_point</span>()</span></code></pre></div>
<p><img src="07-S07-D1_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
</div>
<div id="tasks" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">👉 TASKS<a href="#tasks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="task-1-how-well-did-k-means-clustering-perform" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">TASK 1: How well did k-means clustering perform?<a href="#task-1-how-well-did-k-means-clustering-perform" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Suppose you randomly picked Adelie, Chinstrap, or Gentoo for each penguin. What proportion of penguins would you “correctly” classify?</p>
<blockquote>
<p>Your code here</p>
</blockquote>
<p>Consider how this compares to the performance of the k-means clustering algorithm.</p>
</div>
<div id="task-2-how-many-clusters-should-we-use" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">TASK 2: How many clusters should we use?<a href="#task-2-how-many-clusters-should-we-use" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the above example, we used three clusters because there are three species of penguin in our data. However, in practice, we may not know this “ground-truth” information. That is, we may not know how many species are of penguin are represented in the data.</p>
<p>Additionally, we may wish to cluster the data based on other criteria, such as minimizing the intra-cluster variation. Recall that unsupervised learning is useful for <em>detecting patterns</em> in the data.</p>
<p>One method we can use to determine the optimal number of clusters is the <strong>elbow method</strong>. This method involves plotting the within-cluster sum of squares (WSS; also known as within-cluster variation or intra-cluster variation) for a range values of <em>k</em> (recall that <em>k</em> is the number of clusters). We then look for the location of the “bend” in the in the plot, i.e., the elbow.</p>
<p>Below, we use the elbow method to determine the optimal number of clusters of penguins using the <code>body_mass_g</code> and <code>bill_length_mm</code> features.</p>
<div class="sourceCode" id="cb845"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb845-1"><a href="#cb845-1" tabindex="-1"></a><span class="co"># preprocess data</span></span>
<span id="cb845-2"><a href="#cb845-2" tabindex="-1"></a>pengs_mass_length <span class="ot">&lt;-</span> </span>
<span id="cb845-3"><a href="#cb845-3" tabindex="-1"></a>  pengs <span class="sc">|&gt;</span></span>
<span id="cb845-4"><a href="#cb845-4" tabindex="-1"></a>  <span class="fu">select</span>(body_mass_g, bill_length_mm)</span>
<span id="cb845-5"><a href="#cb845-5" tabindex="-1"></a></span>
<span id="cb845-6"><a href="#cb845-6" tabindex="-1"></a><span class="co"># compute total within-cluster sum of square </span></span>
<span id="cb845-7"><a href="#cb845-7" tabindex="-1"></a>wss <span class="ot">&lt;-</span> <span class="cf">function</span>(k) {</span>
<span id="cb845-8"><a href="#cb845-8" tabindex="-1"></a>  <span class="fu">kmeans</span>(</span>
<span id="cb845-9"><a href="#cb845-9" tabindex="-1"></a>    pengs_mass_length,</span>
<span id="cb845-10"><a href="#cb845-10" tabindex="-1"></a>    <span class="at">centers =</span> k,</span>
<span id="cb845-11"><a href="#cb845-11" tabindex="-1"></a>    <span class="at">nstart =</span> <span class="dv">10</span></span>
<span id="cb845-12"><a href="#cb845-12" tabindex="-1"></a>  )<span class="sc">$</span>tot.withinss</span>
<span id="cb845-13"><a href="#cb845-13" tabindex="-1"></a>}</span>
<span id="cb845-14"><a href="#cb845-14" tabindex="-1"></a></span>
<span id="cb845-15"><a href="#cb845-15" tabindex="-1"></a><span class="co"># Compute and plot WSS for k = 1 to k = 15</span></span>
<span id="cb845-16"><a href="#cb845-16" tabindex="-1"></a>k_values <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span></span>
<span id="cb845-17"><a href="#cb845-17" tabindex="-1"></a></span>
<span id="cb845-18"><a href="#cb845-18" tabindex="-1"></a><span class="co"># plot the WSS values against k</span></span>
<span id="cb845-19"><a href="#cb845-19" tabindex="-1"></a>wss_values <span class="ot">&lt;-</span> <span class="fu">map_dbl</span>(k_values, wss)</span>
<span id="cb845-20"><a href="#cb845-20" tabindex="-1"></a></span>
<span id="cb845-21"><a href="#cb845-21" tabindex="-1"></a><span class="fu">tibble</span>(</span>
<span id="cb845-22"><a href="#cb845-22" tabindex="-1"></a>  k_values,</span>
<span id="cb845-23"><a href="#cb845-23" tabindex="-1"></a>  wss_values</span>
<span id="cb845-24"><a href="#cb845-24" tabindex="-1"></a>) <span class="sc">|&gt;</span></span>
<span id="cb845-25"><a href="#cb845-25" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(k_values, wss_values)) <span class="sc">+</span></span>
<span id="cb845-26"><a href="#cb845-26" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb845-27"><a href="#cb845-27" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb845-28"><a href="#cb845-28" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="st">&quot;Number of clusters, k&quot;</span>, <span class="at">breaks =</span> <span class="fu">unique</span>(k_values)) <span class="sc">+</span></span>
<span id="cb845-29"><a href="#cb845-29" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;Total within-clusters sum of squares&quot;</span>)</span></code></pre></div>
<p><img src="07-S07-D1_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>The results here suggest that the optimal number of clusters is 3 (which neatly aligns with the number of penguin species in the data). Although, 4 also looks like a good choice. The elbow method is useful, but it isn’t always clear where the elbow lies, which often simply reflects the reality of the data.</p>
<p>The elbow method is implemented in <code>fviz_nbclust()</code> function from the <code>factoextra</code> package:</p>
<div class="sourceCode" id="cb846"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb846-1"><a href="#cb846-1" tabindex="-1"></a><span class="fu">fviz_nbclust</span>(pengs_mass_length, kmeans, <span class="at">method =</span> <span class="st">&quot;wss&quot;</span>)</span></code></pre></div>
<p><img src="07-S07-D1_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="task-3-clustering-cars" class="section level3 unnumbered hasAnchor">
<h3 class="unnumbered hasAnchor">TASK 3: Clustering cars<a href="#task-3-clustering-cars" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Cluster the <code>mtcars</code> data using <code>kmeans()</code>.</p>
<div class="sourceCode" id="cb847"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb847-1"><a href="#cb847-1" tabindex="-1"></a><span class="fu">head</span>(mtcars)</span></code></pre></div>
<pre><code>##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1</code></pre>
<p>Use the <code>mpg</code> and <code>hp</code> features to group the cars into three clusters.</p>
<blockquote>
<p>Your code here</p>
</blockquote>
<p>How well do your predicted clusters map onto the <code>cyl</code> feature in the <code>mtcars</code> data? (Note that the <code>cyl</code> feature (cylinders) has three values: 4, 6, 8).</p>
<blockquote>
<p>Your code here</p>
</blockquote>
<p>Next, find the optimal number of clusters in the data using the elbow method. You should start by using the <code>mpg</code> and <code>hp</code> features, although feel free to experiment with using other features.</p>
<blockquote>
<p>Your code here</p>
</blockquote>
<!--chapter:end:07-S07-D1.Rmd-->
</div>
</div>
</div>
<div id="demonstration-2-principal-component-analysis-in-r" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Demonstration 2: Principal Component Analysis in R<a href="#demonstration-2-principal-component-analysis-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div class="file">
<p>For the tasks below, you will require the <strong>World Happiness Report (2021)</strong> dataset.</p>
<p>Click here to download the file:
<a href="data/world_happiness_report_2021.csv" download="world_happiness_report_2021.csv"> world_happiness_report_2021.csv </a>.<br />
Remember to place your data file in a separate subfolder within your R project working directory.</p>
</div>
<p>Prior to beginning the practical, you will be required install two packages <code>easyCODA</code> and <code>janitor</code>.</p>
<div id="load-packages-1" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Load packages<a href="#load-packages-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb849"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb849-1"><a href="#cb849-1" tabindex="-1"></a><span class="fu">library</span>(easyCODA)</span>
<span id="cb849-2"><a href="#cb849-2" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb849-3"><a href="#cb849-3" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb849-4"><a href="#cb849-4" tabindex="-1"></a><span class="fu">library</span>(readr)</span>
<span id="cb849-5"><a href="#cb849-5" tabindex="-1"></a><span class="fu">library</span>(janitor)</span>
<span id="cb849-6"><a href="#cb849-6" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span></code></pre></div>
</div>
<div id="preprocess-the-world-happiness-report-data" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Preprocess the world happiness report data<a href="#preprocess-the-world-happiness-report-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb850"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb850-1"><a href="#cb850-1" tabindex="-1"></a>happy <span class="ot">&lt;-</span></span>
<span id="cb850-2"><a href="#cb850-2" tabindex="-1"></a>  <span class="fu">read_csv</span>(</span>
<span id="cb850-3"><a href="#cb850-3" tabindex="-1"></a>    <span class="at">file =</span> <span class="st">&quot;data/world_happiness_report_2021.csv&quot;</span>,</span>
<span id="cb850-4"><a href="#cb850-4" tabindex="-1"></a>    <span class="at">show_col_types =</span> <span class="cn">FALSE</span></span>
<span id="cb850-5"><a href="#cb850-5" tabindex="-1"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb850-6"><a href="#cb850-6" tabindex="-1"></a>  <span class="fu">clean_names</span>()</span>
<span id="cb850-7"><a href="#cb850-7" tabindex="-1"></a></span>
<span id="cb850-8"><a href="#cb850-8" tabindex="-1"></a>happy_standardized <span class="ot">&lt;-</span> </span>
<span id="cb850-9"><a href="#cb850-9" tabindex="-1"></a>  happy <span class="sc">|&gt;</span></span>
<span id="cb850-10"><a href="#cb850-10" tabindex="-1"></a>  <span class="fu">select</span>(</span>
<span id="cb850-11"><a href="#cb850-11" tabindex="-1"></a>    <span class="at">social =</span> social_support,</span>
<span id="cb850-12"><a href="#cb850-12" tabindex="-1"></a>    <span class="at">life =</span> healthy_life_expectancy,</span>
<span id="cb850-13"><a href="#cb850-13" tabindex="-1"></a>    <span class="at">choices =</span> freedom_to_make_life_choices,</span>
<span id="cb850-14"><a href="#cb850-14" tabindex="-1"></a>    generosity,</span>
<span id="cb850-15"><a href="#cb850-15" tabindex="-1"></a>    <span class="at">corruption =</span> perceptions_of_corruption</span>
<span id="cb850-16"><a href="#cb850-16" tabindex="-1"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb850-17"><a href="#cb850-17" tabindex="-1"></a>  <span class="fu">scale</span>()</span>
<span id="cb850-18"><a href="#cb850-18" tabindex="-1"></a></span>
<span id="cb850-19"><a href="#cb850-19" tabindex="-1"></a><span class="fu">rownames</span>(happy_standardized) <span class="ot">&lt;-</span> happy<span class="sc">$</span>country_name</span></code></pre></div>
</div>
<div id="profile-the-happiness-data" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Profile the happiness data<a href="#profile-the-happiness-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb851"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb851-1"><a href="#cb851-1" tabindex="-1"></a><span class="fu">head</span>(happy)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 20
##   country_name regional_indicator ladder_score standard_error_of_ladder_score
##   &lt;chr&gt;        &lt;chr&gt;                     &lt;dbl&gt;                          &lt;dbl&gt;
## 1 Finland      Western Europe             7.84                          0.032
## 2 Denmark      Western Europe             7.62                          0.035
## 3 Switzerland  Western Europe             7.57                          0.036
## 4 Iceland      Western Europe             7.55                          0.059
## 5 Netherlands  Western Europe             7.46                          0.027
## 6 Norway       Western Europe             7.39                          0.035
## # ℹ 16 more variables: upperwhisker &lt;dbl&gt;, lowerwhisker &lt;dbl&gt;,
## #   logged_gdp_per_capita &lt;dbl&gt;, social_support &lt;dbl&gt;,
## #   healthy_life_expectancy &lt;dbl&gt;, freedom_to_make_life_choices &lt;dbl&gt;,
## #   generosity &lt;dbl&gt;, perceptions_of_corruption &lt;dbl&gt;,
## #   ladder_score_in_dystopia &lt;dbl&gt;, explained_by_log_gdp_per_capita &lt;dbl&gt;,
## #   explained_by_social_support &lt;dbl&gt;,
## #   explained_by_healthy_life_expectancy &lt;dbl&gt;, …</code></pre>
</div>
<div id="plot-happiness-score-against-feature" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Plot happiness score against feature<a href="#plot-happiness-score-against-feature" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb853"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb853-1"><a href="#cb853-1" tabindex="-1"></a>happy <span class="sc">|&gt;</span></span>
<span id="cb853-2"><a href="#cb853-2" tabindex="-1"></a>  <span class="fu">select</span>(</span>
<span id="cb853-3"><a href="#cb853-3" tabindex="-1"></a>    <span class="at">ladder =</span> ladder_score,</span>
<span id="cb853-4"><a href="#cb853-4" tabindex="-1"></a>    <span class="at">social =</span> social_support,</span>
<span id="cb853-5"><a href="#cb853-5" tabindex="-1"></a>    <span class="at">life =</span> healthy_life_expectancy,</span>
<span id="cb853-6"><a href="#cb853-6" tabindex="-1"></a>    <span class="at">choices =</span> freedom_to_make_life_choices,</span>
<span id="cb853-7"><a href="#cb853-7" tabindex="-1"></a>    generosity,</span>
<span id="cb853-8"><a href="#cb853-8" tabindex="-1"></a>    <span class="at">corruption =</span> perceptions_of_corruption</span>
<span id="cb853-9"><a href="#cb853-9" tabindex="-1"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb853-10"><a href="#cb853-10" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="sc">-</span>ladder, <span class="at">names_to =</span> <span class="st">&quot;feature&quot;</span>, <span class="at">values_to =</span> <span class="st">&quot;value&quot;</span>) <span class="sc">|&gt;</span></span>
<span id="cb853-11"><a href="#cb853-11" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(value, ladder)) <span class="sc">+</span></span>
<span id="cb853-12"><a href="#cb853-12" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.4</span>) <span class="sc">+</span></span>
<span id="cb853-13"><a href="#cb853-13" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="st">&quot;Feature value&quot;</span>) <span class="sc">+</span></span>
<span id="cb853-14"><a href="#cb853-14" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;Cantrill Ladder happiness score&quot;</span>) <span class="sc">+</span></span>
<span id="cb853-15"><a href="#cb853-15" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> feature, <span class="at">scales =</span> <span class="st">&quot;free_x&quot;</span>)</span></code></pre></div>
<p><img src="07-S07-D2_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
</div>
<div id="run-pca-using-five-features" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Run PCA using five features<a href="#run-pca-using-five-features" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb854"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb854-1"><a href="#cb854-1" tabindex="-1"></a>happy_pca <span class="ot">&lt;-</span> <span class="fu">PCA</span>(happy_standardized, <span class="at">weight =</span> <span class="cn">FALSE</span>)</span>
<span id="cb854-2"><a href="#cb854-2" tabindex="-1"></a><span class="fu">summary</span>(happy_pca)</span></code></pre></div>
<pre><code>## 
## Principal inertias (eigenvalues):
## 
##  dim    value      %   cum%   scree plot               
##  1      0.466493  47.0  47.0  ************             
##  2      0.243482  24.5  71.5  ******                   
##  3      0.139560  14.1  85.5  ****                     
##  4      0.095018   9.6  95.1  **                       
##  5      0.048735   4.9 100.0  *                        
##         -------- -----                                 
##  Total: 0.993289 100.0                                 
## 
## 
## Rows:
##         name   mass  qlt  inr     k=1 cor ctr     k=2 cor ctr  
## 1   |   Fnln |    7  707   19 | -1378 683  27 |  -257  24   2 |
## 2   |   Dnmr |    7  808   19 | -1403 699  28 |  -553 109   8 |
## 3   |   Swtz |    7  860   14 | -1281 789  24 |  -384  71   4 |
## 4   |   Icln |    7  817   10 | -1021 738  15 |  -334  79   3 |
## 5   |   Nthr |    7  929   13 | -1149 667  19 |  -719 261  14 |
## 6   |   Nrwy |    7  918   17 | -1366 763  27 |  -615 155  10 |
## 7   |   Swdn |    7  871   16 | -1303 699  24 |  -647 172  12 |
## 8   |   Lxmb |    7  834    9 | -1030 803  15 |  -201  30   1 |
## 9   |   NwZl |    7  890   17 | -1322 689  25 |  -713 201  14 |
## 10  |  Austr |    7  973    8 | -1022 919  15 |  -247  54   2 |
## 11  |  Astrl |    7  958   11 | -1104 765  18 |  -554 193   8 |
## 12  |   Isrl |    7  615    4 |  -569 563   5 |   173  52   1 |
## 13  |   Grmn |    7  888    6 |  -882 842  11 |  -206  46   1 |
## 14  |   Cand |    7  951   10 | -1099 824  17 |  -433 128   5 |
## 15  |   Irln |    7  867   10 | -1070 750  16 |  -422 116   5 |
## 16  |   CstR |    7  695    5 |  -602 495   5 |   383 200   4 |
## 17  |   UntK |    7  856   10 |  -914 548  12 |  -685 308  13 |
## 18  |   CzcR |    7  885    6 |  -493 261   3 |   763 624  16 |
## 19  |   UntS |    7  665    2 |  -456 572   3 |  -184  93   1 |
## 20  |   Blgm |    7  864    4 |  -522 483   4 |   463 381   6 |
## 21  |   Frnc |    7  890    6 |  -812 717   9 |   399 173   4 |
## 22  |   Bhrn |    7  717    3 |  -533 573   4 |  -267 144   2 |
## 23  |   Malt |    7  873    6 |  -847 759  10 |  -328 114   3 |
## 24  |   TwPC |    7  883    2 |  -338 506   2 |   292 377   2 |
## 25  |   UnAE |    7  919    4 |  -555 578   4 |  -427 342   5 |
## 26  |   SdAr |    7  707    3 |  -431 485   3 |   291 222   2 |
## 27  |   Span |    7  762    5 |  -531 418   4 |   482 345   6 |
## 28  |   Itly |    7  552    5 |  -140  27   0 |   613 525  10 |
## 29  |   Slvn |    7  747    6 |  -754 606   8 |   364 142   4 |
## 30  |   Gtml |    7  139    2 |  -175 138   0 |   -16   1   0 |
## 31  |   Urgy |    7  893    4 |  -721 870   7 |   117  23   0 |
## 32  |   Sngp |    7  710   25 | -1535 644  34 |  -491  66   7 |
## 33  |   Kosv |    7  307    7 |    62   4   0 |  -544 303   8 |
## 34  |   Slvk |    7  762    4 |  -187  56   1 |   664 706  12 |
## 35  |   Brzl |    7  864    1 |  -199 344   1 |   244 520   2 |
## 36  |   Mexc |    7  646    2 |  -240 179   1 |   388 467   4 |
## 37  |   Jamc |    7  534    4 |  -270 140   1 |   453 394   6 |
## 38  |   Lthn |    7  941    4 |  -233  85   1 |   736 855  15 |
## 39  |   Cypr |    7  214    3 |  -138  42   0 |   277 171   2 |
## 40  |   Estn |    7  824    6 |  -829 815  10 |    85   8   0 |
## 41  |   Panm |    7  777    4 |  -382 243   2 |   566 534   9 |
## 42  |   Uzbk |    7  918   13 |  -775 320   9 | -1059 598  31 |
## 43  |   Chil |    7  542    2 |  -133  61   0 |   373 481   4 |
## 44  |   Plnd |    7  959    3 |  -442 445   3 |   475 514   6 |
## 45  |   Kzkh |    7  584    3 |  -411 457   2 |   217 127   1 |
## 46  |   Romn |    7  684    5 |   -32   1   0 |   699 682  13 |
## 47  |   Kuwt |    7  615    1 |  -272 395   1 |   203 220   1 |
## 48  |   Serb |    7  304    1 |  -127  87   0 |   201 217   1 |
## 49  |   ElSl |    7  112    2 |  -170 102   0 |    55  11   0 |
## 50  |   Mrts |    7  601    2 |  -343 450   2 |   199 151   1 |
## 51  |   Latv |    7  823    4 |   -96  18   0 |   650 806  12 |
## 52  |   Clmb |    7  733    2 |  -162  90   0 |   435 643   5 |
## 53  |   Hngr |    7  893    5 |  -170  41   0 |   775 852  17 |
## 54  |   Thln |    7  283    8 |  -262  56   1 |  -526 227   8 |
## 55  |   Ncrg |    7  982    1 |  -350 909   2 |   -99  73   0 |
## 56  |   Japn |    7  820    7 |  -618 351   5 |   715 469  14 |
## 57  |   Argn |    7  916    3 |  -296 171   1 |   618 745  11 |
## 58  |   Prtg |    7  772    7 |  -467 199   3 |   791 573  17 |
## 59  |   Hndr |    7  227    1 |  -130  79   0 |  -177 148   1 |
## 60  |   Crot |    7  726    5 |  -171  41   0 |   703 685  14 |
## 61  |   Phlp |    7   75    2 |  -154  69   0 |    49   7   0 |
## 62  |   SthK |    7  279    4 |   -65   7   0 |   409 272   5 |
## 63  |   Peru |    7  722    3 |   -64  10   0 |   535 711   8 |
## 64  |   BsnH |    7   44    4 |   132  29   0 |    97  16   0 |
## 65  |  Moldv |    7  446    2 |     6   0   0 |   369 446   4 |
## 66  |   Ecdr |    7  600    2 |  -146  73   0 |   392 527   4 |
## 67  |   Kyrg |    7  137    5 |  -250  81   1 |  -209  56   1 |
## 68  |   Grec |    7  741   11 |   199  24   1 |  1088 717  33 |
## 69  |   Bolv |    7  104    2 |    -8   0   0 |   153 104   1 |
## 70  |   Mngl |    7   28    4 |   129  27   0 |   -11   0   0 |
## 71  |   Prgy |    7  137    3 |  -216 125   1 |    65  11   0 |
## 72  |   Mntn |    7  419    2 |    16   1   0 |   320 418   3 |
## 73  |   DmnR |    7  614    2 |  -270 326   1 |   254 288   2 |
## 74  |   NrtC |    7  476    3 |  -445 474   3 |   -23   1   0 |
## 75  |   Blrs |    7  407    5 |   -66   6   0 |   552 401   8 |
## 76  |   Russ |    7  717    2 |    81  18   0 |   508 699   7 |
## 77  |   HKSA |    7  333   10 |  -645 293   6 |  -237  39   2 |
## 78  |   Tjks |    7  404    2 |  -314 375   1 |   -86  28   0 |
## 79  |   Vtnm |    7  432    3 |  -415 350   2 |   201  82   1 |
## 80  |   Liby |    7  120    1 |    57  29   0 |    99  91   0 |
## 81  |   Mlys |    7  263    3 |  -179  73   0 |  -288 190   2 |
## 82  |   Indn |    7  576   20 |    90   3   0 | -1310 574  47 |
## 83  |   CngB |    7  812    6 |   832 809  10 |   -53   3   0 |
## 84  |   Chin |    7  485    3 |  -360 291   2 |   294 194   2 |
## 85  |   IvrC |    7  840   10 |  1074 781  17 |  -295  59   2 |
## 86  |   Armn |    7  334    2 |  -205 137   1 |   245 197   2 |
## 87  |   Nepl |    7  774    2 |   134  66   0 |  -441 708   5 |
## 88  |   Blgr |    7  597    4 |  -118  26   0 |   556 572   9 |
## 89  |  Mldvs |    7  516    3 |  -444 473   3 |   135  43   1 |
## 90  |   Azrb |    7  262    5 |  -327 152   2 |   278 110   2 |
## 91  |   Cmrn |    7  905    6 |   881 858  11 |  -206  47   1 |
## 92  |   Sngl |    7  964    3 |   672 958   6 |    54   6   0 |
## 93  |   Albn |    7  216    3 |   275 161   1 |   160  55   1 |
## 94  |   NrtM |    7  289    2 |   252 254   1 |    93  35   0 |
## 95  |   Ghan |    7  781    4 |   546 481   4 |  -431 300   5 |
## 96  |  Niger |    7  738    7 |   721 506   7 |  -488 232   7 |
## 97  |   Trkm |    7  202   10 |  -268  49   1 |  -474 153   6 |
## 98  |   Gamb |    7  869   17 |   837 280  10 | -1213 589  41 |
## 99  |   Benn |    7  653   14 |  1070 540  16 |  -490 113   7 |
## 100 |   Laos |    7  738    5 |   109  17   0 |  -705 721  14 |
## 101 |   Bngl |    7  120    2 |    51   7   0 |  -201 113   1 |
## 102 |   Guin |    7  991    8 |   964 801  13 |  -468 189   6 |
## 103 |   SthA |    7  460    3 |   415 361   2 |   218  99   1 |
## 104 |   Trky |    7  563    6 |   371 154   2 |   605 409  10 |
## 105 |   Pkst |    7  965    5 |   764 748   8 |  -412 217   5 |
## 106 |   Mrcc |    7  334   10 |   590 239   5 |   372  95   4 |
## 107 |   Vnzl |    7  746    5 |   279  97   1 |   721 649  14 |
## 108 |   Gerg |    7  211    5 |   272  94   1 |   303 117   3 |
## 109 |   Algr |    7  394   10 |   624 252   6 |   468 141   6 |
## 110 |   Ukrn |    7  355    3 |   170  73   0 |   333 282   3 |
## 111 |   Iraq |    7  921    5 |   766 824   8 |   262  96   2 |
## 112 |   Gabn |    7  851    4 |   492 424   3 |   493 427   7 |
## 113 |   BrkF |    7  906    7 |   912 857  12 |  -217  49   1 |
## 114 |   Cmbd |    7  117    4 |    -3   0   0 |  -269 117   2 |
## 115 |   Mzmb |    7  557    5 |   313 134   1 |  -555 423   8 |
## 116 | Nigeri |    7  773    9 |   962 731  13 |  -232  42   1 |
## 117 |   Mali |    7  855    7 |   955 852  13 |   -56   3   0 |
## 118 |   Iran |    7  417    8 |   522 231   4 |  -468 185   6 |
## 119 |   Ugnd |    7  755    5 |   687 641   7 |  -289 114   2 |
## 120 |   Libr |    7  942    4 |   743 874   8 |  -207  68   1 |
## 121 |   Keny |    7  795    8 |   547 251   4 |  -805 544  18 |
## 122 |   Tuns |    7  752    7 |   577 343   5 |   630 409  11 |
## 123 |   Lbnn |    7  520    9 |   528 205   4 |   654 315  12 |
## 124 |   Namb |    7  728    4 |   553 491   4 |   385 238   4 |
## 125 |   PlsT |    7  859    4 |   438 330   3 |   554 529   8 |
## 126 |   Mynm |    7  857   18 |    64   1   0 | -1526 855  64 |
## 127 |   Jrdn |    7  476    2 |    74  20   0 |   357 457   4 |
## 128 |   Chad |    7  943   17 |  1519 907  33 |  -304  36   3 |
## 129 |   SrLn |    7   53    2 |   -79  24   0 |   -86  29   0 |
## 130 |   Swzl |    7  569   10 |   884 525  11 |   256  44   2 |
## 131 |   Cmrs |    7  839   12 |  1211 806  21 |  -247  33   2 |
## 132 |   Egyp |    7  762    3 |   393 381   2 |   392 380   4 |
## 133 |   Ethp |    7  923    2 |   440 624   3 |  -305 299   3 |
## 134 |   Mrtn |    7  610    8 |   789 529   9 |   307  80   3 |
## 135 |   Mdgs |    7  797    9 |  1025 793  15 |    71   4   0 |
## 136 |   Togo |    7  924   13 |  1272 873  23 |  -308  51   3 |
## 137 |   Zamb |    7  872    4 |   676 702   7 |  -332 169   3 |
## 138 |   SrrL |    7  965   11 |  1163 852  19 |  -424 113   5 |
## 139 |   Indi |    7  538    7 |   452 198   3 |  -592 340  10 |
## 140 |   Brnd |    7  724   18 |  1323 647  25 |  -456  77   6 |
## 141 |   Yemn |    7  693    7 |   697 476   7 |   471 217   6 |
## 142 |   Tnzn |    7  950    6 |   275  82   1 |  -894 868  22 |
## 143 |   Hait |    7  876   26 |  1312 450  25 | -1275 426  45 |
## 144 |   Malw |    7  721   10 |   869 535  11 |  -514 187   7 |
## 145 |   Lsth |    7  654   11 |   992 615  14 |   253  40   2 |
## 146 |   Btsw |    7  436    5 |   281 116   1 |   466 320   6 |
## 147 |   Rwnd |    7  430   22 |   -55   1   0 | -1187 429  39 |
## 148 |   Zmbb |    7  910    5 |   777 901   9 |    79   9   0 |
## 149 |   Afgh |    7  890   37 |  2193 878  69 |   257  12   2 |
## 
## Columns:
##     name   mass  qlt  inr    k=1 cor ctr    k=2 cor ctr  
## 1 | socl |  200  767  200 | -822 680 290 |  294  87  71 |
## 2 | life |  200  816  200 | -860 744 317 |  268  72  59 |
## 3 | chcs |  200  664  200 | -761 583 248 | -284  81  66 |
## 4 | gnrs |  200  782  200 |    7   0   0 | -881 782 638 |
## 5 | crrp |  200  544  200 |  582 341 145 |  449 203 166 |</code></pre>
<div class="question">
<p>What is the percentage of variance explained by each principal component?</p>
</div>
<div class="sourceCode" id="cb856"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb856-1"><a href="#cb856-1" tabindex="-1"></a>happy_pca<span class="sc">$</span>sv <span class="sc">^</span> <span class="dv">2</span> <span class="sc">/</span> <span class="fu">sum</span>(happy_pca<span class="sc">$</span>sv <span class="sc">^</span> <span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.46964540 0.24512744 0.14050280 0.09566051 0.04906384</code></pre>
<div class="question">
<p>What is the cumulative variance explained for each variable?</p>
</div>
<div class="sourceCode" id="cb858"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb858-1"><a href="#cb858-1" tabindex="-1"></a>vexp <span class="ot">&lt;-</span> <span class="cf">function</span>(feature, df, pca) {</span>
<span id="cb858-2"><a href="#cb858-2" tabindex="-1"></a>  reg_pc1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(df[, feature] <span class="sc">~</span> pca<span class="sc">$</span>rowpcoord[, <span class="dv">1</span>])</span>
<span id="cb858-3"><a href="#cb858-3" tabindex="-1"></a>  reg_pc1_pc2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(df[, feature] <span class="sc">~</span> pca<span class="sc">$</span>rowpcoord[, <span class="dv">1</span>] <span class="sc">+</span> pca<span class="sc">$</span>rowpcoord[, <span class="dv">2</span>])</span>
<span id="cb858-4"><a href="#cb858-4" tabindex="-1"></a>  </span>
<span id="cb858-5"><a href="#cb858-5" tabindex="-1"></a>  explained_pc1 <span class="ot">&lt;-</span> <span class="fu">cor</span>(<span class="fu">predict</span>(reg_pc1), df[, feature]) <span class="sc">^</span> <span class="dv">2</span></span>
<span id="cb858-6"><a href="#cb858-6" tabindex="-1"></a>  explained_pc1_pc2 <span class="ot">&lt;-</span> <span class="fu">cor</span>(<span class="fu">predict</span>(reg_pc1_pc2), df[, feature]) <span class="sc">^</span> <span class="dv">2</span></span>
<span id="cb858-7"><a href="#cb858-7" tabindex="-1"></a>  </span>
<span id="cb858-8"><a href="#cb858-8" tabindex="-1"></a>  <span class="fu">c</span>(explained_pc1, explained_pc1_pc2)</span>
<span id="cb858-9"><a href="#cb858-9" tabindex="-1"></a>}</span></code></pre></div>
<div class="sourceCode" id="cb859"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb859-1"><a href="#cb859-1" tabindex="-1"></a>var_explained <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, \(x) <span class="fu">vexp</span>(x, happy_standardized, happy_pca))</span>
<span id="cb859-2"><a href="#cb859-2" tabindex="-1"></a><span class="fu">colnames</span>(var_explained) <span class="ot">&lt;-</span> <span class="fu">colnames</span>(happy_standardized)</span>
<span id="cb859-3"><a href="#cb859-3" tabindex="-1"></a><span class="fu">rownames</span>(var_explained) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;PC1&quot;</span>, <span class="st">&quot;PC2&quot;</span>)</span>
<span id="cb859-4"><a href="#cb859-4" tabindex="-1"></a><span class="fu">t</span>(<span class="fu">round</span>(var_explained, <span class="dv">3</span>))</span></code></pre></div>
<pre><code>##              PC1   PC2
## social     0.680 0.767
## life       0.744 0.816
## choices    0.583 0.664
## generosity 0.000 0.782
## corruption 0.341 0.544</code></pre>
<div class="sourceCode" id="cb861"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb861-1"><a href="#cb861-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">colMeans</span>(<span class="fu">t</span>(var_explained)), <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##   PC1   PC2 
## 0.470 0.715</code></pre>
<div class="question">
<p>How do we build a correlation matrix of all variables?</p>
</div>
<div class="sourceCode" id="cb863"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb863-1"><a href="#cb863-1" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cor</span>(<span class="fu">cbind</span>(<span class="st">&quot;ladder&quot;</span> <span class="ot">=</span> happy<span class="sc">$</span>ladder_score, happy_standardized)), <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##            ladder social   life choices generosity corruption
## ladder      1.000  0.757  0.768   0.608     -0.018     -0.421
## social      0.757  1.000  0.723   0.483     -0.115     -0.203
## life        0.768  0.723  1.000   0.461     -0.162     -0.364
## choices     0.608  0.483  0.461   1.000      0.169     -0.401
## generosity -0.018 -0.115 -0.162   0.169      1.000     -0.164
## corruption -0.421 -0.203 -0.364  -0.401     -0.164      1.000</code></pre>
</div>
<div id="correlations-of-happiness-score-with-pc1-and-pc2" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Correlations of happiness score with PC1 and PC2<a href="#correlations-of-happiness-score-with-pc1-and-pc2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb865"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb865-1"><a href="#cb865-1" tabindex="-1"></a>row_principal <span class="ot">&lt;-</span> <span class="sc">-</span>happy_pca<span class="sc">$</span>rowpcoord[, <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>]</span>
<span id="cb865-2"><a href="#cb865-2" tabindex="-1"></a><span class="fu">colnames</span>(row_principal) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;PC1&quot;</span>, <span class="st">&quot;PC2&quot;</span>)</span>
<span id="cb865-3"><a href="#cb865-3" tabindex="-1"></a><span class="fu">round</span>(<span class="fu">cor</span>(<span class="fu">cbind</span>(<span class="st">&quot;ladder&quot;</span> <span class="ot">=</span> happy<span class="sc">$</span>ladder_score, row_principal)), <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##        ladder  PC1    PC2
## ladder  1.000 0.85 -0.067
## PC1     0.850 1.00  0.000
## PC2    -0.067 0.00  1.000</code></pre>
</div>
<div id="plot-first-two-principal-components" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Plot first two principal components<a href="#plot-first-two-principal-components" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb867"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb867-1"><a href="#cb867-1" tabindex="-1"></a>row_principal <span class="sc">|&gt;</span></span>
<span id="cb867-2"><a href="#cb867-2" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">|&gt;</span></span>
<span id="cb867-3"><a href="#cb867-3" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb867-4"><a href="#cb867-4" tabindex="-1"></a>    <span class="at">region =</span> happy<span class="sc">$</span>regional_indicator,</span>
<span id="cb867-5"><a href="#cb867-5" tabindex="-1"></a>    <span class="at">country =</span> happy<span class="sc">$</span>country_name</span>
<span id="cb867-6"><a href="#cb867-6" tabindex="-1"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb867-7"><a href="#cb867-7" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(PC1, <span class="sc">-</span>PC2, <span class="at">color =</span> region, <span class="at">label =</span> country)) <span class="sc">+</span></span>
<span id="cb867-8"><a href="#cb867-8" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">color =</span> <span class="st">&quot;gray70&quot;</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span></span>
<span id="cb867-9"><a href="#cb867-9" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">0</span>, <span class="at">color =</span> <span class="st">&quot;gray70&quot;</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>) <span class="sc">+</span></span>
<span id="cb867-10"><a href="#cb867-10" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="at">size =</span> <span class="dv">3</span>, <span class="at">show.legend =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb867-11"><a href="#cb867-11" tabindex="-1"></a>  <span class="fu">coord_equal</span>()</span></code></pre></div>
<p><img src="07-S07-D2_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
</div>
<div id="regress-happiness-on-the-five-indicators" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Regress happiness on the five indicators<a href="#regress-happiness-on-the-five-indicators" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb868"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb868-1"><a href="#cb868-1" tabindex="-1"></a>happy_standardized_pc <span class="ot">&lt;-</span></span>
<span id="cb868-2"><a href="#cb868-2" tabindex="-1"></a>  happy_standardized <span class="sc">|&gt;</span></span>
<span id="cb868-3"><a href="#cb868-3" tabindex="-1"></a>  <span class="fu">as_tibble</span>() <span class="sc">|&gt;</span></span>
<span id="cb868-4"><a href="#cb868-4" tabindex="-1"></a>  <span class="fu">mutate</span>(</span>
<span id="cb868-5"><a href="#cb868-5" tabindex="-1"></a>    <span class="at">ladder =</span> happy<span class="sc">$</span>ladder_score,</span>
<span id="cb868-6"><a href="#cb868-6" tabindex="-1"></a>    <span class="at">pc1 =</span> happy_pca<span class="sc">$</span>rowpcoord[, <span class="dv">1</span>],</span>
<span id="cb868-7"><a href="#cb868-7" tabindex="-1"></a>    <span class="at">pc2 =</span> happy_pca<span class="sc">$</span>rowpcoord[, <span class="dv">2</span>],</span>
<span id="cb868-8"><a href="#cb868-8" tabindex="-1"></a>    <span class="at">country =</span> happy<span class="sc">$</span>country_name,</span>
<span id="cb868-9"><a href="#cb868-9" tabindex="-1"></a>    <span class="at">region =</span> happy<span class="sc">$</span>regional_indicator</span>
<span id="cb868-10"><a href="#cb868-10" tabindex="-1"></a>  )</span>
<span id="cb868-11"><a href="#cb868-11" tabindex="-1"></a></span>
<span id="cb868-12"><a href="#cb868-12" tabindex="-1"></a>happiness_full <span class="ot">&lt;-</span> </span>
<span id="cb868-13"><a href="#cb868-13" tabindex="-1"></a>  <span class="fu">lm</span>(ladder <span class="sc">~</span> social <span class="sc">+</span> life <span class="sc">+</span> choices <span class="sc">+</span> generosity <span class="sc">+</span> corruption,</span>
<span id="cb868-14"><a href="#cb868-14" tabindex="-1"></a>     <span class="at">data =</span> happy_standardized_pc)</span>
<span id="cb868-15"><a href="#cb868-15" tabindex="-1"></a></span>
<span id="cb868-16"><a href="#cb868-16" tabindex="-1"></a><span class="fu">summary</span>(happiness_full)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = ladder ~ social + life + choices + generosity + 
##     corruption, data = happy_standardized_pc)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.63771 -0.26591  0.02327  0.37120  1.37329 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  5.53284    0.04581 120.776  &lt; 2e-16 ***
## social       0.40177    0.06975   5.760 4.96e-08 ***
## life         0.38823    0.07217   5.380 2.98e-07 ***
## choices      0.21977    0.05783   3.801 0.000213 ***
## generosity   0.03033    0.04933   0.615 0.539620    
## corruption  -0.13599    0.05301  -2.565 0.011343 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5592 on 143 degrees of freedom
## Multiple R-squared:  0.738,  Adjusted R-squared:  0.7289 
## F-statistic: 80.57 on 5 and 143 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="question">
<p>What is the correlation between observed y and fitted y?</p>
</div>
<div class="sourceCode" id="cb870"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb870-1"><a href="#cb870-1" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">summary</span>(happiness_full)<span class="sc">$</span>r.squared)</span></code></pre></div>
<pre><code>## [1] 0.8590876</code></pre>
</div>
<div id="visualise-happiness-against-principal-components" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Visualise happiness against principal components<a href="#visualise-happiness-against-principal-components" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb872"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb872-1"><a href="#cb872-1" tabindex="-1"></a>happy_standardized_pc <span class="sc">|&gt;</span></span>
<span id="cb872-2"><a href="#cb872-2" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(</span>
<span id="cb872-3"><a href="#cb872-3" tabindex="-1"></a>    <span class="at">cols =</span> <span class="fu">c</span>(pc1, pc2),</span>
<span id="cb872-4"><a href="#cb872-4" tabindex="-1"></a>    <span class="at">names_to =</span> <span class="st">&quot;PC&quot;</span>,</span>
<span id="cb872-5"><a href="#cb872-5" tabindex="-1"></a>    <span class="at">values_to =</span> <span class="st">&quot;value&quot;</span></span>
<span id="cb872-6"><a href="#cb872-6" tabindex="-1"></a>  ) <span class="sc">|&gt;</span></span>
<span id="cb872-7"><a href="#cb872-7" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(value, ladder)) <span class="sc">+</span></span>
<span id="cb872-8"><a href="#cb872-8" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">color =</span> region)) <span class="sc">+</span></span>
<span id="cb872-9"><a href="#cb872-9" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(</span>
<span id="cb872-10"><a href="#cb872-10" tabindex="-1"></a>    <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>,</span>
<span id="cb872-11"><a href="#cb872-11" tabindex="-1"></a>    <span class="at">formula =</span> y <span class="sc">~</span> x,</span>
<span id="cb872-12"><a href="#cb872-12" tabindex="-1"></a>    <span class="at">linewidth =</span> <span class="fl">0.5</span>,</span>
<span id="cb872-13"><a href="#cb872-13" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">&quot;grey20&quot;</span>,</span>
<span id="cb872-14"><a href="#cb872-14" tabindex="-1"></a>    <span class="at">se =</span> <span class="cn">FALSE</span></span>
<span id="cb872-15"><a href="#cb872-15" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb872-16"><a href="#cb872-16" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="st">&quot;PC Value&quot;</span>) <span class="sc">+</span></span>
<span id="cb872-17"><a href="#cb872-17" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="st">&quot;Cantrill Ladder Happiness Score&quot;</span>) <span class="sc">+</span></span>
<span id="cb872-18"><a href="#cb872-18" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span> PC) <span class="sc">+</span></span>
<span id="cb872-19"><a href="#cb872-19" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>))</span></code></pre></div>
<p><img src="07-S07-D2_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
</div>
<div id="regress-happiness-on-first-principal-component" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Regress happiness on first principal component<a href="#regress-happiness-on-first-principal-component" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="sourceCode" id="cb873"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb873-1"><a href="#cb873-1" tabindex="-1"></a>happiness_reduced <span class="ot">&lt;-</span></span>
<span id="cb873-2"><a href="#cb873-2" tabindex="-1"></a>  <span class="fu">lm</span>(<span class="at">formula =</span> ladder <span class="sc">~</span> pc1, <span class="at">data =</span> happy_standardized_pc)</span>
<span id="cb873-3"><a href="#cb873-3" tabindex="-1"></a></span>
<span id="cb873-4"><a href="#cb873-4" tabindex="-1"></a><span class="fu">summary</span>(happiness_reduced)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = ladder ~ pc1, data = happy_standardized_pc)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.19102 -0.26848  0.06941  0.40025  1.20478 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  5.53284    0.04645  119.11   &lt;2e-16 ***
## pc1         -1.33258    0.06801  -19.59   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.567 on 147 degrees of freedom
## Multiple R-squared:  0.7231, Adjusted R-squared:  0.7212 
## F-statistic: 383.9 on 1 and 147 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb875"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb875-1"><a href="#cb875-1" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fu">summary</span>(happiness_reduced)<span class="sc">$</span>r.squared)</span></code></pre></div>
<pre><code>## [1] 0.8503647</code></pre>
</div>
<div id="task" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">👉 TASK<a href="#task" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Now let’s consider PCA using six features. In the example of PCA above, we did not use <code>logged_gdp_per_capita</code> feature for the PCA analysis.</p>
<p>Run PCA using the original five features (<code>ladder</code>, <code>social</code>, <code>life</code>,
<code>choices</code>, <code>corruption</code>) and <code>logged_gdp_per_capita</code>. You may need to do
some preprocessing first.</p>
<blockquote>
<p>Your code here</p>
</blockquote>
<p>Now regress happiness on the first principal component from your analysis.</p>
<blockquote>
<p>Your code here</p>
</blockquote>
<p>How does this compare to the previous analysis with five features? Consider
here the R^2 value and the correlation between the observed and fitted values.</p>
<blockquote>
<p>Your code here</p>
</blockquote>
<!--chapter:end:07-S07-D2.Rmd-->
</div>
</div>
<div id="part-section-8" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">(PART*) Section 8<a href="#part-section-8" class="anchor-section" aria-label="Anchor link to header"></a></h1>
</div>
<div id="overview-7" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Overview<a href="#overview-7" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div style="color: #333; font-size: 24px; font-style: italic; text-align: justify;">
<p>Section 8: Case Study and Formative Assessment</p>
</div>
<p>This section is comprised of a demonstration on utilising ChatGPT for
data science projects developed by Dr. Tatjana Kecojevic, Lecturer in
Social Statistics.</p>
<p><strong>Leveraging ChatGPT for Enhanced Data Science Projects</strong></p>
<p>Engaging with <a href="https://chatgpt.com/">ChatGPT</a> during data science
projects in R offers a versatile and innovative approach to
problem-solving and project management. As an AI-powered assistant,
ChatGPT efficiently addresses complex challenges at various stages of
the project lifecycle, serving as a reliable resource for guidance on
best practices, educational insights, and time-saving solutions. By
providing instant responses and suggestions, ChatGPT streamlines data
preprocessing, model selection, and result interpretation, enabling
users to focus more on analysis and decision-making.</p>
<p>ChatGPT’s comprehensive support spans data visualization, statistical
analysis, machine learning model building, and beyond, making it an
invaluable asset for both novice and experienced data scientists. Its
innovative approach encourages experimentation and exploration,
fostering creativity and pushing the boundaries of traditional data
analysis methodologies. Integrating ChatGPT into data science projects
in R not only enhances efficiency but also promotes continuous learning
and innovation in the field.</p>
<p>In this tutorial, we will learn how to utilise ChatGPT for an end-to-end
data science project. We will use various prompts to create a project
outline, write R code, conduct research, and debug applications.
Additionally, we will provide tips on crafting effective ChatGPT
prompts.</p>
<p>The focus of this tutorial is a single AI project suitable for
intermediate practitioners. This project builds on foundational skills
and is designed to be both challenging and engaging, offering a
practical way to enhance your abilities. You will learn how to handle
datasets, build and train models, and evaluate their performance,
providing a comprehensive understanding of the AI project lifecycle. For
this, we will use the <a href="https://www.kaggle.com/datasets/fedesoriano/spanish-wine-quality-dataset?resource=download">Spanish Wine Quality
Dataset</a>
available from <a href="https://www.kaggle.com/">kaggle</a>.</p>
<div class="ilos">
<p><strong>Learning Outcomes:</strong></p>
<ul>
<li>Utilise ChatGPT to guide data preprocessing and selection of machine
learning algorithms and techniques;</li>
<li>Interpret complex models using ChatGPT;</li>
<li>Summarise findings, create code, and generate reports using ChatGPT;</li>
<li>Write effective prompts.</li>
</ul>
</div>
<!--chapter:end:08-S08-overview.Rmd-->
</div>
<div id="demonstration" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">Demonstration<a href="#demonstration" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Enhancing Data Science Projects with ChatGPT: A Case Study in R.</em></p>
<div id="data-set" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Data Set<a href="#data-set" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Spanish Wine Quality Dataset focuses on red wine variants from Spain, detailing various metrics related to their popularity and characteristics, and their influence on quality. It is suitable for classification or regression tasks, with the goal being to predict the quality or price of the wines. Note that the quality ratings range from nearly 5 to 4 points and are not evenly distributed. The dataset comprises 7,500 different types of Spanish red wines, each described by 11 features including price, rating, and flavor descriptors. The data was meticulously gathered through web scraping from diverse sources, such as specialised wine websites and supermarkets.</p>
<p>Attribute Information</p>
<ul>
<li>winery: Name of the winery</li>
<li>wine: Name of the wine</li>
<li>year: Harvest year of the grapes</li>
<li>rating: Average user rating (1-5)</li>
<li>num_reviews: Number of user reviews</li>
<li>country: Country of origin (Spain)</li>
<li>region: Wine-producing region</li>
<li>price: Price in euros (€)</li>
<li>type: Wine variety</li>
<li>body: Body score, indicating the richness and weight of the wine (1-5)</li>
<li>acidity: Acidity score, indicating the wine’s tartness and refreshing quality (1-5)</li>
</ul>
<p>To start our AI-aided data science project, we’ll need to download the dataset named <code>wines_SPA.csv</code>. Save this dataset in a “data” folder within a new project directory called <code>wines_SPA</code>.</p>
<p><img src="images/data_folder.png" width="75%" style="display: block; margin: auto;" /></p>
<p>Before we begin, let’s take an initial look at the dataset. Open the downloaded csv file to ensure it’s functional and contains the data we need.</p>
<p><img src="images/wines_SPA_csv.png" width="85%" style="display: block; margin: auto;" /></p>
<p>After that, we’ll set up an R project file and dive into our data science project.</p>
<p><img src="images/Rproj.png" width="85%" style="display: block; margin: auto;" /></p>
<p><em>Exciting times ahead!</em></p>
</div>
<div id="project-planning-setting-the-stage-for-success" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Project Planning: Setting the Stage for Success<a href="#project-planning-setting-the-stage-for-success" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Project planning is a critical phase where we assess available resources and goals to develop an optimal strategy. To initiate this process, visit <a href="chat.openai.com">chatGPT</a> and start a new chat. Mention the availability of the Spanish Wine Quality dataset and ask ChatGPT to provide steps for building an end-to-end generic portfolio project. This step lays the groundwork for a successful project by defining clear objectives and identifying the necessary resources.</p>
<p><span style="color:red"><strong>Prompt:</strong> </span> <span style="color:gray"> <em>I have The Spanish Wine Quality Dataset, which centers on red wine variations from Spain. It provides detailed metrics regarding their popularity, characteristics, and how these factors influence quality. This dataset is ideal for classification or regression tasks, aiming to predict the quality or price of wines. It’s worth noting that the quality ratings range from nearly 5 to 4 points and are not evenly distributed. The dataset includes 7,500 different types of Spanish red wines, each described by 11 features such as price, rating, and flavor descriptors. Could you guide me through the necessary steps for this data science project using this dataset</em></span></p>
<p><span style="color:red"><strong>ChatGPT</strong></span>
<img src="images/prompt1.gif" width="85%" style="display: block; margin: auto;" /></p>
<p>To be in line with our adopted practice, we will summarise ChatGPT’s suggestions into the following set of steps:</p>
<ol style="list-style-type: lower-roman">
<li>Data Preprocessing + Feature Engineering<br />
</li>
<li>Exploratory Data Analysis (EDA) + Feature Engineering if still needed<br />
</li>
<li>Model Selection and Model Training<br />
</li>
<li>Model Evaluation<br />
</li>
<li>Model Interpretation<br />
</li>
<li>Conclusion</li>
</ol>
</div>
<div id="data-preprocessing" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Data Preprocessing<a href="#data-preprocessing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>As we use ChatGPT to assist us in this project, some steps will be performed without its aid. These include loading the necessary packages, uploading data, and converting some character-type variables into appropriate factor types.</p>
<p>❗️<span style="color:green"> Please ensure that you have installed all necessary packages before proceeding with these steps. </span></p>
<div class="sourceCode" id="cb877"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb877-1"><a href="#cb877-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Load packages</span></span>
<span id="cb877-2"><a href="#cb877-2" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># Note that you will need to install some of these packages</span></span>
<span id="cb877-3"><a href="#cb877-3" tabindex="-1"></a><span class="er">&gt;</span> </span>
<span id="cb877-4"><a href="#cb877-4" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">library</span>(ggplot2) <span class="co"># For creating visualisations.</span></span>
<span id="cb877-5"><a href="#cb877-5" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">library</span>(dplyr) <span class="co"># For wrangling data.</span></span>
<span id="cb877-6"><a href="#cb877-6" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">library</span>(tidyr) <span class="co"># For data tidying operations.</span></span>
<span id="cb877-7"><a href="#cb877-7" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">library</span>(reshape2) <span class="co"># For reshaping data frames.</span></span>
<span id="cb877-8"><a href="#cb877-8" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">library</span>(caret) <span class="co"># For data preprocessing tasks like scaling and imputing missing values and ML.</span></span>
<span id="cb877-9"><a href="#cb877-9" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">library</span>(glmnet) <span class="co"># For fitting Lasso and Ridge regression models.</span></span>
<span id="cb877-10"><a href="#cb877-10" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">library</span>(randomForest) <span class="co"># For fitting Random Forest models.</span></span>
<span id="cb877-11"><a href="#cb877-11" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">library</span>(rpart) <span class="co"># For fitting and visualising decision trees.</span></span>
<span id="cb877-12"><a href="#cb877-12" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">library</span>(rpart.plot) <span class="co"># For visualising decision trees created with the `rpart` package.</span></span>
<span id="cb877-13"><a href="#cb877-13" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">library</span>(splines) <span class="co"># For creating spline basis functions for use in generalised linear models (GLMs)</span></span>
<span id="cb877-14"><a href="#cb877-14" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb877-15"><a href="#cb877-15" tabindex="-1"></a><span class="er">&gt;</span> <span class="co">#read data</span></span>
<span id="cb877-16"><a href="#cb877-16" tabindex="-1"></a><span class="er">&gt;</span> wine_data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">&#39;data/wines_SPA.csv&#39;</span>,</span>
<span id="cb877-17"><a href="#cb877-17" tabindex="-1"></a><span class="sc">+</span>                     <span class="at">header=</span><span class="cn">TRUE</span>, </span>
<span id="cb877-18"><a href="#cb877-18" tabindex="-1"></a><span class="sc">+</span>                     <span class="at">na.strings=</span><span class="fu">c</span>(<span class="st">&quot;&quot;</span>,<span class="st">&quot;NA&quot;</span>),</span>
<span id="cb877-19"><a href="#cb877-19" tabindex="-1"></a><span class="sc">+</span>                     <span class="at">stringsAsFactor =</span> <span class="cn">FALSE</span>) </span>
<span id="cb877-20"><a href="#cb877-20" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">glimpse</span>(wine_data)</span></code></pre></div>
<pre><code>## Rows: 7,500
## Columns: 11
## $ winery      &lt;chr&gt; &quot;Teso La Monja&quot;, &quot;Artadi&quot;, &quot;Vega Sicilia&quot;, &quot;Vega Sicilia&quot;,…
## $ wine        &lt;chr&gt; &quot;Tinto&quot;, &quot;Vina El Pison&quot;, &quot;Unico&quot;, &quot;Unico&quot;, &quot;Unico&quot;, &quot;Unic…
## $ year        &lt;chr&gt; &quot;2013&quot;, &quot;2018&quot;, &quot;2009&quot;, &quot;1999&quot;, &quot;1996&quot;, &quot;1998&quot;, &quot;2010&quot;, &quot;1…
## $ rating      &lt;dbl&gt; 4.9, 4.9, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8…
## $ num_reviews &lt;int&gt; 58, 31, 1793, 1705, 1309, 1209, 1201, 926, 643, 630, 591, …
## $ country     &lt;chr&gt; &quot;Espana&quot;, &quot;Espana&quot;, &quot;Espana&quot;, &quot;Espana&quot;, &quot;Espana&quot;, &quot;Espana&quot;…
## $ region      &lt;chr&gt; &quot;Toro&quot;, &quot;Vino de Espana&quot;, &quot;Ribera del Duero&quot;, &quot;Ribera del …
## $ price       &lt;dbl&gt; 995.0000, 313.5000, 324.9500, 692.9600, 778.0600, 490.0000…
## $ type        &lt;chr&gt; &quot;Toro Red&quot;, &quot;Tempranillo&quot;, &quot;Ribera Del Duero Red&quot;, &quot;Ribera…
## $ body        &lt;int&gt; 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5…
## $ acidity     &lt;int&gt; 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3…</code></pre>
<p>Next, we will convert character types into factors and change the <code>year</code> variable to an integer type. We will also remove the <code>country</code> variable since all the wine is from Spain.</p>
<div class="sourceCode" id="cb879"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb879-1"><a href="#cb879-1" tabindex="-1"></a><span class="sc">&gt;</span> wine_data [,<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(wine_data[,<span class="dv">1</span>])</span>
<span id="cb879-2"><a href="#cb879-2" tabindex="-1"></a><span class="sc">&gt;</span> wine_data [,<span class="dv">2</span>] <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(wine_data[,<span class="dv">2</span>])</span>
<span id="cb879-3"><a href="#cb879-3" tabindex="-1"></a><span class="sc">&gt;</span> wine_data [,<span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="fu">as.integer</span>(wine_data[,<span class="dv">3</span>])</span>
<span id="cb879-4"><a href="#cb879-4" tabindex="-1"></a><span class="sc">&gt;</span> wine_data [,<span class="dv">7</span>] <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(wine_data[,<span class="dv">7</span>])</span>
<span id="cb879-5"><a href="#cb879-5" tabindex="-1"></a><span class="sc">&gt;</span> wine_data [,<span class="dv">9</span>] <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(wine_data[,<span class="dv">9</span>])</span>
<span id="cb879-6"><a href="#cb879-6" tabindex="-1"></a><span class="sc">&gt;</span> wine_data [,<span class="dv">10</span>] <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(wine_data[,<span class="dv">10</span>])</span>
<span id="cb879-7"><a href="#cb879-7" tabindex="-1"></a><span class="sc">&gt;</span> wine_data [,<span class="dv">11</span>] <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(wine_data[,<span class="dv">11</span>])</span>
<span id="cb879-8"><a href="#cb879-8" tabindex="-1"></a><span class="sc">&gt;</span> wine_data <span class="ot">&lt;-</span> wine_data[, <span class="sc">!</span>(<span class="fu">names</span>(wine_data) <span class="sc">%in%</span> <span class="fu">c</span>(<span class="st">&quot;country&quot;</span>))]</span>
<span id="cb879-9"><a href="#cb879-9" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">summary</span>(wine_data)</span></code></pre></div>
<pre><code>##               winery                wine           year          rating     
##  Contino         : 457   Reserva      : 467   Min.   :1910   Min.   :4.200  
##  Artadi          : 261   Gran Reserva : 458   1st Qu.:2011   1st Qu.:4.200  
##  La Rioja Alta   : 254   Rioja Reserva: 240   Median :2015   Median :4.200  
##  Sierra Cantabria: 237   El Viejo     : 224   Mean   :2013   Mean   :4.255  
##  Matarromera     : 232   Corimbo I    : 223   3rd Qu.:2017   3rd Qu.:4.200  
##  Vina Pedrosa    : 230   Mirto        : 223   Max.   :2021   Max.   :4.900  
##  (Other)         :5829   (Other)      :5665   NA&#39;s   :290                   
##   num_reviews                   region         price        
##  Min.   :   25.0   Rioja           :2440   Min.   :   4.99  
##  1st Qu.:  389.0   Ribera del Duero:1413   1st Qu.:  18.90  
##  Median :  404.0   Priorato        : 686   Median :  28.53  
##  Mean   :  451.1   Toro            : 300   Mean   :  60.10  
##  3rd Qu.:  415.0   Vino de Espana  : 263   3rd Qu.:  51.35  
##  Max.   :32624.0   Rias Baixas     : 252   Max.   :3119.08  
##                    (Other)         :2146                    
##                    type        body      acidity    
##  Rioja Red           :2357   2   :  34   1   :  35  
##  Ribera Del Duero Red:1407   3   : 553   2   : 268  
##  Red                 : 864   4   :4120   3   :6028  
##  Priorat Red         : 674   5   :1624   NA&#39;s:1169  
##  Toro Red            : 296   NA&#39;s:1169              
##  (Other)             :1357                          
##  NA&#39;s                : 545</code></pre>
<div class="sourceCode" id="cb881"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb881-1"><a href="#cb881-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">dim</span>(wine_data)</span></code></pre></div>
<pre><code>## [1] 7500   10</code></pre>
<p>We have observed the presence of missing values in our dataset. Given the substantial size of our dataset, we can opt to remove rows that contain <code>NA</code> values.</p>
<div class="sourceCode" id="cb883"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb883-1"><a href="#cb883-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># contains only the rows where there are no NAs in the specified columns.</span></span>
<span id="cb883-2"><a href="#cb883-2" tabindex="-1"></a><span class="er">&gt;</span> wine_data <span class="ot">&lt;-</span> wine_data[<span class="fu">complete.cases</span>(wine_data[, <span class="fu">c</span>(<span class="st">&quot;body&quot;</span>, <span class="st">&quot;acidity&quot;</span>, <span class="st">&quot;type&quot;</span>)]), ]</span>
<span id="cb883-3"><a href="#cb883-3" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Handle missing values in the &#39;year&#39; variable</span></span>
<span id="cb883-4"><a href="#cb883-4" tabindex="-1"></a><span class="er">&gt;</span> wine_data<span class="sc">$</span>year[<span class="fu">is.na</span>(wine_data<span class="sc">$</span>year)] <span class="ot">&lt;-</span> <span class="fu">median</span>(wine_data<span class="sc">$</span>year, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb883-5"><a href="#cb883-5" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Check the number of missing values after removal</span></span>
<span id="cb883-6"><a href="#cb883-6" tabindex="-1"></a><span class="er">&gt;</span> missing_values <span class="ot">&lt;-</span> <span class="fu">colSums</span>(<span class="fu">is.na</span>(wine_data))</span>
<span id="cb883-7"><a href="#cb883-7" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">print</span>(missing_values)</span></code></pre></div>
<pre><code>##      winery        wine        year      rating num_reviews      region 
##           0           0           0           0           0           0 
##       price        type        body     acidity 
##           0           0           0           0</code></pre>
<div class="sourceCode" id="cb885"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb885-1"><a href="#cb885-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Summary of the dataset</span></span>
<span id="cb885-2"><a href="#cb885-2" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">summary</span>(wine_data)</span></code></pre></div>
<pre><code>##               winery                 wine           year          rating    
##  Contino         : 414   Reserva       : 422   Min.   :1910   Min.   :4.20  
##  Artadi          : 239   Gran Reserva  : 415   1st Qu.:2011   1st Qu.:4.20  
##  La Rioja Alta   : 228   Rioja Reserva : 218   Median :2015   Median :4.20  
##  Sierra Cantabria: 215   Corimbo I     : 202   Mean   :2013   Mean   :4.26  
##  Vina Pedrosa    : 207   El Viejo      : 202   3rd Qu.:2017   3rd Qu.:4.20  
##  Imperial        : 206   Rioja Graciano: 202   Max.   :2021   Max.   :4.90  
##  (Other)         :4822   (Other)       :4670                                
##   num_reviews                     region         price        
##  Min.   :   25.0   Rioja             :2221   Min.   :   4.99  
##  1st Qu.:  388.0   Ribera del Duero  :1281   1st Qu.:  19.98  
##  Median :  402.0   Priorato          : 622   Median :  29.15  
##  Mean   :  444.1   Toro              : 264   Mean   :  65.71  
##  3rd Qu.:  415.0   Vino de Espana    : 240   3rd Qu.:  60.95  
##  Max.   :32624.0   Jerez-Xeres-Sherry: 225   Max.   :3119.08  
##                    (Other)           :1478                    
##                    type      body     acidity 
##  Rioja Red           :2143   2:  34   1:  35  
##  Ribera Del Duero Red:1278   3: 553   2: 268  
##  Red                 : 787   4:4120   3:6028  
##  Priorat Red         : 620   5:1624           
##  Tempranillo         : 268                    
##  Toro Red            : 261                    
##  (Other)             : 974</code></pre>
<div class="sourceCode" id="cb887"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb887-1"><a href="#cb887-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Structure of the dataset</span></span>
<span id="cb887-2"><a href="#cb887-2" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">glimpse</span>(wine_data)</span></code></pre></div>
<pre><code>## Rows: 6,331
## Columns: 10
## $ winery      &lt;fct&gt; Teso La Monja, Artadi, Vega Sicilia, Vega Sicilia, Vega Si…
## $ wine        &lt;fct&gt; Tinto, Vina El Pison, Unico, Unico, Unico, Unico, Unico, U…
## $ year        &lt;dbl&gt; 2013, 2018, 2009, 1999, 1996, 1998, 2010, 1995, 2015, 2011…
## $ rating      &lt;dbl&gt; 4.9, 4.9, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8, 4.8…
## $ num_reviews &lt;int&gt; 58, 31, 1793, 1705, 1309, 1209, 1201, 926, 643, 630, 591, …
## $ region      &lt;fct&gt; Toro, Vino de Espana, Ribera del Duero, Ribera del Duero, …
## $ price       &lt;dbl&gt; 995.0000, 313.5000, 324.9500, 692.9600, 778.0600, 490.0000…
## $ type        &lt;fct&gt; Toro Red, Tempranillo, Ribera Del Duero Red, Ribera Del Du…
## $ body        &lt;fct&gt; 5, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 4, 5…
## $ acidity     &lt;fct&gt; 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 3, 3, 3, 3…</code></pre>
<p>As we conclude the data preprocessing step, we have transformed character types into factors, changed the year variable to an integer type, and removed the country variable, considering that all wines are from Spain. Additionally, we have addressed missing values by removing rows containing NA values. With these preparations completed, we are now ready to proceed to the next step: Exploratory Data Analysis (EDA), where we will delve deeper into the dataset to gain insights and prepare for the model selection process.</p>
</div>
<div id="exploratory-data-analysis-eda" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Exploratory Data Analysis (EDA)<a href="#exploratory-data-analysis-eda" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Exploratory Data Analysis (EDA) is a crucial step in the data science process that involves exploring and understanding the structure, patterns, and relationships in a dataset. By visually and statistically analysing the data, data scientists can uncover insights, identify patterns, and detect anomalies that can inform subsequent analysis and modelling decisions. In this section, we will perform EDA on the wine dataset to gain a deeper understanding of its features and prepare for the modelling phase. Through visualisations and summary statistics, we aim to uncover meaningful patterns and relationships in the data, which will guide our further analysis and modelling strategies.</p>
<p>Let’s begin by presenting the dataset’s size and complexity, starting with the number of rows and columns. Additionally, we’ll compute summary statistics for numerical variables to gain insights into their distribution and range. This analysis can help us identify potential outliers and better understand the overall structure of the data.</p>
<div class="sourceCode" id="cb889"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb889-1"><a href="#cb889-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Display the number of rows and columns</span></span>
<span id="cb889-2"><a href="#cb889-2" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">cat</span>(<span class="st">&quot;Number of rows:&quot;</span>, <span class="fu">nrow</span>(wine_data), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Number of rows: 6331</code></pre>
<div class="sourceCode" id="cb891"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb891-1"><a href="#cb891-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">cat</span>(<span class="st">&quot;Number of columns:&quot;</span>, <span class="fu">ncol</span>(wine_data), <span class="st">&quot;</span><span class="sc">\n\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Number of columns: 10</code></pre>
<div class="sourceCode" id="cb893"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb893-1"><a href="#cb893-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Statistical summary for numerical variables</span></span>
<span id="cb893-2"><a href="#cb893-2" tabindex="-1"></a><span class="er">&gt;</span> num_vars <span class="ot">&lt;-</span> <span class="fu">sapply</span>(wine_data, is.numeric)</span>
<span id="cb893-3"><a href="#cb893-3" tabindex="-1"></a><span class="sc">&gt;</span> num_data <span class="ot">&lt;-</span> wine_data[, num_vars]</span>
<span id="cb893-4"><a href="#cb893-4" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">summary</span>(num_data)</span></code></pre></div>
<pre><code>##       year          rating      num_reviews          price        
##  Min.   :1910   Min.   :4.20   Min.   :   25.0   Min.   :   4.99  
##  1st Qu.:2011   1st Qu.:4.20   1st Qu.:  388.0   1st Qu.:  19.98  
##  Median :2015   Median :4.20   Median :  402.0   Median :  29.15  
##  Mean   :2013   Mean   :4.26   Mean   :  444.1   Mean   :  65.71  
##  3rd Qu.:2017   3rd Qu.:4.20   3rd Qu.:  415.0   3rd Qu.:  60.95  
##  Max.   :2021   Max.   :4.90   Max.   :32624.0   Max.   :3119.08</code></pre>
<p>From the summary statistics, we can draw the following conclusions:</p>
<ul>
<li><code>year</code>: The wines in the dataset span a wide range of years, from 1910 to 2021, with the majority falling between 2011 and 2017.<br />
</li>
<li><code>rating</code>: The ratings for the wines range from 4.2 to 4.9, with an average rating of approximately 4.26. The ratings appear to be quite high, with the median also at 4.2.<br />
</li>
<li><code>num_reviews</code>: The number of reviews for the wines varies widely, with a minimum of 25 reviews and a maximum of 16505 reviews. The average number of reviews is approximately 440, indicating that some wines have received significant attention and feedback.<br />
</li>
<li><code>price</code>: The prices of the wines range from 6.26 to 3119.08, with an average price of approximately 67.40. However, the distribution of prices appears to be skewed, as the median price is 31.63, which is much lower than the mean.</li>
</ul>
<p>Now, we will utilise ChatGPT to aid us in conducting Exploratory Data Analysis (EDA) on the <code>wine_data</code> dataset. Through a combination of visualisations and statistical analyses, we aim to explore the distribution of key numerical variables such as rating, num_reviews, and price. Additionally, we will investigate the potential impact of categorical variables such as region, type and acidity on these key metrics.</p>
<p><span style="color:red"><strong>Prompt:</strong> </span> <span style="color:gray">
Please provide an R code for creating visually appealing visualisations of key numerical variables such as rating, num_reviews, and price. Additionally, analyse the potential impact of categorical variables such as region, and type on these key metrics. The dataset has a structure of 6,331 rows and 10 columns, with variables including winery, wine, year, rating, num_reviews, region, price, type, body, and acidity.</span></p>
<pre><code>&gt; glimpse(wine_data)
Rows: 6,331
Columns: 10
$ winery      &lt;fct&gt; Teso La Monja, Artadi, Vega…
$ wine        &lt;fct&gt; Tinto, Vina El Pison, Unico…
$ year        &lt;dbl&gt; 2013, 2018, 2009, 1999, 199…
$ rating      &lt;dbl&gt; 4.9, 4.9, 4.8, 4.8, 4.8, 4.…
$ num_reviews &lt;int&gt; 58, 31, 1793, 1705, 1309, 1…
$ region      &lt;fct&gt; Toro, Vino de Espana, Riber…
$ price       &lt;dbl&gt; 995.00, 313.50, 324.95, 692…
$ type        &lt;fct&gt; Toro Red, Tempranillo, Ribe…
$ body        &lt;fct&gt; 5, 4, 5, 5, 5, 5, 5, 5, 5, …
$ acidity     &lt;fct&gt; 3, 2, 3, 3, 3, 3, 3, 3, 3, …</code></pre>
<p>❗️<span style="color:green"> Make sure to include the <code>glimpse()</code> output in your prompt question to give an overview of your data’s structure!</span></p>
<p><span style="color:red"><strong>ChatGPT</strong></span>
<img src="images/prompt2.gif" width="85%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb896"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb896-1"><a href="#cb896-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Create visualisations</span></span>
<span id="cb896-2"><a href="#cb896-2" tabindex="-1"></a><span class="er">&gt;</span> </span>
<span id="cb896-3"><a href="#cb896-3" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># Histogram of Ratings</span></span>
<span id="cb896-4"><a href="#cb896-4" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">ggplot</span>(wine_data, <span class="fu">aes</span>(<span class="at">x =</span> rating)) <span class="sc">+</span> </span>
<span id="cb896-5"><a href="#cb896-5" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">geom_histogram</span>(<span class="at">binwidth =</span> <span class="fl">0.1</span>, <span class="at">fill =</span> <span class="st">&quot;skyblue&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb896-6"><a href="#cb896-6" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Distribution of Ratings&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Rating&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Frequency&quot;</span>)</span></code></pre></div>
<p><img src="08-S08-D1_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<div class="sourceCode" id="cb897"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb897-1"><a href="#cb897-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Histogram of Price</span></span>
<span id="cb897-2"><a href="#cb897-2" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">ggplot</span>(wine_data, <span class="fu">aes</span>(<span class="at">x =</span> price)) <span class="sc">+</span> </span>
<span id="cb897-3"><a href="#cb897-3" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">geom_histogram</span>(<span class="at">binwidth =</span> <span class="dv">20</span>, <span class="at">fill =</span> <span class="st">&quot;skyblue&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb897-4"><a href="#cb897-4" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Distribution of Wine Prices&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Price&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Frequency&quot;</span>)</span></code></pre></div>
<p><img src="08-S08-D1_files/figure-html/unnamed-chunk-5-2.png" width="672" /></p>
<div class="sourceCode" id="cb898"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb898-1"><a href="#cb898-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Scatterplot of Rating vs. Num_reviews</span></span>
<span id="cb898-2"><a href="#cb898-2" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">ggplot</span>(wine_data, <span class="fu">aes</span>(<span class="at">x =</span> num_reviews, <span class="at">y =</span> rating)) <span class="sc">+</span> </span>
<span id="cb898-3"><a href="#cb898-3" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">&quot;skyblue&quot;</span>) <span class="sc">+</span></span>
<span id="cb898-4"><a href="#cb898-4" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Rating vs. Number of Reviews&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Number of Reviews&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Rating&quot;</span>)</span></code></pre></div>
<p><img src="08-S08-D1_files/figure-html/unnamed-chunk-5-3.png" width="672" /></p>
<div class="sourceCode" id="cb899"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb899-1"><a href="#cb899-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Barplot of Wine Types</span></span>
<span id="cb899-2"><a href="#cb899-2" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">ggplot</span>(wine_data, <span class="fu">aes</span>(<span class="at">x =</span> type, <span class="at">fill =</span> type)) <span class="sc">+</span> </span>
<span id="cb899-3"><a href="#cb899-3" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">geom_bar</span>() <span class="sc">+</span></span>
<span id="cb899-4"><a href="#cb899-4" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Distribution of Wine Types&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Type&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Count&quot;</span>) <span class="sc">+</span></span>
<span id="cb899-5"><a href="#cb899-5" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">45</span>, <span class="at">hjust =</span> <span class="dv">1</span>),</span>
<span id="cb899-6"><a href="#cb899-6" tabindex="-1"></a><span class="sc">+</span>         <span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>)</span></code></pre></div>
<p><img src="08-S08-D1_files/figure-html/unnamed-chunk-5-4.png" width="672" /></p>
<div class="sourceCode" id="cb900"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb900-1"><a href="#cb900-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Barplot of Regions</span></span>
<span id="cb900-2"><a href="#cb900-2" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">ggplot</span>(wine_data, <span class="fu">aes</span>(<span class="at">x =</span> region, <span class="at">fill =</span> region)) <span class="sc">+</span> </span>
<span id="cb900-3"><a href="#cb900-3" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">geom_bar</span>() <span class="sc">+</span></span>
<span id="cb900-4"><a href="#cb900-4" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Distribution of Regions&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Region&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Count&quot;</span>) <span class="sc">+</span></span>
<span id="cb900-5"><a href="#cb900-5" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">45</span>, <span class="at">hjust =</span> <span class="dv">1</span>),</span>
<span id="cb900-6"><a href="#cb900-6" tabindex="-1"></a><span class="sc">+</span>         <span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>)</span></code></pre></div>
<p><img src="08-S08-D1_files/figure-html/unnamed-chunk-5-5.png" width="672" /></p>
<p>The Barplot of Regions is challenging to interpret due to the numerous regions displayed. Since there are too many regions to effectively visualise, let’s seek assistance from ChatGPT for a solution.</p>
<p><span style="color:red"><strong>Prompt:</strong> </span>Please filter and display only the top 10 regions for better clarity in this visualization.</p>
<pre><code>ggplot(wine_data, aes(x = region, fill = region)) + 
  geom_bar() +
  labs(title = &quot;Distribution of Regions&quot;, x = &quot;Region&quot;, y = &quot;Count&quot;) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = &quot;none&quot;)</code></pre>
<p><span style="color:gray"></p>
<p>❗️<span style="color:green"> Once more, please ensure that the code is included to enhance clarity</span></p>
<p><span style="color:red"><strong>ChatGPT</strong></span>
<img src="images/prompt3.png" width="85%" style="display: block; margin: auto;" /></p>
<p>We can enhance the ggplot by removing the legend, as the region names are already displayed below the bars.</p>
<div class="sourceCode" id="cb902"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb902-1"><a href="#cb902-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Get the top 10 regions by frequency</span></span>
<span id="cb902-2"><a href="#cb902-2" tabindex="-1"></a><span class="er">&gt;</span> top_regions <span class="ot">&lt;-</span> wine_data <span class="sc">%&gt;%</span></span>
<span id="cb902-3"><a href="#cb902-3" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">group_by</span>(region) <span class="sc">%&gt;%</span></span>
<span id="cb902-4"><a href="#cb902-4" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">summarize</span>(<span class="at">count =</span> <span class="fu">n</span>()) <span class="sc">%&gt;%</span></span>
<span id="cb902-5"><a href="#cb902-5" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">arrange</span>(<span class="fu">desc</span>(count)) <span class="sc">%&gt;%</span></span>
<span id="cb902-6"><a href="#cb902-6" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">head</span>(<span class="dv">10</span>)</span>
<span id="cb902-7"><a href="#cb902-7" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Filter the dataset to include only the top 10 regions</span></span>
<span id="cb902-8"><a href="#cb902-8" tabindex="-1"></a><span class="er">&gt;</span> wine_data_top_regions <span class="ot">&lt;-</span> wine_data <span class="sc">%&gt;%</span></span>
<span id="cb902-9"><a href="#cb902-9" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">filter</span>(region <span class="sc">%in%</span> top_regions<span class="sc">$</span>region)</span>
<span id="cb902-10"><a href="#cb902-10" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Barplot of Top 10 Regions</span></span>
<span id="cb902-11"><a href="#cb902-11" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">ggplot</span>(wine_data_top_regions, <span class="fu">aes</span>(<span class="at">x =</span> region, <span class="at">fill =</span> region)) <span class="sc">+</span> </span>
<span id="cb902-12"><a href="#cb902-12" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">geom_bar</span>() <span class="sc">+</span></span>
<span id="cb902-13"><a href="#cb902-13" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Top 10 Regions by Frequency&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Region&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Count&quot;</span>) <span class="sc">+</span></span>
<span id="cb902-14"><a href="#cb902-14" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">45</span>, <span class="at">hjust =</span> <span class="dv">1</span>),</span>
<span id="cb902-15"><a href="#cb902-15" tabindex="-1"></a><span class="sc">+</span>         <span class="at">legend.position =</span> <span class="st">&quot;none&quot;</span>)</span></code></pre></div>
<p><img src="08-S08-D1_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Let’s assess the degree of association between the key variables. To do this, we’ll need to convert the variables ‘body’ and ‘acidity’.</p>
<div class="sourceCode" id="cb903"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb903-1"><a href="#cb903-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Convert factors to numeric (if appropriate)</span></span>
<span id="cb903-2"><a href="#cb903-2" tabindex="-1"></a><span class="er">&gt;</span> wine_data<span class="sc">$</span>body <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(wine_data<span class="sc">$</span>body))</span>
<span id="cb903-3"><a href="#cb903-3" tabindex="-1"></a><span class="sc">&gt;</span> wine_data<span class="sc">$</span>acidity <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">as.character</span>(wine_data<span class="sc">$</span>acidity))</span>
<span id="cb903-4"><a href="#cb903-4" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb903-5"><a href="#cb903-5" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># Compute the correlation matrix</span></span>
<span id="cb903-6"><a href="#cb903-6" tabindex="-1"></a><span class="er">&gt;</span> correlation_matrix <span class="ot">&lt;-</span> <span class="fu">cor</span>(wine_data[, <span class="fu">c</span>(<span class="st">&quot;year&quot;</span>, <span class="st">&quot;rating&quot;</span>, <span class="st">&quot;num_reviews&quot;</span>, <span class="st">&quot;price&quot;</span>, <span class="st">&quot;body&quot;</span>, <span class="st">&quot;acidity&quot;</span>)])</span>
<span id="cb903-7"><a href="#cb903-7" tabindex="-1"></a><span class="sc">&gt;</span> correlation_matrix</span></code></pre></div>
<pre><code>##                    year        rating   num_reviews       price        body
## year         1.00000000 -0.2957784867  0.0354872722 -0.38526005 -0.10277253
## rating      -0.29577849  1.0000000000 -0.0001653593  0.55209571  0.16303349
## num_reviews  0.03548727 -0.0001653593  1.0000000000 -0.03393994  0.06710569
## price       -0.38526005  0.5520957084 -0.0339399360  1.00000000  0.15362352
## body        -0.10277253  0.1630334935  0.0671056895  0.15362352  1.00000000
## acidity      0.14806938 -0.0945527010  0.0401375039 -0.03286992 -0.01795032
##                 acidity
## year         0.14806938
## rating      -0.09455270
## num_reviews  0.04013750
## price       -0.03286992
## body        -0.01795032
## acidity      1.00000000</code></pre>
<p>Let us ask chatGPT to visualise this outcome.</p>
<p><span style="color:red"><strong>Prompt:</strong> </span> <span style="color:gray">Please provide R code to create a heatmap visualising the correlation matrix shown below. Ensure the heatmap includes a gradient color scheme, correlation coefficients, axis labels, and a title.</span></p>
<pre><code>&gt; correlation_matrix
                   year        rating   num_reviews       price
year         1.00000000 -0.2957784867  0.0354872722 -0.38526005
rating      -0.29577849  1.0000000000 -0.0001653593  0.55209571
num_reviews  0.03548727 -0.0001653593  1.0000000000 -0.03393994
price       -0.38526005  0.5520957084 -0.0339399360  1.00000000
body        -0.10277253  0.1630334935  0.0671056895  0.15362352
acidity      0.14806938 -0.0945527010  0.0401375039 -0.03286992
                   body     acidity
year        -0.10277253  0.14806938
rating       0.16303349 -0.09455270
num_reviews  0.06710569  0.04013750
price        0.15362352 -0.03286992
body         1.00000000 -0.01795032
acidity     -0.01795032  1.00000000</code></pre>
<p><span style="color:red"><strong>ChatGPT</strong></span>
<img src="images/prompt4.png" width="85%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb906"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb906-1"><a href="#cb906-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Create a heatmap of the correlation matrix with correlation coefficients</span></span>
<span id="cb906-2"><a href="#cb906-2" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">ggplot</span>(<span class="at">data =</span> <span class="fu">melt</span>(correlation_matrix), <span class="fu">aes</span>(<span class="at">x=</span>Var1, <span class="at">y=</span>Var2, <span class="at">fill=</span>value)) <span class="sc">+</span></span>
<span id="cb906-3"><a href="#cb906-3" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">geom_tile</span>() <span class="sc">+</span></span>
<span id="cb906-4"><a href="#cb906-4" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">label =</span> <span class="fu">round</span>(value, <span class="dv">2</span>)), <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">size =</span> <span class="dv">3</span>) <span class="sc">+</span>  <span class="co"># Add text labels for correlation coefficients</span></span>
<span id="cb906-5"><a href="#cb906-5" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">scale_fill_gradient2</span>(<span class="at">low=</span><span class="st">&quot;blue&quot;</span>, <span class="at">mid=</span><span class="st">&quot;white&quot;</span>, <span class="at">high=</span><span class="st">&quot;red&quot;</span>, <span class="at">midpoint=</span><span class="dv">0</span>, <span class="at">limit=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)) <span class="sc">+</span></span>
<span id="cb906-6"><a href="#cb906-6" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb906-7"><a href="#cb906-7" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">labs</span>(<span class="at">title=</span><span class="st">&quot;Correlation Heatmap&quot;</span>, <span class="at">x=</span><span class="st">&quot;Variables&quot;</span>, <span class="at">y=</span><span class="st">&quot;Variables&quot;</span>) <span class="sc">+</span></span>
<span id="cb906-8"><a href="#cb906-8" tabindex="-1"></a><span class="sc">+</span>   <span class="fu">theme</span>(<span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle =</span> <span class="dv">90</span>, <span class="at">vjust =</span> <span class="fl">0.5</span>, <span class="at">hjust=</span><span class="dv">1</span>))</span></code></pre></div>
<p><img src="08-S08-D1_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>The correlation matrix offers valuable insights into the relationship between wine attributes and prices. It reveals that older wines tend to be less expensive, as seen in the negative correlation between year and price. Conversely, higher-rated wines are associated with higher prices, supported by the positive correlation between rating and price. Surprisingly, there is almost no relationship between the number of reviews or acidity and price, suggesting limited impact on pricing decisions. Additionally, wines with a higher body may command slightly higher prices, indicated by a weak positive correlation. Overall, the rating emerges as the strongest predictor of wine price, underscoring the significance of quality perception in pricing strategies.</p>
<p>In conclusion, the EDA phase has shed light on the key variables influencing wine prices. Factors such as the year of production, wine rating, and body appear to be significant determinants. However, variables like the number of reviews and acidity show minimal impact. With these insights, we can now transition to the next phase: model selection and training. This step will involve choosing suitable models to predict wine prices based on the identified variables and training them on our dataset to make accurate price predictions.</p>
</div>
<div id="model-selection-and-model-training" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Model Selection and Model Training<a href="#model-selection-and-model-training" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Model selection and training are pivotal stages in any data analysis project, especially when predicting intricate phenomena like wine prices. During this phase, we will leverage the insights gleaned from our exploratory data analysis (EDA) to select the most appropriate machine learning models for predicting wine prices. Our goal is to choose models that can adeptly capture the relationships between key variables identified in the EDA, thereby crafting robust and accurate models for predicting wine prices. This process will entail evaluating various regression models, including linear regression, lasso regression, ridge regression, regression tree, and Generalized Additive Model (GAM), to ascertain which one performs optimally for our dataset. Through the process of model selection and training, our aim is to develop a dependable pricing model that can facilitate stakeholders in making well-informed decisions within the wine industry.</p>
<p>However, considering that certain attribute variables in <code>wine_data</code> comprise a large number of levels, we will exclude the fitting of the multiple regression model. Linear models assume a linear relationship between the predictors and the outcome variable, which can be challenging when dealing with numerous factor levels. In such instances, the model may attempt to fit each level individually, leading to overfitting and a lack of generalisation to new data. Furthermore, estimating coefficients for each factor level can be computationally intensive and may not be viable for sizable datasets. The interpretation of coefficients in a linear model with many factor levels can also be intricate, making it challenging to discern the impact of each level on the outcome variable. Therefore, alternative modeling techniques that can effectively handle factor variables with a large number of levels, such as tree-based models, may be more suitable in these scenarios.</p>
<p>So, let us start by asking chatGPT to build for us those suggested models.</p>
<p><span style="color:red"><strong>Prompt:</strong> </span> <span style="color:gray">Perform machine learning to predict wine prices using lasso regression, ridge regression, regression tree, and Generalized Additive Model (GAM) to identify the most effective model for the wine_data dataset.</span></p>
<pre><code>&gt; glimpse(wine_data)
Rows: 6,331
Columns: 10
$ winery      &lt;fct&gt; Teso La Monja, Artadi, Vega…
$ wine        &lt;fct&gt; Tinto, Vina El Pison, Unico…
$ year        &lt;dbl&gt; 2013, 2018, 2009, 1999, 199…
$ rating      &lt;dbl&gt; 4.9, 4.9, 4.8, 4.8, 4.8, 4.…
$ num_reviews &lt;int&gt; 58, 31, 1793, 1705, 1309, 1…
$ region      &lt;fct&gt; Toro, Vino de Espana, Riber…
$ price       &lt;dbl&gt; 995.00, 313.50, 324.95, 692…
$ type        &lt;fct&gt; Toro Red, Tempranillo, Ribe…
$ body        &lt;dbl&gt; 5, 4, 5, 5, 5, 5, 5, 5, 5, …
$ acidity     &lt;dbl&gt; 3, 2, 3, 3, 3, 3, 3, 3, 3, …</code></pre>
<p><span style="color:red"><strong>ChatGPT</strong></span>
<img src="images/prompt5.png" width="85%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb908"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb908-1"><a href="#cb908-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Set seed for reproducibility</span></span>
<span id="cb908-2"><a href="#cb908-2" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb908-3"><a href="#cb908-3" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb908-4"><a href="#cb908-4" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># Split the data into training and testing sets</span></span>
<span id="cb908-5"><a href="#cb908-5" tabindex="-1"></a><span class="er">&gt;</span> train_indices <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(wine_data), <span class="fl">0.7</span> <span class="sc">*</span> <span class="fu">nrow</span>(wine_data))</span>
<span id="cb908-6"><a href="#cb908-6" tabindex="-1"></a><span class="sc">&gt;</span> train_data <span class="ot">&lt;-</span> wine_data[train_indices, ]</span>
<span id="cb908-7"><a href="#cb908-7" tabindex="-1"></a><span class="sc">&gt;</span> test_data <span class="ot">&lt;-</span> wine_data[<span class="sc">-</span>train_indices, ]</span>
<span id="cb908-8"><a href="#cb908-8" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb908-9"><a href="#cb908-9" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># Fit lasso regression</span></span>
<span id="cb908-10"><a href="#cb908-10" tabindex="-1"></a><span class="er">&gt;</span> lasso_model <span class="ot">&lt;-</span> glmnet<span class="sc">::</span><span class="fu">glmnet</span>(<span class="fu">as.matrix</span>(<span class="fu">select</span>(train_data, <span class="sc">-</span>price)), train_data<span class="sc">$</span>price, <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb908-11"><a href="#cb908-11" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb908-12"><a href="#cb908-12" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># Fit ridge regression</span></span>
<span id="cb908-13"><a href="#cb908-13" tabindex="-1"></a><span class="er">&gt;</span> ridge_model <span class="ot">&lt;-</span> glmnet<span class="sc">::</span><span class="fu">glmnet</span>(<span class="fu">as.matrix</span>(<span class="fu">select</span>(train_data, <span class="sc">-</span>price)), train_data<span class="sc">$</span>price, <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb908-14"><a href="#cb908-14" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb908-15"><a href="#cb908-15" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># Fit regression tree</span></span>
<span id="cb908-16"><a href="#cb908-16" tabindex="-1"></a><span class="er">&gt;</span> tree_model <span class="ot">&lt;-</span> rpart<span class="sc">::</span><span class="fu">rpart</span>(price <span class="sc">~</span> ., <span class="at">data =</span> train_data)</span>
<span id="cb908-17"><a href="#cb908-17" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb908-18"><a href="#cb908-18" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># Fit Generalized Additive Model (GAM)</span></span>
<span id="cb908-19"><a href="#cb908-19" tabindex="-1"></a><span class="er">&gt;</span> gam_model <span class="ot">&lt;-</span> mgcv<span class="sc">::</span><span class="fu">gam</span>(price <span class="sc">~</span> <span class="fu">bs</span>(year, <span class="at">df =</span> <span class="dv">3</span>) <span class="sc">+</span> <span class="fu">bs</span>(rating, <span class="at">df =</span> <span class="dv">3</span>) <span class="sc">+</span> <span class="fu">bs</span>(num_reviews, <span class="at">df =</span> <span class="dv">3</span>) <span class="sc">+</span> <span class="fu">bs</span>(body, <span class="at">df =</span> <span class="dv">3</span>) <span class="sc">+</span> <span class="fu">bs</span>(acidity, <span class="at">df =</span> <span class="dv">3</span>), <span class="at">data =</span> train_data)</span></code></pre></div>
<p>In this section, we fitted various models for predicting wine prices, including lasso regression, ridge regression, regression tree, and Generalized Additive Model (GAM). Each model offers unique advantages and complexities, making it crucial to select the most suitable one for our dataset. Following model selection and training, the next step is to evaluate the performance of these models to determine which one best predicts wine prices. This evaluation will provide insights into the model’s accuracy and reliability, guiding us in making informed decisions about the most effective model for our wine_data dataset.</p>
</div>
<div id="model-evaluation" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Model Evaluation<a href="#model-evaluation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the Model Evaluation section, we assess the performance of various machine learning models in predicting wine prices for the <code>wine_data</code> dataset. By examining metrics such as RMSE and MAE, we aim to determine the most accurate and reliable model for our predictive tasks. This evaluation will provide valuable insights into the effectiveness of each model and guide us in selecting the best approach for predicting wine prices.</p>
<p>We will ask chatGPT to generate the code for this task.</p>
<p><span style="color:red"><strong>Prompt:</strong> </span> <span style="color:gray">Please generate R code to evaluate the performance of the fitted models (lasso regression, ridge regression, regression tree, and Generalized Additive Model) on the test dataset. Compute and print the RMSE and MAE for each model in the code given to assess their predictive accuracy.</span></p>
<pre><code># Set seed for reproducibility
set.seed(123)

# Split the data into training and testing sets
train_indices &lt;- sample(1:nrow(wine_data), 0.7 * nrow(wine_data))
train_data &lt;- wine_data[train_indices, ]
test_data &lt;- wine_data[-train_indices, ]

# Fit lasso regression
lasso_model &lt;- glmnet::glmnet(as.matrix(select(train_data, -price)), train_data$price, alpha = 1)

# Fit ridge regression
ridge_model &lt;- glmnet::glmnet(as.matrix(select(train_data, -price)), train_data$price, alpha = 0)

# Fit regression tree
tree_model &lt;- rpart::rpart(price ~ ., data = train_data)

# Fit Generalized Additive Model (GAM)
gam_model &lt;- mgcv::gam(price ~ bs(year, df = 3) + bs(rating, df = 3) + bs(num_reviews, df = 3) + bs(body, df = 3) + bs(acidity, df = 3), data = train_data)</code></pre>
<p><span style="color:red"><strong>ChatGPT</strong></span>
<img src="images/prompt6.gif" width="85%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb910"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb910-1"><a href="#cb910-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Predictions for lasso regression</span></span>
<span id="cb910-2"><a href="#cb910-2" tabindex="-1"></a><span class="er">&gt;</span> lasso_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(lasso_model, <span class="at">newx =</span> <span class="fu">as.matrix</span>(<span class="fu">select</span>(test_data, <span class="sc">-</span>price)))</span>
<span id="cb910-3"><a href="#cb910-3" tabindex="-1"></a><span class="sc">&gt;</span> lasso_rmse <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((lasso_pred <span class="sc">-</span> test_data<span class="sc">$</span>price)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb910-4"><a href="#cb910-4" tabindex="-1"></a><span class="sc">&gt;</span> lasso_mae <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">abs</span>(lasso_pred <span class="sc">-</span> test_data<span class="sc">$</span>price))</span>
<span id="cb910-5"><a href="#cb910-5" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb910-6"><a href="#cb910-6" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># Predictions for ridge regression</span></span>
<span id="cb910-7"><a href="#cb910-7" tabindex="-1"></a><span class="er">&gt;</span> ridge_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(ridge_model, <span class="at">newx =</span> <span class="fu">as.matrix</span>(<span class="fu">select</span>(test_data, <span class="sc">-</span>price)))</span>
<span id="cb910-8"><a href="#cb910-8" tabindex="-1"></a><span class="sc">&gt;</span> ridge_rmse <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((ridge_pred <span class="sc">-</span> test_data<span class="sc">$</span>price)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb910-9"><a href="#cb910-9" tabindex="-1"></a><span class="sc">&gt;</span> ridge_mae <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">abs</span>(ridge_pred <span class="sc">-</span> test_data<span class="sc">$</span>price))</span>
<span id="cb910-10"><a href="#cb910-10" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb910-11"><a href="#cb910-11" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># Predictions for regression tree</span></span>
<span id="cb910-12"><a href="#cb910-12" tabindex="-1"></a><span class="er">&gt;</span> tree_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(tree_model, <span class="at">newdata =</span> test_data)</span>
<span id="cb910-13"><a href="#cb910-13" tabindex="-1"></a><span class="sc">&gt;</span> tree_rmse <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((tree_pred <span class="sc">-</span> test_data<span class="sc">$</span>price)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb910-14"><a href="#cb910-14" tabindex="-1"></a><span class="sc">&gt;</span> tree_mae <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">abs</span>(tree_pred <span class="sc">-</span> test_data<span class="sc">$</span>price))</span>
<span id="cb910-15"><a href="#cb910-15" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb910-16"><a href="#cb910-16" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># Predictions for GAM</span></span>
<span id="cb910-17"><a href="#cb910-17" tabindex="-1"></a><span class="er">&gt;</span> gam_pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(gam_model, <span class="at">newdata =</span> test_data)</span>
<span id="cb910-18"><a href="#cb910-18" tabindex="-1"></a><span class="sc">&gt;</span> gam_rmse <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">mean</span>((gam_pred <span class="sc">-</span> test_data<span class="sc">$</span>price)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb910-19"><a href="#cb910-19" tabindex="-1"></a><span class="sc">&gt;</span> gam_mae <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">abs</span>(gam_pred <span class="sc">-</span> test_data<span class="sc">$</span>price))</span>
<span id="cb910-20"><a href="#cb910-20" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb910-21"><a href="#cb910-21" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># Print RMSE and MAE for each model</span></span>
<span id="cb910-22"><a href="#cb910-22" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">cat</span>(<span class="st">&quot;Lasso Regression RMSE:&quot;</span>, lasso_rmse, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Lasso Regression RMSE: 142.121</code></pre>
<div class="sourceCode" id="cb912"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb912-1"><a href="#cb912-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">cat</span>(<span class="st">&quot;Lasso Regression MAE:&quot;</span>, lasso_mae, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Lasso Regression MAE: 47.67765</code></pre>
<div class="sourceCode" id="cb914"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb914-1"><a href="#cb914-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">cat</span>(<span class="st">&quot;Ridge Regression RMSE:&quot;</span>, ridge_rmse, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Ridge Regression RMSE: 160.9951</code></pre>
<div class="sourceCode" id="cb916"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb916-1"><a href="#cb916-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">cat</span>(<span class="st">&quot;Ridge Regression MAE:&quot;</span>, ridge_mae, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Ridge Regression MAE: 52.27333</code></pre>
<div class="sourceCode" id="cb918"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb918-1"><a href="#cb918-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">cat</span>(<span class="st">&quot;Regression Tree RMSE:&quot;</span>, tree_rmse, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Regression Tree RMSE: 88.45144</code></pre>
<div class="sourceCode" id="cb920"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb920-1"><a href="#cb920-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">cat</span>(<span class="st">&quot;Regression Tree MAE:&quot;</span>, tree_mae, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Regression Tree MAE: 24.06545</code></pre>
<div class="sourceCode" id="cb922"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb922-1"><a href="#cb922-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">cat</span>(<span class="st">&quot;Generalized Additive Model RMSE:&quot;</span>, gam_rmse, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Generalized Additive Model RMSE: 123.112</code></pre>
<div class="sourceCode" id="cb924"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb924-1"><a href="#cb924-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">cat</span>(<span class="st">&quot;Generalized Additive Model MAE:&quot;</span>, gam_mae, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Generalized Additive Model MAE: 40.67011</code></pre>
<p>We can consolidate the results and display them in a unified table for easier comparison.</p>
<div class="sourceCode" id="cb926"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb926-1"><a href="#cb926-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Combine results into a table</span></span>
<span id="cb926-2"><a href="#cb926-2" tabindex="-1"></a><span class="er">&gt;</span> results <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb926-3"><a href="#cb926-3" tabindex="-1"></a><span class="sc">+</span>   <span class="at">Model =</span> <span class="fu">c</span>(<span class="st">&quot;Lasso Regression&quot;</span>, <span class="st">&quot;Ridge Regression&quot;</span>, <span class="st">&quot;Regression Tree&quot;</span>, <span class="st">&quot;Generalized Additive Model&quot;</span>),</span>
<span id="cb926-4"><a href="#cb926-4" tabindex="-1"></a><span class="sc">+</span>   <span class="at">RMSE =</span> <span class="fu">c</span>(lasso_rmse, ridge_rmse, tree_rmse, gam_rmse),</span>
<span id="cb926-5"><a href="#cb926-5" tabindex="-1"></a><span class="sc">+</span>   <span class="at">MAE =</span> <span class="fu">c</span>(lasso_mae, ridge_mae, tree_mae, gam_mae)</span>
<span id="cb926-6"><a href="#cb926-6" tabindex="-1"></a><span class="sc">+</span> )</span>
<span id="cb926-7"><a href="#cb926-7" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb926-8"><a href="#cb926-8" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># Print the table</span></span>
<span id="cb926-9"><a href="#cb926-9" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">print</span>(results)</span></code></pre></div>
<pre><code>##                        Model      RMSE      MAE
## 1           Lasso Regression 142.12096 47.67765
## 2           Ridge Regression 160.99508 52.27333
## 3            Regression Tree  88.45144 24.06545
## 4 Generalized Additive Model 123.11196 40.67011</code></pre>
<p>Upon analysis, it becomes evident that the Regression Tree model outperforms other models in this dataset, particularly due to its ability to handle numerous features effectively. This conclusion is drawn from a comparative evaluation of the models, highlighting the Regression Tree’s superior performance in managing the complexity of the dataset’s features.</p>
</div>
<div id="model-interpretation" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Model Interpretation<a href="#model-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the model interpretation section of this report, we delve into the insights gained from the trained machine learning models, aiming to understand the underlying factors that influence wine prices. By analysing the feature importance, decision rules, and predicted values of the models, we seek to unravel the complex relationships between wine characteristics and their market values. This section not only sheds light on the predictive performance of the models but also provides actionable insights for stakeholders in the wine industry, guiding them in making informed decisions related to pricing strategies and product development.</p>
<p>We will begin by plotting the regression tree for the <code>wine_data</code>, providing a visual representation of the decision-making process of the model. You can interpret the tree by starting at the root node and following the branches to the leaf nodes, where the predictions are made. Each split in the tree represents a decision based on a feature in the dataset, and the leaf nodes represent the predicted wine prices.</p>
<p>Let us visualise the tree.</p>
<div class="sourceCode" id="cb928"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb928-1"><a href="#cb928-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">rpart.plot</span>(tree_model, </span>
<span id="cb928-2"><a href="#cb928-2" tabindex="-1"></a><span class="sc">+</span>             <span class="at">box.palette =</span> <span class="st">&quot;RdBu&quot;</span>, </span>
<span id="cb928-3"><a href="#cb928-3" tabindex="-1"></a><span class="sc">+</span>             <span class="at">shadow.col =</span> <span class="st">&quot;gray&quot;</span>, </span>
<span id="cb928-4"><a href="#cb928-4" tabindex="-1"></a><span class="sc">+</span>             <span class="at">nn =</span> <span class="cn">TRUE</span>, </span>
<span id="cb928-5"><a href="#cb928-5" tabindex="-1"></a><span class="sc">+</span>             <span class="at">type =</span> <span class="dv">4</span>, </span>
<span id="cb928-6"><a href="#cb928-6" tabindex="-1"></a><span class="sc">+</span>             <span class="at">fallen.leaves =</span> <span class="cn">TRUE</span>, </span>
<span id="cb928-7"><a href="#cb928-7" tabindex="-1"></a><span class="sc">+</span>             <span class="at">branch.lty =</span> <span class="dv">3</span>, </span>
<span id="cb928-8"><a href="#cb928-8" tabindex="-1"></a><span class="sc">+</span>             <span class="at">branch.lwd =</span> <span class="dv">2</span>,</span>
<span id="cb928-9"><a href="#cb928-9" tabindex="-1"></a><span class="sc">+</span>             <span class="at">extra =</span> <span class="dv">1</span>,</span>
<span id="cb928-10"><a href="#cb928-10" tabindex="-1"></a><span class="sc">+</span>             <span class="at">cex =</span> <span class="fl">0.6</span>,  <span class="co"># Reduce the size of nodes and text</span></span>
<span id="cb928-11"><a href="#cb928-11" tabindex="-1"></a><span class="sc">+</span>             <span class="at">faclen =</span> <span class="dv">3</span>)  <span class="co"># Adjust the faclen value to increase space between nodes</span></span></code></pre></div>
<p><img src="08-S08-D1_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<div class="sourceCode" id="cb929"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb929-1"><a href="#cb929-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">summary</span>(tree_model)</span></code></pre></div>
<pre><code>## Call:
## rpart::rpart(formula = price ~ ., data = train_data)
##   n= 4431 
## 
##           CP nsplit rel error    xerror       xstd
## 1 0.54735715      0 1.0000000 1.0006206 0.17076673
## 2 0.08338496      1 0.4526429 0.5384705 0.10301520
## 3 0.07689143      2 0.3692579 0.5090425 0.09812500
## 4 0.03461715      3 0.2923665 0.4213455 0.08404526
## 5 0.03452879      4 0.2577493 0.4161366 0.08524950
## 6 0.01482093      5 0.2232205 0.3842724 0.08380711
## 7 0.01208131      6 0.2083996 0.3696184 0.08374828
## 8 0.01148682      7 0.1963183 0.3525272 0.08166889
## 9 0.01000000      8 0.1848315 0.3686921 0.08631320
## 
## Variable importance
##        wine      winery      region        type        year num_reviews 
##          57          19           6           5           5           3 
##        body      rating 
##           3           2 
## 
## Node number 1: 4431 observations,    complexity param=0.5473571
##   mean=63.84701, MSE=24260.23 
##   left son=2 (4358 obs) right son=3 (73 obs)
##   Primary splits:
##       wine        splits as  --LLL--LLL--LLLLLLLLLLLL-LL--LLLL-LLL--LLLL--LLLLL-LLLLL-LLLLLLLL-LL-LLL-LLL--LLLLL--L--LL-L--LLLL---LL-LL--LLLLLL-LLLL-LLLLLLLLLL-L-LL-LLLL-L--LLLL-L-LLLLL-LLLLLLLLLLLL-LLLLLLLLLLLLLLLL---LLLLRL-LL-LL-LL-LLLL---LLLLLLLL-LL--L--LLLLLLLLLLL--LLL-LLL--LLLLLL--LLLLL----LL-LLL-LLL-L-L-LLLLLLLLL-LLL-LLLLLLL--LLLL--L-L-L-L-LLL-LLLL-LL-R-L--LLL-L-LLLLLLLL-LLLLL-L-L-LLL-LL-L-L---LLL--L-LLLL-L-R-LLLLLLLLL--LLLRLLL-L-LLLLLL-----L----L--LLLL-L--LLLLL-LLL--L--LLLL-LLLLLLLLL--LLL--LLLLL--LLLL--LL-LLLLLLLLL--LL-LL-LL-LLL--L--LL-L-L--L--LL--L--LL-LL-LLLLL-LLL-LLLLLLLL-L-L-LLL-LLL-LL-LLLL--LRL--LL-L-LLLL-L-LLLLL-LL-L--LLL-LLL--LLLLLLLLLLLL-LL-LLLLL-LLL-RLLL-LLLLLLL-L-LLLLLLLLLLLL---LLL---LLLL---L-L---LLLLLLLLLLL----LLLL---L-LLLLLLLLLLL-L---L-LL-LLLLL-LLLR-LLL-LLLL-LLLLLL--LLL--L-LLL-RRL-L-LL--LL-LLL---LL-L-L-L----LLLLLLLLL-LLLL-LLLLLLL-L-LLL-LL-LLLLLL, improve=0.5473571, (0 missing)
##       winery      splits as  -LLLLLLLLLLL-LL-LLLLLLLLRL-LLLLLLLLLLL---L-LLL--L--LLLL---LL-LLRL-L--LLLL-L-LLLLL-LLLL-LL-LLLL-LLLLLLLLLLLLL---LL-LLLLLL-----L-L-LL-LLL-LLLLLL-LLL-LLRLLLLLLLLLLLLLL-LL-LLLR---LLLL-L--RLL-LL-LLLLL-L-L-LL-LL-LLLLLL-L-LL-LLLL--L-LL-L-LLLLL--L-LLLLLLLLL-L-LL-LLLLL-LLLLL-LLLLLL-L--L-LL-LLLLLL-LL--LLLLL-LLLL--LLL--LLLLLLLLL-LLLLLLLL-LLL-L-LLLL-R--LLLLLLLLLL-LLLL-LLLL-LLL---LLL-L-LLLLLLL-L--LLLLLLLLLLLL--L--LL-LLLLL----LL-RLLLRLLLL-LL-LLL-L-LLLL-LLLLLR--LLLLLL-LLLLLL-LLL-L-LLLLL-L-L, improve=0.3886168, (0 missing)
##       rating      &lt; 4.55   to the left,  improve=0.3049241, (0 missing)
##       year        &lt; 2003.5 to the right, improve=0.1302212, (0 missing)
##       num_reviews &lt; 325.5  to the right, improve=0.1113139, (0 missing)
##   Surrogate splits:
##       winery splits as  -LLLLLLLLLLL-LL-LLLLLLLLLL-LLLLLLLLLLL---L-LLL--L--LLLL---LL-LLRL-L--LLLL-L-LLLLL-LLLL-LL-LLLL-LLLLLLLLLLLLL---LL-LLLLLL-----L-L-LL-LLL-LLLLLL-LLL-LLLLLLLLLLLLLLLLL-LL-LLLR---LLLL-L--LLL-LL-LLLLL-L-L-LL-LL-LLLLLL-L-LL-LLLL--L-LL-L-LLLLL--L-LLLLLLLLL-L-LL-LLLLL-LLLLL-LLLLLL-L--L-LL-LLLLLL-LL--LLLLL-LLLL--LLL--LLLLLLLLL-LLLLLLLL-LLL-L-LLLL-L--LLLLLLLLLL-LLLL-LLLL-LLL---LLL-L-LLLLLLL-L--LLLLLLLLLLLL--L--LL-LLLLL----LL-LLLLLLLLL-LL-LLL-L-LLLL-LLLLLR--LLLLLL-LLLLLL-LLL-L-LLLLL-L-L, agree=0.988, adj=0.274, (0 split)
##       rating &lt; 4.75   to the left,  agree=0.984, adj=0.014, (0 split)
## 
## Node number 2: 4358 observations,    complexity param=0.07689143
##   mean=48.93278, MSE=4646.262 
##   left son=4 (4087 obs) right son=5 (271 obs)
##   Primary splits:
##       wine        splits as  --LLR--LLL--LLLLLLLLLLLL-LL--RLLL-RLL--LLLL--LLLLL-LLLLL-RLLLLLLL-RL-LLL-LLL--LRLLL--L--RL-L--LLLL---LL-LL--LLLLRR-LLLL-RLLLLRLLLL-L-RL-LLLR-L--LLLL-L-LLLLL-LLLLRLLRLLRL-LLLLLLRLLLLLRLLL---LLLL-L-LL-LL-LL-LLLR---LLLRLLLL-LR--L--LLLRLRRLLLL--LLL-RLL--LRRLLL--LLLLL----LL-LLL-LLL-L-L-LLLLLLLLL-LLL-LLLLLLL--LLLL--L-L-L-L-RLL-LLLL-LL---L--LLR-R-LRLLRLLL-LLLRL-L-R-RLL-LL-L-L---RLL--L-LLLL-L---LRLLLLLLL--RLL-LLL-L-LLLLLL-----L----L--LLLL-L--LRLLR-LRL--L--RLLL-LLLLLRRLL--LRL--RLLLL--RLLL--LL-LLRLLLLLL--LR-LL-LL-LLL--R--RL-L-L--L--LL--L--LL-RL-LLLLL-LLL-LLLLLLLL-L-L-LRL-LLL-RR-LLRL--L-L--LL-L-LLLL-L-RLLLL-LL-L--RLL-LLL--LLLLLLLLRLLL-LL-LLLLL-LLL--LRL-LLLLLLL-L-LLLLRLLRLLLR---LLL---LRLL---L-L---LLLLLLLLLLL----LLLL---L-LLLLLLRRRLR-L---L-LL-LLLLL-LLL--LLL-LLLL-RLLRLL--LLL--L-LLL---R-R-LR--LL-LRL---LL-L-L-L----LLLLLLLLL-LRLL-RRRLLLL-L-LLL-LL-LLLLLL, improve=0.4082101, (0 missing)
##       winery      splits as  -LLLLLLLRLLL-LR-LLLLLLRLLL-LLRLLLLLLLL---L-LLL--L--LLLL---LL-LLLL-L--LLLL-L-LLLLR-LLLR-LL-LLLL-LLLLLLLLLLLRL---LL-LLLLLL-----L-L-LL-LRL-LLLLLL-LLL-LLRLLLLLLLLLLLLLL-LL-LLLR---LLLL-L--LRL-LL-LLLRL-L-L-LL-LL-LLLRLL-L-LL-LLLR--L-LL-L-LRLLL--L-LLLLLLLLL-L-LL-LLRLL-LLLLL-LLLLLL-L--L-LL-LLLLLL-LL--LLLLR-LLLL--LLR--RLRRLLLLL-LLLLLLLL-LLL-L-LLLL-R--LLLLRRLLLL-LLLL-LLLL-LLL---RLL-L-LLLLLLL-L--LLLLLLLLLLLL--L--RL-LLLLL----LL-RLRRRLLLL-RL-LLL-L-LLLR-LLLLLR--LLLLLL-RLLLLL-LLL-L-LLLLL-L-L, improve=0.2804499, (0 missing)
##       rating      &lt; 4.45   to the left,  improve=0.2244645, (0 missing)
##       num_reviews &lt; 386    to the right, improve=0.1585326, (0 missing)
##       year        &lt; 2010.5 to the right, improve=0.1194112, (0 missing)
##   Surrogate splits:
##       winery splits as  -LLLLLRLRLLL-LL-LLLLLLRLLL-LLRLLLLLLLL---L-LLL--L--LLLL---LL-LLLL-L--LLLL-L-LLLLR-LRLR-LL-LLLL-LLLRLLLLLLLRL---LL-LLLLLL-----L-L-LL-LRL-LLLLLL-LLL-LRRLLRLLLLLLLLLLL-LL-LLLL---LLLL-L--LRL-LL-LLLLL-R-L-LL-RL-LLLLLL-L-LL-LLLR--L-LL-L-LLLLL--L-LLLLLLLLL-L-LL-LLLLL-LLLLL-LLLLLL-L--L-LL-LRLLLL-LL--LLLLR-LLLL--LLR--RLRRLLLLL-LLLLRLLL-LLL-L-LLLL-R--LLLLLRLLLL-LLLL-LLLL-LLL---RLL-L-LLLLLLR-L--LRLLLLLRLLLL--L--RL-LLLLL----LL-RLLRRLLLL-RL-LLL-R-LLLR-LLLLLR--LLLLLL-LLLLLL-LLL-L-RLLLL-L-L, agree=0.966, adj=0.450, (0 split)
##       rating &lt; 4.55   to the left,  agree=0.945, adj=0.111, (0 split)
##       year   &lt; 1990   to the right, agree=0.944, adj=0.103, (0 split)
##       region splits as  -LLLLLLLL-LLLLLLRLLR-LLRLLL--LL-LLLLL-L-RLRLRLLLL-LLLLL--L-LLLLLLLLLLLL--LLL, agree=0.941, adj=0.059, (0 split)
## 
## Node number 3: 73 observations,    complexity param=0.08338496
##   mean=954.2058, MSE=389169.5 
##   left son=6 (50 obs) right son=7 (23 obs)
##   Primary splits:
##       wine        splits as  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------L-----------------------------------------------------------------------------------------------------------------------------------------L--------------------------------------------------------R---------------R---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------R--------------------------------------------------------------L------------------------------------------------------------------------------------------------------L-----------------------------LL-------------------------------------------------------------------, improve=0.3155169, (0 missing)
##       winery      splits as  ------------------------R----------------------------------L---L---------------------------------------L-------------------------------------------------------------------R-----------R-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------L------------------L-------------------------------, improve=0.3155169, (0 missing)
##       region      splits as  --------R-----------------------------------L--------R---L-L---------L------, improve=0.2168240, (0 missing)
##       type        splits as  --------LRRLL------L-, improve=0.2168240, (0 missing)
##       num_reviews &lt; 109.5  to the right, improve=0.1796302, (0 missing)
##   Surrogate splits:
##       region      splits as  --------R-----------------------------------L--------R---L-L---------L------, agree=0.877, adj=0.609, (0 split)
##       type        splits as  --------LRRLL------L-, agree=0.877, adj=0.609, (0 split)
##       body        &lt; 4.5    to the right, agree=0.836, adj=0.478, (0 split)
##       num_reviews &lt; 67.5   to the right, agree=0.795, adj=0.348, (0 split)
##       year        &lt; 2014.5 to the left,  agree=0.712, adj=0.087, (0 split)
## 
## Node number 4: 4087 observations,    complexity param=0.01482093
##   mean=37.71838, MSE=630.4267 
##   left son=8 (2687 obs) right son=9 (1400 obs)
##   Primary splits:
##       wine        splits as  --LL---RRL--LLRRLLRLRLRL-LL---RRL--LL--LLLR--LLLRL-RLLLL--RLRRRRR--L-LLL-RRR--L-LLL--L---R-R--LLLL---RL-LL--LRRL---LLLL--LRLL-RRRL-L--L-RLR--L--RRLL-L-LRLLR-LLLL-RL-RR-L-RRLRRR-RRLLL-LLR---LLRR-L-LL-LL-LL-RRR----LLR-LRLL-L---L--LLL-L--RRLL--RLL--RL--R--RLR--RRLLR----LL-RLL-LRL-L-L-LRRRLRRLL-LLR-LLRRLLL--LLLL--R-R-L-L--LR-LLLR-LR---R--LR----R-LL-RLR-LLR-R-R----LL-LR-R-R----LR--R-LLLL-L---L-LRLRRLL---LR-LLL-L-RLRLLR-----R----R--LRRL-L--L-LL--R-R--L---RLL-LLLLL--LL--R-R---LLLR---RLR--LR-LR-RRLRLR--L--LR-LL-LLR------L-L-L--L--LL--L--RL--R-LLLLL-LRR-LRRRRLRL-R-L-L-L-LRR----RR-R--R-L--LR-R-RRRL-L--LLRL-LL-L---RL-RRL--LLRLRLRL-LLL-LL-LLRLL-RLL--R-L-LRLLRLL-L-RRLR-RL-RRR----LRL---R-RL---R-L---RRLLLRLRLRR----LLLR---L-LRLLLR---L--R---L-LL-LRLRL-RLL--RLR-LRLL--RR-RL--RLR--R-LLR-------R---RR-L-L---LL-R-L-L----RLLLLRLLL-R-RL----LLRR-L-LLL-RL-LLRRLR, improve=0.6183481, (0 missing)
##       winery      splits as  -RLRRR-L-RLL-RR-RLLRLR-LRR-RL-LLRRRLRL---R-LLR--L--LRLR---LR-RRRL-L--LLRL-L-LLRLR-LLR--RL-RRLR-LLRRLRRRLLL-R---LL-RLRRRL-----R-L-RR-L-L-RLLLRL-RRL-R-RRLRLRLLRLRRRRR-LL-RLRR---LLRL-R--RRR-LL-LLLRR-L-L-LR--L-LRRRLL-R-RL-LLR---L-LL-L-RRLRL--L-LRRLRLRLR-R-LL-RRRLL-LLLLL-RLLLLL-L--L-LR-R-LLLL-RL--LRLR--RRLR--LR---LL--LRRRL-RLRL-LRR-LRR-L-LLLL----RLLLLRRRLL-RLLL-LLLL-LLL----LR-R-LRRRRLL-L--R-RRRRL-LLLR--R---R-LRLLR----RR--LR-LRRLL-RL-LRL---LLR--LRRLL---LRLRRR-RLRRLR-LLL-R--LLLL-R-L, improve=0.4686453, (0 missing)
##       year        &lt; 2014.5 to the right, improve=0.2151698, (0 missing)
##       num_reviews &lt; 389.5  to the right, improve=0.1672325, (0 missing)
##       region      splits as  -LLLLLRRL-RLRRLR-RLR-LL-LRL--LR-RRRRL-R-RR-LRRLRL-RLLRL--R-RRLLRLRLRRRL--LRL, improve=0.1543332, (0 missing)
##   Surrogate splits:
##       winery      splits as  -RLRLL-L-RLL-RR-RLLLLR-LRR-RL-LLRRLLRL---R-LLL--L--LRLL---LR-LRLL-L--LLRL-L-LLRLR-LLR--RL-LRLR-LLLRLRRRLLL-R---LL-RLRRLL-----L-L-RR-L-L-RLLLLL-RRL-R-LRLRLRLLRLRRRRR-LL-LLRR---LLRL-R--RLR-LL-LLLRR-L-L-LR--L-LRLRLL-R-RL-LLR---L-LL-R-LRLRL--L-LLRLRLRLL-R-LL-LRRLL-LLLLL-RLLLLL-L--L-LL-L-LLLL-LL--LLLR--RRLL--LR---LL--LRLRL-RLRL-LRR-LLR-L-LLLL----RLLLLRLRLL-RRLL-LLLL-LLL----LR-R-LRRRRLL-L--L-LRLLL-LLLR--R---R-LRLLR----LR--LR-LLRLL-LL-LLL---LLL--RRRLL---LRLRRL-RLRRLL-LLL-L--LLLL-L-L, agree=0.930, adj=0.794, (0 split)
##       year        &lt; 2011.5 to the right, agree=0.780, adj=0.359, (0 split)
##       region      splits as  -LLLLLRLL-LLLRLL-RLR-LL-LLL--LL-RRLLL-R-LL-LRLLRL-LLLRL--L-LRLLRLLLLLLL--LRL, agree=0.710, adj=0.153, (0 split)
##       type        splits as  LRRLLLLLLRLLLLLLLLRLL, agree=0.707, adj=0.145, (0 split)
##       num_reviews &lt; 386    to the right, agree=0.698, adj=0.119, (0 split)
## 
## Node number 5: 271 observations,    complexity param=0.03461715
##   mean=218.0592, MSE=34709.41 
##   left son=10 (257 obs) right son=11 (14 obs)
##   Primary splits:
##       winery splits as  -L----L-L-----L-------L-L----L---LL------------------------L----------L---------L--L-L----L------LL-----L-L--------------------------L--------------LR--L------------------L------L-----L--------L--L-L----L---L-------L-----L----------L-L----------------------L-----------L-------------L-------------L-L------LL--L-LL---L------L------L--------L------LL-----L-L-------------L--------L--L-----L--L--L---------L--L--L--------L-LLRL----L------L---LL------L------L--L------------L--------, improve=0.39561390, (0 missing)
##       type   splits as  --LLLL-LLLLLLL-LL-LR-, improve=0.16341780, (0 missing)
##       region splits as  ---L-L--L-------LL-L-L-L-L---L---L-L----L-L-LL----L--L---L-LLL---L---R---LL-, improve=0.16341780, (0 missing)
##       rating &lt; 4.75   to the left,  improve=0.13420570, (0 missing)
##       year   &lt; 2013.5 to the right, improve=0.09244459, (0 missing)
##   Surrogate splits:
##       wine   splits as  ----L------------------------L----R----------------------L--------L------------L--------L-----------------------LL------L----L-------L-----L---------------------L--L--L--------L-----L-------------------------L------R------L--------L-LL----------L-----LL------------------------------------------------------------------L------------------L-L--L--L-------L----L-L------------L----------------L---------L-------------------------------------L--L--L------L---------LL-----L---L------L----------L---------L------------L--L--------------------L--------------------------L------LL---L--------------------L-----------L----------------L-------------------L----------------L--L---L----------L-----------------------------------------LLL-L------------------------------L--L-----------------L-L--L------L---------------------------L---LLL--------------------, agree=0.959, adj=0.214, (0 split)
##       region splits as  ---L-L--L-------LL-L-L-L-L---L---L-L----L-L-LL----L--L---L-LLL---L---R---LL-, agree=0.956, adj=0.143, (0 split)
##       type   splits as  --LLLL-LLLLLLL-LL-LR-, agree=0.956, adj=0.143, (0 split)
## 
## Node number 6: 50 observations,    complexity param=0.03452879
##   mean=716.5438, MSE=226049.2 
##   left son=12 (23 obs) right son=13 (27 obs)
##   Primary splits:
##       year        &lt; 2004.5 to the right, improve=0.32840130, (0 missing)
##       num_reviews &lt; 309.5  to the right, improve=0.16080040, (0 missing)
##       rating      &lt; 4.65   to the left,  improve=0.08287315, (0 missing)
##       winery      splits as  -----------------------------------------------------------L---L---------------------------------------R-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------R------------------R-------------------------------, improve=0.04522092, (0 missing)
##       wine        splits as  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------L-----------------------------------------------------------------------------------------------------------------------------------------R-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------R------------------------------------------------------------------------------------------------------L-----------------------------RR-------------------------------------------------------------------, improve=0.04522092, (0 missing)
##   Surrogate splits:
##       wine        splits as  -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------L-----------------------------------------------------------------------------------------------------------------------------------------R-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------R------------------------------------------------------------------------------------------------------L-----------------------------RL-------------------------------------------------------------------, agree=0.76, adj=0.478, (0 split)
##       winery      splits as  -----------------------------------------------------------L---L---------------------------------------R-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------R------------------R-------------------------------, agree=0.68, adj=0.304, (0 split)
##       region      splits as  --------------------------------------------R------------R-L---------L------, agree=0.68, adj=0.304, (0 split)
##       type        splits as  --------R--RL------L-, agree=0.68, adj=0.304, (0 split)
##       num_reviews &lt; 124    to the right, agree=0.64, adj=0.217, (0 split)
## 
## Node number 7: 23 observations,    complexity param=0.01208131
##   mean=1470.862, MSE=354055.4 
##   left son=14 (9 obs) right son=15 (14 obs)
##   Primary splits:
##       year        &lt; 2011.5 to the right, improve=0.15948200, (0 missing)
##       rating      &lt; 4.65   to the right, improve=0.13463400, (0 missing)
##       num_reviews &lt; 62.5   to the right, improve=0.12780900, (0 missing)
##       body        &lt; 4.5    to the right, improve=0.02788812, (0 missing)
##       winery      splits as  ------------------------R--------------------------------------------------------------------------------------------------------------------------------------------------R-----------L--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------, improve=0.02788812, (0 missing)
##   Surrogate splits:
##       rating      &lt; 4.75   to the right, agree=0.696, adj=0.222, (0 split)
##       num_reviews &lt; 41     to the left,  agree=0.696, adj=0.222, (0 split)
##       winery      splits as  ------------------------R--------------------------------------------------------------------------------------------------------------------------------------------------L-----------R--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------, agree=0.652, adj=0.111, (0 split)
##       wine        splits as  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------R---------------L---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------R------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------, agree=0.652, adj=0.111, (0 split)
##       region      splits as  --------L--------------------------------------------R---R------------------, agree=0.652, adj=0.111, (0 split)
## 
## Node number 8: 2687 observations
##   mean=23.46677, MSE=77.10961 
## 
## Node number 9: 1400 observations
##   mean=65.0713, MSE=554.3952 
## 
## Node number 10: 257 observations,    complexity param=0.01148682
##   mean=190.7092, MSE=11604.8 
##   left son=20 (149 obs) right son=21 (108 obs)
##   Primary splits:
##       wine   splits as  ----R------------------------L---------------------------L--------R------------L--------L-----------------------LR------L----L-------R-----L---------------------R--L--R--------L-----L-------------------------L-------------L--------L-LR----------L-----LL------------------------------------------------------------------R------------------L-R--R--R-------R----R-R------------L----------------R---------L-------------------------------------R--R--L------R---------LL-----L---L------L----------L---------R------------L--R--------------------R--------------------------L------RL---R--------------------L-----------R----------------R-------------------R----------------L--L---R----------L-----------------------------------------LLL-R------------------------------L--R-----------------L-R--R------R---------------------------R---LRL--------------------, improve=0.41402420, (0 missing)
##       winery splits as  -L----L-R-----R-------L-R----R---RL------------------------R----------L---------R--L-R----L------RL-----L-R--------------------------L--------------L---L------------------R------R-----R--------R--R-R----L---R-------L-----L----------R-L----------------------R-----------R-------------L-------------L-R------LR--R-RR---R------L------R--------R------RR-----L-L-------------L--------L--R-----L--R--L---------R--L--L--------R-RR-L----R------L---LR------R------R--R------------L--------, improve=0.34110450, (0 missing)
##       year   &lt; 2016.5 to the right, improve=0.11247530, (0 missing)
##       region splits as  ---L-L--R-------LL-L-L-L-R---L---R-L----R-L-LL----L--L---L-LLR---L---L---LR-, improve=0.09828861, (0 missing)
##       rating &lt; 4.55   to the left,  improve=0.08983699, (0 missing)
##   Surrogate splits:
##       winery splits as  -L----L-R-----L-------L-R----R---RL------------------------L----------L---------L--L-L----L------LL-----L-R--------------------------L--------------L---L------------------R------R-----R--------R--L-L----L---R-------L-----L----------R-L----------------------L-----------R-------------L-------------L-R------LR--L-LL---R------L------L--------R------RL-----L-L-------------L--------L--L-----L--L--L---------R--L--L--------R-RR-L----L------L---LL------R------L--R------------L--------, agree=0.899, adj=0.759, (0 split)
##       region splits as  ---L-L--R-------LL-R-L-L-R---L---R-L----R-L-LL----L--L---R-LLR---L---L---LR-, agree=0.658, adj=0.185, (0 split)
##       rating &lt; 4.55   to the left,  agree=0.650, adj=0.167, (0 split)
##       year   &lt; 2005.5 to the right, agree=0.638, adj=0.139, (0 split)
##       type   splits as  --LRLR-LLLLRLR-LL-RL-, agree=0.626, adj=0.111, (0 split)
## 
## Node number 11: 14 observations
##   mean=720.1264, MSE=193040.9 
## 
## Node number 12: 23 observations
##   mean=421.34, MSE=56728.43 
## 
## Node number 13: 27 observations
##   mean=968.0137, MSE=232813.5 
## 
## Node number 14: 9 observations
##   mean=1174.492, MSE=45966.15 
## 
## Node number 15: 14 observations
##   mean=1661.386, MSE=459348.1 
## 
## Node number 20: 149 observations
##   mean=131.6958, MSE=5364.493 
## 
## Node number 21: 108 observations
##   mean=272.1258, MSE=8780.789</code></pre>
<p>In our analysis, the decision tree model started with 4431 observations and successfully split them into child nodes based on certain criteria, resulting in a deviance of 107497100.0 and a predicted value of 63.84701 for the root node. The complexity of interpreting the tree is compounded by the large number of levels in some of the attribute variables, which makes it challenging to understand the model’s decision-making process using this tree.</p>
<p>One particular challenge arises from the inclusion of a large number of wine names as features in the decision tree model. The sheer volume of features and their unique names can make interpretation difficult. However, by using feature importance, we can gain insight into which wine attributes, i.e. features are most influential in predicting the target variable. Features with higher importance values are more critical in the model’s decision-making process.</p>
<p>Understanding feature importance is crucial in data science as it not only helps prioritise efforts but also enhances the transparency of the model. By identifying the most impactful factors, we can improve the interpretability of the model’s results and make our data science reports more informative and actionable.</p>
<div class="sourceCode" id="cb931"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb931-1"><a href="#cb931-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="co"># Get feature importance</span></span>
<span id="cb931-2"><a href="#cb931-2" tabindex="-1"></a><span class="er">&gt;</span> importance <span class="ot">&lt;-</span> <span class="fu">varImp</span>(tree_model)</span>
<span id="cb931-3"><a href="#cb931-3" tabindex="-1"></a><span class="sc">&gt;</span> </span>
<span id="cb931-4"><a href="#cb931-4" tabindex="-1"></a><span class="er">&gt;</span> <span class="co"># Print the feature importance</span></span>
<span id="cb931-5"><a href="#cb931-5" tabindex="-1"></a><span class="er">&gt;</span> <span class="fu">print</span>(importance)</span></code></pre></div>
<pre><code>##                Overall
## body        0.02788812
## num_reviews 0.90531862
## rating      0.97093847
## region      0.63286356
## type        0.38024180
## wine        2.34867729
## winery      2.26305636
## year        1.15760534
## acidity     0.00000000</code></pre>
<p>The feature importance scores provide insight into the relative importance of each feature in the model for predicting the target variable. Features with higher importance scores have more influence on accurate predictions. In this case, the wine feature has the highest importance score of 2.3487, indicating its significant impact on predictions. The winery feature follows closely behind with a score of 2.2631, followed by rating with a score of 0.9709, and year with a score of 1.1576. Conversely, features with lower importance scores, such as body and acidity, have less influence on predictions.</p>
<p>We realise that interpreting a decision tree with a large number of features like wine names requires careful analysis and consideration of the unique characteristics of the dataset. Features such as wine names can lead to a high-dimensional feature space, making it challenging to visualise and comprehend the decision-making process of the model. It is important to understand the underlying relationships between features and the target variable to ensure the model’s predictions are reliable and actionable.</p>
</div>
<div id="report-conclusion" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Report Conclusion<a href="#report-conclusion" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The proposed <code>tree_model</code> offers valuable insights into the dataset, highlighting key features like <code>wine</code>, <code>winery</code>, <code>rating</code>, and <code>year</code> that significantly influence predictions. However, the model’s complexity, particularly with a large number of levels in some attribute variables, poses challenges for interpretation. Future models could benefit from simplification techniques or alternative algorithms to improve interpretability. Additionally, while the feature importance analysis provides useful information, it would be beneficial to explore the interaction effects between features, which could further enhance the model’s predictive power. Furthermore, the high-dimensional feature space created by including a large number of wine names as features underscores the importance of feature selection techniques to identify the most relevant features for the model. Overall, future modelling efforts could focus on improving interpretability, exploring feature interactions, and refining feature selection processes to build more robust and insightful models.</p>
</div>
<div id="key-insights" class="section level2 unnumbered hasAnchor">
<h2 class="unnumbered hasAnchor">Key Insights<a href="#key-insights" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the case study of enhancing data science projects with ChatGPT using the <code>wine_data</code> dataset in R, several positives and negatives emerged. On the positive side, ChatGPT proved to be a useful tool in facilitating data exploration and preprocessing, contributing to the cleaning and preparation of datasets. Additionally, ChatGPT’s ability to summarise findings, create visualisations, and generate reports enhanced the communication of results and insights. However, there are some considerations to note. ChatGPT’s effectiveness may be limited by the quality and quantity of the data it is trained on, potentially leading to biased or inaccurate recommendations. Furthermore, while ChatGPT can assist in selecting machine learning algorithms, it may not always suggest the most optimal approach for every dataset. Therefore, it is essential to use ChatGPT as a supportive tool rather than relying solely on its suggestions, ensuring a balanced approach to data science project development.</p>
<p>Despite these considerations, integrating ChatGPT into the data science workflow can bring valuable insights, streamline processes, and ultimately enhance the overall success of the project.</p>
</div>
</div>
<div id="your-turn" class="section level1 unnumbered hasAnchor">
<h1 class="unnumbered hasAnchor">👉 Your Turn!<a href="#your-turn" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>Conduct a data science analysis using the <a href="https://www.kaggle.com/datasets/austinreese/craigslist-carstrucks-data">Used Cars Dataset</a> available from <a href="https://www.kaggle.com/datasets/">Kaggle</a> and apply the ideas presented in this case study “Enhancing Data Science Projects with ChatGPT: A Case Study in R.”</p>
<ul>
<li>Begin by exploring and preprocessing the dataset with ChatGPT’s assistance, focusing on data cleaning and preparation.<br />
</li>
<li>Utilise ChatGPT to guide the selection of machine learning algorithms and techniques based on the project’s objectives and dataset characteristics.<br />
</li>
<li>During model development, use ChatGPT to interpret complex models, such as decision trees, by providing explanations and insights into the model’s predictions.<br />
</li>
<li>Finally, leverage ChatGPT to communicate the results and insights effectively, whether through summarising findings, creating visualisations, or generating reports.<br />
</li>
<li>Evaluate the impact of integrating ChatGPT into your workflow and note any positives and negatives that emerge from this approach.</li>
</ul>
<!--chapter:end:08-S08-D1.Rmd-->
</div>
<!--bookdown:body:end-->
            </section>

          </div>
        </div>
      </div>
<!--bookdown:link_prev-->
<!--bookdown:link_next-->
    </div>
  </div>
<!--bookdown:config-->

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
