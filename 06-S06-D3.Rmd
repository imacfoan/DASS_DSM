---
editor_options: 
  markdown: 
    wrap: 72
---

# Demonstration 3: PCR and PLS Regression {-}

::: file
For the tasks below, you require the **Hitters** dataset from the `ISRL2` package.

You will also need the `pls` package; please make sure to install and load it before you begin the practical.
:::

The **Hitters** dataset contains Major League Baseball Data from the 1986 and 1987 season. It is a dataframe with 322 observations and 20 variables. To learn more about the variables type `?Hitters` in your console. 

The goal of this demonstration is to predict salary of major league players using ridge regression and lasso.

Loading the required packages: 

```{r warning = FALSE, message = FALSE}
library(ISLR2)
library(pls)
```

Removing missing values: 

```{r}
attach(Hitters)
Hitters <- na.omit(Hitters)
```

Preparing the data: 

```{r}
x <- model.matrix(Salary ~ ., Hitters)[, -1]
y <- Hitters$Salary
```

## Principal Components Regression {-}

Principal components regression (PCR) can be performed using the `pcr()` function, which is part of the `pls` package. 

In this demonstration, we continue using the **Hitters** dataset to predict salary of major league players. We first set a random seed so that the results obtained will be reproducible and then split the data into training and test sets. 

```{r}
set.seed(1)
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)
y.test <- y[test]
```

The syntax for the `pcr()` function is similar to that for `lm()`, with a few additional options. Setting `scale = TRUE` has the effect of *standardising* each predictor, prior to generating the principal components, so that the scale on which each variable is measured will not have an effect.
 Setting `validation = "CV"` causes `pcr()` to compute the ten-fold cross-validation error for each possible value of $M$, the number of principal components used. 

```{r}
set.seed(2)
pcr.fit <- pcr(Salary ~ ., data = Hitters, scale = TRUE,
               validation = "CV")
```

As usual, the results can be explored using `summary()`.

```{r}
summary(pcr.fit)
```

The CV score is provided for each possible number of components. Note that  `pcr()` reports the *root mean squared error*; in order to obtain the usual MSE, we must square this quantity. For instance, a root mean squared error of $352.8$ corresponds to an MSE of $352.8^2=124{,}468$.

We could also plot the cross-validation scores using the `validationplot()` function. Using `val.type = "MSEP"` will cause the cross-validation MSE to be plotted.

```{r}
validationplot(pcr.fit, val.type = "MSEP")
```

We see that the smallest cross-validation error occurs when $M=18$ components are used. This is barely fewer than $M=19$, which amounts to simply performing least squares, because when all of the components are used in PCR no dimension reduction occurs. However, from the plot we also see that the cross-validation error is roughly the same when only one component is included in the model. This suggests that a model that uses just a small number of components might suffice.

The `summary()` function also provides the *percentage of variance explained* in the predictors and in the response using different numbers of components. We can think of this as the amount of information about the predictors or the response that is captured using $M$ principal components. For example, setting $M=1$ only captures $38.31 \%$ of all the variance, or information, in the predictors. In contrast, using $M=5$ increases the value to $84.29 \%$. If we were to use all $M=p=19$ components, this would increase to $100 \%$.

We now perform PCR on the training data and evaluate its test set performance.

```{r}
set.seed(1)
pcr.fit <- pcr(Salary ~ ., data = Hitters, subset = train,
               scale = TRUE, validation = "CV")
validationplot(pcr.fit, val.type = "MSEP")
```

Now we find that the lowest cross-validation error occurs when $M=5$ components are used. We compute the test MSE as follows.

```{r}
pcr.pred <- predict(pcr.fit, x[test, ], ncomp = 5)
mean((pcr.pred - y.test)^2)
```

Finally, we fit PCR on the full data set, using $M=5$, the number of components identified by cross-validation.

```{r}
pcr.fit <- pcr(y ~ x, scale = TRUE, ncomp = 5)
summary(pcr.fit)
```

The test set MSE is competitive with the results obtained using ridge regression and the lasso. However, as a result of the way PCR is implemented, the final model is more difficult to interpret because it does not perform any kind of variable selection or even directly produce coefficient estimates.

## Partial Least Squares {-}

We implement partial least squares (PLS) using the `plsr()` function, also in the `pls` library. The syntax is identical to that of the `pcr()` function.

```{r}
set.seed(1)
pls.fit <- plsr(Salary ~ ., data = Hitters, subset = train, 
                scale = TRUE, validation = "CV")
summary(pls.fit)
validationplot(pls.fit, val.type = "MSEP")
```

The lowest cross-validation error occurs when only $M=1$ partial least squares directions are used. We now evaluate the corresponding test set MSE.

```{r}
pls.pred <- predict(pls.fit, x[test, ], ncomp = 1)
mean((pls.pred - y.test)^2)
```

The test MSE is comparable to, but slightly higher than, the test MSE obtained using ridge regression, the lasso, and PCR.   

Finally, we perform PLS using the full data set, using $M=1$, the number of components identified by cross-validation.

```{r}
pls.fit <- plsr(Salary ~ ., data = Hitters, scale = TRUE,
    ncomp = 1)
summary(pls.fit)
```

Notice that the percentage of variance in **Salary** that the one-component PLS fit explains, $43.05 \%$, is almost as much as that explained using the final five-component model PCR fit, $44.90 \%$. This is because PCR only attempts to maximise the amount of variance explained in the predictors, while PLS searches for directions that explain variance in both the predictors and the response.



