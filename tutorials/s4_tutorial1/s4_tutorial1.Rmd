---
title: "SOST70033 Section 4"
output: learnr::tutorial
runtime: shiny_prerendered
---

```{r setup, include = FALSE}
library(learnr)
```

```{r, echo=FALSE, results='asis'}
cat('<link href="https://fonts.googleapis.com/css?family=Open+Sans&display=swap" rel="stylesheet">')
```

```{css, echo=FALSE}
.topicsFooter .resetButton {
  font-size: 1em !important;
}

body {
  font-family: 'Open Sans', sans-serif;
  color: black
}
```


```{r setup-chunk, include=FALSE}
library(ISLR2)
set.seed(1)
Auto <- ISLR2::Auto
training_data <- sample(392, 196) 
training_data
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = training_data)
mean((Auto$mpg - predict(lm.fit, Auto))[-training_data]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = training_data)
mean((Auto$mpg - predict(lm.fit2, Auto))[-training_data]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = training_data)
mean((Auto$mpg - predict(lm.fit3, Auto))[-training_data]^2)
```

```{r setup-chunk2, include=FALSE}
library(ISLR2)
set.seed(2)   
Auto <- ISLR2::Auto
training_data2 <- sample(392, 196)
lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = training_data2) 
mean((Auto$mpg - predict(lm.fit, Auto))[-training_data2]^2)
lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = training_data2)  
mean((Auto$mpg - predict(lm.fit2, Auto))[-training_data2]^2)
lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = training_data2) 
mean((Auto$mpg - predict(lm.fit3, Auto))[-training_data2]^2)
```

```{r setup-chunk3, include=FALSE}
library(ISLR2)
library(boot)
Auto <- ISLR2::Auto
glm.fit <- glm(mpg ~ horsepower, data = Auto)
lm.fit <- lm(mpg ~ horsepower, data = Auto)
glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err <- cv.glm(Auto, glm.fit)
cv.error <- rep(0, 10)
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
```

```{r setup-chunk4, include=FALSE}
library(ISLR2)
library(boot)
Auto <- ISLR2::Auto
set.seed(17)
cv.error.10 <- rep(0, 10)
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
```

# Tutorial 1 - Cross-Validation in R

How can we implement cross-validation in R? Follow the steps outlined in this interactive tutorial to build basic coding skills required to perform cross-validation. This tutorial serves as a step-by-step guide to performing cross-validation in R. It has a fully interactive console in which you can type and run the suggested code. To run code, click on the *Run Code* button. 

This interactive tutorial is based on **Section 5.3 Lab: Cross-Validation and the Bootstrap** from the core textbook for this course: James, G., Witten, D., Hastie, T. and Tibshirani, R. (2021). *An introduction to statistical learning with applications in R*. 2nd ed. New York: Springer.  https://www.statlearning.com/

## 1. The Validation Set Approach

You will learn how to estimate and compare tests error rates from fitting multiple linear models using the **Auto** dataset, with **mpg** as the output (response) and **horsepower** as the input (predictor). 

The **Auto** dataset has 392 observations and 9 variables. We begin by using the base R `sample()` function to create our training and test datasets. We can split the 392 observations in half, thus we will have 196 observations for our training dataset and 192 for our test dataset later. 

### Training and Test Datasets

Type the following code in the console below to create a new object called training_data and then look at the contents by typing the name of the object in console also:  

`training_data <- sample(392, 196)`   
`training_data`   

Then, click on *Run Code*.

```{r chunk2, exercise = TRUE, exercise.setup="setup-chunk"}
```

What does this code actually do?  

The first argument within `sample()` is 392. This will represent the total number of observations available to us from the `Auto` dataset. The second argument is 196. This represents the number of observations we want to select from the total we have available which is 392. Therefore, the `sample()` function will return a vector of 196 unique integers which represents a subset of 196 indices from a total of 392. You can confirm this by typing the name of the data object to see that it contains a random sample of 196 indices from a total of 392 indices. Note that by default, the `sample()` function conducts the sampling without replacement. 

### Fit Linear Regression Model using Training Dataset

Now we need to tell R to fit a linear regression using only the observations corresponding to the training dataset.  

Type the following code in the console. Don't forget to click on **Run Code**.

`lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = training_data)`

```{r chunk3, exercise = TRUE, exercise.setup="setup-chunk"}
```

Essentially, this line of code is telling R to build a model with **mpg** as the response and **horsepower** as the predictor using the **Auto** dataset. The subset argument is telling R to only select 192 observations that are indexed at the specific positions as defined by the  **training_data** vector object. In this way, the model will be fitted using only the observations from **Auto** dataset that are defined by this vector of indices. 

### Estimated Test Mean Squared Error

Let's now calculate the Mean Squared Error for the test dataset.

Type the code below in the console:  

`mean((Auto$mpg - predict(lm.fit, Auto))[-training_data]^2)`  

```{r chunk4, exercise = TRUE, exercise.setup="setup-chunk"}
```

Using base r, we use the **predict()** function to estimate the response for all 392 observations in the **Auto** dataset and then we use the **mean()** function to calculate the MSE of the 196 observations in the test (or validation) dataset. This is why we included the `[-training_data]` component such that we only select the observations not defined by the **training_data** object.

### Quadratic and Cubic Regression 

Let's try applying some transformations to our predictor **horsepower** using the **poly()** function and then calculate the test MSE values. 

Fit a second-degree polynomial regression model:  

`lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = training_data)`  

Then calculate the test MSE:   
`mean((Auto$mpg - predict(lm.fit2, Auto))[-training_data]^2)`  

```{r chunk5, exercise = TRUE, exercise.setup="setup-chunk"}
```


Now fit a third-degree polynomial regression model  

`lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = training_data)`  

Then calculate the test MSE:    

`mean((Auto$mpg - predict(lm.fit3, Auto))[-training_data]^2)`  

```{r chunk6, exercise = TRUE, exercise.setup="setup-chunk"}
```

### Different Training Datasets

Now what happens if we choose a different training dataset instead? We can try this out by setting the seed using `set.seed(2)`.  

Run the code below to change the seed option to 2 and create a new training dataset.  

`set.seed(2)`   

`training_data2 <- sample(392, 196)`  

```{r chunk7, exercise = TRUE, exercise.setup="setup-chunk2"}
```

Then run a new linear regression model, a second degree polynomial model, and then a third degree polynomial model. Calculate the test MSE values for each of the three models.   

- linear regression model and corresponding test MSE  

`lm.fit <- lm(mpg ~ horsepower, data = Auto, subset = training_data2)`  

`mean((Auto$mpg - predict(lm.fit, Auto))[-training_data2]^2)`

```{r chunk8, exercise = TRUE, exercise.setup="setup-chunk2"}
```

- quadratic regression model and corresponding test MSE  

`lm.fit2 <- lm(mpg ~ poly(horsepower, 2), data = Auto, subset = training_data2)`  

`mean((Auto$mpg - predict(lm.fit2, Auto))[-training_data2]^2)`

```{r chunk9, exercise = TRUE, exercise.setup="setup-chunk2"}
```

- cubic regression model and corresponding test MSE  

`lm.fit3 <- lm(mpg ~ poly(horsepower, 3), data = Auto, subset = training_data2)`  

`mean((Auto$mpg - predict(lm.fit3, Auto))[-training_data2]^2)`

```{r chunk10, exercise = TRUE, exercise.setup="setup-chunk2"}
```

As you can see, if we were to choose different training datasets, we will obtain somewhat different errors on the validation set. Nevertheless, the results obtained using the second training dataset are consistent with the findings from the first training dataset whereby a model that predicts **mpg** using a quadratic function of **horsepower** performs better than a model that involves only a linear function of **horsepower**. Also, there is little evidence in favor of a model that uses a cubic function of **horsepower**.

## 2. Leave-One-Out Cross-Validation

The LOOCV estimate can be automatically computed for any generalized linear model  using the `glm()` and `cv.glm()` functions. You'll remember that by specifying the **family** argument in the `glm()` function, we can, for example, perform logistic regression. If we do not specify this argument, we can perform linear regression in the same way as we would with the `lm()` function. 

Try it yourself and compare the results. Run a simple regression model with **mpg** as the response and **horsepower** as the predictor using both functions. 

Use `glm()` and obtain the coefficients.   

`glm.fit <- glm(mpg ~ horsepower, data = Auto)`  

`coef(glm.fit)`  


```{r chunk11, exercise = TRUE, exercise.setup="setup-chunk3"}
``` 

Then use `lm()` and obtain the coefficients.   

`lm.fit <- lm(mpg ~ horsepower, data = Auto)`  

`coef(lm.fit)`  

```{r chunk12, exercise = TRUE, exercise.setup="setup-chunk3"}
``` 
 
As you can see, both functions yield identical linear regression models. In the context of cross-validation, we will continue using the `glm()` function to perform linear regression since we need to use additional functions from the `boot` package. *This package will need to be installed and loaded before you can use it.*

```{r chunk13}

glm.fit <- glm(mpg ~ horsepower, data = Auto)
cv.err <- cv.glm(Auto, glm.fit)
cv.err$delta
```

The `cv.glm()` function produces a list with several components.  The two numbers in the `delta` vector contain the cross-validation results. In this case the numbers are identical (up to two decimal places) and correspond to the LOOCV statistic given in (5.1). Below, we discuss a situation in which the two numbers differ. Our cross-validation estimate for the test error is approximately $24.23$.

We can repeat this procedure for increasingly complex polynomial fits.
 To automate the process, we use the  `for()` function to initiate a *for loop* which iteratively fits polynomial regressions for polynomials of order $i=1$ to $i=10$, computes the associated cross-validation error, and stores it in the $i$th element of the vector `cv.error`.
 We begin by initializing the vector. 

```{r chunk14}
cv.error <- rep(0, 10)
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error[i] <- cv.glm(Auto, glm.fit)$delta[1]
}
cv.error
```

As in Figure 5.4, we see a sharp drop in the estimated test MSE between the linear and quadratic fits, but then no clear improvement from using higher-order polynomials.

## 3. $k$-Fold Cross-Validation
The `cv.glm()` function can also be used to implement $k$-fold CV. Below we use $k=10$, a common choice for $k$, on the `Auto` data set.
We once again set a random seed and initialize a vector in which we will store the CV errors corresponding to the polynomial fits of orders one to ten.

```{r chunk15}
set.seed(17)
cv.error.10 <- rep(0, 10)
for (i in 1:10) {
  glm.fit <- glm(mpg ~ poly(horsepower, i), data = Auto)
  cv.error.10[i] <- cv.glm(Auto, glm.fit, K = 10)$delta[1]
}
cv.error.10
```

Notice that the computation time is shorter than that of LOOCV.
(In principle, the computation time for LOOCV for a least squares linear model should be faster than for $k$-fold CV, due to the availability
of the formula (5.2) for LOOCV; however, unfortunately the `cv.glm()` function does not make use of this formula.)
We still see little evidence that using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit.

We saw in Section 5.3.2 that the two numbers associated with `delta` are essentially the same when LOOCV is performed.
When we instead perform $k$-fold CV, then the two numbers associated with `delta` differ slightly. The first is the standard $k$-fold CV estimate,
as in (5.3). The second is a bias-corrected version. On this data set, the two estimates are very similar to each other.
